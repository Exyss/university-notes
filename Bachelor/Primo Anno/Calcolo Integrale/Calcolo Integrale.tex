\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%

\def\useItalian{1}  % 1 = Italian, 0 = English

\def\courseName{Calcolo Integrale}

\def\coursePrerequisites{Apprendimento del materiale relativo al corso \textit{Calcolo Differenziale}.}

\def\book{\curlyquotes{Calculus}: A Complete Course",\\R. Adams, C. Essex}

\def\authorName{Simone Bianco}
\def\email{bianco.simone@outlook.it}
\def\github{https://github.com/Exyss/university-notes}
\def\linkedin{https://www.linkedin.com/in/simone-bianco}

% \def\authorName{Alessio Bandiera}
% \def\email{alessio.bandiera02@gmail.com}
% \def\github{https://github.com/aflaag-notes}
% \def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../../packages/Nyx/nyx-packages}
\usepackage{../../../packages/Nyx/nyx-styles}
\usepackage{../../../packages/Nyx/nyx-frames}
\usepackage{../../../packages/Nyx/nyx-macros}
\usepackage{../../../packages/Nyx/nyx-title}
\usepackage{../../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../../packages/Nyx/logo.png}

\if\useItalian1
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} Università di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \ifdefined\book
        \subtitle{Appunti integrati con il libro \book}
    \fi
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \ifdefined\book
        \subtitle{Lecture notes integrated with the book \book}
    \fi
    \author{\textit{Author}\\\authorName}
\fi


\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%


    \chapter{Serie Numeriche}
    
    \section{Successioni e Serie numeriche}
    
    In matematica, con il termine \textbf{successione} viene indicato l'insieme ordinato dei valori assunti da una funzione $a : S \to \mathbb{R} : k \mapsto a_k$, dove $S \subseteq \N$, ossia una funzione che associa dei valori ad un sottoinsieme dei numeri naturali. Formalmente, una successione descritta dalla funzione $a$ andrebbe quindi descritta come l'insieme ordinato $(a_k)_{k \in S} = \{a(1), a(2), a(3), ..., a(n)\}$. Tuttavia, per questioni di praticità, utilizzeremo la notazione $(a_k)_{k \in S} = a_1, a_2, a_3, ... , a_n$, dove il pedice sotto ad ogni valore viene detto \textbf{indice della successione}.
    
    Esempi di successioni:
    \begin{itemize}
        \item Se $a_k = 2k$, allora $(a_k)_{n \in S} = 2, 4, 6, 8, 10, ..., 2n$
        \item Se $a_k = 2^k$ allora $(a_k)_{n \in S} = 2, 4, 8, 16, 32..., 2^n$
        \item Se $a_k = \frac{1}{k}$ allora $(a_k)_{k \in S} = \frac{1}{2},\frac{1}{3},\frac{1}{4},\frac{1}{5}, ..., \frac{1}{n}$
    \end{itemize}
    
    A questo punto, notiamo che ogni successione dia vita ad una somma dei suoi termini e viceversa. Difatti, possiamo considerare ogni termine della successione come uno dei termini di una sommatoria e viceversa. In particolare, indichiamo con $S_n$ la sommatoria dei primi $n$ termini di una successione:
    \[S_n = \sum_{k = 1}^{n} a_k = a_1 + a_2 + a_3 + ... + a_n\]
    
    E se l'insieme dei termini da sommare fosse \textbf{illimitato}, ossia se $S = \N$? Come possiamo sapere il risultato finale della somma? In tali somme, il numero di termini da sommare risulta infinito. Tuttavia, poiché l'\textit{infinito} non è un numero, ma solo un concetto, possiamo definire tali somme infinite come il \textbf{limite} per $n \to +\infty$ di una somma $S_n$, dunque come se stessimo sommando i "primi infiniti termini" della successione $(a_k)_{k \in \N^+}$. Tali limiti delle somme vengono detti \textbf{serie numeriche}.
    
    \begin{frameddefn}{Serie numerica}
        Data una successione di termine generico $a_k$, si dice \textbf{serie numerica} il limite per $n \to +\infty$ della somma $S_n$:
        \[ \sum_{k = 1}^{\infty} a_k := \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} \sum_{k = 1}^{n} a_k\]
    \end{frameddefn}

    Analizziamo ora quindi le \textbf{proprietà} che tali serie possono assumere. Vediamo qualche esempio:
    
    \begin{enumerate}
        \item Consideriamo le somma dettata dal termine $a_k = 0$, ossia:
        \[S_n = \sum_{k = 1}^n 0\]
        Cosa accade a questa somma considerando $n \to +\infty$? Risulta evidente che il risultato della somma sarà sempre 0, dunque
        \[ \sum_{k = 1}^{\infty} 0 = \lim_{n \to +\infty} S_n = 0 \]
    
        \item Consideriamo  invece la successione dettata dal termine
        \[a_k = \frac{1}{k(k+1)} = \frac{1}{k} - \frac{1}{(k+1)}\]
        
        Analizziamo prima cosa accade alla serie per piccoli valori di $k$:
        \begin{itemize}
            \item $S_1 = a_1 = 1 - \frac{1}{2}$
            \item $S_2 = a_1 + a_2 = (1 - \frac{1}{2}) + (\frac{1}{2} - \frac{1}{3}) = 1 - \frac{1}{3}$
            \item $S_3 = a_1 + a_2 + a_3 = 1 - (1 - \frac{1}{2}) + (\frac{1}{2} - \frac{1}{3}) + (\frac{1}{3} - \frac{1}{4}) = 1 - \frac{1}{4}$
            \item ...
            \item $S_n = a_1 + a_2 + a_3 + \ldots + a_n = 1 - (1 - \frac{1}{2}) + (\frac{1}{2} - \frac{1}{3}) + (\frac{1}{3} - \frac{1}{4}) + \ldots + (\frac{1}{n} - \frac{1}{(n+1)})= 1 - \frac{1}{n}$
        \end{itemize}
        
        Notiamo quindi per per ogni $k$ abbiamo che $S_k = 1 - \frac{1}{k+1}$. Possiamo quindi affermare che
        \[S_n = \sum_{k = 1}^{n} \left (\frac{1}{k} - \frac{1}{k+1} \right ) = 1 - \frac{1}{n+1}\]
        
        Dunque, il limite per $n \to +\infty$ di questa serie, sarà
        \[ \sum_{k = 1}^{\infty} \left (\frac{1}{k} - \frac{1}{k+1} \right ) = \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} 1 - \frac{1}{n+1} = 1\]
    \end{enumerate}
    
    In entrambi gli esempi mostrati, per $n \to +\infty$, le due serie \textbf{tendono ad un valore finito} (dunque hanno limite in $\ell$). In questi casi si parla di \textbf{serie convergenti}.
    
    \begin{frameddefn}{Serie Convergente}
        Se una serie numerica tende ad un valore $\ell$, ossia
        \[ \lim_{n \to +\infty} S_n = \sum_{k = 0}^{\infty} a_k = \ell\]    
        allora tale serie è detta \textbf{convergente} ad $\ell$
    \end{frameddefn}
    
    Analizziamo ora le ulteriori due serie:
    \begin{enumerate}
        \item Sia data la successione costante $a_k = 1$. Vediamo il comportamento delle sue prime serie parziali.
        \begin{itemize}
            \item $S_0 = 1$
            \item $S_1 = 1 + 1 = 2$
            \item $S_2 = 1 + 1 + 1 = 3$
            \item ...
            \item $S_n = \underbrace{1 + 1 + 1 + \ldots + 1}_{\text{n volte}} = n$
        \end{itemize}
                
        Applicando il limite su tale successione, dunque, otteniamo che
        \[\sum_{k = 0}^{\infty} 1 = \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} n+1 = + \infty \]
        dunque il valore di tale serie cresce all'infinito
        
        \item Sia data la successione $a_k = \log \rbk{\frac{1}{k}}$. Per le proprietà dei logaritmi, notiamo che:
        \[S_n = \sum_{k = 1}^n \log \rbk{\frac{1}{k}} = \log \rbk{\frac{1}{1}} + \ldots \log \rbk{\frac{1}{k}} = \log \rbk{\frac{1}{1} \cdot \ldots \cdots \frac{1}{n}} = \log \rbk{\frac{1}{n!}}\]
        
        Applicando il limite su tale successione, dunque, otteniamo che:
        \[\sum_{k = 1}^{+\infty} \log \rbk{\frac{1}{k}} = \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} \log \rbk{\frac{1}{n!}} = -\infty\]
        dunque il valore di tale serie decresce all'infinito
    \end{enumerate}

    Quando il valore di una serie cresce (o decresce) all'infinito, tale serie viene detta divergente ad un valore infinito.

    \begin{frameddefn}{Serie Divergente}
        Se una serie numerica tende a $+\infty$ o $-\infty$, ossia
        \[ \lim_{n \to +\infty} S_n = \sum_{k = 0}^{\infty} a_k = \pm \infty\]
        allora si dice che la serie numerica è \textbf{divergente}.
    \end{frameddefn}
    
    \label{no_converge}
    
    \begin{framedobs}{}
        Una serie che non converge non è detto che sia automaticamente divergente e viceversa. Difatti, una serie può anche non convergere e non divergere.
    \end{framedobs}
    
    Ad esempio, consideriamo la serie della seguente successione $a_n = (-1)^n$
    \[ S_n = \sum_{k=0}^{n} (-1)^k \]
    
    Notiamo che:
    \begin{itemize}
        \item $S_0 = 1$
        \item $S_1 = 1 - 1 = 0$
        \item $S_2 = 1 - 1 + 1 = 1$
        \item $S_3 = 1 - 1 + 1 - 1 = 0$
        \item ...
    \end{itemize}
    
    In altre parole, otteniamo che:
    $$
    S_n = \left \{ \begin{array}{ll}
            1 & \text{se } n \text{ è pari}\\
            0 & \text{se } n \text{ è dispari}\\
        \end{array} \right .
    $$
    
    Dunque tale serie per $n \to +\infty$ non è né convergente né divergente.
    
    Nell'esempio appena mostrato, siamo in grado di arrivare a tale conclusione solo perché, ai tempi moderni, siamo a conoscenza del \textbf{concetto di limite}. In passato, molti matematici hanno provato a rispondere al quesito posto dalla serie numerica $(-1)^n$, giungendo a \textbf{tre conclusioni errate}.
    
    Partendo dalla serie $S_n = 1 - 1 + 1 - 1 + 1 - ...$ possiamo aggiungere aggiungere delle parentesi in tre modi:
    \begin{itemize}
        \item $S_n = (1 - 1) + (1 - 1) + (1 - ...$, in questo modo otterremo che $S_n = 0 + 0 + 0 + ... = 0$
        \item $S_n = 1 +(- 1 + 1) + (-1 + 1) + ...$, in questo modo otterremo che $S_n = 1 + 0 + 0 + ... = 1$
        \item $S_n = 1 - (1 + 1 - 1 + 1 - ...$, in questo modo otterremo che $S_n = 1 - S_n$. A questo punto possiamo risolvere l'equazione, ottenendo che $S_n = \frac{1}{2}$
    \end{itemize}
    
    Il motivo per cui tali procedimenti restituiscono risultati errati è semplice: \underline{non è possibile} \underline{trattare una somma infinita come una somma finita}. Esempio evidente di ciò è il terzo procedimento: non ha alcun senso aprire una parentesi senza mai chiuderla alla fine.
    
    Tale problema, tuttavia, può essere risolto dal concetto di limite: immaginando una \textbf{serie parziale} fino ad $a_n$, possiamo applicare le normali proprietà matematiche su di essa, per \textbf{\underline{poi}} estenderne il risultato per $n \to +\infty$.
    
    In questo caso, quindi, abbiamo già detto che:
    $$
    S_n = \sum_{k = 0}^n (-1)^k= \left \{ \begin{array}{ll}
            1 & \text{se } n \text{ è pari}\\
            0 & \text{se } n \text{ è dispari}\\
        \end{array} \right .
    \Longrightarrow \lim_{n \to +\infty} S_n = \nexists
    $$
   
    \quad

    \subsection{Serie telescopiche}

    Un primo esempio di convergenza di somme infinite di termini sono le cosiddette \textbf{serie telescopiche}. Tali serie vengono chiamate così per via del modo in cui ogni termine della serie vada in realtà ad eliminare un intero termine precedente (o solo una parte di esso). Consideriamo ad esempio la seguente serie simile ad una già vista precedentemente:
    \[\sum_{k = 2}^{+\infty} \frac{1}{k-1} - \frac{1}{k}\]

    Espandendo i termini della serie parziale ad essa associata, risulta evidente che i termini si cancellino tra loro, "chiudendosi" come un vero e proprio telescopio:
    \[\begin{split}
        S_n &= \sum_{k = 2}^{n} \frac{1}{k-1} - \frac{1}{k} \\
        &= \rbk{\frac{1}{1} - \frac{1}{2}} + \rbk{\frac{1}{2} - \frac{1}{3}} + \rbk{\frac{1}{3} - \frac{1}{4}} + \ldots + \rbk{\frac{1}{n-1} - \frac{1}{n}} \\
        &= \frac{1}{1} \color{red}- \frac{1}{2} + \frac{1}{2} - \frac{1}{3} + \frac{1}{3} - \frac{1}{4} + \ldots + \frac{1}{n-1} \color{black} - \frac{1}{n} \\
        &= 1 - \frac{1}{n}
    \end{split}\]
    dunque considerando il limite per $n \to +\infty$ otteniamo che:
    \[\sum_{k = 2}^{+\infty} \frac{1}{k-1} - \frac{1}{k} = \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} 1 - \frac{1}{n} = 1\]

    Vediamo ora un altro esempio. Consideriamo la seguente serie:
    \[\sum_{k = 3}^{+\infty} \frac{1}{k^2} - \frac{1}{(k-2)^2}\]
    Espandendo i termini notiamo che anche tale serie risulta essere telescopica in quanto i termini si cancellino tra loro a "distanza di 2":
    \[\begin{split}
        S_n &= \sum_{k = 3}^{n} \frac{1}{k-1} - \frac{1}{k} \\
        &= \rbk{\frac{1}{3^2} - \frac{1}{1^2}} + \rbk{\frac{1}{4^2} - \frac{1}{2^2}} + \rbk{\frac{1}{5^2} - \frac{1}{3^2}} + \ldots + \rbk{\frac{1}{n^2} - \frac{1}{(n-2)^2}} \\
        &= \frac{1}{1^2} - \frac{1}{2^2} + \frac{1}{(n-1)^2} + \frac{1}{n^2}
    \end{split}\]
    Applicando il limite dunque concludiamo che:
    \[\sum_{k = 3}^{+\infty} \frac{1}{k^2} - \frac{1}{(k-2)^2} = \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} \frac{1}{1^2} - \frac{1}{2^2} + \frac{1}{(n-1)^2} + \frac{1}{n^2} = -\frac{5}{4}\]

    \quad

    \subsection{Serie geometriche}
    
    Dato $q \in \R$, consideriamo la successione $(q^k)_{k \in \N^+}$. Tale successione viene detta \textbf{progressione geometrica} per via di alcune proprietà matematiche che non analizzeremo. Tale successione e la sua serie associata risultano fondamentali nell'intero studio della matematica poiché esse "spuntano un po' ovunque". Vediamo quindi cosa accade ad una serie che utilizza tale successione:
    \begin{itemize}
        \item Se $q=1$, allora 
        \[ \sum_{k = 0}^{+\infty} q^k = \lim_{n \to +\infty} \sum_{k = 0}^n 1 = \lim_{n \to +\infty} n+1 = +\infty\]
        
        \item Se $q \neq 1$, allora notiamo che:
        \[\begin{split}
            S_n - qS_n &= \sum_{k=0}^n q^k - q \sum_{k=0}^n q^k \\
            &= \sum_{k=0}^n q^k - \sum_{k=0}^n q^{k+1} \\
            &= (1 + q + q^2 + ... + q^n) - (q + q^2 + ... + q^n + q^{n+1}) \\
            &= 1 - q^{n+1}
        \end{split}\]

        Di conseguenza, poiché $q \neq 1$ in ipotesi, possiamo concludere che:
        \[S_n - qS_n = 1 - q^{n+1} \implies S_n = \frac{1-q^{n+1}}{1-q}\]
    \end{itemize}
    
    Quindi, otteniamo che:
    $$
    S_n = \sum_{k = 0}^n q^k= \left \{ \begin{array}{ll}
            \frac{1 - q^{n+1}}{1-q} & \text{ se } q \neq 1\\
            n+1 & \text{ se } q = 1\\
        \end{array} \right .
    $$

    Per vedere cosa accade alla \textbf{serie geometrica} per $n \to +\infty$, dobbiamo prima analizzare cosa accade a $q^{n+1}$ nel caso in cui $q \neq 1$:
    $$
    \lim_{n \to +\infty} q^{n+1} = \left \{ \begin{array}{ll}
            0 & \text{ se } -1 < q < 1\\
            +\infty & \text{ se } q > 1\\
            \nexists & \text{ se } q \leq -1\\
        \end{array} \right .
    $$
    
    Unendo i due risultati, dunque, concludiamo la seguente proposizione:
    
    \begin{framedprop}{Serie geometrica}
    
        Data la \textbf{serie geometrica}, si ha che
        $$
        \sum_{k = 0}^{\infty} q^k  = \left \{ \begin{array}{ll}
                \frac{1}{1-q} & \text{ se } -1 < q < 1\\
                +\infty & \text{ se } q \geq 1\\
                \nexists & \text{ se } q \leq -1\\
            \end{array} \right .
        $$
    \end{framedprop}
        
    \quad

    \subsection{Serie armoniche}
    
    Prendiamo in considerazione la seguente serie:
    \[ \sum_{k=1}^{+\infty} \frac{1}{k} = 1 + \frac{1}{2} +\frac{1}{3} +\frac{1}{4} + ...\]

    Tale serie viene detta \textbf{serie armonica} per via di alcune proprietà legate alla musica, le quali non analizzeremo. Di primo occhio, si potrebbe pensare che tale serie sia convergente in quanto per $k \to +\infty$ vengano sommati dei termini sempre più vicini allo 0. Tuttavia, tale somma è in realtà \textbf{divergente}.

    \begin{framedprop}{Serie armonica}
        La seguente serie, detta \textbf{serie armonica}, è divergente:
        \[\sum_{k=1}^{+\infty} \frac{1}{k} = +\infty\]
    \end{framedprop}

    \proofenv{

        Supponiamo per assurdo che $\exists \ell \in \R$ tale che $\lim_{m \to +\infty} S_m = \ell$. Ovviamente, poiché $m \to +\infty$, tale assunzione implica che $S_m$ tenda a $\ell$ per ogni $m \in \N^+$.

        A questo punto, notiamo che:
        \[\begin{split}
            S_{2n} &=  1 + \frac{1}{2} +\frac{1}{3} +\frac{1}{4} + \frac{1}{5} +\frac{1}{6} +\frac{1}{7} + \frac{1}{8} + ... + \frac{1}{2n-1} + \frac{1}{2n} \\
            &\geq 1 + \frac{1}{2} + \rbk{\frac{1}{4} +\frac{1}{4}} + \rbk{\frac{1}{6} +\frac{1}{6}} + \rbk{\frac{1}{8} + \frac{1}{8}} + ... + \rbk{\frac{1}{2n} + \frac{1}{2n}}\\
            &= 1 + \frac{1}{2} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots + \frac{1}{n}\\
            &= \frac{1}{2} + S_{n}
        \end{split}\]
        e che:
        \[S_{2n+1} =  S_{2n} + \frac{1}{2n+1} \geq \frac{1}{2} + \frac{1}{2n+1} + S_{n}\]

        Di conseguenza, otteniamo che:
        \[\lim_{n \to +\infty} S_{2n} \geq \lim_{n \to +\infty} \frac{1}{2} + S_{n} \implies \ell \geq \frac{1}{2} + \ell\]
        il che è assurdo. Analogamente, otteniamo che:
        \[\lim_{n \to +\infty} S_{2n+1} \geq \lim_{n \to +\infty} \frac{1}{2} + \frac{1}{2n+1} + S_{n} \implies \ell \geq \frac{1}{2} + 0 + \ell\]
        che ancora una volta risulta assurdo. Per tanto, è impossibile che la serie converga ad un valore finito. Infine, notiamo che $\forall n \in \N^+$ si abbia che $S_n < S_{n+1}$, ossia che la serie sia strettamente crescente. Per tanto, visto che essa non converge, ne segue che essa diverga a $+\infty$.

    } 

    Consideriamo invece una serie simile alla serie appena vista:
    \[ \sum_{k=1}^{+\infty} \frac{1}{k^2} = 1 + \frac{1}{4} +\frac{1}{9} +\frac{1}{16} + ...\]

    Sorprendentemente, tale serie risulta invece \textbf{convergere}. Difatti, notiamo che per ogni $k \geq 2$ si abbia che:
    \[\frac{1}{k^2} \leq \frac{1}{k^2-k} = \frac{1}{k-1} - \frac{1}{k}\]
    dunque otteniamo che:
    \[\sum_{k=1}^{+\infty} \frac{1}{k^2} \leq 1 + \sum_{k=2}^{+\infty} \frac{1}{k-1} - \frac{1}{k}\]
    Tuttavia, tale seconda serie ottenuta non è nient'altro che la nostra primissima serie telescopica analizzata, la quale sappiamo convergere ad 1. Per tanto, poiché l'intera somma è minore di un valore costante (in particolare minore di 2), ne segue che essa converga.

    A questo punto, possiamo vedere cosa accade alla serie armonica una volta \textbf{generalizzata} per un qualsiasi esponente 

    \begin{framedprop}{Serie armonica generalizzata}

        Data la \textbf{serie armonica generalizzata} si ha che
        $$
        \sum_{k = 1}^{\infty} \frac{1}{k^\alpha}  = \left \{ \begin{array}{ll}
                +\infty & \text{ se } \alpha \leq 1\\
                < +\infty & \text{ se } \alpha > 1\\
            \end{array} \right .
        $$
        
        dove $< +\infty$ indica la convergenza ad un valore
    \end{framedprop}

    \proofenv{
        Se $\alpha = 1$ allora essa coincide con la serie armonica standard, la quale sappiamo divergere.
        
        Se $\alpha < 1$ allora notiamo che:
        \[\frac{1}{k^\alpha} > \frac{1}{k} \implies \sum_{k=1}^{+\infty} \frac{1}{k^\alpha} > \sum_{k=1}^{+\infty} \frac{1}{k} = +\infty\]
        dunque essa diverge
        
        Se $\alpha > 1$ allora possiamo generalizzare i procedimenti svolti nel caso $\alpha = 2$. Posto $\alpha = 1 + \beta$, notiamo che per $k \geq 2$ si abbia che:
        \[\begin{split}
            \frac{\beta}{k^{1+\beta}} &= \frac{\beta}{k \cdot k^\beta} \\
            &\leq \frac{\beta}{k(k-1)^\beta}\\
            &= \frac{\frac{\beta}{k}}{(k-1)^\beta} \\
            &\leq \frac{1-\rbk{1-\frac{1}{k}}^\beta}{(k-1)^\beta} \\
            &= \frac{k^\beta - (k-1)^\beta}{(k-1)^\beta k^\beta} \\
            &= \frac{1}{(k-1)^\beta} - \frac{1}{k^\beta}
        \end{split}\]
        dunque otteniamo che $ \frac{1}{k^{1+\beta}} \leq \frac{1}{\beta}\rbk{\frac{1}{(k-1)^\beta} - \frac{1}{k^\beta}}$. A questo punto, sappiamo che:
        \[\begin{split}
            \sum_{k=1}^{+\infty} \frac{1}{k^\alpha} &\leq \sum_{k=2}^{+\infty} \frac{1}{\beta}\rbk{\frac{1}{(k-1)^\beta} - \frac{1}{k^\beta}}\\
            &= \frac{1}{\beta} \sum_{k=2}^{+\infty} \frac{1}{(k-1)^\beta} - \frac{1}{k^\beta} \\
        \end{split}\]
        infine, tale serie finale ottenuta risulta ovviamente essere telescopica, implicando che essa converga e di conseguenza che anche la nostra serie armonica generalizzata converga 

    }

    \newpage
    
    \subsection{Esercizi svolti}

    \begin{framedprob}{}
        Determinare se la seguenti serie convergano, divergano o nessuno dei due:
        \begin{enumerate}
            \item $\sum\limits_{k = 0}^{\infty} 2^k$
            \item $\sum\limits_{k = 0}^{\infty} \dfrac{1}{3^k}$
            \item $\sum\limits_{k = 0}^{\infty} \dfrac{1}{k+9} - \dfrac{1}{k+7}$
            \item $\sum\limits_{k = 5}^{\infty} \rbk{-\dfrac{1}{2}}^k$
        \end{enumerate}
        Nel caso in cui convergano, determinare il loro limite.
    \end{framedprob}

    \textit{Soluzione:}

    \begin{enumerate}
        \item Trattandosi di una serie geometrica con $q \geq 1$, tale serie diverge
        
        \item Notiamo che:
        \[\sum\limits_{k = 0}^{\infty} \frac{1}{3^k} = \sum\limits_{k = 0}^{\infty} \rbk{\frac{1}{3}}^k = \dfrac{1}{1-\frac{1}{3}} = \dfrac{3}{2}\]
        poiché si tratta di una serie geometrica con $-1 < q < 1$
        
        \item Notiamo che la serie sia telescopica:
        \[\begin{split}
            S_n &= \sum\limits_{k = 0}^{n} \frac{1}{k+9} - \frac{1}{k+7} \\
            &= \frac{1}{9} - \frac{1}{7} + \frac{1}{10} - \frac{1}{8} + \frac{1}{11} - \frac{1}{9} + \ldots + \frac{1}{n+7} - \frac{1}{n+5} + \frac{1}{n+8} - \frac{1}{n+6} + \frac{1}{n+9} - \frac{1}{n+7} \\
            &= - \frac{1}{7} - \frac{1}{8} + \frac{1}{n+8} + \frac{1}{n+9}
        \end{split}\]

        Dunque abbiamo che:
        \[\sum\limits_{k = 0}^{\infty} \dfrac{1}{k+9} - \dfrac{1}{k+7} = \lim_{n \to +\infty} - \frac{1}{7} - \frac{1}{8} + \frac{1}{n+8} + \frac{1}{n+9} = -\frac{15}{56}\]
        
        \item Prima di tutto, dobbiamo riassestare l'incide della serie ponendo $i = k-5$:
        \[\sum\limits_{k = 5}^{\infty} \rbk{-\dfrac{1}{2}}^k = \sum\limits_{i = 0}^{\infty} \rbk{-\dfrac{1}{2}}^{i+5}\]

        A questo punto, otteniamo che
        \[\sum\limits_{i = 0}^{\infty} \rbk{-\dfrac{1}{2}}^{i+5} = \rbk{-\frac{1}{2}}^5 \sum\limits_{i = 0}^{\infty} \rbk{-\dfrac{1}{2}}^{i} = \rbk{-\frac{1}{2}}^5 \rbk{\frac{1}{1+\frac{1}{2}}} = -\frac{1}{48}\]

    \end{enumerate}

    \section{Teoremi, Condizioni e Criteri di Convergenza}

    \subsection{Serie a termini di segno costante}
    
    Supponiamo che $(a_k)_{k \in \N^+}$ sia una successione tale che $\forall k \in \N^+$ si abbia che $a_{k} \geq 0$, ossia che ogni termine della successione sia positivo. Consideriamo ora la serie associata a tale successione e, in particolare, le sue serie parziali $S_{n+1}$ e $S_n$ per ogni $n \in \N$. Notiamo facilmente che:
    \[S_{n+1} - S_n = a_{n+1} \geq 0 \implies S_{n+1} \geq S_n\]
    Dunque, la serie risulta essere \textbf{crescente}. Di conseguenza, considerando il limite per $n \to +\infty$, è impossibile che il valore della serie oscilli tra più valori dunque è impossibile che il suo limite non esista, concludendo che la serie debba necessariamente \textbf{convergere} ad valore $\ell \geq 0$ o \textbf{divergere} a $+\infty$.

    Procedendo analogamente, se $(b_k)_{k \in \N^+}$ è una successione tale che $\forall k \in \N^+$ si abbia che $b_k \leq 0$, la sua serie deve necessariamente \textbf{convergere} ad valore $\ell' \leq 0$ o \textbf{divergere} a $-\infty$.

    \begin{framedthm}{Serie a termini di segno costante}
        Sia $(a_k)_{k \in \N^+}$. Se $\forall k \in \N^+$ si ha che $a_k \geq 0$ allora la serie $\sum\limits_{k = 0}^{+\infty} a_k$ \textbf{converge} ad un valore non-negativo o \textbf{diverge} a $+\infty$.
        
        Analogamente, se $\forall k \in \N^+$ si ha che $a_k \leq 0$ allora la serie $\sum\limits_{k = 0}^{+\infty} a_k$ \textbf{converge} ad un valore non-positivo o \textbf{diverge} a $-\infty$.
    \end{framedthm}

    \textbf{Esempi:}

    \begin{itemize}
        \item La seguente serie:
        \[\sum_{k = 0}^{+\infty} \frac{1}{k}\]
        deve necessariamente convergere o divergere poiché di segno costante
        \item La seguente serie:
        \[\sum_{k = 0}^{+\infty} \sin \rbk{\frac{1}{k}}\]
        deve necessariamente convergere o divergere poiché di segno costante
        \item La seguente serie:
        \[\sum_{k = 0}^{+\infty} \sin k\]
        non è detto se converga o diverga poiché non di segno costante
    \end{itemize}

    \quad

    \subsection{Condizione necessaria per la convergenza di una serie}
    
    Data la seguente serie, supponiamo che essa converga ad un valore $\ell \in \R$:
    \[ \sum_{k = 0}^{\infty} a_k = \lim_{n \to +\infty} S_n = \ell\]
    
    Risulta banale notare che $\lim_{n \to +\infty} S_n = \ell \implies \lim_{n \to +\infty} S_{n-1} = \ell$
    Di conseguenza, come nella sezione precedente, notiamo che:
    \[a_{n} = S_{n} - S_{n-1} \implies \lim_{n \to +\infty} a_{n} = \lim_{n \to +\infty} (S_{n} - S_{n-1}) \implies \lim_{n \to +\infty} a_{n} = \ell - \ell = 0\]

    Per definizione stessa della sommatoria, sappiamo che la \textbf{differenza} tra $S_{n+1}$ e $S_n$ risulta in:
    \[ S_{n+1} - S_n = \sum_{k=0}^{n+1} a_k - \sum_{k=0}^{n} a_k = a_{n+1}\]

    In altre parole, se una serie converge allora il suo termine deve tendere 0. Riformuliamo quindi tale risultato nel seguente teorema:
    
    \begin{framedthm}{Condizione necessaria per la convergenza}
        Data la successione $(a_k)_{k \in \N^+}$, si ha che:
        \[\sum_{k = 0}^{\infty} a_k \text{ converge} \Longrightarrow \lim_{n \to +\infty} a_n = 0\]
    \end{framedthm}
    
    È opportuno sottolineare che trattandosi di un'implicazione logica, tale teorema afferma che se la premessa è vera allora la conclusione deve essere vera, ma non il contrario. Il tipico esempio di ciò è la serie armonica vista precedentemente: nonostante per $n \to +\infty$ abbiamo che $\frac{1}{n} \to 0$, non è vero che la serie converga. 
        
    Tuttavia, ciò che ci interessa veramente di tale teorema risulta essere la sua affermazione contronominale. Tale contronominale viene solitamente chiamata \textbf{criterio di non convergenza}.

    \begin{framedcor}{Criterio di non convergenza}
        Data la successione $(a_k)_{k \in \N^+}$, si ha che:
        \[\lim_{n \to +\infty} a_n \neq 0 \implies \sum_{k = 0}^{\infty} a_k \text{ non converge}\]
    \end{framedcor}
    
    Per capire meglio, consideriamo la seguente serie:
    \[ \sum_{k=0}^{\infty} \frac{k}{k+1} \]
    
    Possiamo facilmente calcolare il limite di $a_k$ per $k \to +\infty$:
    \[ \lim_{k \to +\infty} \frac{k}{k+1} = \lim_{k \to +\infty} \frac{k}{k(1 + \frac{1}{k})} = 1\]
    
    Di conseguenza, per il criterio di non convergenza tale serie non può convergere.
    
    In particolare, tale criterio risulta molto utile se applicato sulle \textbf{serie a segno costante}. Difatti, poiché $\forall k \in \N$ si ha che $\frac{k}{k+1} \geq 0$, sappiamo che la serie appena analizzata è anche di segno costante, implicando che essa debba necessariamente convergere o divergere. Infine, poiché tale serie non può convergere grazie al criterio di non convergenza, possiamo facilmente concludere che tale serie diverga.

    \begin{framedcor}{}
        Data la successione $(a_k)_{k \in \N^+}$ di segno costante, si ha che:
        \[\lim_{n \to +\infty} a_n \neq 0 \implies \sum_{k = 0}^{\infty} a_k \text{ diverge}\]
    \end{framedcor}
    
    \quad

    \subsection{Criterio del confronto diretto}
    
    Proviamo ora ad analizzare una serie più complessa rispetto a quelle già viste:
    \[\sum_{k=1}^{\infty} \sin \left ( \frac{1}{k^2} \right )\]
    
    Notiamo facilmente che si tratta di una \textbf{serie a termini positivi}, poiché dunque la serie deve necessariamente convergere e o divergere. Tuttavia, notiamo anche che:
    \[\lim_{n \to +\infty} \sin \left ( \frac{1}{n^2}\right ) = \lim_{y \to 0} \sin \left ( y \right ) = 0\]
    Dunque il criterio di non convergenza fallisce per tale serie. Proviamo allora a \textbf{confrontare} i termini della serie con quelli di una serie con una per cui sia più facile stabilire se essa converga.

    Prima di tutto, dimostriamo che $\forall x \in \R^+$ si abbia che $\sin x < x$. Data la funzione $f(x) = \sin x - x$, consideriamo la sua derivata $f'(x) = \cos x - 1$. Notiamo che per $x \geq 0$ si abbia che $f'(x) \leq 0$, dunque la derivata è decrescente, implicando automaticamente che per $x \geq 0$ si abbia che $\sin x - x\leq 0$ e dunque che $\sin x \leq x$.

    A questo punto, stabiliamo facilmente che $\forall k \in \N^+$ si abbia che $\sin \rbk{\frac{1}{k^2}} \leq \frac{1}{k^2}$. Di conseguenza, otteniamo che:
    \[ 0 \leq \sum_{k=1}^{\infty} \sin \left ( \frac{1}{k^2} \right ) \leq \sum_{k=1}^{\infty} \frac{1}{k^2}\]

    Tuttavia, poiché la seconda serie ottenuta non è nient'altro che la serie armonica generalizzata con $\alpha > 1$, sappiamo che essa sia convergente ad un certo valore $\ell$. Dunque, poiché la nostra serie è limitata da tale serie armonica e quest'ultima converga ad un valore $\ell$, concludiamo che la nostra serie possa crescere al massimo fino ad $\ell$. Infine, poiché la nostra serie è limitata tra 0 e $\ell$, essa deve necessariamente \textbf{convergere} ad un valore finito. Possiamo quindi formulare il seguente teorema:
    
    \begin{framedthm}{Criterio del confronto diretto}
        Siano $(a_k)_{k \in \N^+}$ e $(b_k)_{k \in \N^+}$ due successioni tali che esista un valore $N \in \N$ tale che $\forall n \geq N$ valga $0 \leq a_n \leq b_n$, allora se la serie di $(b_k)_{k \in \N^+}$ \textbf{converge} anche la serie di $(a_k)_{k \in \N^+}$ \textbf{converge}:
        \[\sum_{k = 0}^{+\infty} b_k \text{ converge} \implies \sum_{k = 0}^{+\infty} a_k \text{ converge}\]
    \end{framedthm}

    Per contronominale, tale risultato può essere riformulato anche come:
    \[\sum_{k = 0}^{+\infty} a_k \text{ non converge} \implies \sum_{k = 0}^{+\infty} b_k \text{ non converge}\]
    
    \quad

    \subsection{Criterio del confronto asintotico}
    
    Vediamo ora una serie simile alla precedente, ma profondamente diversa:
    
    \[\sum_{k=1}^{\infty} \sin \left ( \frac{1}{k} \right )\]
    
    Notiamo facilmente che anch'essa sia una \textbf{serie a termini positivi}, poiché dunque la serie deve necessariamente convergere e o divergere. Tuttavia, notiamo anche che:
    \[\lim_{n \to +\infty} \sin \left ( \frac{1}{n}\right ) = \lim_{y \to 0} \sin \left ( y \right ) = 0\]
    Dunque il criterio di non convergenza fallisce per tale serie. Proviamo allora a \textbf{confrontare} i termini della serie con quelli di una serie con una per cui sia più facile stabilire se essa converga, procedendo in modo analogo alla sezione precedente:
    \[ 0 \leq \sum_{k=1}^{\infty} \sin \left ( \frac{1}{k} \right ) \leq \sum_{k=1}^{\infty} \frac{1}{k}\]
    
    Tuttavia, la serie usata per il confronto diretto risulta essere la serie armonica standard, la quale diverge. Di conseguenza, il criterio del confronto diretto fallisce in tal caso, a meno che non riusciamo a trovare una serie convergente che limiti la nostra serie (\textit{Spoiler}: non vi è poiché la nostra serie è in realtà divergente!).
    
    Abbiamo quindi bisogno di un'altra strategia. Tramite i limiti notevoli, sappiamo che:
    \[\lim_{n \to +\infty} \frac{\sin \rbk{\frac{1}{n}}}{\frac{1}{n}} = \lim_{y \to 0} \frac{\sin y}{y} = 1\]

    In altre parole, abbiamo che $\sin \rbk{\frac{1}{n}} \sim \frac{1}{n}$ (letto come \textit{"è simile a"}), ossia i due termini si comportano in modo del tutto equivalente per $n \to +\infty$. Tale risultato implica che per $n \to +\infty$ anche le due serie si comportano in modo equivalente. Per tanto, l'una \textbf{converge} se e solo se anche l'altra \textbf{converge}.

    Riformuliamo tale idea in un risultato più generale, il quale dimostriamo in modo più rigoroso rispetto a quello appena effettuato.

    \begin{framedthm}{Criterio del confronto asintotico}
        Siano $(a_k)_{k \in \N^+}$ e $(b_k)_{k \in \N^+}$ due successioni tali che
        \[ \lim_{n \to +\infty}  \frac{a_n}{b_n} = \delta\]
        dove $0 < \delta < +\infty$ allora si ha che:
        \[ \sum_{k=0}^{\infty} a_k \text{ converge} \iff \sum_{k=0}^{\infty} b_k \text{ converge}\]
    \end{framedthm}

    \proofenv{
        Data l'ipotesi:
        \[\lim_{n \to +\infty}  \frac{a_n}{b_n} = \delta\]

        per definizione di limite sappiamo che $\exists N \in \N$ tale che $\forall n \geq N$ esiste $\varepsilon > 0$ per cui valga:
        \[\delta -\varepsilon <  \frac{a_n}{b_n} < \delta + \varepsilon\]
        Da tale risultato, otteniamo automaticamente che:
        \[\sum\limits_{k = 0}^{+\infty} (\delta-\varepsilon) b_n < \sum\limits_{k = 0}^{+\infty} a_n < \sum\limits_{k = 0}^{+\infty} (\delta+\varepsilon) b_n \implies \]
        \[(\delta-\varepsilon) \sum\limits_{k = 0}^{+\infty} b_n < \sum\limits_{k = 0}^{+\infty} a_n < (\delta+\varepsilon) \sum\limits_{k = 0}^{+\infty}b_n \]

        Supponiamo quindi che la serie $\sum\limits_{k = 0}^{+\infty} a_n$ converga. In tal caso, per il criterio del confronto diretto anche la serie $(\delta-\varepsilon) \sum\limits_{k = 0}^{+\infty} b_n$ convergerà. Inoltre, poiché il fattore moltiplicativo $(\delta-\varepsilon)$ non influenza la convergenza tale ultima serie, ne segue che anche la serie $\sum\limits_{k = 0}^{+\infty} b_n$ converga. Viceversa, supponiamo che la serie $\sum\limits_{k = 0}^{+\infty} a_n$ converga. In tal caso, anche la serie $(\delta+\varepsilon) \sum\limits_{k = 0}^{+\infty} b_n$ convergerà poiché il fattore moltiplicativo non influenza la convergenza. A questo punto, ancora una volta per il criterio del confronto diretto ne segue automaticamente che anche la serie $\sum\limits_{k = 0}^{+\infty} a_n$ converga.
    }
    
    Tale criterio risulta uno degli strumenti principali per determinare la convergenza o la divergenza di una serie poiché riduce l'intero problema a dover calcolare un semplice limite. In particolare, per alcuni casi può risultare utile introdurre l'\textbf{approssimazione di Stirling}.
    
    \begin{framedprop}{Approssimazione di Stirling}
        Per $k \to +\infty$, si ha che $k! \sim k^k e^{-k} \sqrt{2\pi k}$, ossia:
        \[\lim_{k \to +\infty} \frac{k!}{k^k e^{-k} \sqrt{2\pi k}} = 1\]
    \end{framedprop}
    
    Tale risultato permette di lavorare con alcuni limiti altresì difficili da confrontare. Ad esempio, consideriamo la seguente serie:
    \[\sum_{k=0}^{\infty} \frac{k^k}{e^k \cdot k!} \]

    Utilizzando l'approssimazione di Stirling, notiamo facilmente che:
    \[\sum_{k=1}^{\infty} \frac{k^k}{e^k \cdot k!} \sim \sum_{k=1}^{\infty} \frac{k^k}{e^k \cdot k^k e^{-k} \sqrt{2\pi k}} = \frac{1}{\sqrt{2\pi}}\sum_{k=1}^{\infty} \frac{1}{\sqrt{ k}} \]
    
    A questo punto, notiamo che la serie ottenuta risulta essere un multiplo della serie armonica generalizzata con $\alpha < 1$. Per tanto, essa diverge e dunque anche la nostra serie iniziale diverge.

    \newpage
    
    \subsection{Criterio del rapporto e Criterio della radice}
    
    Assieme al criterio del confronto asintotico, il \textbf{criterio del rapporto} e il \textbf{criterio della radice} risultano essere due degli strumenti principali nello studio delle convergenze delle serie. Tuttavia, a differenza dei criteri precedenti, non vi è una vera e propria intuizione basilare dietro il perché essi funzionino. Per tanto, forniremo direttamente i loro enunciati e le loro dimostrazioni.
    
    \begin{framedthm}{Criterio del rapporto}
        Data la successione $(a_k)_{k \in \N^+}$ a termini positivi e tale che
        \[ \lim_{n \to +\infty} \frac{a_{n+1}}{a_n} = \delta\]
        
        dove $0 \leq \delta < +\infty$ allora si verifica che:
        \begin{itemize}
            \item Se $\delta < 1$ allora la serie associata \textbf{converge}
            \item Se $\delta > 1$ allora la serie associata \textbf{diverge}
        \end{itemize}
        
        \textit{Nota}: se $\delta = 1$ allora non possiamo concludere nulla
    \end{framedthm}

    \proofenv{
        Data l'ipotesi:
        \[\lim_{n \to +\infty} \frac{a_{n+1}}{a_n} = \delta\]
    
        per definizione di limite sappiamo che $\exists N \in \N$ tale che $\forall n \geq N$ esiste $\varepsilon > 0$ per cui valga:
        \[\delta -\varepsilon <  \frac{a_{n+1}}{a_n} < \delta + \varepsilon\]

        Supponiamo quindi che $\delta < 1$. Sia $q \in \R$ tale che $\delta < q < 1$. Per scelta di $r$ e poiché è possibile scegliere un termine $\varepsilon$ sempre più piccolo, diventando insignificante, esisterà un indice $m \geq N$ per cui si abbia che:
        \[\frac{a_{m+1}}{a_m} < q \implies a_{m+1} < q a_m\]

        A questo punto, notiamo che:
        \[\begin{split}
            a_{m+1} &< q a_m \\
            a_{m+2} &< q a_{m+1} < q^2 a_{m}\\
            \vdots
        \end{split}\]

        Difatti, è facilmente dimostrabile, ad esempio per induzione, che il risultato ottenuto sia generalizzabile come $a_{m+i} < q^i a_m$ per ogni valore $i \geq 1$. Scomponendo la nostra serie associata e utilizzando il risultato ottenuto, abbiamo che:
        \[\begin{split}
            \sum_{k = 1}^{+\infty} a_k &= \sum_{k = 1}^{m} a_k + \sum_{k = m+1}^{+\infty} a_k\\
            &= \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} a_{m+i} \\
            &< \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} q^ia_{m} \\
            &= \sum_{k = 1}^{m} a_k + a_m \sum_{i = 1}^{+\infty} q^i \\
            &= \sum_{k = 1}^{m} a_k + a_m \ell \\
        \end{split}\]

        In altre parole, la nostra serie iniziale risulta limitata da una serie geometrica dettata da $-1 < q < 1$, la quale sappiamo essere convergente ad un limite $\ell$. Di conseguenza, per il criterio del confronto diretto, anche la serie iniziale deve convergere (notiamo che i primi $m$ termini della serie sono finiti, per tanto la serie iniziale è una somma di due valori finiti).

        Supponiamo ora invece che $\delta > 1$. In tal caso, abbiamo che:
        \[1 - \varepsilon < \delta - \varepsilon <  \frac{a_{n+1}}{a_n}\]
        Poiché è possibile scegliere un termine $\varepsilon$ sempre più piccolo, diventando insignificante, esisterà un indice $m \geq N$ tale che:
        \[1 < \frac{a_{m+1}}{a_m} \implies a_m < a_{m+1}\]

        A questo punto, notiamo che:
        \[\begin{split}
            a_{m+1} &> a_m \\
            a_{m+2} &> {m+1} > a_{m}\\
            \vdots
        \end{split}\]

        Difatti, è facilmente dimostrabile, ad esempio per induzione, che il risultato ottenuto sia generalizzabile come $a_{m+i} > a_m$ per ogni valore $i \geq 1$.  Per tanto, scomponendo la nostra associata e utilizzando il risultato ottenuto, abbiamo che:
        \[\begin{split}
            \sum_{k = 1}^{+\infty} a_k &= \sum_{k = 1}^{m} a_k + \sum_{k = m+1}^{+\infty} a_k\\
            &= \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} a_{m+i} \\
            &> \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} a_{m} \\
            &= \sum_{k = 1}^{m} a_k + \infty \\
        \end{split}\]
        Dunque per il criterio del confronto diretto anche la serie iniziale deve necessariamente divergere.

    }

    Vediamo quindi due esempi di applicazione di tale criterio. Consideriamo la seguente serie:
    \[\sum_{k = 0}^{+\infty} \frac{k^3}{2^k}\]

    Applicando il criterio, notiamo che:
    \[\lim_{n \to +\infty} \frac{\frac{(n+1)^3}{2^{n+1}}}{\frac{n^3}{2^n}} = \lim_{n \to +\infty} \frac{(n+1)^3}{2^{n+1}} \cdot \frac{2^n}{n^3} = \frac{1}{2} < 1\]
    dunque la serie converge.

    Consideriamo invece la seguente serie:
    \[\sum_{k = 0}^{+\infty} \frac{k!}{2^k}\]

    Applicando il criterio, notiamo che:
    \[\lim_{n \to +\infty} \frac{\frac{(n+1)!}{2^{n+1}}}{\frac{n!}{2^n}} = \lim_{n \to +\infty} \frac{(n+1)!}{2^{n+1}} \cdot \frac{2^n}{n!} = \lim_{n \to +\infty} \frac{n+1}{2} = +\infty > 1\]
    dunque la serie diverge.

    Vediamo ora invece il criterio della radice, il cui funzionamento e dimostrazione sono del tutto analoghi a quello del rapporto.

    \begin{framedthm}{Criterio della radice}
        Data la successione $(a_k)_{k \in \N^+}$ a termini positivi e tale che
        \[ \lim_{n \to +\infty} \sqrt[n]{a_n} = \delta\]
        
        dove $0 \leq \delta < +\infty$ allora si verifica che:
        \begin{itemize}
            \item Se $\delta < 1$ allora la serie associata \textbf{converge}
            \item Se $\delta > 1$ allora la serie associata \textbf{diverge}
        \end{itemize}
        
        \textit{Nota}: se $\delta = 1$ allora non possiamo concludere nulla
    \end{framedthm}


    \proofenv{
        Data l'ipotesi:
        \[ \lim_{n \to +\infty} \sqrt[n]{a_n} = \delta\]
    
        per definizione di limite sappiamo che $\exists N \in \N$ tale che $\forall n \geq N$ esiste $\varepsilon > 0$ per cui valga:
        \[\delta -\varepsilon <  \sqrt[n]{a_n} < \delta + \varepsilon\]

        Supponiamo quindi che $\delta < 1$. Sia $q \in \R$ tale che $\delta < q < 1$. Per scelta di $q$ e poiché è possibile scegliere un termine $\varepsilon$ sempre più piccolo, diventando insignificante, esisterà un indice $m \geq N$ per cui si abbia che:
        \[\sqrt[m]{a_m} < q \implies a_m < q^m\]
        
        Il risultato ottenuto è generalizzabile come $a_{m+i} < q^{m+i}$ per ogni valore $i \geq 1$. Scomponendo la nostra serie associata e utilizzando il risultato ottenuto, abbiamo che:
        \[\begin{split}
            \sum_{k = 1}^{+\infty} a_k &= \sum_{k = 1}^{m} a_k + \sum_{k = m+1}^{+\infty} a_k\\
            &= \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} a_{m+i} \\
            &< \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} q^{m+i} \\
            &= \sum_{k = 1}^{m} a_k + q^m \sum_{i = 1}^{+\infty} q^i \\
            &= \sum_{k = 1}^{m} a_k + q^m \ell \\
        \end{split}\]

        In altre parole, la nostra serie iniziale risulta limitata da una serie geometrica dettata da $-1 < q < 1$, la quale sappiamo essere convergente ad un limite $\ell$. Di conseguenza, per il criterio del confronto diretto, anche la serie iniziale deve convergere (notiamo che i primi $m$ termini della serie sono finiti, per tanto la serie iniziale è una somma di due valori finiti).

        Supponiamo ora invece che $\delta > 1$. In tal caso, abbiamo che:
        \[1 - \varepsilon < \delta - \varepsilon < \sqrt[n]{a_n}\]
        Poiché è possibile scegliere un termine $\varepsilon$ sempre più piccolo, diventando insignificante, esisterà un indice $m \geq N$ tale che:
        \[1 < \sqrt[m]{a_m} \implies 1 < a_m\]

        Il risultato ottenuto è generalizzabile come $a_{m+i} > 1$ per ogni valore $i \geq 1$. 
        Per tanto, scomponendo la nostra associata e utilizzando il risultato ottenuto, abbiamo che:
        \[\begin{split}
            \sum_{k = 1}^{+\infty} a_k &= \sum_{k = 1}^{m} a_k + \sum_{k = m+1}^{+\infty} a_k\\
            &= \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} a_{m+i} \\
            &> \sum_{k = 1}^{m} a_k + \sum_{i = 1}^{+\infty} 1 \\
            &= \sum_{k = 1}^{m} a_k + \infty \\
        \end{split}\]
        Dunque per il criterio del confronto diretto anche la serie iniziale deve necessariamente divergere.

    }
    
    Come già accennato, i due criteri risultano estremamente simili e spesso il limite calcolato tramite entrambi risulta coincidere. Per tanto, la scelta tra l'uno e l'altro ricade interamente sulla preferenza personale e sulla serie che ci troviamo ad affrontare. 
    
    Ad esempio, considerando ancora la serie $\sum\limits_{k=0}^{\infty} \dfrac{k!}{k^2}$, abbiamo mostrato come il criterio del rapporto su essa risulti ottimale. Invece, applicare il criterio della radice su di esso potrebbe risultare più complesso.
    
    \newpage
    
    \subsection{Criterio di Leibniz}
    
    Fino ad ora abbiamo trattato tipologie di serie in cui il segno rimane \textbf{costante} (serie a termini positivi e serie a termini negativi), fatta eccezione per la serie $a_k = (-1)^k$, che abbiamo decretato come \textbf{nè convergente nè divergente}.
    
    Rimane però il dubbio per le serie in cui $(-1)^k$ costituisce solo una parte di ogni termine della successione e non la sua totalità. In particolare, dunque, siamo interessati al trattare serie in cui i segni dei termini si alternano. Il \textbf{criterio di Leibniz} è in grado di determinare se una serie di questo tipo sia in grado di convergere oppure no in base a \textbf{tre requisiti}:
    
    \begin{framedthm}{Criterio di Leibniz}
        Sia $(a_k)_{k \in \N^+}$ una serie a segno alterno, ossia $(a_k)_{k \in \N^+} = (-1)^k \cdot b_k$, dove $(b_k)_{k \in \N^+}$.
        
        Se si verifica che:
        \begin{itemize}
            \item $(b_k)_{k \in \N^+}$ è una successione di termini di segno non negativo
            \item $\forall k \in \N^+$ si ha che $b_{k+1} \leq b_k$, ossia è una successione decrescente
            \item $\lim\limits_{n \to +\infty} b_n = 0$
        \end{itemize}
        allora $\sum\limits_{k = 0}^{+\infty} a_k$ converge.
    \end{framedthm}

    \proofenv{
        Procediamo con la dimostrazione al fine di mostrare che le serie parziali di pedice pari e di pedice dispari convergano allo stesso valore per $n \to +\infty$. 

        Prima di tutto, notiamo che $S_{2n+1} - S_{2n} = (-1)^{2n+1} \cdot b_{2n+1} = -b_{2n+1} \leq 0$ poiché $(b_k)_{n \in \N^+}$ è una successione di termini di segno non negativo, implicando quindi che $S_{2n+1} \leq S_{2n}$.

        A questo punto, notiamo che:
        \[S_{2(n+1)} = S_{2n+2} = S_{2n} + (-1)^{2n+1} b_{2n+1} + (-1)^{2n+2} b_{2n+2} = S_{2n} - b_{2n+1} + b_{2n+2}\]

        Poiché per ipotesi abbiamo che $b_{i+1} \leq b_i$ per ogni $i \in \N^+$, otteniamo che $b_{2n+2} \leq b_{2n+1}$ e dunque che $b_{2n+2} - b_{2n+1} \leq 0$. Per tanto, concludiamo che:
        \[S_{2(n+1)} = S_{2n} - b_{2n+1} + b_{2n+2} \leq S_{2n}\]
        
        In altre parole, le serie parziali di pedice pari sono decrescenti, ossia:
        \[S_{2(n+1)} \leq S_{2n} \leq \ldots \leq S_{2} = b_2 - b_1\]

        Similmente, notiamo che:
        \[S_{2(n+1)+1} = S_{2n+3} = S_{2n+1} + (-1)^{2n+2} b_{2n+2} + (-1)^{2n+3} b_{2n+3} = S_{2n+1} + b_{2n+2} - b_{2n+3}\]

        Poiché per ipotesi abbiamo che $b_{i+1} \leq b_i$ per ogni $i \in \N^+$, otteniamo che $b_{2n+3} \leq b_{2n+2}$ e dunque che $b_{2n+2} - b_{2n+3} \geq 0$. Per tanto, concludiamo che:
        \[S_{2(n+1)+1} = S_{2n+1} + b_{2n+2} - b_{2n+3} \geq S_{2n+1}\]

        In altre parole, le serie parziali di pedice dispari sono crescenti, ossia:
        \[S_{2(n+1)+1} \geq S_{2n+1} \geq \ldots \geq S_{3} \geq S_1 = -b_1\]
        
        A questo punto, unendo i tre risultati ottenuti, concludiamo che:
        \[ -b_1 = S_1 \leq \ldots \leq S_{2(n+1)} \leq S_{2n+1} \leq S_{2n} \leq \ldots \leq S_{2} = b_2 - b_1\]

        Dunque, indipendentemente dalla parità del pedice della serie parziale, essa convergerà ad un valore compreso tra $-b_1$ e $b_2 - b_1$. Ciò ci permette quindi di stabilire che esistano due valori $-b_1 \leq \ell, \ell' \leq b_2 - b_1$ tali che:
        \[\lim_{n \to +\infty} S_{2n+1} = \ell \qquad\qquad \lim_{n \to +\infty} S_{2n} = \ell'\]

        Infine, poiché $\lim_{n \to +\infty} b_n = 0$, notiamo che:
        \[\lim_{n \to +\infty} b_n = 0 \implies \lim_{n \to +\infty} a_n = 0  \implies \lim_{n \to +\infty} a_{2n+1} = 0 \]

        A questo punto, poiché i due limiti esistono, otteniamo che: 
        \[\lim_{n \to +\infty} S_{2n+1} - S_{2n} = \lim_{n \to +\infty} a_{2n+1} = 0 \implies \lim_{n \to +\infty} S_{2n+1} = \lim_{n \to +\infty} S_{2n} \implies \ell = \ell'\]

        In altre parole, sia che il pedice della serie sia pari sia che esso sia dispari, la serie parziale tenderà sempre al valore $\ell = \ell'$. Per tanto, la serie convergerà sempre allo stesso valore.

    }
    
    Vediamo quindi un esempio di applicazione di tale criterio. Consideriamo le seguente serie:
    \[ \sum_{k=1}^{\infty} \frac{(-1)^k}{k}\]
    
    Notiamo facilmente che non possiamo applicare nessuno dei criteri visti precedentemente. Vediamo quindi se essa rispetta le tre condizioni del \textbf{criterio di Leibniz}.
    Prima di tutto, mettiamo in evidenza $b_k$ ricordando che $a_k = (-1)^k \cdot b_k$:
    \[ \sum_{k=1}^{\infty} \frac{(-1)^k}{k} = \sum_{k=0}^{\infty} (-1)^k \cdot \frac{1}{k}\]
    
    \newpage

    Successivamente, verifichiamo le condizioni
    \begin{itemize}
        \item $b_k = \frac{1}{k}$ è a \textbf{termini positivi}
        \item $b_{k+1} \leq b_k \Longrightarrow \frac{1}{k+1} \leq \frac{1}{k}$ è vero per ogni $k$, dunque è \textbf{decrescente}
        \item $b_k \to 0$ per $k \to +\infty$
    \end{itemize}
    
    Tutte e tre le condizioni sono soddisfatte, dunque si tratta di una serie \textbf{convergente}
    
    
    \quad
    
    \subsection{Criterio di convergenza assoluta}
    
    Nonostante il criterio di Leibniz riesca a trattare anche le serie a segno alterno, esso è strettamente dipendente da una caratteristica: l'alternanza tra i segni deve essere regolare, ossia deve ricorrere dopo ogni termine. Ma cosa accade se l'alternanza non è regolare?
    
    Consideriamo la seguente serie
    \[ \sum_{k = 1}^{\infty} \frac{\sin(k)}{k^2}\]
    
    In questa situazione, ogni termine compreso tra $0 \leq k \leq \pi$ è positivo mentre quelli tra $\pi \leq k \leq 2\pi$ sono negativi, dunque il criterio di Leibniz non è applicabile.
    
    Sarà quindi necessario applicare quello che viene chiamato \textbf{criterio di convergenza assoluta}:
    
    \begin{framedthm}{Criterio di convergenza assoluta}
        Data la serie $(a_k)_{k \in \N^+}$, si ha che:
        \[ \sum_{k=0}^{\infty} \abs{a_k} \text{ converge} \implies \sum_{k=0}^{\infty} a_k \text{ converge}\]
        
        Inoltre, in tal caso diciamo che la serie \textbf{converge assolutamente}
    \end{framedthm}

    \proofenv{
        Supponiamo che $\sum\limits_{k=0}^{\infty} \abs{a_k}$ converga ad un valore $\ell$. Consideriamo quindi la seguente serie:
        \[\sum\limits_{k=0}^{\infty} a_k + \abs{a_k}\]
        Risulta evidente che:
        \[0 \leq \sum\limits_{k=0}^{\infty} a_k + \abs{a_k} \leq \sum\limits_{k=0}^{\infty} 2 \abs{a_k}\]

        Di conseguenza, per il criterio del confronto diretto si ha che anche $\sum\limits_{k=0}^{\infty} a_k - \abs{a_k}$ converga ad un valore $\ell'$. A questo punto, notiamo che:
        \[\sum\limits_{k=0}^{\infty} a_k = \sum\limits_{k=0}^{\infty} a_k + \abs{a_k} - \sum\limits_{k=0}^{\infty} \abs{a_k} = \ell - \ell'\]
        concludendo che anche la serie iniziale converga.
    }

    Tornando al nostro esempio precedente, dunque, proviamo a vedere se la serie converge assolutamente
    \[\sum_{k=1}^{\infty} \abs{\frac{\sin(k)}{k^2}} = \sum_{k=1}^{\infty} \frac{\abs{\sin(k)}}{k^2}\]
    Applicando il criterio del confronto diretto, la serie risulta limitata da una serie armonica generalizzata con $\alpha > 1$:
    \[ 0 \leq \sum_{k=1}^{\infty} \frac{\abs{\sin(k)}}{k^2} \leq \sum_{k=1}^{\infty} \frac{1}{k^2}\]
    dunque la serie converge assolutamente e per tanto converge anche normalmente.
    
    Di primo occhio, si potrebbe pensare che una volta introdotto il criterio di convergenza assoluta esso possa rimpiazzare completamente il criterio di Leibniz. Tuttavia, non sempre ciò risulta possibile.

    Ad esempio, abbiamo visto come la serie $\sum\limits_{k=1}^{\infty} \frac{(-1)^k}{k}$ converga tramite il criterio di Leibniz. Per quanto riguarda il criterio della convergenza assoluta, invece, esso fallisce in quanto la serie in realtà \textit{diverga assolutamente}.
    
    \subsection{Esercizi svolti}

    \begin{framedprob}{}
        Determinare se le seguenti serie convergano, divergano o nessuno dei due:
        \begin{enumerate}
            \item $\sum\limits_{k = 0}^{+\infty} \sin^2 \rbk{\dfrac{1}{k}}$
            \item $\sum\limits_{k = 0}^{+\infty} e^{\frac{1}{k}}$
            \item $\sum\limits_{k = 0}^{+\infty} \dfrac{\sin^2 (k) + 1}{3^n + n} $
        \end{enumerate}
    \end{framedprob}

    \textit{Soluzione:}

    \begin{enumerate}
        \item Notiamo che:
        \[\lim_{n \to +\infty} \frac{\sin^2 \rbk{\dfrac{1}{n}}}{\frac{1}{n^2}} = \lim_{y \to 0} \frac{\sin^2 y}{y^2} = 1\]
        dunque abbiamo che:
        \[\sum\limits_{k = 0}^{+\infty} \sin^2 \rbk{\dfrac{1}{k}} \sim \sum\limits_{k = 0}^{+\infty} \frac{1}{k^2}\]
        di conseguenza, poiché la serie ottenuta converge in quanto sia una serie armonica con $\alpha > 1$, per il criterio del confronto asintotico abbiamo che anche la serie iniziale converga

        \item Notiamo che $\forall k \in \N^+$ si abbia che $e^{\frac{1}{k}} \geq 0$. Dunque, trattandosi di una serie a termini positivi, essa deve necessariamente convergere o divergere. Tuttavia, tramite il criterio di non convergenza notiamo che:
        \[\lim_{n \to +\infty} e^{\frac{1}{n}} = e^0 = 1\]
        dunque la serie non può convergere, concludendo che essa diverga

        \item Risulta evidente che per $n \to +\infty$ si abbia che $\frac{1}{3^n + n} \sim \frac{1}{3^n}$, implicando dunque che:
        \[ \sum_{k = 0}^{+\infty}  \frac{\sin^2 (k) + 1}{3^k + k} \sim  \sum_{k = 0}^{+\infty}  \frac{\sin^2 (k) + 1}{3^k}\]

        Inoltre, sappiamo che per ogni $x \in \R$ si abbia che $0 \leq \sin^2 x \leq 1$. Dunque, abbiamo che:
        \[\sum_{k = 0}^{+\infty} \frac{1}{3^k} \leq \sum_{k = 0}^{+\infty}  \frac{\sin^2 (k) + 1}{3^k} \leq \sum_{k = 0}^{+\infty} \frac{2}{3^k}\]

        La serie con cui abbiamo effettuato il confronto asintotico, dunque, risulta schiacciata tra due serie geometriche con $-1 < q < 1$, le quali convergono, implicando che anche essa converga. Per tanto, anche la serie originale converge.

    \end{enumerate}

    \begin{framedprob}{}
        Determinare se le seguenti serie convergano, divergano o nessuno dei due:
        \begin{enumerate}
            \item $\sum\limits_{k = 0}^{+\infty} \dfrac{k!}{2^k}$
            \item $\sum\limits_{k = 0}^{+\infty} \dfrac{(-2)^k}{k^k}$
            \item $\sum\limits_{k = 0}^{+\infty} \dfrac{\cos k}{k^2}$
            \item $\sum\limits_{k = 0}^{+\infty} \dfrac{k}{(-3)^k}$
        \end{enumerate}
    \end{framedprob}

    \begin{enumerate}
        \item Tramite il criterio del rapporto abbiamo che:
        \[\lim_{n \to +\infty} \frac{\frac{(n+1)!}{2^{n+1}}}{\frac{n!}{2^n}} = \lim_{n \to +\infty} \frac{n+1}{2} = +\infty\]
        dunque la serie diverge
        \item Prima di tutto, esplicitiamo i termini della serie:
        \[\sum\limits_{k = 0}^{+\infty} \dfrac{(-2)^k}{k^k} = \sum\limits_{k = 0}^{+\infty} (-1)^k \rbk{\dfrac{2}{k}}^k\]
        a questo punto, notiamo che:
        \begin{itemize}
            \item $\rbk{\rbk{\frac{2}{k}}^k}_{k \in \N^+}$ è una successione di termini non negativi
            \item $\forall k \in \N^+$ si ha che $\rbk{\frac{2}{k}}^{k+1} \leq \rbk{\frac{2}{k}}^k$
            \item $\lim\limits_{n \to +\infty} \rbk{\frac{2}{n}}^n = \rbk{0}^n = 0$
        \end{itemize}
        dunque la serie converge per il criterio di Leibniz

        \item Sappiamo che $0 \leq \abs{\cos x} \leq 1$ per ogni $x \in \R$. Dunque abbiamo che:
        \[0 \leq \sum\limits_{k = 0}^{+\infty} \abs{\dfrac{\cos k}{k^2 }} \leq \sum\limits_{k = 0}^{+\infty} \dfrac{1}{k^2}\]
        Poiché la serie con cui stiamo maggiorando è una serie armonica con $\alpha > 1$, ne segue che essa converga. Di conseguenza, la serie iniziale converge assolutamente e dunque anche normalmente.

        \item Consideriamo la serie assoluta equivalente:
        \[\sum_{k = 0}^{+\infty} \abs{\frac{k}{(-3)^k}} = \sum_{k = 0}^{+\infty} \frac{k}{3^k}\]
        Tramite il criterio della radice abbiamo che:
        \[\lim_{n \to +\infty} \sqrt[n]{\frac{n}{3^n}} = \lim_{n \to +\infty} \frac{\sqrt[n]{n}}{\sqrt[n]{3^n}} = \frac{1}{3} < 1\]
        dunque la serie iniziale converge assolutamente e dunque anche normalmente
    \end{enumerate}

    \newpage

    \section{Serie di potenze}

    Abbiamo visto come la serie geometrica converga alla funzione $f(x) = \frac{1}{1-x}$ per ogni $x \in (-1, 1)$. Tale risultato ci permette quindi di rappresentare tale funzione come un \textbf{polinomio di infiniti termini}.
    \[\frac{1}{1-x} = \sum_{k = 0}^{+\infty} x^k = 1 + x + x^2 + x^3 + \ldots\]
    a patto ovviamente che $x \in (-1, 1)$. Nel caso generale, tali polinomi di "grado infinito" vengono detti \textbf{serie di potenze}.

    \begin{frameddefn}{Serie di Potenze}
        Data una successione $(a_k)_{k \in \N^+}$ e un valore $x_0 \in \R$, definiamo come \textbf{serie di potenze di centro $x_0$ associata a tale successione} come:
        \[\sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
        
        dove la successione è detta \textbf{successione dei coefficienti}, e $x_0$ è detto \textbf{centro della serie}.
    \end{frameddefn}

    Nel caso della serie geometrica, dunque, la successione dei coefficienti è dettata da $a_k = 1$, mentre il centro della serie è $x_0 = 0$. In particolare, notiamo che non tutte le serie trattate fino ad ora siano delle serie di potenze. Ad esempio, la serie armonica non è una serie di potenze in quanto essa non rappresenti un polinomio di "grado infinito".

    Caratteristica principale delle serie di potenze risulta essere il loro \textbf{insieme di convergenza}, ossia l'insieme $X \subset \R$ per cui la serie converge ad un valore finito.

    \begin{frameddefn}{Insieme di convergenza}
        Data una serie di potenze, definiamo come \textbf{insieme di convergenza} l'intervallo $X \subseteq \R$ per cui si per ogni $x \in X$ esiste un valore $\ell \in \R$ tale che:
        \[\ell = \sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
    \end{frameddefn}

    Ad esempio, nel caso della serie geometrica sappiamo che per $x \in (-1, 1)$ tale serie converga al valore $\frac{1}{1-x}$. In particolare, notiamo che nel caso generico tale insieme di convergenza contenga sempre almeno un valore, ossia il centro stesso della serie. Difatti, abbiamo che:
    \[\sum_{k = 0}^{\infty} a_k \cdot (x_0-x_0)^k = a_0 \cdot 0^0 + a_1 \cdot 0^1 + a_2 \cdot 0^2 + \ldots = a_0 \cdot 1 + 0 + 0 + \ldots = a_0\]

    \begin{framedobs}{}
        Data la serie di potenze
        \[\sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
        avente insieme di convergenza $X$, si ha sempre che $x_0 \in X$
    \end{framedobs}

    In particolare, tale insieme risulta sempre essere un \textbf{intervallo} avente centro corrispondente proprio al centro della serie. Difatti, vale la seguente proposizione.

    \begin{framedprop}{}
        Data la serie di potenze
        \[\sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
        se la serie converge per un valore $\overline{x} \neq x_0$ allora per ogni $x \in \R$ tale che $\abs{x} < \abs{\overline{x}}$ la serie converge
    \end{framedprop}

    \proofenv{
        Supponiamo che la serie converga per $\overline{x} \neq x_0$. In tal caso, la condizione necessaria di convergenza ci dice che:
        \[\sum_{k = 0}^{\infty} a_k \cdot (\overline{x}-x_0)^k \text{ converge} \implies \lim_{n \to +\infty} a_n (\overline{x}-x_0)^n = 0\]

        Di conseguenza, per definizione di limite di successione esiste un intero $N$ tale che per ogni $n \geq N$ si abbia che $a_n (\overline{x}-x_0)^n \leq 1$. A questo punto, notiamo che:
        \[\abs{a_k(x-x_0)^n} = \abs{a_k (\overline{x}-x_0)^n} \cdot \abs{\frac{(x-x_0)^n}{(\overline{x}-x_0)^n}} \leq \abs{\frac{(x-x_0)}{(\overline{x}-x_0)}}^n\]

        Di conseguenza, otteniamo che:
        \[0 \leq \sum_{k = 0}^{\infty} \abs{a_k \cdot (x-x_0)^k} \leq \sum_{k = 0}^{\infty} \abs{\frac{(x-x_0)}{(\overline{x}-x_0)}}^n\]

        Poiché la serie con cui viene effettuato il confronto è una serie geometrica, sappiamo che essa converga se:
        \[\abs{\frac{(x-x_0)}{(\overline{x}-x_0)}} < 1 \implies \abs{x-x_0} < \abs{\overline{x}-x_0} \implies \abs{x} < \abs{\overline{x}}\]

        Di conseguenza, $\forall x \in \R$ tali che $\abs{x} < \abs{\overline{x}}$, per il criterio del confronto la serie convergerà assolutamente e dunque anche normalmente.

    }

    \begin{framedcor}{Intervallo di convergenza}
        Data la serie di potenze
        \[\sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
        se la serie converge per un valore $\overline{x} \neq x_0$ allora per ogni $x \in (-\overline{x}, \overline{x})$ la serie converge
    \end{framedcor}

    \quad

    \subsection{Raggio di convergenza}

    A questo punto, introduciamo un concetto che unisce il centro della serie al corollario appena ottenuto, ossia il \textbf{raggio di convergenza}: poiché è assicurato che la serie di potenze converga nel suo centro, tramite il corollario risulta intuitivo pensare che il centro della serie sia anche il centro dell'intervallo di convergenza.

    \begin{frameddefn}{Raggio di convergenza}
        Data la serie di potenze
        \[\sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
        definiamo come \textbf{raggio di convergenza della serie} l'estremo superiore di tutti i valori $x \geq 0$ per cui la serie converge, ossia:
        \[\rho := \sup \{x \geq 0 \mid \sum_{k = 0}^{\infty} a_k \cdot (\rho-x_0)^k \text{ converge}\}\]
    \end{frameddefn}

    Ovviamente, dalla definizione segue che $\rho$ sia un valore in $[0, +\infty]$. Inoltre, il seguente risultato segue dalla definizione stessa:

    \begin{framedprop}{}
        Data una serie di potenze di centro $x_0$ e raggio di convergenza $\rho$, si ha che:
        \begin{itemize}
            \item Per ogni $x \in \R$ tale che $\abs{x-x_0} < \rho$ la serie \textbf{converge}
            \item Per ogni $x \in \R$ tale che $\abs{x-x_0} > \rho$ la serie \textbf{non converge}
            \item Per ogni $x \in \R$ tale che $\abs{x-x_0} = \rho$ il comportamento della serie è \textbf{ignoto}
        \end{itemize} 
    \end{framedprop}

    Nel caso della serie geometrica, sappiamo già che il suo centro sia $x_0 = 0$ e che il suo intervallo di convergenza sia $[1, -1]$, concludendo dunque che il raggio di convergenza sia $\rho = 1$. Nel caso generale, invece, per calcolare il raggio di convergenza di una serie ci serviamo del seguente teorema.

    \begin{framedthm}{Calcolo del raggio di convergenza}
        Data la serie di potenze
        \[\sum_{k = 0}^{\infty} a_k \cdot (x-x_0)^k\]
        il suo raggio di convergenza $\rho$ è dato da:
        $$
        \rho =  \lim_{n \to +\infty} \frac{1}{\abs{\frac{a_{n+1}}{n_k}}} = \lim_{k \to +\infty} \frac{1}{\sqrt[n]{\abs{a_n}}}
        $$

        \textit{(dimostrazione omessa)}
    \end{framedthm}

    In forma impropria, dunque, potremmo dire che dato il limite:
    \[\ell \text{ "=" }\lim_{n \to +\infty} \abs{\frac{a_{n+1}}{n_k}} = \lim_{k \to +\infty} \sqrt[n]{\abs{a_n}}\]
    abbiamo che $\rho \text{ "=" } \dfrac{1}{\ell}$.

    Consideriamo quindi la seguente serie di potenze:
    \[\sum_{k=0}^{\infty} \frac{(x-2)^k}{(k+1)(k+2)}\]

    Prima di tutto, isoliamo la successione di coefficienti della serie, in modo da facilitare i passaggi seguenti:
    \[\sum_{k=0}^{\infty} \frac{1}{(k+1)(k+2)} (x-2)^k\]

    Notiamo facilmente che il suo centro sia $x_0 = 2$. Utilizziamo quindi il precedente teorema per calcolare il raggio di convergenza $\rho$:
    \[\ell \text{ "=" } \lim_{n \to +\infty} \abs{\frac{\frac{1}{(n+2)(n+3)}}{\frac{1}{(n+1)(n+2)}}} = \lim_{n \to +\infty} \abs{\frac{(n+1)(n+2)}{(n+2)(n+3)}} = 1\]
    ottenendo che $\rho \text{ "=" } \dfrac{1}{\ell} = 1$

    Di conseguenza, sappiamo che $\forall x \in (2 - \rho, 2 + \rho) = (1,3)$ la serie converga. Vediamo quindi cosa accade agli estremi di tale intervallo aperto:
    \begin{itemize}
        \item Per $x = 1$ abbiamo che:
        \[\sum_{k=0}^{\infty} \frac{(1-2)^k }{(k+1)(k+2)}= \sum_{k=0}^{\infty} \frac{(-1)^k}{k^2 + 3k + 2}\]
        la quale converge tramite il criterio di Leibniz

        \item Per $x = 3$ abbiamo che:
        \[\sum_{k=0}^{\infty} \frac{(3-2)^k }{(k+1)(k+2)}= \sum_{k=0}^{\infty} \frac{1}{k^2 + 3k + 2}\]
        la quale converge poiché confrontabile asintoticamente con una serie armonica generalizzata con $\alpha > 1$
    \end{itemize}

    Per tanto, concludiamo che l'intervallo di convergenza di tale serie sia $[1, 3]$. Consideriamo invece la seguente serie di potenze:
    \[ \sum_{k=0}^{\infty} \frac{3^k(2x-1)^k}{k!} \]

    Come nel caso precedente, riscriviamo la serie in modo da isolare la sua successione di coefficienti:
    \[ \sum_{k=0}^{\infty} \frac{6^k}{k!}\rbk{x-\frac{1}{2}}^k\]

    Otteniamo quindi che il centro di tale serie sia $x_0 = \frac{1}{2}$. Calcoliamo quindi il suo raggio di convergenza:
    \[\ell \text{ "=" } \lim_{n \to +\infty} \abs{\frac{\frac{6^{n+1}}{(n+1)!}}{\frac{6^n}{n!}}} = \lim_{n \to +\infty} \abs{\frac{6^{n+1}}{(n+1)!}\frac{n!}{6^n}} = 0\]
    ottenendo che $\rho \text{ "=" } \dfrac{1}{\ell} = +\infty$ e dunque che l'intervallo di convergenza della serie sia $\rbk{\frac{1}{2} - \infty, \frac{1}{2} + \infty} = \R$.

    \quad

    \subsection{Derivazione di serie di potenze}

    Consideriamo una serie di potenze generica:
    \[ f(x) = \sum_{k=0}^{\infty} a_k (x-x_0)^k = a_0 + a_1 (x-x_0) + a_2 (x-x_0)^2 + \ldots\]
    
    Come sappiamo, in matematica le \textbf{funzioni} godono di alcune proprietà, come la \textbf{continuità} in un intervallo o la \textbf{derivabilità}.
    
    Poiché abbiamo già affermato che le funzioni possano essere espresse anche in termini di serie di potenze, ne consegue che anche quest'ultime godano delle stesse proprietà, in particolare la \textbf{derivabilità}: se una funzione può essere espressa sotto forma di una somma infinita di termini, ne segue che la derivata di tale funzione possa essere espressa sotto forma di una somma infinita delle derivate di ogni termine originale.

    In particolare, notiamo che derivando tale serie in $x$ i termini della successione dei coefficienti siano una costante, mentre per $(x-x_0)^k$ è sufficiente utilizzare la regola della derivazione di potenze. 

    Ad esempio, per la derivata di primo ordine abbiamo che:
    \[ \begin{split}
        f'(x) &= \frac{d}{d x} \; a_0 + a_1 (x-x_0) + a_2 (x-x_0)^2 + a_3 (x-x_0)^3 + \ldots\\
        &= 0 + a_1 + 2 a_2 (x-x_0) + 3 a_3 (x-x_0)^2 + \ldots \\
        &= \sum_{k=1}^{\infty} k \, a_k \, (x-x_0)^{k-1}
    \end{split}\]

    \textit{Nota:} la notazione $\frac{d}{dx} \; g(x)$ è un'alternativa alla notazione $g'(x)$

    Analogamente, per la derivata di secondo ordine avremo che:
    \[ \begin{split}
        f''(x) &= \frac{d}{d x} \; a_1 + 2 a_2 (x-x_0) + 3 a_3 (x-x_0)^2 + 4 a_4 (x-x_0)^3 + \ldots\\
        &= 0 + 2a_2 + 6 a_3 (x-x_0) + 12 a_4 (x-x_0)^2 + \ldots \\
        &= \sum_{k=2}^{\infty} k (k-1) \, a_k \, (x-x_0)^{k-2}
    \end{split}\]

    Difatti, nel caso generale avremo che la derivata di ordine $j$ di una serie di potenze corrisponda a:
    \[f^{(j)}(x) = \sum_{k=j}^{\infty} k (k-1) \ldots (k-j+1) \, a_k \, (x-x_0)^{k-j}\]

    Tuttavia, rimane ancora da analizzare quale sia l'intervallo di convergenza di tali derivate di ordine $j$. Prima di tutto, effettuiamo un cambio di variabile per ottenere una serie di potenze standard:
    \[f^{(j)}(x) = \sum_{k=j}^{\infty} k (k-1) \ldots (k-j+1) \, a_k \, (x-x_0)^{k-j} = \sum_{i=0}^{\infty} (i+j)(i+j-1) \ldots (i+1) \, a_{i+j} \, (x-x_0)^{i}\]

    Notiamo quindi che il centro della serie rimanga invariato rispetto alla serie iniziale. Per quanto riguarda il raggio di convergenza, notiamo che:
    \[\ell \text{ "=" } \lim_{i \to +\infty} \abs{\frac{(i+j+1)(i+j) \ldots (i+2) \, a_{i+j+1}}{(i+j)(i+j-1) \ldots (i+1) \, a_{i+j}}} = \lim_{i \to +\infty} \abs{\frac{a_{i+j+1}}{a_{i+j}}} = \lim_{n \to +\infty} \abs{\frac{a_{n+1}}{a_{n}}}\]
    dunque anch'esso rimane invariato, concludendo quindi che l'intervallo di convergenza della serie e della sua $j$-esima derivata siano equivalenti.

    \begin{framedprop}{Derivazione di una serie di potenze}
        Data la serie di potenze:
        \[f(x) = \sum_{k=0}^{\infty} a_k (x-x_0)^k \]
        avente intervallo di convergenza $X$, in tale intervallo la serie è \textbf{derivabile infinite volte} e la sua $j$-esima derivata equivale a:
        \[f^{(j)}(x) = \sum_{k=j}^{\infty} k (k-1) \ldots (k-j+1) \, a_k \, (x-x_0)^{k-j}\]
    \end{framedprop}

    Le derivazioni di serie di potenze ci permettono di ottenere in modo rapido le serie di potenze di alcune funzioni per cui sappiamo già l'espressione in serie della propria \textbf{anti-derivata}. Ad esempio, consideriamo la seguente serie:
    \[f(x) = \sum_{k = 0}^{+\infty} \frac{(x+2)^k}{k(k-1)}\]

    Notiamo facilmente che il suo intervallo di convergenza sia $[-3, 3]$. Calcolando la sua derivata di secondo ordine, notiamo che:
    \[f''(x) = \sum_{k = 2}^{+\infty} k(k-1)\frac{(x+2)^{k-2}}{k(k-1)} =  \sum_{k = 2}^{+\infty} (x+2)^{k-2}\]

    A questo punto, ponendo $i = k-2$, notiamo che:
    \[f''(x) = \sum_{i = 0}^{+\infty} (x+2)^{i}\]
    
    Effettuando un'ulteriore sostituzione ponendo $y = x+2$, notiamo che:
    \[f''(y-2) = \sum_{i = 0}^{+\infty} y^{i} = \frac{1}{1-y} = - \frac{1}{1+x}\]
    per valori $y \in [-3, 3]$ (ossia $x \in [-1, 1]$). Procedendo a ritroso, notiamo che per valori di $x \in [-1, 1]$ si abbia anche che:
    \[-\frac{1}{1+x} = -\sum_{k = 0}^{+\infty} (-x)^{k} = -\sum_{k = 0}^{+\infty} (-1)^{k} x^k\]

    Concludendo quindi che per $x \in [1, -1]$ si abbia che:
    \[f''(x) = - \frac{1}{1+x} = \sum_{i = 0}^{+\infty} (x+2)^{i} = -\sum_{k = 0}^{+\infty} (-1)^{k} x^k \]

    \subsection{Serie di Taylor}
    
    \begin{frameddefn}{Polinomio di Taylor}
        Siano $[a,b] \subseteq \R$, $\func{f}{[a,b]}{\R}$ e $x_0 \in (a,b)$. Definiamo il \textbf{polinomio di Taylor di ordine $n$ di $f$ centrato in $x_0$}, indicato come $T_n(f, x_0)$, come il polinomio:
        \[T_n(x;x_0) = f(x_0) + f'(x_0) (x-x_0) + \frac{f''(x_0)}{k!}(x-x_0)^2 + \ldots + \frac{f^{(n)}(x_0)}{n!} (x-x_0)^{n}\]
        dove $f^{(k)}$ indica la derivata di ordine $k$ della funzione $f$
    \end{frameddefn}

    Volendo esprimere il polinomio di Taylor in forma contratta, possiamo descriverlo tramite la seguente sommatoria:
    \[T_n(x;x_0) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^{k} \]
    
    Il polinomio di Taylor risulta fondamentale per lo studio di funzioni più complesse. Difatti, il \textbf{teorema di Taylor} afferma che, al crescere dell'ordine del polinomio di Taylor utilizzato, l'approssimazione tra la funzione $f(x)$ e il polinomio $T_n(x;x_0)$ tenda ad essere trascurabile per valori di $x$ molto vicini al centro $x_0$ (in particolare, $f(x_0) = T_n(x; x_0)(x_0)$ in quanto ogni termine ogni termine del polinomio si annullerà eccetto il primo). In altre parole, possiamo approssimare una funzione tramite un polinomio di Taylor di ordine sufficientemente alto, ma solo per valori vicini al centro.

    \begin{framedthm}{Teorema di Taylor}
        Siano $[a,b] \subseteq \R$, $\func{f}{[a,b]}{\R}$ e $x_0 \in (a,b)$. Esiste sempre una funzione $R_n(x)$ detta \textbf{resto infinitesimale} per cui si abbia che:
        \[f(x) = T_n(x;x_0) + R_n(x; x_0)\]
        e inoltre:
        \[\lim_{x \to x_0} \frac{R_n(x; x_0)}{(x-x_0)^n} = 0\]

        \textit{(dimostrazione omessa)}
    \end{framedthm}

    Esistono vari modi per rappresentare il resto infinitesimale dato dal teorema di Taylor. In particolare, in questo corso vedremo il \textbf{resto di Lagrange}
    \[R_n(x; x_0) = \frac{f^{(n+1)}(\xi)}{(n+1)!} (x-x_0)^{n+1}\]
    dove $\xi \in (x, x_0)$.

    Consideriamo ad esempio la seguente funzione $f(x) = e^{\sin x}$ e supponiamo di volerla approssimare intorno al centro $\pi$. Cerchiamo quindi di trovare una risposta tramite il polinomio di Taylor di ordine $1$ centrato in $\pi$:
    \[\begin{split}
        T_1(x; \pi) &= f(\pi) + f'(\pi) (x - \pi) \\
        &= e^{\sin \pi} + \cos(\pi) e^{\sin \pi} (x-\pi) \\
        &= 1-(x-\pi)
    \end{split} \]

    Dando una veloce occhiata al grafico del polinomio ottenuto e comparandolo con quello di $f(x)$, notiamo che l'approssimazione risulta corretta esclusivamente per il centro $\pi$.

    \begin{center}
        \includegraphics[scale=0.5]{resources/images/taylor_1.png}
    \end{center}

    Calcolando il polinomio di ordine 2, invece, notiamo come l'intorno di approssimazione sia aumentato:
    \[\begin{split}
        T_2(x; \pi) &= f(\pi) + f'(\pi) (x - \pi) + \frac{f''(\pi)}{2!}(x-\pi)^2\\
        &= e^{\sin \pi} + \cos(\pi) e^{\sin \pi} (x-\pi) + \frac{(\cos^2 \pi - \sin \pi) e^{\sin \pi}}{2}(x-\pi)^2\\
        &= 1-(x-\pi) + \frac{1}{2}(x-\pi)^2
    \end{split} \]

    \begin{center}
        \includegraphics[scale=0.55]{resources/images/taylor_2.png}
    \end{center}

    Dopo aver dato un'intuizione dietro al polinomio di Taylor, una domanda sorge spontanea: poiché l'approssimazione diminuisce al crescere del ordine, cosa succede se tale ordine tende all'infinito? La risposta risulta anch'essa intuitiva: il resto infinitesimale diventerà talmente insignificante da far coincidere la funzione con il polinomio.

    Difatti, notiamo che:
    \[\begin{split}
        \lim_{n \to +\infty} f(x) &= \lim_{n \to +\infty} T_n(x;x_0) + R_n(x; x_0) \\
        &= \lim_{n \to +\infty} \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^{k} + \frac{f^{(n+1)}(\xi)}{(n+1)!} (x-x_0)^{n+1} \\
        &= \sum_{k = 0}^{+\infty} \frac{f^{(k)}(x_0)}{k!} (x-x_0)^{k}
    \end{split}\]
    
    Ovviamente, $\lim\limits_{n \to +\infty} f(x) = f(x)$ poiché il valore $n$ non compare all'interno della funzione. Estendendo l'ordine a $+\infty$, dunque, otteniamo che la funzione coincida esattamente con una serie, la quale viene detta \textbf{serie di Taylor}. Un'occhio allenato può notare facilmente che \underline{ogni} serie di Taylor sia difatti anche una \textbf{serie di potenze}. Per tanto, tale convergenza al valore della funzione si verifica \underline{solo ed esclusivamente} all'interno del raggio di convergenza della serie stessa.

    \begin{frameddefn}{Serie di Taylor}
        Siano $[a,b] \subseteq \R$, $\func{f}{[a,b]}{\R}$ e $x_0 \in (a,b)$. Definiamo come \textbf{serie di Taylor di $f$ di centro $x_0$} il polinomio di Taylor di ordine $+\infty$.
        
        In particolare, $\forall x \in [x_0-\rho, x_0+\rho]$, dove $\rho$ è il raggio di convergenza di tale serie, si ha che:
        \[f(x) = \sum_{k = 0}^{+\infty} \frac{f^{(k)}(x_0)}{k!} (x-x_0)^{k}\]
    \end{frameddefn}

    Consideriamo ad esempio la funzione $f(x) = e^x$. Come ben noto, la derivata di ordine $k \in \N^+$ di tale funzione coincide sempre con la funzione stessa. Per tanto, la sua serie di Taylor di centro $0$ sarà data da:
    \[\begin{split}
        e^x &= \lim_{n \to +\infty} T_n(x; 0) \\
        &= \lim_{n \to +\infty} \sum_{k = 0}^n \frac{f^{(k)}(0)}{k!} (x-0)^{k} \\
        &= \lim_{n \to +\infty} \sum_{k = 0}^n \frac{e^{0}}{k!} x^{k} \\
        &= \sum_{k = 0}^{+\infty} \frac{x^{k}}{k!} \\
    \end{split}\]

    A questo punto, non ci resta che calcolare l'intervallo di convergenza di tale serie, la quale sappiamo avere centro in $0$:
    \[\lim_{n \to +\infty} \abs{\frac{\frac{1}{(n+1)!}}{\frac{1}{n!}}} = \lim_{n \to +\infty} \abs{n} = +\infty\]
    dunque $\forall x \in \R$ tale serie converge a $e^x$

    \begin{framedprop}{Serie di Taylor di $e^x$}
        Per ogni $x \in \R$ si ha che:
        \[e^x =  \sum_{k=0}^{\infty} \frac{x^k}{k!}\]
    \end{framedprop}

    A questo punto, è opportuno sottolineare che \underline{ogni} serie di Taylor possa essere combinata con i \textbf{cambi di variabile}. Ad esempio, possiamo facilmente ottenere la serie di Taylor della funzione $e^{x^2}$ ponendo $y = x^2$:
    \[e^{x^2} = e^y = \sum_{k=0}^{\infty} \frac{y^k}{k!} = \sum_{k=0}^{\infty} \frac{(x^2)^k}{k!} = \sum_{k=0}^{\infty} \frac{x^{2k}}{k!}\]
                
    Ovviamente, tale procedimento risulta possibile \underline{solo ed esclusivamente} se l'\textbf{intervallo di convergenza} permette ciò. Ad esempio, per la funzione $e^x$ sappiamo che il suo raggio $\R$, dunque risulta possibile applicare tale procedimento per ogni funzione presente all'esponente.

    Vediamo ora il calcolo di un'altra serie nota.
    
    \begin{framedprop}{Serie di Taylor di $\sin x$}
        Per ogni $x \in \R$ si ha che:
        \[ \sin x =  \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{(2k+1)!}\]
    \end{framedprop}

    \proofenv{
        Data $f(x) := \sin x$, notiamo che:
        \begin{itemize}
            \item $f^{(0)}(0) = \sin(0) = 0$
            \item $f^{(1)} = \cos(0) = 1$
            \item $f^{(2)}(0) = -\sin(0) = 0$
            \item $f^{(3)}(0) = -\cos(0) = -1$
            \item $f^{(4)}(0) = \sin(0) = 0$
        \end{itemize}
        In generale, difatti, abbiamo che:
        \[f^{(j)}(0) = \soe{ll}{
            0 & \text{se } j = 4k \\
            1 & \text{se } j = 4k+1 \\
            0 & \text{se } j = 4k+2 \\
            -1 & \text{se } j = 4k+3 \\
        }\]

        Calcoliamo la serie di Taylor di $\sin x$ con centro $x_0 = 0$, spezzando i suoi indici seguendo il pattern trovato precedentemente. In particolare, tutti gli indici la cui derivata in 0 equivale a 0 verranno eliminati, ossia gli indici $4k$ e $4k+2$:
        \[\begin{split}
            T(x; 0)& =  \sum_{k = 0}^{+\infty} \frac{f^{(k)}(0)}{k!} x^{k} \\
            &= \sum_{k = 0}^{+\infty} \frac{f^{(4k)}(0)}{4k!} x^{4k} + \sum_{k = 0}^{+\infty} \frac{f^{(4k+1)}(0)}{(4k+1)!} x^{4k+1} \\
            &+\sum_{k = 0}^{+\infty} \frac{f^{(4k+2)}(0)}{(4k+2)!} x^{4k+2} + \sum_{k = 0}^{+\infty} \frac{f^{(4k+3)}(0)}{(4k+3)!} x^{4k+3} \\
            &= \sum_{k = 0}^{+\infty} \frac{1}{(4k+1)!} x^{4k+1} + \sum_{k = 0}^{+\infty} \frac{(-1)}{(4k+3)!} x^{4k+3} \\
        \end{split}\]

        A questo punto, effettuiamo un cambio di variabile per l'indice ponendo $i = 2k$
        \[T(x; 0) = \sum_{i = 0}^{+\infty} \frac{1}{(2i+1)!} x^{2i+1} + \sum_{k = 0}^{+\infty} \frac{(-1)}{(2i+3)!} x^{2i+3}\]

        Poiché $2i+3 = 2(i+1) +1$ e poiché $i \to +\infty$ in tale sommatorie, abbiamo che:
        \[\begin{split}
            T(x; 0) &= \sum_{i = 0}^{+\infty} \frac{1}{(2i+1)!} x^{2i+1} + \sum_{i = 0}^{+\infty} \frac{(-1)}{(2i+1)!} x^{2i+1} \\
            & = \sum_{i = 0}^{+\infty} \frac{(-1)^i}{(2i+1)!} x^{2i+1}
        \end{split}\]
        
        Calcoliamo quindi il raggio di convergenza di tale serie. Ovviamente, non possiamo calcolare il raggio tramite la serie semplificata che abbiamo ottenuto, bensì è opportuno calcolarlo tramite la serie originale.
        \[\ell \text{ "=" } \lim_{n \to +\infty} \abs{\frac{\frac{f^{(n+1)}(0)}{(n+1)!}}{\frac{f^{(n)}(0)}{n!}}} = \lim_{n \to +\infty} \abs{\frac{f^{(n+1)}(0)}{f^{(n)}(0) (n+1)}} = 0\]
        dunque $\rho \text{ "=" } \frac{1}{\ell} = +\infty$, concludendo che $\forall x \in \R$ la serie converge a $\sin x$ 

    }

    \subsection{Espansioni di Taylor notevoli e non}

    Di seguito vengono elencate le espansioni di Taylor delle funzioni più comuni assieme al loro intervallo di convergenza:
    \begin{itemize}
        \item Serie di Taylor di $f(x) = e^x$, avente intervallo di convergenza pari a $\R$:
        \[ e^x =  \sum_{k=0}^{\infty} \frac{x^k}{k!}\]
        
        \item Serie di Taylor di $f(x) = \cos x$, avente intervallo di convergenza pari a $\R$:
        \[ \cos(x) =  \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k}}{(2k)!}\]
        
        \item Serie di Taylor di $f(x) = \sin x$, avente intervallo di convergenza pari a $\R$:
        \[ \sin(x) =  \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{(2k+1)!}\]
        
        \item Serie di Taylor di $f(x) = \frac{1}{1-x}$, avente intervallo di convergenza pari a $(-1, 1)$ \\\textit{(già vista nell'ambito delle serie geometriche)}
        \[ \frac{1}{1-x} =  \sum_{k=0}^{\infty} x^k\]
                
        \item Serie di Taylor di $f(x) = ln(1+x)$, avente intervallo di convergenza pari a $(-1, 1)$
        \[ ln(1+x) =  \sum_{k=0}^{\infty} \frac{(-1)^k x^{k+1}}{k+1}\]
        
        \item Serie di Taylor di $f(x) = \arctan(x)$, avente intervallo di convergenza pari a $(-1, 1)$
        \[\arctan(x) =  \sum_{k=0}^{\infty} \frac{(-1)^k x^{2k+1}}{2k+1}\]
    \end{itemize}

    Conoscere tali serie risulta fondamentale per poter ottenere serie di Taylor notevoli che possano esprimere \textbf{altre funzioni}. Ad esempio, consideriamo la funzione $f(x) = x e^x$. Tramite l'espansione notevole di $e^x$, otteniamo facilmente che:
    \[x e^x = x \sum_{k=0}^{\infty} \frac{x^k}{k!} = \sum_{k=0}^{\infty} \frac{x^{k+1}}{k!}\]

    Inoltre, è opportuno ricordare che le serie di Taylor siamo anche delle serie di potenze. Per tanto, possiamo sfruttare i cambi di variabile (se permesso dall'intervallo di convergenza) per ottenere le espansioni di funzioni composte:
    \[\cos(x^2) = \cos y = \sum_{k=0}^{\infty} \frac{(-1)^k y^{2k}}{(2k)!} = \sum_{k=0}^{\infty} \frac{(-1)^k (x^2)^{2k}}{(2k)!} = \sum_{k=0}^{\infty} \frac{(-1)^k x^{4k}}{(2k)!}\]
            
    \subsection{Numeri Complessi e Formula di Eulero}
    
    Come da matematica elementare, sappiamo che \textbf{la radice} di esponente pari di un \textbf{numero negativo non esiste}. Ad esempio, non esiste alcun numero equivalente a $\sqrt{-1}$, poiché nessun numero elevato al quadrato può dare come risultato $-1$:
    \begin{itemize}
        \item Ipotizzando che tale numero esista e che sia un numero positivo, allora il prodotto di tale numero con se stesso dovrebbe dare vita ad un numero positivo, poiché avremmo un prodotto tra due numeri positivi, mentre $-1$ è negativo
        \item Ipotizzando che tale numero esista e che sia un numero negativo, allora il prodotto di tale numero con se stesso dovrebbe dare vita ad un numero positivo, poiché avremmo un prodotto tra due numeri negativi, mentre $-1$ è negativo.
    \end{itemize}
        
    Tuttavia, possiamo effettuare un "salto della fede" e dare per valida l'esistenza di tale numero. Come abbiamo visto, tale numero non può essere un numero reale e per tale motivo viene definito col termine di \textbf{numero "immaginario"}, venendo indicato con l'\textbf{unità immaginaria $i$}, dove $i^2 = -1$. Definiamo quindi il seguente insieme numerico che estende i numeri reali, detto \textbf{insieme dei numeri complessi}.

    \begin{frameddefn}{Insieme dei numeri complessi}
        L'\textbf{insieme dei numeri complesso} è definito come:
        \[\C = \{a+ib \mid a,b \in \R\}\]
        dove $i^2 = -1$
    \end{frameddefn}

    Graficamente, i numeri complessi possono essere interpretati tramite un piano bidimensionale dotato di un'\textbf{asse reale} e un'\textbf{asse immaginaria}:
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_1/complex_numbers.jpg}
    \end{center}
        
    L'utilizzo di tali numeri apre molte porte all'interno della matematica, ad esempio la \textbf{fattorizzazione} di alcuni numeri primi che, per loro definizione stessa, normalmente non sarebbero fattorizzabili:
    \[ (2+i)(2-i) = 4-2i+2i-i^2 = 4-(-1) = 5\]
    
    Un'ulteriore modo per visualizzare i numeri complessi è dato dalla \textbf{formula di Eulero}, la quale ci permette di rappresentarli come \textbf{rotazioni} di un piano. Tale formula può facilmente essere dimostrata estendendo il concetto di serie di Taylor anche ai numeri complessi.
        
    \begin{framedthm}{Formula di Eulero}
        $\forall \theta \in \mathbb{R}$ vale la seguente equivalenza:
        \[ e^{i \theta} = \cos(\theta)+ i \cdot \sin(\theta)\]
    \end{framedthm}

    \proofenv{
        Estendendo ai numeri complessi la serie della funzione $e^{x}$, otteniamo che:
        \[e^{i \theta} = \sum_{k=0}^{\infty} \frac{(i\theta)^k}{k!} = \sum_{k=0}^{\infty} \frac{i^k \theta^k}{k!}\]
        
        A questo punto, osserviamo il comportamento di $i^k$ al crescere di $k$:
        \begin{center}
            \begin{tabular}{c | c | c | c | c | c | c | c | c}
            $k$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 & ...\\
            \hline
            $i^k$ & $1$ & $i$ & $-1$ & $-i$ & $1$ & $i$ & $-1$ & ...\\
            \end{tabular}
        \end{center}
        
        dunque, affermiamo che:
        $$
        i^k = \left \{ \begin{array}{l l}
            (-1)^h & \text{se } k = 2h \\
            (-1)^h \cdot i & \text{se } k = 2h +1
        \end{array} \right .
        $$

        A questo punto, separiamo i termini pari ed i termini dispari della sommatoria:
        \[ \begin{split}
            e^{i \theta} &= \sum_{k=0}^{\infty} \frac{i^k \theta^k}{k!} = \sum_{h=0}^{\infty} \frac{i^{2h} \theta^{2h}}{(2h)!} + \sum_{h=0}^{\infty} \frac{i^{2h+1} \theta^{2h+1}}{(2h+1)!}\\
            &= \sum_{h=0}^{\infty} \frac{(-1)^h \theta^{2h}}{(2h)!} + \sum_{h=0}^{\infty} \frac{(-1)^h \cdot i \cdot  \theta^{2h+1}}{(2h+1)!} \\
            &= \cos(\theta) + i \cdot \sin(\theta)
        \end{split}\]
    }

    \subsection{Esercizi svolti}

    \begin{framedprob}{}
        Determinare l'intervallo di convergenza delle seguenti serie di potenze:
        \begin{enumerate}
            \item $\sum\limits_{k = 0} \dfrac{(x-3)^k}{k^2-1}$
            \item $\sum\limits_{k = 0} \dfrac{(6-x)^k}{k!}$
            \item $\sum\limits_{k = 0} \dfrac{(2x+7)^k}{4^k}$
        \end{enumerate}
    \end{framedprob}

    \textit{Soluzione:}
    \begin{enumerate}
        \item Prima di tutto, normalizziamo la serie riscrivendola in forma standard:
        \[\sum\limits_{k = 0} \frac{(x-3)^k}{k^2-1} = \sum\limits_{k = 0} \frac{1}{k^2-1}(x-3)^k\]
        ottenendo che il centro sia $x_0= 3$. Calcoliamo quindi il suo raggio:
        \[\ell \text{ "=" }\lim_{n \to +\infty} \abs{\frac{\frac{1}{(n+1)^2-1}}{\frac{1}{n^2-1}}} = 1\]
        ottenendo quindi che il raggio sia $\rho \text{ "=" } \frac{1}{\ell} = 1$ e dunque che la serie converga per $x \in (2, 4)$.

        Analizziamo quindi i due estremi dell'intervallo:
        \begin{itemize}
            \item Per $x = 2$ abbiamo che:
            \[\sum\limits_{k = 0} \frac{(x-3)^k}{k^2-1} = \sum\limits_{k = 0} \frac{(-1)^k}{k^2-1}\]
            la quale converge assolutamente in quanto la sua versione assoluta sia asintoticamente simile ad una serie armonica con $\alpha > 1$:
            \[\sum\limits_{k = 0} \abs{\frac{1}{k^2-1}(-1)^k} = \sum\limits_{k = 0} \abs{\frac{1}{k^2-1}} \sim \sum\limits_{k = 0} \frac{1}{k^2}\]
            concludendo quindi che anche per $x = 2$ la serie converga
            
            \item Per $x = 4$ abbiamo che:
            \[\sum\limits_{k = 0} \frac{(x-3)^k}{k^2-1} = \sum\limits_{k = 0} \frac{1}{k^2-1}\]
            la quale converge in quanto asintoticamente simile ad una serie armonica con $\alpha > 1$:
            \[\sum\limits_{k = 0} \frac{1}{k^2-1}(-1)^k \sim \sum\limits_{k = 0} \frac{1}{k^2}\]
            concludendo quindi che anche per $x = 4$ la serie converga
        \end{itemize}

        Per tanto, l'intervallo di convergenza della serie risulta essere $[2, 4]$. 

        \item Prima di tutto, normalizziamo la serie riscrivendola in forma standard:
        \[\sum\limits_{k = 0} \dfrac{(6-x)^k}{k!} = \sum\limits_{k = 0} \frac{(-1)^k}{k!}(x-6)^k\]
        ottenendo che il centro sia $x_0= 6$. Calcoliamo quindi il suo raggio:
        \[\ell \text{ "=" }\lim_{n \to +\infty} \abs{\frac{\frac{(-1)^{n+1}}{(n+1)!}}{\frac{(-1)^n}{n!}}} = 0\]
        ottenendo quindi che il raggio sia $\rho \text{ "=" } \frac{1}{\ell} = +\infty$ e dunque che la serie converga per $x \in \R$. In alternativa, un occhio allenato avrebbe potuto notare che la serie analizzata sia in realtà la serie di $e^{6-x}$, concludendo immediatamente che l'intervallo di convergenza sia $\R$.

        \item Prima di tutto, normalizziamo la serie riscrivendola in forma standard:
        \[\sum\limits_{k = 0} \dfrac{(2x+7)^k}{4^k} = \sum\limits_{k = 0} \dfrac{2^k}{4^k}(x+\frac{7}{2})^k = \sum\limits_{k = 0} \dfrac{1}{2^k}(x+\frac{7}{2})^k\]
        ottenendo che il centro sia $x_0= -\frac{7}{2}$. Calcoliamo quindi il suo raggio:
        \[\ell \text{ "=" }\lim_{n \to +\infty} \abs{\frac{\frac{1}{2^{n+1}}}{\frac{1}{2^n}}} = \frac{1}{2}\]
        ottenendo quindi che il raggio sia $\rho \text{ "=" } \frac{1}{\ell} = 2$ e dunque che la serie converga per $x \in \rbk{-\frac{11}{2}, -\frac{3}{2}}$. 

        Analizziamo quindi i due estremi dell'intervallo:
        \begin{itemize}
            \item Per $x = -\frac{11}{2}$ abbiamo che:
            \[\sum\limits_{k = 0} \dfrac{(2x+7)^k}{4^k} = \sum\limits_{k = 0} \dfrac{(-4)^k}{4^k} = \sum\limits_{k = 0} (-1)^k\]
            la quale sappiamo nè convergere nè divergere, concludendo quindi per $x = -\frac{11}{2}$ la serie non converga

            \item Per $x = -\frac{11}{2}$ abbiamo che:
            \[\sum\limits_{k = 0} \dfrac{(2x+7)^k}{4^k} = \sum\limits_{k = 0} \dfrac{4^k}{4^k} = \sum\limits_{k = 0} 1\]
            la quale sappiamo divergere, concludendo quindi per $x = -\frac{11}{2}$ la serie non converga
        
        \end{itemize}

        Per tanto, l'intervallo di convergenza della serie risulta essere $(2, 4)$. In alternativa, un occhio allenato avrebbe potuto notare che la serie analizzata sia in realtà una serie geometrica, concludendo immediatamente che l'intervallo di convergenza sia dato da:
        \[-1 < \frac{2x+7}{4} < 1 \implies -\frac{11}{2} < x < -\frac{3}{2}\]
    \end{enumerate}

    \begin{framedprob}{}
        Calcolare la serie di Taylor delle seguenti funzioni:
        \begin{enumerate}
            \item $f(x) = \sin \frac{1}{x}$
            \item $g(x) = x^5e^{x^2}$
            \item $h(x) = \log_2 x$
        \end{enumerate}
    \end{framedprob}

    \textit{Soluzione:}

    \begin{enumerate}
        \item Ponendo $y = \frac{1}{x}$, notiamo che:
        \[f(x) = \sin \frac{1}{x} = \sin y = \sum_{k = 0}^{+\infty} \frac{(-1)^k y^{2k+1}}{(2k+1)!} = \sum_{k = 0}^{+\infty} \frac{(-1)^k}{x^{2k+1}(2k+1)!} \]

        \item Ponendo $y = x^2$, abbiamo che:
        \[g(x) = x^5 e^{x^2} = x^5 e^y = x^5 \sum_{k = 0}^{+\infty} \frac{y^k}{k!} = \sum_{k = 0}^{+\infty} \frac{x^{2k+5}}{k!}\]

        \item Tramite le proprietà dei logaritmi sappiamo che:
        \[h(x) = \log_2 x = \frac{\ln x}{\ln 2}\]

        Ponendo $1+y = x$, abbiamo che:
        \[h(x) = \log_2 x = \frac{\ln x}{\ln 2} = \frac{\ln (1+y)}{\ln 2} = \frac{1}{\ln 2}\sum_{k = 0}^{+\infty} \frac{(-1)^k y^{2k+1}}{k+1} = \frac{1}{\ln 2}\sum_{k = 0}^{+\infty} \frac{(-1)^k (x-1)^{2k+1}}{k+1}\]
    \end{enumerate}

    \chapter{Integrali}
    
    \section{Definizione}
    
    Immaginiamo di trovarci nella seguente situazione: vogliamo calcolare l'\textbf{area della figura sottostante alla funzione $f(x)$ nell'intervallo $[a, b]$}
    
    \begin{center}
        \includegraphics[scale=0.575]{resources/images/chapter_2/integral_area.png}
    \end{center}
    
    Notiamo però che tale figura \textbf{non corrisponde ad un poligono,} ne tanto meno ad una figura geometricamente nota. Come possiamo dunque calcolarne l'area senza poter applicare una formula precisa? Potremmo pensare di \textbf{scomporre la figura in dei poligoni} di cui possiamo effettivamente calcolare l'area, per poi \textbf{sommare tutte le aree calcolate} ed ottenere l'area della figura. Proviamo quindi a scomporre l'area in \textbf{due triangoli}.
    
    \begin{center}
        \includegraphics[scale=0.725]{resources/images/chapter_2/integral_area2.png}
    \end{center}
    
    Essendo il lato superiore della figura \textbf{curvilineo}, non siamo in grado di trovare una \textbf{scomposizione perfetta} della figura che possa coprire l'area originale nel suo totale.
    
    L'unica via, quindi, è stimare l'area della figura tramite una \textbf{serie di approssimazioni}: se scomponessimo l'area in una \textbf{quantità elevata di triangolini} potremmo arrivare a lasciare scoperta una minuscolissima parte della figura, rendendo la somma delle aree \textbf{estremamente vicina all'area effettiva} della figura.
    
    \begin{center}
        \includegraphics[scale=0.575]{resources/images/chapter_2/integral_area3.png}
    \end{center}
    
    Rimane tuttavia un problema fondamentale, ossia il \textit{come calcolare} tali aree dei triangoli. Notiamo come non è presente un \textbf{rigore matematico} in tale approccio, poiché ogni triangolo è estremamente diverso dall'altro, risultando anche nella presenza di alcuni rettangoli non retti. Stimare l'area della figura utilizzando dei triangoli, quindi, risulta estremamente \textbf{inefficiente} e di \textbf{difficoltà pari} (se non superiore) \textbf{al problema originale}.
    
    Proviamo un nuovo approccio: proviamo a scomporre la figura in una \textbf{serie di rettangoli}, ossia la figura geometrica la cui area è la \textbf{più semplice da calcolare}. Cerchiamo inoltre di seguire un approccio \textbf{più rigoroso} rispetto alla stima precedente.
    
    \quad
    
    Aggiungiamo al piano la seguente retta $r(x) = M$.
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/integral_area4.png}
    \end{center}
    
    Consideriamo quindi il rettangolo di base $(b-a)$ e di altezza $M$. Tale rettangolo ha sicuramente un'area \textbf{maggiore} rispetto all'area che stiamo cercando. Consideriamo inoltre il rettangolo di base $(b-a)$ e di altezza $0$. Tale rettangolo ha sicuramente un'area \textbf{minore} dell'area che stiamo cercando. Possiamo quindi definire la seguente disequazione:
    
    \[ 0 \cdot  (b-a) \leq A \leq M \cdot (b-a)\]
    
    dove $A$ è l'\textbf{area della figura}.
    
    \newpage
    
    Attualmente, la stima dell'area della figura risulta estremamente incorretta. Proviamo quindi ad affinare la nostra stima.
    
    Aggiungiamo al piano una seconda retta $r(x) = m$, dove $m = min(y_a, y_b)$. Inoltre, "aggiorniamo" la retta precedente, ponendo $M = max(y_a, y_b)$.
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/integral_area5.png}
    \end{center}
    
    Analogamente a prima, stimiamo il valore di A confrontandolo con un'\textbf{area maggiore} (ossia $M \cdot (b-a)$) e con un'\textbf{area minore} (ossia $m \cdot (b-a)$):
    \[ m \cdot (b-a) \leq A \leq M \cdot (b-a)\]
    
    Notiamo però come la stima risulti ancora troppo imprecisa. Decidiamo quindi di \textbf{scomporre il problema in due figure}, raddoppiando il numero di rettangoli.
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/integral_area6.png}
    \end{center}
    
    Una volta scomposto in due figure il problema, notiamo come possiamo individuare un \textbf{valore $M$} ed un \textbf{valore $m$} per ognuna delle \textbf{due figure}, corrispondenti ai \textbf{punti di massimo e di minimo dei due intervalli} definiti dalla scomposizione, ossia $[a, x_1]$ e $[x_1, b]$, dove $x_1$ corrisponde al \textbf{punto medio tra $a$ e $b$}.
    
    \begin{itemize}
        \item $x_1 = \frac{b-a}{2}$
        \item $M_0 = max_{[a, x_1]} f(x)$
        \item $m_0 = min_{[a, x_1]} f(x)$
        \item $M_1 = max_{[x_1, b]} f(x)$
        \item $m_1 = min_{[x_1, b]} f(x)$
    \end{itemize}
    
    A questo punto, riscriviamo nuovamente la stima di $A$, utilizzando la somma dei rettangoli minori e la somma dei rettangoli maggiori.
    \[ m_0 \cdot (b-a) + m_1 \cdot (b-a) \leq A \leq M_0 \cdot (b-a) + M_1 \cdot (b-a) \]
    
    A questo punto, possiamo fare alcuni accorgimenti:
    
    \begin{itemize}
        \item Le basi dei quattro rettangoli equivalgono tutte a $\frac{b-a}{2}$, poiché non abbiamo fatto altro che dividere l'intervallo $[a, b]$ in due
        \item Per via della proprietà distributiva, possiamo riscrivere ognuna delle due somme come il prodotto tra la \textbf{somma delle altezze dei rettangoli} e la \textbf{base}
    \end{itemize}
    \[ m_0 \cdot \frac{b-a}{2} + m_1 \cdot \frac{b-a}{2} \leq A \leq M_0 \cdot \frac{b-a}{2} + M_1 \cdot \frac{b-a}{2}\]
    \[\frac{b-a}{2}(m_0 + m_1) \leq A \leq \frac{b-a}{2}(M_0 + M_1)\]
    
    L'approssimazione risulta quindi \textbf{più accurata} rispetto alla precedente. Procediamo quindi sulla stessa riga, questa volta \textbf{dividendo l'intervallo in quattro}.
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/integral_area7.png}
    \end{center}
    
    A questo punto, è facile rendersi conto che questa volta la stima corrisponderà a
    
    \[\frac{b-a}{4}(m_0 + m_1 + m_2 + m_3) \leq A \leq \frac{b-a}{4}(M_0 + M_1 + M_2 + M_3)\]
    
    che possiamo riscrivere come
    
    \[ \frac{b-a}{2^2} \cdot \sum_{k=0}^{2^2-1} m_k \leq A \leq \frac{b-a}{2^2} \cdot \sum_{k=0}^{2^2-1} M_k\]
    
    ricordando che $M_k$ e $m_k$ corrispondono rispettivamente al \textbf{massimo e al minimo dell'k-esimo intervallo}.
    
    Possiamo quindi definire la seguente \textbf{forma generalizzata}:
    
    \[ \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} min_{[x_k, x_{k+1}]} f(x) \leq A \leq \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} max_{[x_k, x_{k+1}]} f(x)\]
    
    dove $n$ corrisponde al \textbf{numero di suddivisioni} e ogni $x_k$ corrisponde a
    \[ x_k = a + k \cdot \frac{b-a}{2}\]
    
    Facciamo ora alcune osservazioni: dando un rapido sguardo ai grafici delle ultime tre approssimazioni, notiamo come all'aumentare del numero degli intervalli la \textbf{somma dei rettangoli maggiori}, che d'ora in poi chiameremo $\overline{S_n}$, vada man mano a \textbf{diminuire}, mentre la \textbf{somma dei rettangoli minori}, che d'ora in poi chiameremo $\underline{S_n}$, vada man mano ad \textbf{aumentare}.
    \[ \underline{S_n} \leq \underline{S_{n+1}} \leq ... \leq A \leq ... \leq \overline{S_{n+1}} \leq \overline{S_n}\]
    
    Immaginiamo ora di suddividere la figura un numero infinito di volte. Intuitivamente, riusciamo a concludere che l'\textbf{errore nella stima} si riduca ad un valore \textbf{infinitesimale}, così come la \textbf{differenza tra $\underline{S_n}$ e $\overline{S_n}$}.
    \[ \lim_{n \to +\infty} \underline{S_n} = \underline{S} = A = \overline{S} \lim_{n \to +\infty} \overline{S_n}\]
    
    Dunque, possiamo concludere che effettuando il limite per $n \to +\infty$, le somme $\underline{S_n}$ e $\overline{S_n}$ \textbf{convergano al valore dell'area $A$}. Se $f(x)$ è una funzione su cui può essere applicato tale concetto, allora si dice che $f$ è \textbf{integrabile secondo Riemann}.
    
    \begin{frameddefn}{Integrazione secondo Riemann}
        Sia $f: [a,b] \to \mathbb{R}$. Si dice che $f$ è \textbf{integrabile secondo Riemann} se dati
        
        \begin{itemize}
            \item Il limite per $n \to +\infty$ della somma $\underline{S_n}$
        
            \[ \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} min_{[x_k, x_{k+1}]} f(x) = \underline{S} \]
        
            \item Il limite per $n \to +\infty$ della somma $\overline{S_n}$
            
            \[ \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} max_{[x_k, x_{k+1}]} f(x) = \overline{S}\]
            
        \end{itemize}
        
        si verifica che
        \[\underline{S} = \overline{S}\]
        
        L'\textbf{integrale definito nell'intervallo $[a,b]$ di $f(x)$} viene denominato con
        
        \[ \int_{a}^{b} f(x) \; dx\]
    \end{frameddefn}
    
    \newpage

    \textbf{Esempi}
    
    Vediamo ora alcuni esempi di calcolo di un integrale utilizzando la sua definizione geometrica:
    
    \begin{enumerate}
        \item Si calcoli l'integrale definito in $[0, 1]$ di $f(x) = 2$
        
        \[ \int_{0}^{1} 2 \; dx\]
        
        \begin{itemize}
            \item Prima di tutto, è necessario individuare il massimo e il minimo di ogni intervallo in cui andremo a suddividere la figura. Ovviamente, trattandosi di una funzione costante, il \textbf{massimo} e il \textbf{minimo} avranno sempre valore 2 indipendentemente dall'intervallo.
            
            \[min_{[x_k, x_{k+1}]} f(x) = 2\]
            \[max_{[x_k, x_{k+1}]} f(x) = 2\]
            
            \item Calcoliamo ora i valori di $\overline{S}$ e $\underline{S}$
            
            \[ \underline{S} = \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} min_{[x_k, x_{k+1}]} f(x) = \lim_{n \to +\infty} \frac{1-0}{2^n} \cdot \sum_{k=0}^{2^n-1} 2 =\]
            \[ = \lim_{n \to +\infty} \frac{1}{2^n} \cdot 2(2^n-1) = \lim_{n \to +\infty} \frac{2^+1 - 2}{2^n} = 2\]
            
            \[ \overline{S} = \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} max_{[x_k, x_{k+1}]} f(x) = \lim_{n \to +\infty} \frac{1-0}{2^n} \cdot \sum_{k=0}^{2^n-1} 2 =\]
            \[ = \lim_{n \to +\infty} \frac{1}{2^n} \cdot 2(2^n-1) = \lim_{n \to +\infty} \frac{2^+1 - 2}{2^n} = 2\]
            
            \item Concludiamo quindi che 
            
            \[ \int_{0}^{1} 2 \; dx = 2\]
            
            \item Ovviamente, tale calcolo risulta corretto, poiché l'area richiesta corrisponde esattamente ad un rettangolo di base pari ad 1 ed altezza pari a 2. 
        \end{itemize}
        
        \newpage
        
        \item Si calcoli l'integrale definito in $[0, 1]$ di $f(x) = x$
        
        \[ \int_{0}^{1} x \; dx\]
        
        \begin{itemize}
            \item Poiché la funzione $f(x) = x$ è una funzione \textbf{monotona crescente}, il minimo dell'intervallo $[x_k, x_{k+1}]$ corrisponde sempre al valore assunto dalla funzione nel suo estremo sinistro, ossia $f(x_k)$, mentre il massimo dell'intervallo corrisponderà sempre al valore assunto nel suo estremo destro, ossia $f(x_{k+1})$. 
            
            \[min_{[x_k, x_{k+1}]} f(x) = f(x_k) = a+k \frac{b-a}{2^n} = \frac{k}{2^n}\]
            \[max_{[x_k, x_{k+1}]} f(x) = f(x_{k+1}) = a+(k+1) \frac{b-a}{2^n} = \frac{k+1}{2^n}\]
            
            \item Calcoliamo ora i valori di $\overline{S}$ e $\underline{S}$
            
            \[ \underline{S} = \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} min_{[x_k, x_{k+1}]} f(x) = \lim_{n \to +\infty} \frac{1-0}{2^n} \cdot \sum_{k=0}^{2^n-1} \frac{k}{2^n} =\]
            \[\lim_{n \to +\infty} \frac{1}{2^n} \cdot \frac{1}{2^n} \cdot \sum_{k=0}^{2^n-1} k = \lim_{n \to +\infty} \frac{1}{2^{2n}} \cdot \frac{(2^n-1) \cdot 2^n}{2} = \lim_{n \to +\infty} \frac{2^{2n}-2^n}{2^{2n+1}} = \frac{1}{2}\]
            
            \[ \overline{S} = \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} max_{[x_k, x_{k+1}]} f(x) = \lim_{n \to +\infty} \frac{1-0}{2^n} \cdot \sum_{k=0}^{2^n-1} \frac{k+1}{2^n} =\]
            \[\lim_{n \to +\infty} \frac{1}{2^n} \cdot \frac{1}{2^n} \cdot \sum_{k=0}^{2^n-1} (k+1) = \lim_{n \to +\infty} \frac{1}{2^{2n}} \cdot \frac{2^n \cdot (2^n+1)}{2} = \lim_{n \to +\infty} \frac{2^{2n}+2^n}{2^{2n+1}} = \frac{1}{2}\]
            
            \item Concludiamo quindi che 
            
            \[ \int_{0}^{1} x \; dx = \frac{1}{2}\]
            
            \item Ovviamente, tale calcolo risulta corretto, poiché l'area richiesta corrisponde esattamente ad un triangolo di base pari ad 1 ed altezza pari a 1.
        \end{itemize}
        
        \quad
        
        \textit{Nota: all'interno dei calcoli è stato omesso il calcolo della sommatoria poiché è stata usata seguente la sommatoria notevole}
        
        \[ \sum_{k=0}^{n} k = \frac{n \cdot (n+1)}{2}\]
        \newpage
        
        \item Si calcoli l'integrale definito in $[0, 2]$ di $f(x) = x^2$
        
        \[ \int_{0}^{2} x^2 \; dx\]
        
        \begin{itemize}
            \item Poiché la funzione $f(x) = x^2$ è una funzione \textbf{monotona crescente}, il minimo dell'intervallo $[x_k, x_{k+1}]$ corrisponde sempre al valore assunto dalla funzione nel suo estremo sinistro, ossia $f(x_k)$, mentre il massimo dell'intervallo corrisponderà sempre al valore assunto nel suo estremo destro, ossia $f(x_{k+1})$. 
            
            \[min_{[x_k, x_{k+1}]} f(x) = f(x_k) = \left ( a+k \frac{b-a}{2^n} \right)^2 = \frac{4k^2}{2^{2n}}\]
            \[min_{[x_k, x_{k+1}]} f(x) = f(x_{k+1}) = \left ( a+(k+1) \frac{b-a}{2^n} \right)^2 = \frac{4(k+1)^2}{2^{2n}}\]
            
            \item Calcoliamo ora i valori di $\overline{S}$ e $\underline{S}$
            
            \[ \underline{S} = \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} min_{[x_k, x_{k+1}]} f(x) = \lim_{n \to +\infty} \frac{2}{2^n} \cdot \sum_{k=0}^{2^n-1} \frac{4k^2}{2^{2n}} = \]
            \[\lim_{n \to +\infty} \frac{8}{2^{3n}} \cdot \sum_{k=0}^{2^n-1} k^2 = \lim_{n \to +\infty} \frac{8}{2^{3n}} \cdot \frac{1}{3} \cdot 2^{n-1}(2^n-1)(2^{n+1}-1) =  \]
            \[\lim_{n \to +\infty} \frac{8}{3} \cdot \frac{2^{3n}-2^{2n-1}-2^{2n}+2^{n-1}}{2^{3n}} = \lim_{n \to +\infty} \frac{8}{3} \cdot \frac{2^{3n}}{2^{3n}} = \frac{8}{3}\]
            
            \[ \overline{S} = \lim_{n \to +\infty} \frac{b-a}{2^n} \cdot \sum_{k=0}^{2^n-1} max_{[x_k, x_{k+1}]} f(x) = \lim_{n \to +\infty} \frac{2}{2^n} \cdot \sum_{k=0}^{2^n-1} \frac{4(k+1)^2}{2^{2n}} = \]
            \[\lim_{n \to +\infty} \frac{8}{2^{3n}} \cdot \sum_{k=0}^{2^n-1} (k+1)^2 = \lim_{n \to +\infty} \frac{8}{2^{3n}} \cdot \frac{1}{3} \cdot 2^{n-1}(2^n+1)(2^{n+1}+1) =  \]
            \[\lim_{n \to +\infty} \frac{8}{3} \cdot \frac{2^{3n}+2^{2n-1}+2^{2n}+2^{n-1}}{2^{3n}} = \lim_{n \to +\infty} \frac{8}{3} \cdot \frac{2^{3n}}{2^{3n}} = \frac{8}{3}\]
            
            \item Concludiamo quindi che 
            
            \[ \int_{0}^{2} x^2 \; dx = \frac{8}{3}\]
            
        \end{itemize}
        
        \quad
        
        \textit{Nota: all'interno dei calcoli è stato omesso il calcolo della sommatoria poiché è stata usata seguente la sommatoria notevole}
            
            \[ \sum_{k=0}^{n} k^2 = \frac{1}{6} \cdot n (n+1)(2n+1)\]
    \end{enumerate}
    
    \section{Proprietà delle funzioni integrabili}
    
    Una volta appreso il concetto di integrale, resta da chiedersi quali siano le \textbf{tipologie di funzioni integrabili} e le \textbf{proprietà} che esse possiedono.
    
    Dopo aver visto la definizione geometrica di integrale, è facile ragionare su quali siano le tipologie di funzioni integrabili, ossia qualsiasi funzione \textbf{continua} o \textbf{monotona} (anche monotona discontinua) nell'intervallo $[a,b]$. 
    
    Come sappiamo, le funzioni che non rispettano tali caratteristiche sono poche. Infatti, sostanzialmente le uniche \textbf{funzioni non integrabili} sono delle funzioni di cui è \textbf{difficile calcolare il valore} assunto dalla funzione stessa. Esempio tipico di ciò è la \textbf{funzione di Dirichlet}, che assume valore 1 nel caso in cui $x$ sia un numero razionale e valore 0 nel caso in cui sia un numero irrazionale. Tale funzione risulta difficile da integrare poiché all'interno di un intervallo vi sono infiniti numeri razionali ed infiniti numeri irrazionali.
    
    \quad
    
    \textbf{Proprietà degli integrali}
    
    Tenendo a mente la definizione geometrica di integrale, dunque del fatto che corrisponda esattamente all'area della funzione in un intervallo specifico, possiamo formulare alcune \textbf{proprietà} che essi rispettano in qualsiasi caso:
    
    \begin{framedthm}{Linearità dell'integrale}
        Date \textbf{due funzioni $f$ e $g$ integrabili} nell'intervallo $[a,b]$, la funzione $h(x) = \alpha f(x) + \beta g(x)$ è integrabile nell'intervallo $[a,b]$ e l'integrale equivale alla \textbf{somma dell'integrale di $\alpha  f(x)$ e di $\beta  g(x)$ nell'intervallo $[a,b]$}:
        
        \[ \int_{a}^{b} [\alpha f(x) + \beta g(x)] \; dx = \alpha \int_{a}^{b} f(x) \; dx + \beta \int_{a}^{b} g(x) \; dx\]
    \end{framedthm}
    
    \begin{center}
        \includegraphics[scale=0.8]{resources/images/prop_int1.png}
        
        \textit{Notiamo facilmente come la somma delle aree di $\alpha f(x)$ e $\beta g(x)$ nell'intervallo $[a,b]$  corrispondano all'area di $h(x) = \alpha f(x) + \beta g(x)$ nello stesso intervallo}
    \end{center}
    
    \newpage
    
    \begin{framedthm}{Additività dell'integrale}
        Sia $[a,b] = [a,c] \cup [c,b]$, dove $c \in [a,b]$, e sia $f$ una funzione \textbf{integrabile in $[a,c]$ e in $[c,b]$}. 
        In tal caso, $f$ è integrabile in $[a,b]$ ed l'integrale equivale alla \textbf{somma tra l'integrale in $[a,c]$ e l'integrale in $[c,b]$}
        
        \[ \int_{a}^{b} f(x) \; dx = \int_{a}^{c} f(x) \; dx + \int_{c}^{b} f(x) \; dx\]
    \end{framedthm}
    
    \begin{center}
        \includegraphics[scale=0.85]{resources/images/prop_int2.png}
        
        \textit{Tale proprietà risulta essere banale, poiché sfruttata anche \\ nella definizione geometrica stessa di integrale}
    \end{center}
    
    Seppur semplice, tale proprietà ci permette di fare delle \textbf{assunzioni aggiuntive}. Immaginiamo di voler calcolare l'integrale in $[a,b]$ di una \textbf{funzione discontinua e non strettamente monotona}. 
    
    \begin{center}
        \includegraphics[scale=0.8]{resources/images/prop_int3.png}
    \end{center}
    
    A primo impatto, ci sembra impossibile stabilire il valore assunto dall'integrale in $[a,b]$. Tuttavia, sfruttare la proprietà dell'additività dell'integrale, \textbf{spezzando il singolo integrale nella somma tra tre integrali}: uno nell'intervallo $[a,c]$, dove la funzione è \textbf{strettamente decrescente}, uno nell'intervallo $[c,d]$, dove la funzione è \textbf{strettamente crescente}, ed uno in $[d,b]$, dove la funzione è \textbf{discontinua}.
    
    A questo punto, utilizzando il \textbf{metodo di Rienmann}, ci risulta facile calcolare i tre singoli integrali, per poi sommarli ed ottenere il valore dell'integrale in $[a,b]$.
    
    \[  \int_{a}^{b} f(x) \; dx = \int_{a}^{c} f(x) \; dx + \int_{c}^{d} f(x) \; dx + \int_{d}^{b} f(x) \; dx\]
    
    \begin{framedprop}{Integrabilità di una funzione}
        Una funzione $f$ è integrabile in un intervallo $[a,b]$ se all'interno di tale intervallo presenta un \textbf{numero finito di cambi di monotonia e un numero finito di discontinuità}.
    \end{framedprop}
    
    \quad
    
    Inoltre, tale proprietà ci permette di calcolare integrali in cui la funzione \textbf{non assume valori strettamente positivi} in un intervallo.
    
    \begin{center}
        \includegraphics[scale=0.8]{resources/images/prop_int4.png}
    \end{center}
    
    In questo caso, l'integrale in $[a,b]$ può essere comodamente spezzato nella \textbf{somma tra l'integrale in $[a,c]$ e l'integrale in $[c,b]$}. Tuttavia, notiamo come nell'intervallo $[a,c]$ la funzione assuma \textbf{valori negativi}, rendendo \textbf{negativo} il risultato di tale integrale.
    
    Di conseguenza, l'integrale in $[a,b]$ corrisponderebbe alla \textbf{somma tra un'area negativa ed un'area positiva}. Nel caso in cui invece volessimo ottenere l'\textbf{area assoluta}, è necessario \textbf{negare l'integrale in $[a,c]$}, in modo da ottenere la somma effettiva tra le due aree.
    
    \[ \int_{a}^{b} f(x) \; dx = - \int_{a}^{c} f(x) \; dx + \int_{c}^{b} f(x) \; dx\]
    
    \begin{framedprop}{Integrale con Area assoluta}
        Se una funzione $f$ assume \textbf{valori negativi} $\forall x \in [x_1,x_2]$ e viene integrata in tale intervallo, allora è necessario \textbf{negare il risultato} dell'integrale.
    \end{framedprop}
    
    \newpage
    
    Fino ad ora abbiamo visto solo casi in cui abbiamo integrato una funzione in un intervallo $[a,b]$ dove $a < b$. Ma cosa accade se \textbf{invertiamo i due estremi} di un integrale? A livello quantitativo, il risultato non cambierà, poiché la \textbf{quantità di area} sottostante alla funzione sarà sempre la stessa. Tuttavia, ciò che \textbf{cambierà sarà il segno del risultato}, ottenendo una versione negata dell'integrale originale.
    
    \begin{framedthm}{Inversione dell'intervallo di integrazione}
        Se $f$ è una funzione integrabile in $[a,b]$ dove $a < b$, allora vale che
        
        \[ \int_{a}^{b} f(x) \; dx = - \int_{b}^{a} f(x) \; dx\]
    \end{framedthm}
    
    \quad
    
    Infine, l'ultima proprietà degli integrali prevede il \textbf{calcolo di un'area tramite la differenza tra due aree}. Consideriamo la seguente situazione: vogliamo calcolare l'integrale in $[c,b]$ della seguente funzione
    
    \begin{center}
        \includegraphics[scale=0.8]{resources/images/prop_int5.png}
    \end{center}
    
    In questo caso, ci viene naturale affermare che l'integrale in $[c,b]$ corrisponde esattamente alla \textbf{differenza tra l'integrale in $[a,b]$ e l'integrale in $[a,c]$}.
    
    \begin{framedthm}{Differenza tra integrali}
        Sia $[a,b]$ un intervallo e sia $c \in [a,b]$. Se $f$ è una funzione integrabile in $[a,b]$, allora l'integrale di $f$ in $[c,b]$ è esprimibile come
        
        \[ \int_{c}^{b} f(x) \; dx = \int_{a}^{b} f(x) \; dx - \int_{a}^{c} f(x) \; dx\]
    \end{framedthm}
    
    \newpage
    
    \section{Teorema Fondamentale del Calcolo Integrale}
    
    Riprendiamo ora il discorso del calcolo dell'integrale della funzione $f(x) = x$.
    
    \begin{center}
        \includegraphics[scale=0.8]{resources/images/prop_int6.png}
    \end{center}
    
    Vogliamo calcolare l'integrale di $f$ nell'intervallo $[t, t+h]$. Scegliamo quindi di utilizzare l'ultima proprietà degli integrali discussa nella sezione precedente, ossia la\textbf{differenza tra integrali}.
    
    \[ \int_{t}^{t+h} x \; dx = \int_{0}^{t+h} x \; dx - \int_{0}^{t} x \; dx\]
    
    Poiché le aree al di sotto di $f(x) = x$ corrispondono a dei \textbf{triangoli}, possiamo \textit{"barare"} e calcolarci velocemente tali aree:
    
    \[ \int_{t}^{t+h} x \; dx = \int_{0}^{t+h} x \; dx - \int_{0}^{t} x \; dx = \frac{(t+h) \cdot (t+h)}{2} - \frac{t \cdot t}{2} \]
    
    Notiamo come le aree calcolate corrispondono sono valide per qualsiasi valore di $t$. Definiamo quindi una \textbf{funzione ausiliaria $F(x)$} corrispondente a \textbf{qualsiasi integrale di $f(x) = x$ in un intervallo $[0,t]$}.
    
    \[ F(t) = \int_{0}^{t} x \; dx = \frac{t^2}{2}\]
    
    Dunque, riscriviamo l'integrale precedente come
    
    \[ \int_{t}^{t+h} x \; dx = F(t+h) - F(t) = \frac{(t+h)^2}{2} - \frac{t^2}{2}\]
    
    Arrivati a questo punto, ci chiediamo quale sia il \textbf{limite del rapporto incrementale} (dunque la \textbf{derivata}) di tale funzione ausiliaria, in modo da sapere quanto il suo valore cambi al variare del suo argomento. 
    
    \[ F'(x) = \lim_{h \to 0} \frac{F(t+h)-F(t)}{h} = \lim_{h \to 0} \frac{\frac{(t+h)^2} {2} - \frac{t^2}{2}}{h} = t\]
    
    Notiamo quindi come la \textbf{derivata di $F(x)$} corrisponda esattamente al \textbf{valore stesso di $t$}. Difatti, ragionando graficamente, riducendo al limite la distanza tra il punto $t$ e il punto $t+h$ (dunque calcolando la derivata), ciò che otteniamo non è altro che l'\textbf{"altezza" di un rettangolo di base infinitesimale}, poiché
    
    \begin{itemize}
        \item $F(t+h)-F(t)$ corrisponde all'\textbf{area} di $f(x)$ nell'intervallo $[t, t+h]$
        \item $h$ corrisponde alla \textbf{base} di tale area
        \item Dunque il limite del rapporto incrementale sarà
        
        \[ \lim_{h \to 0} \frac{F(t+h)-F(t)}{h} = \frac{Area}{Base} = Altezza\]
    \end{itemize}
    
    Tuttavia, sappiamo anche che t\textbf{ale altezza corrisponde esattamente a $f(t)$}. Dunque, la \textbf{derivata della funzione ausiliaria in $t$} corrisponde esattamente al \textbf{valore della funzione originale in $t$.}
    
    \[ F'(t) = f(t)\]
    
    \begin{framedthm}{Teorema Fondamentale del Calcolo Integrale}
        Se $F(t)$ è la funzione ricavata dall'integrale di $f(x)$ in $[0, t]$, allora $F'(t) = f(t)$.
        
        \[ F(t) = \int_{0}^{t} f(x) \; dx\]
        
        \[ F'(t) = f(t)\]
        
        Possiamo quindi informalmente affermare che \textbf{l'integrale corrisponde all'operazione matematica inversa della derivata}.
    \end{framedthm}
    
    Tornando all'esempio con $f(x) = x$, difatti, notiamo come la funzione $F(x)$ corrisponda esattamente ad una funzione la cui derivata coincide esattamente con $f(x)$.
    
    Tale \textbf{teorema fondamentale} ci permette di calcolare con estrema facilità il valore di un qualsiasi integrale di una funzione definito in un certo intervallo, limitando il calcolo al \textbf{dover trovare il valore assunto dalla funzione la cui derivata coincide con $f(x)$ negli estremi $a$ e $b$}.
    
    Per comodità, rappresenteremo i calcoli nel seguente formato
    
    \[ \int_{a}^{b} f(x) \; dx = \left . F(x) \; \right |_{a}^{b} = F(b) - F(a)\]
    
    \newpage
    
    \textbf{Esempi}
    
    \begin{itemize}
        \item Consideriamo il seguente integrale
        
        \[ \int_{0}^{2} x^2 \; dx\]
        
        \begin{itemize}
            \item Prima di tutto cerchiamo quale sia la funzione $F(x)$ la cui derivata coincide con $f(x)$. Ricordando le regole di derivazione, riusciamo a ricavare che
            
            \[ F(x) = \frac{1}{3} x^3 \Longrightarrow F'(x) = \frac{1}{3} \cdot 3 \cdot x^{3-1} = x^2 = f(x)\]
            
            \item Dunque il valore dell'integrale sarà
            
            \[ \int_{0}^{2} x^2 \; dx = \left . \frac{1}{3} x^3 \right |_{0}^{2} = \frac{1}{3} \cdot 2^3 - \frac{1}{3} \cdot 0^3 = \frac{8}{3}\]
        \end{itemize}
        
        \item Consideriamo il seguente integrale
        
        \[ \int_{2}^{8} x^2+5x \; dx\]
        
        \begin{itemize}
            \item Cerchiamo la funzione $F(x)$ la cui derivata coincide con $f(x)$
            
            \[ F(x) = \frac{1}{3}x^3+\frac{5}{2}x^2\]
            
            \item Dunque il valore dell'integrale sarà
            
            \[ \int_{2}^{8} x^2+5x \; dx = \left . \frac{1}{3}x^3+\frac{5}{2}x^2 \right |_{2}^{8} = \frac{1}{3} \cdot 8^3 +\frac{5}{2} \cdot 8^2 -  \frac{1}{3} \cdot 2^3 +\frac{5}{2} \cdot 2^2 = 318\]
        \end{itemize}
        
        \item Consideriamo il seguente integrale
        
        \[ \int_{0}^{\frac{\pi}{2}} \cos(x) \; dx\]
        
        \begin{itemize}
            \item Cerchiamo la funzione $F(x)$ la cui derivata coincide con $f(x)$
            
            \[ F(x) = \sin(x)\]
            
            \item Dunque il valore dell'integrale sarà
            
            \[ \int_{0}^{\frac{\pi}{2}} \cos(x) \; dx = \left . \sin(x) \right |_{0}^{\frac{\pi}{2}} = \sin \left (\frac{\pi}{2} \right ) - \sin(0) = 1\]
        \end{itemize}
    \end{itemize}
    
    \subsection{Funzioni primitive, integrale definito e indefinito}
    
    Abbiamo quindi visto come al fine di poter calcolare l'integrale di una funzione $f(x)$ sia necessario trovare una \textbf{funzione} $F(x)$ tale che $F'(x) = f(x)$.
    
    Tale funzione viene detta \textbf{primitiva di $f(x)$} e ne possono esistere \textbf{infinite}: se $F(x)$ è una primitiva di $f(x)$, allora anche $G(x) = F(x) + c$ (per \textbf{qualsiasi valore costante $c$}) è una \textbf{primitiva} di $f(x)$, poiché $G'(x) = f(x)$.
    
    \begin{framedprop}{Funzioni primitive}
        Se $f(x)$ è una funzione integrabile, allora esistono \textbf{infinite funzioni primitive} $G(x)$ tali che $G'(x) = f(x)$, dove $G(x) = F(x) + c$ per ogni $c \in \mathbb{R}$
    \end{framedprop}
    
    Notiamo però come l'esistenza di infinite primitive \underline{non influisca} sul \textbf{Teorema Fondamentale del Calcolo Integrale}, poiché considerando una qualsiasi primitiva $G(x) = F(x) + c$, allora abbiamo che
    
    \[ \int_{a}^{b} f(x) \; dx = \left . G(x) \right |_{a}^{b} = G(b) - G(a) = F(b) + c - (F(a) + c) = F(b) - F(a)\]
    
    dunque, considerando una \textbf{qualsiasi primitiva di} $f(x)$, otterremo \textbf{sempre lo stesso risultato}, poiché i due valori costanti $c$ si eliminano a vicenda.
    
    Abbiamo inoltre già concluso come si possa impropriamente considerare l'\textbf{integrazione} come l'\textbf{operazione matematicamente inversa} alla \textbf{derivazione}, nonostante fino ad ora abbiamo utilizzato gli integrali per calcolare l'area al di sotto di una funzione.
    
    Definiamo quindi la differenza tra \textbf{integrale definito}, ossia quello visto fin'ora dove viene calcolata l'area al di sotto di una funzione in un intervallo definito, ed \textbf{integrale indefinito}, ossia l'operazione inversa alla derivazione.
    
    \begin{framedprop}{Integrale definito e indefinito}
    
        Definiamo come \textbf{integrale definito} il calcolo dell'area al di sotto di una funzione $f(x)$ in un intervallo $[a,b]$ come
        
        \[ \int_{a}^{b} f(x) = \left . F(x) \right |_{a}^{b} = F(b) - F(a) \]
        
        mentre definiamo come \textbf{integrale indefinito} l'operazione inversa alla derivazione, ossia trovare la primitiva $F(x)$ di una funzione $f(x)$
        
        \[ \int f(x) = F(x) + c\]
    \end{framedprop}
    
    \newpage
    
    \section{Tecniche di integrazione}
    
    \subsection{Integrali immediati}
    
    Essendo l'integrazione l'operazione inversa alla derivazione, ne segue logicamente che esistano degli \textbf{integrali immediati} strettamente legati alle \textbf{derivate immediate}:
    
    \begin{center}
        
        \begin{tabular}{c c c}
            \begin{tabular}{c | c}
                \multicolumn{2}{c}{\textbf{Derivate immediate}}\\
                $f(x)$ & $f'(x)$ \\
                \hline
                $x^{\alpha}$ & $\alpha x^{\alpha -1}$ \\
                $\sin(x)$ & $\cos(x)$ \\
                $\cos(x)$ & $-\sin(x)$ \\
                $tg(x)$ & $\frac{1}{\cos^2(x)}$ \\
                $arctg(x)$ & $\frac{1}{1+x^2}$ \\
                $e^x$ & $e^x$ \\
                $\alpha^x$ & $\alpha^x ln(\alpha)$ \\
                $ln(\abs{x})$ & $\frac{1}{x}$ \\
            \end{tabular}
            
            &\qquad\qquad\qquad&
            
            \begin{tabular}{c | c}
                \multicolumn{2}{c}{\textbf{Integrali immediati}}\\
                $f(x)$ & $F(x)$ \\
                \hline
                $x^{\alpha}$ & $\frac{x^{\alpha +1}}{\alpha+1} + c$ \\
                $\sin(x)$ & $-\cos(x) + c$ \\
                $\cos(x)$ & $\sin(x) + c$ \\
                $\frac{1}{\cos^2(x)}$ & $tg(x) + c$\\
                $\frac{1}{1+x^2}$ & $arctg(x)+c$\\
                $e^x$ & $e^x+c$ \\
                $\alpha^x$ & $\frac{\alpha^x}{ln(\alpha)}+c$ \\
                $\frac{1}{x}$ & $ln(\abs{x})+c$ \\
            \end{tabular}
        \end{tabular}
    \end{center}
    
    \quad
    
    
    \subsection{Integrazione per sostituzione}
    
    Consideriamo il seguente integrale:
    
    \[ \int_{1}^{4} e^{3x} \; dx\]
    
    Notiamo come tale integrale risulti essere estremamente \textbf{simile} all'integrale di $f(x) = e^x$, la cui primitiva sappiamo essere $F(x) = e^x$. Inoltre, ricordando che gli integrali corrispondono in realtà a nient'altro che ad una \textbf{serie numerica} di somme di piccole aree, risulta intuitivo applicare le normali proprietà di \textbf{sostituzione} che abbiamo già visto all'interno delle serie numeriche.
    
    Tuttavia, è necessario sottolineare che effettuare un \textbf{cambio di variabile} nell'ambito dell'integrazione comporta il dover andare a sostituire \textbf{ogni riferimento alla variabile originale} presente nell'integrale, inclusi gli \textbf{estremi dell'intervallo} e la \textbf{variabile di integrazione}:
    
    
    \begin{itemize}
        \item Poniamo $y = 3x$ al fine di trasformare $f(x) = e^{3x} = e^y$
        \item Di conseguenza, gli \textbf{estremi dell'intervallo} devono essere modificati, poiché quando $x = 1$ abbiamo che $y = 3$, mentre quando $x=4$ abbiamo che $y = 12$
        \item Anche la \textbf{variabile di integrazione} deve essere modificata, poiché se $y = 3x$, ne segue che $y' = [3x]' = 3$
        \[ y = 3x\]
        \[ dy = [3x]' \,dx\]
        \[ dy = 3 \,dx \]
        \[ \frac{dy}{3} = dx \]
        \item Dunque, una volta sostituiti \textbf{tutti i riferimenti} alla variabile di integrazione, l'integrale che otterremo sarà:
        \[ \int_{1}^{4} e^{3x} \; dx = \int_{3}^{12} e^y \; \frac{dy}{3} = \frac{1}{3} \int_{3}^{12} e^y \; dy \]
        \item A questo punto, ci basterà calcolare l'\textbf{integrale immediato} ottenuto, per poi riportare il risultato ottenuto in termini della variabile di integrazione originale
        \[ \frac{1}{3} \int_{3}^{12} e^y \; dy = \left . \frac{e^y}{3} \right |_{3}^{12} = \left . \frac{e^{3x}}{3} \right |_{1}^{4} = \frac{e^{12}-e^3}{3}\]
    \end{itemize}
    
    \quad
    
    \textbf{Ulteriori esempi}
    \begin{enumerate}
        \item Consideriamo il seguente integrale
        \[ \int_{0}^{\pi} \cos(6x) \; dx\]
        
        \begin{itemize}
        \item Sostituendo per $y=6x$, otteniamo che
            \begin{itemize}
                \item Gli estremi dell'intervallo diventano $[0, 6\pi]$
                \item La variabile di integrazione diventa
                \[ y = 6x\]
                \[ dy = 6 \, dx\]
                \[\frac{dy}{6} = dx\]
            \end{itemize}
        \item Dunque l'integrale viene trasformato in
        \[ \int_{0}^{\pi} \cos(6x) \; dx = \frac{1}{6} \int_{0}^{6\pi} \cos(y) \; dy = \left . \frac{\sin(y)}{6} \right |_{0}^{6\pi} = \frac{0 - 0}{6} = 0\]
        
        \end{itemize}
        
        \item Consideriamo il seguente integrale
        \[ \int x^2 e^{x^3} \; dx\]
        
        \begin{itemize}
        \item Sostituendo per $y=x^3$, otteniamo che
            \begin{itemize}
                \item La variabile di integrazione diventa
                \[ y = x^3\]
                \[ dy = 3x^2 \, dx\]
                \[ \frac{dy}{3} = x^2 \, dx\]
            \end{itemize}
            
        \item Dunque l'integrale viene trasformato in
        \[ \int x^2 e^{x^3} \; dx = \int e^y \frac{dy}{3} = \frac{e^y}{3} +c = \frac{e^{x^3}}{3}+c\]
        
        \end{itemize}
        
        \item Consideriamo il seguente integrale
        \[ \int_{-1}^{3} \frac{1}{2x+15} \; dx\]
        
        \begin{itemize}
        \item Gli estremi dell'intervallo diventano $[13, 21]$
        \item Sostituendo per $y=2x+15$, otteniamo che
            \begin{itemize}
                \item La variabile di integrazione diventa
                \[ y = 2x+15\]
                \[ dy = 3 \, dx\]
                \[ \frac{dy}{2} = dx\]
            \end{itemize}
            
        \item Dunque l'integrale viene trasformato in
        \[ \int_{13}^{21} \frac{1}{y} \frac{dy}{3} = \frac{1}{3} \int_{13}^{21} \frac{1}{y} \; dy = \left . \frac{ln (\abs{y})}{3} \right |_{13}^{21} = \frac{ln(21)-ln(13)}{3}\]
        
        \end{itemize}
        
        \item Consideriamo il seguente integrale
        \[ \int \frac{4x+6}{x^2+3x+6} \; dx\]
        
        \begin{itemize}
        \item Sostituendo per $y=x^2+3x+6$, otteniamo che
            \begin{itemize}
                \item La variabile di integrazione diventa
                \[ y = x^2+3x+6\]
                \[ dy = 2x+3 \, dx\]
            \end{itemize}
            
        \item Dunque l'integrale viene trasformato in
        \[ \int \frac{4x+6}{x^2+3x+6} \; dx = 2 \int \frac{2x+3}{x^2+3x+6} \; dx = 2\int \frac{1}{y} \; dy = \]
        \[ = 2ln(\abs{y}) = 2ln(\abs{x^2+3x+6})+c\]
        
        \end{itemize}
        
    \end{enumerate}
    
    \newpage
    
    \subsection{Integrazione per parti}
    
    Consideriamo il seguente integrale
    
    \[ \int x^3 e^{x^2} \; dx\]
    
    Proviamo a risolverlo applicando l'\textbf{integrazione per parti} vista nella sezione precedente. Ponendo $y = x^2$ otteniamo che
        \[ y = x^2\]
        \[ dy = 2x \, dx \]
        \[ \frac{dy}{2} = x \, dx \]
    
    dunque possiamo riscrivere l'integrale come
    
    \[ \int x^3 e^{x^2} \; dx = \int x \cdot x^2 e^{x^2} \; dx = \frac{1}{2} \int y e^{y} \; dy\]
    
    A questo punto, non siamo riusciti a ricondurre l'integrale originale ad un \textbf{integrale immediato}, dunque non sappiamo calcolarne la primitiva.
    
    Proviamo quindi un altro approccio:
    
    \begin{itemize}
        \item Consideriamo la seguente equazione, descrivente la \textbf{derivazione di un prodotto di funzioni}
        \[ [ f(x) \cdot g(x)]' = f'(x) \cdot g(x) + f(x) \cdot g'(x)\]
        
        \item Applichiamo quindi l'operazione di integrazione su entrambe le parti
        \[ \int [ f(x) \cdot g(x)]' \; dx = \int f'(x) \cdot g(x) + f(x) \cdot g'(x) \; dx\]
        
        \item A questo punto, notiamo come nel lato sinistro dell'equazione abbiamo l'\textbf{integrazione di una derivata} che, essendo \textbf{l'una l'inversa dell'altra}, restituiscono il prodotto originale tra le funzioni $f(x)$ e $g(x)$
        \[ f(x) \cdot g(x) = \int f'(x) \cdot g(x) + f(x) \cdot g'(x) \; dx\]
        
        \item Utilizzando le proprietà degli integrali, \textbf{spezziamo l'integrale nella parte destra in due}
        \[ f(x) \cdot g(x) = \int f'(x) \cdot g(x) \; dx + \int f(x) \cdot g'(x) \; dx\]
        
        \item Infine, non ci rimane che portare uno dei due integrali sul lato sinistro dell'\textbf{equazione}, ottenendo quindi che
        \[ \int f'(x) \cdot g(x) = f(x) \cdot g(x) - \int f(x) \cdot g'(x) \; dx\]
    \end{itemize}
    
    Abbiamo ottenuto quindi una \textbf{formula} che ci permette di calcolare l'\textbf{integrale di un prodotto tra una funzione derivata ed una funzione}.
    
    
    \begin{framedprop}{Integrazione per parti}
    Se il prodotto tra funzioni $f'(x) \cdot g(x)$ è integrabile, allora

    \[ \int f'(x) \cdot g(x) = f(x) \cdot g(x) - \int f(x) \cdot g'(x) \; dx\]
    \end{framedprop}
    A questo punto, riprendiamo l'integrale precedentemente calcolato applicando l'integrazione per parti
    
    \[ \frac{1}{2} \int y e^{y} \; dy\]
    
    Per poter applicare l'\textbf{integrazione per parti}, è necessario \textbf{stabilire attentamente quale tra le due funzioni} presenti all'interno dell'integrale \textbf{corrisponda a }$f'(x)$ e quale \textbf{corrisponda a} $g(x)$.
    
    Vediamo cosa accade in entrambi i casi:
    
    \begin{enumerate}
        \item \begin{itemize}
            \item Scegliamo $f'(x) = e^y$ e $g(x) = y$.
            \item Poiché $f'(x) = e^y$, ne segue che, tramite \textbf{integrazione}, $f(x) = e^y$, mentre poiché $g(x) = x$, ne segue che, tramite \textbf{derivazione}, $g'(x) = 1$
            \item Dunque, applicando l'\textbf{integrazione per parti} otteniamo che
            
            \[ \frac{1}{2} \int y e^{y} \; dy = \frac{1}{2} \left ( ye^y - \int e^y \cdot 1 \; dy \right )\]
            
            \item A questo punto, il secondo integrale ottenuto corrisponde ad un integrale immediato, dunque siamo in grado di calcolarne la primitiva
            
            \[ \frac{1}{2} \left ( ye^y - \int e^y \cdot 1 \; dy \right ) = \frac{ye^y - e^y}{2} + c= \frac{x^2e^{x^2} - e^{x^2}}{2} + c\]
        \end{itemize}
        
        \quad
        
        \item \begin{itemize}
            \item Scegliamo ora invece $f'(x) = x$ e $g(x) = e^x$.
            \item Poiché $f'(x) = x$, ne segue che, tramite \textbf{integrazione}, $f(x) = \frac{x^2}{2}$, mentre poiché $g(x) = e^x$, ne segue che, tramite \textbf{derivazione}, $g'(x) = e^x$
            \item Dunque, applicando l'\textbf{integrazione per parti} otteniamo che
            
            \[ \frac{1}{2} \int y e^{y} \; dy = \frac{1}{2} \left ( \frac{y^2 e^y}{2} - \int \frac{y^2 e^y}{2}  \; dy \right )\]
            
            \item A questo punto, notiamo come il secondo integrale ottenuto sia in realtà \textbf{più complesso} di quello iniziale, richiedendo inoltre un'ennesima applicazione del metodo appena utilizzato. L'integrazione per parti, dunque, risulta essere uno \textbf{strumento potente ma di difficile gestione}, richiedendo molta pratica.
        \end{itemize}
    \end{enumerate}
    
    \textbf{Ulteriori esempi}
    
    \begin{enumerate}
        \item Consideriamo il seguente integrale
        \[ \int x^3 e^x \; dx\]
        
        \begin{itemize}
            \item Applicando l'integrazione per parti abbiamo che
            \item $f'(x) = e^x \Longrightarrow f(x) = e^x$
            \item $g(x) = x^3 \Longrightarrow g'(x) = 3x^2$
        \end{itemize}
        
        \[ \int x^3 e^x \; dx = x^3e^x - \int 3x^2e^x \; dx\]
        
        \begin{itemize}
            \item Applichiamo ancora una volta l'integrazione per parti
            \item $f'(x) = e^x \Longrightarrow f(x) = e^x$
            \item $g(x) = 3x^2 \Longrightarrow g'(x) = 6x$
        \end{itemize}
        
        \[ \int x^3 e^x \; dx = x^3e^x - \int 3x^2e^x \; dx = x^3e^x - \left ( 3x^2e^x - \int 6xe^x \; dx \right )\]
        
        \begin{itemize}
            \item E applicandola ancora un'ultima volta otteniamo che
            \item $f'(x) = e^x \Longrightarrow f(x) = e^x$
            \item $g(x) = 6x \Longrightarrow g'(x) = 6$
        \end{itemize}
        
        \[ \int x^3 e^x \; dx = x^3e^x - \int 3x^2e^x \; dx = x^3e^x - \left ( 3x^2e^x - \int 6xe^x \; dx \right ) =\]
        \[ = x^3e^x - \left ( 3x^2e^x - \left ( 6xe^x - \int 6e^x \; dx \right ) \right ) = x^3e^x - 3x^2e^x + 6xe^x - 6e^x + c= \]
        \[ = e^x(x^3 - 3x^2 + 6x - 6) + c\]
        
        \item Consideriamo il seguente integrale
        
        \[ \int_{0}^{\pi} x \cos(2x) \; dx\]
        
        \begin{itemize}
            \item Applicando l'integrazione per parti abbiamo che
            \item $f'(x) = \cos(2x) \Longrightarrow f(x) = \frac{\sin(2x)}{2}$
            \item $g(x) = x \Longrightarrow g'(x) = 1$
        \end{itemize}
        
        \[ \int_{0}^{\pi} x \cdot \cos(2x) \; dx = \frac{x \cdot \sin(2x)}{2} - \int \frac{\sin(2x)}{2} \; dx = \frac{x \cdot \sin(2x)}{2} + \frac{\cos(2x)}{4} = \]
        \[ = \left . \frac{2x \cdot \sin(2x) + \cos(2x)}{4} \right |_{0}^{\pi} = \frac{1}{4} - \frac{1}{4} = 0\]
        
        
        \item Consideriamo il seguente integrale
        
        \[ \int ln(x) \; dx\]
        
        \begin{itemize}
            \item Riscriviamo l'integrale come
            \[ \int ln(x) \; dx = \int 1 \cdot ln(x) \; dx\]
            
            \item Applicando l'integrazione per parti abbiamo che
            \item $f'(x) = 1 \Longrightarrow f(x) = x$
            \item $g(x) = ln(x) \Longrightarrow g'(x) = \frac{1}{x}$
        
        \[ \int 1 \cdot ln(x) \; dx = x ln(x) - \int \frac{x}{x} \; dx = x ln(x) - x + c\]
        
        \item In alternativa, avremmo potuto sviluppare l'integrale procedendo per sostituzione
        
        \[ y = ln(x)\]
        \[ dy = \frac{1}{x} \, dx \]
        
        dove di conseguenza abbiamo che
        
        \[ y = ln(x) \Longrightarrow e^y = x \]
        
        \item Riscriviamo quindi l'integrale come
        
        \[ \int ln(x) \; dx = \int ln(x) \cdot \frac{x}{x} \; dx = \int ye^y \; dy\]
        
        \item A questo punto sviluppiamo l'integrale per parti ponendo
        
        \item $f'(x) = e^y \Longrightarrow f(x) = e^y$
        \item $g(x) = y \Longrightarrow g'(x) = 1$
        
            \[\int ye^y \; dy = ye^y - \int e^y \; dx = ye^y - e^y = ln(x) \cdot e^{ln(x)} - e^{ln(x)} = xln(x)-x+c\]
            
        \end{itemize}
    \end{enumerate}
    
    \newpage
    
    \subsection{Integrazioni pre-calcolate}
    
    In questa sezione vedremo alcune \textbf{"tecniche rapide"} che permettono di calcolare gli \textbf{integrali più comuni} in modo da \textbf{saperne già lo sviluppo} senza dover effettuare alcun calcolo per sostituzione e/o per parti.
    
    \begin{enumerate}
        \item Consideriamo il seguente integrale generico
        
        \[ \int e^{\alpha x} \; dx\]
        
        dove $\alpha \neq 0$.
        
        \begin{itemize}
            \item Procedendo per sostituzione, abbiamo che
            
            \[ y = \alpha x\]
            \[ dy = \alpha \, dx \]
            \[ \frac{dy}{\alpha} = dx \]
            
            \item Dunque, indipendentemente dal valore di $\alpha$ (dunque $\forall \alpha \in \mathbb{R}$), l'integrale sarà
            
            \[ \int e^{\alpha x} \; dx = \frac{1}{\alpha} \int e^y \; dy = \frac{e^y}{\alpha} = \frac{e^{\alpha x}}{\alpha}+c\]
            
        \end{itemize}
        
        \quad
        
        \quad
        
        
        \item Consideriamo il seguente integrale generico
        
        \[ \int \sin(\alpha x) \; dx\]
        
        \begin{itemize}
            \item Procedendo per sostituzione, abbiamo che
            
            \[ y = \alpha x\]
            \[ dy = \alpha \, dx \]
            \[ \frac{dy}{\alpha} = dx \]
            
            \item Dunque, indipendentemente dal valore di $\alpha$ (dunque $\forall \alpha \in \mathbb{R}$), l'integrale sarà
            
            \[ \int \sin(\alpha x) \; dx = \frac{1}{\alpha} \int \sin(y) \; dy = -\frac{\cos(y)}{\alpha} = -\frac{\cos(\alpha x)}{\alpha}+c\]
            
        \end{itemize}
        
        \newpage
        
        \item Consideriamo il seguente integrale generico
        
        \[ \int \cos(\alpha x) \; dx\]
        
        \begin{itemize}
            \item Procedendo per sostituzione, abbiamo che
            
            \[ y = \alpha x\]
            \[ dy = \alpha \, dx \]
            \[ \frac{dy}{\alpha} = dx \]
            
            \item Dunque, indipendentemente dal valore di $\alpha$ (dunque $\forall \alpha \in \mathbb{R}$), l'integrale sarà
            
            \[ \int \cos(\alpha x) \; dx = \frac{1}{\alpha} \int \cos(y) \; dy = \frac{\sin(y)}{\alpha} = \frac{\sin(\alpha x)}{\alpha}+c\]
            
        \end{itemize}
        
        \quad
        
        \quad
        
        \item Consideriamo il seguente integrale generico
        
        \[ \int \frac{1}{\alpha x + \beta} \; dx\]
        
        \begin{itemize}
            \item Procedendo per sostituzione, abbiamo che
            
            \[ y = \alpha x + \beta\]
            \[ dy = \alpha \, dx \]
            \[ \frac{dy}{\alpha} = dx \]
            
            \item Dunque, indipendentemente dal valore di $\alpha x + \beta$ (dunque $\forall \alpha, \beta \in \mathbb{R}$), l'integrale sarà
            
            \[ \int \frac{1}{\alpha x + \beta} \; dx = \frac{1}{\alpha} \int \frac{1}{y} \; dy = \frac{ln(\abs{y})}{\alpha} = \frac{ln(\abs{\alpha x + \beta})}{\alpha}+c\]
            
        \end{itemize}
        
        \newpage
        
        \item Consideriamo il seguente integrale generico, dove $f(x)$ è una qualsiasi funzione e $f'(x)$ è la sua derivata
        
        \[ \int \frac{f'(x)}{f(x)} \; dx\]
        
        \begin{itemize}
            \item Procedendo per sostituzione, abbiamo che
            
            \[ y = f(x)\]
            \[ dy = f'(x) \, dx \]
            \[ \frac{dy}{f'(x)} = dx \]
            
            \item Dunque, indipendentemente dalla funzione $f(x)$ (dunque $\forall \alpha, \beta \in \mathbb{R}$), l'integrale sarà
            
            \[ \int \frac{f'(x)}{f(x)} \; dx = \int \frac{f'(x)}{y} \cdot \frac{dy}{f'(x)} = \int \frac{1}{y} \; dy = ln(\abs{y}) = ln(\abs{f(x)})+c\]
            
        \end{itemize}
        
        \quad
        
        \quad
        
        \item Consideriamo il seguente integrale generico 
        
        \[ \int e^{\alpha x} \cos(\beta x) \; dx\]
        
        \begin{itemize}
            \item Procedendo per parti, abbiamo che
            
            \begin{itemize}
                \item $f'(x) = e^{\alpha x} \Longrightarrow f(x) = \frac{e^{\alpha x}}{\alpha}$
                \item $g(x) = \cos(\beta x) \Longrightarrow -\beta \sin(\beta x)$
            \end{itemize}
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx = \frac{\cos(\beta x)e^{\alpha x}}{\alpha} - \int \frac{-\beta \sin(\beta x)e^{\alpha x}}{\alpha} \; dx = \]
            \[ = \frac{\cos(\beta x)e^{\alpha x}}{\alpha} + \frac{\beta}{\alpha} \int \sin(\beta x)e^{\alpha x} \; dx\]
            
            \item Procedendo ancora per parti otteniamo che
            
            \begin{itemize}
                \item $f'(x) = e^{\alpha x} \Longrightarrow f(x) = \frac{e^{\alpha x}}{\alpha}$
                \item $g(x) = \sin(\beta x) \Longrightarrow \beta \cos(\beta x)$
            \end{itemize}
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx = \frac{\cos(\beta x)e^{\alpha x}}{\alpha} + \frac{\beta}{\alpha} \left [ \frac{\sin(\beta x)e^{\alpha x}}{\alpha} - \frac{\beta}{\alpha} \int \cos(\beta x)e^{\alpha x} \; dx \right ] \]
            
            \item A questo punto impostiamo e risolviamo la seguente equazione
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx = \frac{\cos(\beta x)e^{\alpha x}}{\alpha} + \frac{\beta}{\alpha} \left [ \frac{\sin(\beta x)e^{\alpha x}}{\alpha} - \frac{\beta}{\alpha} \int \cos(\beta x)e^{\alpha x} \; dx \right ] \]
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx = \frac{\cos(\beta x)e^{\alpha x}}{\alpha} + \frac{\beta \sin(\beta x)e^{\alpha x}}{\alpha^2} - \frac{\beta^2}{\alpha^2} \int  e^{\alpha x} \cos(\beta x) \; dx \]
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx +  \frac{\beta^2}{\alpha^2} \int  e^{\alpha x} \cos(\beta x) \; dx = \frac{\alpha \cos(\beta x)e^{\alpha x} + \beta \sin(\beta x)e^{\alpha x}}{\alpha^2}\]
            
            \[ \left ( \frac{\beta^2 + \alpha^2}{\alpha^2} \right ) \int e^{\alpha x} \cos(\beta x) \; dx = \frac{\alpha \cos(\beta x)e^{\alpha x} + \beta \sin(\beta x)e^{\alpha x}}{\alpha^2}\]
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx = \frac{\alpha \cos(\beta x)e^{\alpha x} + \beta \sin(\beta x)e^{\alpha x}}{\alpha^2} \cdot \frac{\alpha^2}{\beta^2 + \alpha^2}\]
            
            \[ \int e^{\alpha x} \cos(\beta x) \; dx = \frac{e^{\alpha x} (\alpha \cos(\beta x) + \beta \sin(\beta x))}{\beta^2 + \alpha^2}+c\]
        \end{itemize}
        
        
        \quad
        
        \quad
        
        \item Consideriamo il seguente integrale generico 
        
        \[ \int e^{\alpha x} \sin(\beta x) \; dx\]
        
        \begin{itemize}
            \item Procedendo per parti, abbiamo che
            
            \begin{itemize}
                \item $f'(x) = e^{\alpha x} \Longrightarrow f(x) = \frac{e^{\alpha x}}{\alpha}$
                \item $g(x) = \sin(\beta x) \Longrightarrow \beta \cos(\beta x)$
            \end{itemize}
            
            \[ \int e^{\alpha x} \sin(\beta x) \; dx = \frac{e^{\alpha x}\sin(\beta x)}{\alpha}-\frac{\beta}{\alpha} \int e^{\alpha x}\cos(\beta x) \; dx\]
            
            \item Sostituendo l'integrale ottenuto con la formula rapida precedentemente calcolata otteniamo che
            
            \[ \int e^{\alpha x} \sin(\beta x) \; dx = \frac{e^{\alpha x}\sin(\beta x)}{\alpha}-\frac{\beta}{\alpha} \left [ \frac{e^{\alpha x} (\alpha \cos(\beta x) + \beta \sin(\beta x))}{\beta^2 + \alpha^2} \right ] =\]
            
            \[ = \frac{e^{\alpha x} (\alpha \sin(\beta x) - \beta \cos(\beta x))}{\beta^2 + \alpha^2}+c\]
            
        \end{itemize}
        
        \item Consideriamo il seguente integrale generico, dove $P_n(x)$ è un qualsiasi polinomio di grado $n$
        
        \[ \int P_n(x) e^x\; dx\]
        
        \begin{itemize}
            \item Ricordando che la formula dell'integrazione per parti ha origine dalla \textbf{derivazione di un prodotto di funzioni}, sappiamo che per un qualsiasi polinomio $Q_n(x)$ si verifica che
            
            \[ [Q_n(x)e^x]' = Q'_n(x)e^x+Q_n(x)e^x \]
            
            \item A questo punto, fattorizziamo l'espressione e poniamo il polinomio $P_n(x)$ come la somma tra il polinomio $Q_n(x)$ e la sua derivata (dunque $P_n(x) = Q'_n(x) + Q_n(x)$)
            
            \[ Q_n(x)e^x+Q'_n(x)e^x = e^x(\underbrace{Q_n(x)}_{n} + \underbrace{Q'_n(x)}_{n-1}) = P_n(x)e^x\]
            
            \item Dunque, possiamo dire che la \textbf{derivazione di un polinomio $Q_n(x)$ di grado $n$ per un esponenziale è un polinomio di grado $n$ per lo stesso esponenziale}, ossia $P_n(x)$, ottenuto dalla somma del polinomio originale e la sua derivata.
            
            Quest'ultima corrisponderà ad un \textbf{polinomio di grado $n-1$,} poiché derivato da un polinomio di grado $n$ (derivazione di potenze). 
            
            \item Quindi, sapendo che 
            
            \[ P_n(x) = Q_n(x) + Q'_n(x)\]
            
            e che
            
            \[ \int P_n(x) e^x \; dx = Q_n(x)e^x\]
            
            è possibile ricavare $Q_n(x)$ tramite $P_x(n)$ risolvendo un \textbf{sistema di equazioni}
            
            \item Esempio:
            \begin{itemize}
                \item Si consideri il seguente integrale
                
                \[ \int (x^3-3x^2+8x-11)e^x \; dx \]
                
                \item Poiché $P_n(x) = x^3-3x^2+8x-11$, tramite le assunzioni fatte precedentemente, sappiamo che 
                
                \[ \int (x^3-3x^2+8x-11)e^x \; dx = \int P_3(x) e^x \; dx = Q_3(x)e^x\]
                
                e quindi che
                
                \[ P_3(x) = Q_3(x)+Q'_3(x)\]
                \[ x^3-3x^2+8x-11 = Q_3(x)+Q'_3(x)\]
                
                \item Inoltre, poiché $Q_3(x)$ è un polinomio di grado 3, ne segue che
                
                \[ Q_3(x) = \alpha x^3 + \beta x^2 + \gamma x + \delta\]
                
                e, poiché $Q'_3(x)$ corrisponde alla sua derivata, ne segue che
                
                \[ Q'_3(x) = 3 \alpha x^2 + 2 \beta x + \gamma\]
                
                \item Riscriviamo quindi l'equazione precedente come
                
                \[ P_3(x) = Q_3(x)+Q'_3(x)\]
                \[ x^3-3x^2+8x-11 = (\alpha x^3 + \beta x^2 + \gamma x + \delta)+(3 \alpha x^2 + 2 \beta x + \gamma)\]
                \[ x^3-3x^2+8x-11 = \alpha x^3 + (3\alpha+\beta)x^2 + (2\beta+\gamma)x + \gamma + \delta\]
                
                \item A questo punto, ci basta risolvere il sistema di equazioni per ottenere il risultato
                
                \begin{center}
                    $$
                    \left \{ \begin{array}{l}
                        x^3 = \alpha x^3 \\
                        -3x^2 = (3\alpha+\beta)x^2 \\
                        8x = (2\beta+\gamma)x \\
                        -11 = \gamma + \delta
                    \end{array}
                    \right .
                    \qquad\Longrightarrow\qquad
                    \left \{ \begin{array}{l}
                        \alpha = 1 \\
                        \beta = -6 \\
                        \gamma = 20 \\
                        \delta = -31
                    \end{array}
                    \right .
                    $$
                \end{center}
                
                \[ Q_3(x) = \alpha x^3 + \beta x^2 + \gamma x + \delta\]
                \[ Q_3(x) = x^3 -6x^2 + 20x - 31\]
                
                \item Infine, quindi, concludiamo che
                
                \[ \int (x^3-3x^2+8x-11)e^x \; dx = (x^3 -6x^2 + 20x - 31)e^x+c\]
            \end{itemize}
            
            \item Seguendo un ragionamento analogo, possiamo arrivare a formulare che
            
            \[ \int P_n(x)e^{kx} \; dx = Q_n(x)e^{kx}\]
            
            dove 
            
            \[ P_n(x) = k \cdot Q_n(x)+Q'_n(x) \]
        \end{itemize}
    \end{enumerate}
    
    \subsection{Integrazione di $\sin^k(x)$ e $\cos^k(x)$ }
    
    Di seguito vedremo due approcci consigliati per sviluppare gli integrali relativi alle funzioni trigonometriche del seno e del coseno a seconda del loro grado di elevazione.
    
    \begin{itemize}
        \item Se si ha un'integrale nella forma
        
        \[ \int \sin^k(x) \; dx \text{ \quad oppure \quad} \int \cos^k(x) \; dx\]
        
        dove $k$ è una potenza \textbf{dispari}, allora è consigliato procedere per \textbf{sostituzione}
        
        \item \textbf{Esempio:} (gli stessi passaggi possono essere utilizzati \textbf{anche con il coseno})
        
        \begin{itemize}
            \item Si consideri il seguente integrale
            
            \[ \int \sin^5(x) \; dx\]
            
            \item Possiamo riscrivere tale integrale come
            
            \[\int \sin^5(x) \; dx = \int \sin^4(x) \cdot \sin(x) \; dx = \int (\sin^2(x))^2 \sin(x) \; dx\]
            
            \item A questo punto, ricordando l'\textbf{identità trigonometrica fondamentale}, ossia $\sin^2(x) + \cos^2(x) = 1$, possiamo riscrivere l'integrale come
            
            \[ \int (\sin^2(x))^2  \sin(x) \; dx = \int (1-\cos^2(x))^2 \sin(x) \; dx\]
            
            \item Procedendo per \textbf{sostituzione}, otteniamo che
            
            \[ y=\cos(x)\]
            \[ dy = -\sin(x) \, dx \]
            
            \[ \int (1-\cos^2(x))^2 \sin(x) \; dx = -\int (1-y^2)^2 \; dy \]
            
            \item Siamo quindi riusciti a \textbf{ricondurre} l'integrale di una funzione trigonometrica ad un \textbf{integrale di un polinomio}, il quale risulta estremamente semplice da calcolare
            
            \[ -\int (1-y^2)^2 \; dy =  -\int 1 - 2y^2 + y^4 \; dy = -y + \frac{2}{3}y^3 - \frac{1}{5}y^5 = \]
            \[ -\cos(x)+\frac{2}{3}\cos^3(x)-\frac{1}{5}\cos^5(x)+c\]
        \end{itemize}
        
        \item Nel caso in cui $k$ sia una potenza \textbf{pari}, invece, è consigliato procedere per \textbf{parti}
        
        \item \textbf{Esempio:} (gli stessi passaggi possono essere utilizzati \textbf{anche con il coseno})
        
        \begin{itemize}
            \item Si consideri il seguente integrale
            
            \[ \int \sin^4(x) \; dx\]
            
            \item Possiamo riscrivere tale integrale come
            
            \[ \int \sin^4(x) \; dx = \int \sin^3(x) \sin(x) \; dx\]
            
            \item Procedendo per parti, otteniamo che
            
            \begin{itemize}
                \item $f'(x) = \sin(x) \Longrightarrow f(x) = -\cos(x)$
                \item $g(x) = \sin^3(x) \Longrightarrow g'(x) = 3sin^2(x)\cos(x)$
            \end{itemize}
            
            \[ \int \sin^4(x) \; dx = -\sin^3(x)\cos(x)+3\int \sin^2(x)\cos^2(x) \; dx\]
            
            \item Utilizzando l'\textbf{identità trigonometrica fondamentale} otteniamo che
            \[ \int \sin^4(x) \; dx = -\sin^3(x)\cos(x)+3\int \sin^2(x)(1-\sin^2(x)) \; dx\]
            \[ \int \sin^4(x) \; dx = -\sin^3(x)\cos(x)+3\int \sin^2(x) - 3 \int \sin^4(x) \; dx\]
            
            \item A questo punto portiamo l'integrale ottenuto dall'altra parte dell'equazione
            \[ \int \sin^4(x) \; dx + 3 \int \sin^4(x) \; dx = -\sin^3(x)\cos(x)+3\int \sin^2(x)\]
            \[ (1+3)\int \sin^4(x) \; dx = -\sin^3(x)\cos(x)+3\int \sin^2(x)\]
            \[ 4 \int \sin^4(x) \; dx = -\sin^3(x)\cos(x)+3\int \sin^2(x)\]
            \[ \int \sin^4(x) \; dx = \frac{1}{4} \left [ -\sin^3(x)\cos(x)+3\int \sin^2(x) \right ]\]
            
            \item Siamo quindi riusciti a \textbf{ridurre il grado} dell'integrale, poiché a questo punto ci resta da \textbf{calcolare l'integrale di $\sin^2(x)$}, calcolabile utilizzando lo \textbf{stesso metodo}, per ottenere l'integrale di $\sin^4(x)$
            
            \[ \frac{1}{4} \left [ -\sin^3(x)\cos(x)+3\int \sin^2(x) \right ] = \frac{-2sin^3(x)\cos(x)-3sin(x)\cos(x)+3x}{8}\]
        \end{itemize}
    \end{itemize}
    
    \subsection{Integrazione di funzioni razionali}
    
    Fino ad ora abbiamo visto solo situazioni in cui viene integrato il prodotto tra un polinomio ed un'altra funzione generica. Vediamo ora i casi in cui viene richiesto di integrare il \textbf{rapporto tra due polinomi}, esprimibile quindi con la forma generica
    
    \[ \int \frac{P(x)}{Q(x)} \; dx \]
    
    dove $P(x)$ e $Q(x)$ sono due polinomi (principalmente di grado 0, 1 o 2).
    
        \begin{enumerate}
            \item \textbf{Caso 1}: se il rapporto tra polinomi so trova nella seguente forma
            
            \[ \int \frac{\beta}{\gamma x + \delta} \; dx\]
            
            allora è possibile procedere nel seguente modo:
            
            \begin{itemize}
                \item Porto $\beta$ fuori dall'integrale e sostituisco per $y = \gamma x + \delta$
                
            \[ \int \frac{\beta}{\gamma x + \delta} \; dx = \beta \int \frac{1}{\gamma x + \delta} \; dx = \frac{\beta}{\gamma} \int \frac{1}{y} \; dy\]
            
            \item A questo punto sviluppo l'integrale e riapplico la sostituzione
            
            \[\frac{\beta}{\gamma} \int \frac{1}{y} \; dy = \frac{\beta}{\gamma} ln(\abs{y}) = \frac{\beta}{\gamma} ln(\abs{\gamma x + \delta}) \]
            
            \item Dunque, concludiamo che
            
            \[ \int \frac{\beta}{\gamma x + \delta} \; dx =  \frac{\beta}{\gamma} ln(\abs{\gamma x + \delta}) \]
            \end{itemize}
            
            \item \textbf{Caso 2}: se il rapporto tra polinomi so trova nella seguente forma (dove $\alpha \neq 0$)
            
            \[ \int \frac{\alpha x + \beta}{\gamma x + \delta} \; dx\]
            
            allora è possibile procedere nel seguente modo:
            
            \begin{itemize}
                \item Riscriviamo l'integrale nella seguente forma
                
                \[ \int \frac{\alpha x + \beta}{\gamma x + \delta} \; dx = \int \frac{\frac{\alpha}{\gamma} (\gamma x) + \beta}{\gamma x + \delta} \; dx = \int \frac{\frac{\alpha}{\gamma} (\gamma x + \delta - \delta) + \beta}{\gamma x + \delta} \; dx \]
                
                \item A questo punto, distribuiamo parzialmente per spezzare l'integrale
                
                \[ \int \frac{\frac{\alpha}{\gamma} (\gamma x + \delta) - \frac{\alpha}{\gamma} \delta +  \beta}{\gamma x + \delta} \; dx =
                \int \frac{\frac{\alpha}{\gamma} (\gamma x + \delta)}{\gamma x + \delta} \; dx + \int \frac{-\frac{\alpha}{\gamma} \delta +  \beta}{\gamma x + \delta} \; dx \]
                
                \item Dunque, semplifichiamo il semplificabile
                
                \[ \int \frac{\frac{\alpha}{\gamma} (\gamma x + \delta)}{\gamma x + \delta} \; dx + \int \frac{-\frac{\alpha}{\gamma} \delta +  \beta}{\gamma x + \delta} \; dx = \int \frac{\alpha}{\gamma} \; dx + \int \frac{\beta \gamma - \alpha \delta}{\gamma ( \gamma x + \delta)} \; dx\]
                
                \item Ed infine svolgiamo i due integrali ottenuti
                
                \[ \int \frac{\alpha}{\gamma} \; dx + \int \frac{\beta \gamma - \alpha \delta}{\gamma ( \gamma x + \delta)} \; dx = \frac{\alpha}{\gamma} x + \frac{\beta \gamma - \alpha \delta}{\gamma^2} \cdot ln(\abs{\gamma x + \delta})+c\]
                
                \item Dunque, concludiamo che
                
                \[ \int \frac{\alpha x + \beta}{\gamma x + \delta} \; dx = \frac{\alpha \gamma x + (\beta \gamma - \alpha \delta) \cdot ln(\abs{\gamma x + \delta})}{\gamma^2}+c\]
            \end{itemize}
            
            \quad
            
            \item \textbf{Caso 3}: se il rapporto tra polinomi so trova nella seguente forma (dove $\alpha \neq 0$)
            
            \[ \int \frac{1}{\alpha x^2 + \beta + \gamma} \; dx\]
            
            allora è necessario ricondurlo alla seguente forma
            
            \[ \int \frac{1}{ax^2 + bx + c} \; dx = \frac{1}{a} \int \frac{1}{x^2 + \frac{b}{a}x + \frac{c}{a}} \; dx = \frac{1}{a} \int \frac{1}{x^2 + \beta x + \gamma} \; dx\]
            
            dove $\beta = \frac{b}{a}$ e $\gamma = \frac{c}{a}$. Successivamente, sarà necessario sviluppare l'integrale ottenuto procedendo in base ad \textbf{uno dei tre casi elencati di seguito}:
            
            \begin{enumerate}
                \item \textbf{Caso 3.a}: se il \textbf{delta} di tale polinomio di secondo grado è \textbf{pari a zero} ($\Delta = 0$), allora le \textbf{radici del polinomio $x_1$ ed $x_2$} saranno uguali, dunque è possibile procedere nel seguente modo
                
                \begin{itemize}
                    \item Riscrivo l'integrale come
                    
                    \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \int \frac{1}{(x-x_1)^2} \; dx \qquad \text{ se e solo se } \Delta = 0\]
                    
                    \item A questo punto procedo per sostituzione ponendo $y = x-x_1$
                    
                    \[ \int \frac{1}{(x-x_1)^2} \; dx = \int \frac{1}{y}^2 \; dy = \int y^{-2} \; dy = - \frac{1}{y} = - \frac{1}{x-x_1}\]
                    
                    \item Dunque concludiamo che
                    
                    \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = - \frac{1}{x-x_1}+c \qquad \text{ se e solo se } \Delta = 0\]
                \end{itemize}
                
                \item \textbf{Caso 3.b}: se il \textbf{delta} di tale polinomio di secondo grado è \textbf{maggiore di zero} ($\Delta > 0$), allora le \textbf{radici del polinomio $x_1$ ed $x_2$} saranno diverse, dunque è possibile procedere nel seguente modo
                
                \begin{itemize}
                    \item Riscrivo l'integrale come
                    
                    \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \int \frac{1}{(x-x_1)(x-x_2)} \; dx \qquad \text{ se e solo se } \Delta > 0\]
                    
                    \item A questo punto, sfrutto una \textbf{proprietà matematica} secondo cui esistono due costanti $C$ e $D$ ($\exists C, D \in \mathbb{R})$ tali che
                    
                    \[ \frac{1}{(x-x_1)(x-x_2)} = \frac{C}{(x-x_1)}+ \frac{D}{x-x_2}\]
                
                    \item Per trovare tali costanti $C$ e $D$, è necessario considerare i seguenti due casi:
                    
                    \begin{itemize}
                        \item Se $x = x_1$, allora
                        
                        \[ \frac{1}{(x_1-x_1)(x_1-x_2)} = \frac{C}{(x_1-x_1)}+ \frac{D}{x_1-x_2}\]
                        
                        \[ \frac{1}{x_1-x_2} = C+ \frac{D(x_1-x_1)}{x_1-x_2}\]
                        
                        \[ \frac{1}{x_1-x_2} = C+ \frac{D \cdot 0}{x_1-x_2}\]
                        
                        \[ \frac{1}{x_1-x_2} = C\]
                        
                        \item Se $x = x_2$, allora
                        
                        \[ \frac{1}{(x_2-x_1)(x_2-x_2)} = \frac{C}{(x_2-x_1)}+ \frac{D}{x_2-x_2}\]
                        
                        \[ \frac{1}{x_2-x_1} = \frac{C(x_2-x_2)}{(x_2-x_1)}+D\]
                        
                        \[ \frac{1}{x_2-x_1} = \frac{C \cdot 0}{(x_2-x_1)}+D\]
                        \[ \frac{1}{x_2-x_1} = D\]
                    \end{itemize}
                    
                    \item Dunque, concludiamo che
                
                    \[ \frac{1}{(x-x_1)(x-x_2)} = \frac{C}{(x-x_1)}+ \frac{D}{x-x_2}\]
                
                    \[ \frac{1}{(x-x_1)(x-x_2)} = \frac{\frac{1}{x_1-x_2}}{(x-x_1)}+ \frac{\frac{1}{x_2-x_1}}{x-x_2}\]
                    
                    \[ \frac{1}{(x-x_1)(x-x_2)} = \frac{1}{(x-x_1)(x_1-x_2)}+ \frac{1}{(x-x_2)(x_2-x_1)}\]
                
                \item A questo punto, tornando all'integrale, abbiamo che
                
                \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \int \frac{1}{(x-x_1)(x_1-x_2)}+ \frac{1}{(x-x_2)(x_2-x_1)} \; dx = \]
                
                \[ = \frac{1}{x_1-x_2}\int \frac{1}{x-x_1}+ \frac{1}{x_2-x_1} \int \frac{1}{x-x_2} \; dx =\]
                
                \[ = \frac{1}{x_1-x_2} \left [ \int \frac{1}{x-x_1} - \int \frac{1}{x-x_2} \; dx \right ] =\]
                
                \[ = \frac{1}{x_1-x_2} \left [ ln(\abs{x-x_1}) - ln(\abs{x-x_2}) \right ] =\]
                
                \[= \frac{1}{x_1-x_2} \cdot ln \left ( \abs{\frac{x-x_1}{x-x_2}} \right )+c \qquad \text{ se e solo se } \Delta > 0\]
                
                \end{itemize}

                \newpage
                
                \item \textbf{Caso 3.c}: se il \textbf{delta} di tale polinomio di secondo grado è \textbf{minori di zero} ($\Delta < 0$), allora le \textbf{radici del polinomio $x_1$ ed $x_2$} saranno due \textbf{numeri complessi diversi}, dunque è possibile procedere nel seguente modo
                
                \begin{itemize}
                
                \item Riscriviamo il polinomio \textbf{completando il quadrato}
                \[ x^2 + \beta x + \gamma = x^2 + \beta x + \frac{\beta^2}{4} - \frac{\beta^2}{4} + \gamma = \left ( x + \frac{\beta}{2} \right )^2 + \frac{4 \gamma - \beta^2}{4} = \]
                
                \[ = \frac{4 \gamma - \beta^2}{4} \left ( \frac{ \left ( x + \frac{\beta}{2} \right )^2}{\frac{4 \gamma - \beta^2}{4}}  + 1 \right ) =
                \frac{4 \gamma - \beta^2}{4} \left ( \left ( \frac{2x+ \beta}{\sqrt{4 \gamma-\beta^2}} \right )^2  + 1 \right )\]
                
                    \item Dunque, riscriviamo l'integrale come
                    
                    \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \int \frac{1}{\frac{4 \gamma - \beta^2}{4} \left ( \left ( \frac{2x+ \beta}{\sqrt{4 \gamma-\beta^2}} \right )^2  + 1 \right )} \; dx\ \]
                    
                \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \frac{4}{4 \gamma - \beta^2} \int \frac{1}{\left (\frac{2x+ \beta}{\sqrt{4 \gamma-\beta^2}} \right )^2  + 1} \; dx \]
                
                \item A questo punto, procediamo per sostituzione
                
                \[ y = \frac{2x+ \beta}{\sqrt{4 \gamma-\beta^2}} \]
                \[ dy = \frac{2}{\sqrt{4 \gamma-\beta^2}} \, dx \]
                
                \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \frac{4}{(4 \gamma - \beta^2)\left ( \frac{2}{\sqrt{4 \gamma-\beta^2}} \right )} \int \frac{1}{y^2 + 1}\; dy= \]
                
                \[ = \frac{2}{\sqrt{4 \gamma - \beta^2}}  \cdot arctg(y) = \frac{2}{\sqrt{4 \gamma - \beta^2}} \cdot arctg \left ( \frac{2x+ \beta}{\sqrt{4 \gamma-\beta^2}}\right )+c\]
                
                \item Inoltre, dato che in questo caso abbiamo sempre $\Delta = \beta^2 - 4 \gamma$ (poiché $\alpha = 1$), possiamo riscrivere il tutto anche come
                
                \[ \int \frac{1}{x^2 + \beta x + \gamma} \; dx = \frac{2}{\sqrt{-\Delta}} \cdot arctg \left ( \frac{2x+ \beta}{\sqrt{-\Delta}}\right )+c \qquad \text{ se e solo se } \Delta < 0\]
                
                \end{itemize}
            \end{enumerate}
            
            \newpage
            
            \item \textbf{Caso 4:} Se $P(x)$ è un polinomio di \textbf{grado maggiore di 2}, allora è opportuno effettuare una \textbf{divisione tra polinomi}, in modo da \textbf{abbassare il grado} di $P(x)$ e poter rientrare in uno dei \textbf{precedenti casi}
            
            \[ \text{Poiché } P(x) = S(x) \cdot Q(x)+R(x)\]
            
            dove $S(x)$ è il \textbf{risultato della divisione} $\frac{P(x)}{Q(x)}$ e $R(x)$ è il suo \textbf{resto}, allora
            
            \[ \int \frac{P(x)}{Q(x)} \; dx = \int \frac{S(x) \cdot Q(x) + R(x)}{Q(x)} \; dx = \int S(x) \; dx + \int \frac{R(x)}{Q(x)} \; dx\]
            
            \textbf{Esempio:}
            \begin{itemize}
                \item Consideriamo il seguente integrale:
                
                \[ \int \frac{x^3}{x^2+2x+2} \; dx\]
                
                \item Effettuiamo la divisione tra polinomi:
                
                \begin{center}
                    \begin{tabular}{c|c}
                        \begin{tabular}{r}
                            $x^3 + 0 x^2 + 0 x + 0$\\
                            $x^3 + 2x^2 + 2x + 0$\\
                            \hline
                            $- 2x^2 - 2x + 0$\\
                            $- 2x^2 - 4x - 4$\\
                            \hline
                            $2x + 4$
                            
                        \end{tabular}
                        &
                        \begin{tabular}{l}
                        $x^2+2x+2$\\
                        \hline
                        $x-2$\\
                        \\
                        \\
                        \\
                        
                        \end{tabular}
                    \end{tabular}
                \end{center}
                
                Trovando quindi che $x^3$ è riscrivibile come $(x^2+2x+2)(x-2) + 2x + 4$
                
                \item Riscriviamo quindi l'integrale come
                
                \[ \int \frac{x^3}{x^2+2x+2} \; dx = \int \frac{(x^2+2x+2)(x-2)+2x+4}{x^2+2x+2} \; dx =\] \[ = \int x-2 \; dx + \int \frac{2x+4}{x^2+2x+2} \; dx\]
                
                \item A questo punto, siamo riusciti a ricondurre l'integrale originale alla \textbf{somma tra due integrali} di cui siamo in grado di calcolare la primitiva
                
                \[ \int x-2 \; dx + \int \frac{2x+4}{x^2+2x+2} \; dx = \frac{1}{2}x^2 -2x + \int \frac{2x+4-2+2}{x^2+2x+2} \; dx = \]
                \[ = \frac{1}{2}x^2 -2x + \int \frac{2x+2}{x^2+2x+2} \; dx + \int \frac{2}{x^2+2x+2} \; dx =\]
                \[ = \frac{1}{2}x^2 -2x + ln(\abs{x^2+2x+2}) + 2arctg(x+1) \; dx +c\]
            \end{itemize}
        \end{enumerate}
        
        \section{Studio di funzioni definite tramite integrali}
        
        Consideriamo il seguente integrale:
        
        \[ F(t) = \int_{0}^{t} \cos^2(x^3)+e^{x^2} \, dx\]
        
    Per definizione di \textbf{integrazione secondo Rienmann}, sappiamo che la funzione $g(x) = e^{x^2}$ sia integrabile poiché continua $\forall x \in \mathbb{R}$. Tuttavia, dopo alcune prove (si consiglia di tentare), notiamo come \textbf{non siamo in grado di trovare la primitiva} di tale funzione. Di conseguenza, concludiamo che la funzione $F(t)$ \textbf{esista}, tuttavia risultando \textbf{indefinita}.
    
    Tuttavia, poiché la funzione $F(t)$ esiste, possiamo trarre alcune conclusioni su essa:
    \begin{itemize}
        \item \textbf{$F(t)$ è ben definita?} Sì, poiché $f(x)$ è continua $\forall x \in \mathbb{R}$
        \item \textbf{$F(t)$ è derivabile?} Sì, poiché per il teorema fondamentale del calcolo integrale abbiamo che
        \[ F'(t) = f(t) = \cos^2(t^4)+e^{t^2}\]
        \item \textbf{Quanto vale $F(0)$?} Per definizione di integrale, sappiamo che
        \[ F(0) = \int_{0}^{0} \cos^2(x^3)+e^{x^2} \; dx = 0\]
        \item \textbf{Quanto vale $F'(1)$?} Poiché $F'(t) = f(t)$, abbiamo che
        \[ F'(1) = \cos^2(1^3)+e^{1^2} = \cos^2(1)+e\]
        \item \textbf{Quanto vale $F(1)$?} Non siamo in grado di rispondere, poiché la primitiva $F(t)$ è indefinita
        \item \textbf{$F(1)$ è maggiore o minore di 0?}
        Ricordando le proprietà di monotonia degli integrali, possiamo affermare che $F(1) \geq 0 \Longleftrightarrow f(1) \geq 0$:
        \begin{itemize}
            \item Sapendo che $\cos^2(x^3) \geq 0, \, \forall x \in \mathbb{R}$ e che $e^{x^2} \geq 1 \, \forall x \in \mathbb{R}$, concludiamo che $f(x) \geq 1\, \forall x \in \mathbb{R}$
            \item Poiché che $f(x) > 0\, \forall x \in \mathbb{}$, ne traiamo che che $F(x) > 0\, \forall x \in \mathbb{R}$, dunque $F(1) \geq 1$
        \end{itemize}
        \item \textbf{$F(-2)$ è maggiore o minore di 0?} Prima di procedere analogamente al punto precedente, è necessario ricordare che
        
        \[ F(-2) = \int_{0}^{-2} f(x) \; dx = - \int_{-2}^{0} f(x) \; dx\]
        
        \begin{itemize}
            \item Come abbiamo visto nel punto precedente, sappiamo che $F(t) > 0\, \forall x \in \mathbb{R}$
            \item Dunque, poiché abbiamo invertito il segno dell'integrale come conseguenza dell'inversione dell'intervallo di integrazione, ne segue che $F(-2) < 0$
        \end{itemize}
        
        \item \textbf{$F(t)$ è crescente?} Sì, poiché la sua derivata (ossia $f(x)$) è sempre positiva
        
        \item \textbf{$F(t)$ è pari o dispari?} Per rispondere a questa domanda è necessario ricordare che
        \begin{itemize}
            \item Se una funzione è pari, allora $f(x) = f(-x)$
            \item Se una funzione è dispari, allora $f(-x) = -f(x)$
        \end{itemize}
        Dunque, per verificare se $F(t)$ sia pari o dispari, dobbiamo verificare se $F(t) = F(-t)$ oppure se $F(-t) = -F(t)$.
        
        Prima di procedere, è necessario evidenziare che $f(x) = \cos^2(x^3) + e^{x^2}$ è una \textbf{funzione pari}, dunque l'\textbf{intervallo} $[0, t]$ sarà una versione \textbf{specchiata e coincidente dell'intervallo }$[-t, 0]$.
        
        A questo punto, possiamo quindi affermare che 
        \[ \int_{0}^{t} \cos^2(x^3)+e^{x^2} \, dx = \int_{-t}^{0} \cos^2(x^3)+e^{x^2} \, dx \]
        
        \[ \underbrace{\int_{0}^{t} \cos^2(x^3)+e^{x^2} \, dx}_{F(t)} = \underbrace{- \int_{0}^{-t} \cos^2(x^3)+e^{x^2} \, dx}_{-F(-t)}\]
        
        \[ F(t) = -F(-t)\]
        \[ -F(t) = F(-t)\]
        
        Poiché abbiamo ottenuto che $F(-t) = -F(t)$, possiamo concludere che $F(t)$ è una funzione dispari.
        
    \end{itemize}
    
    \quad
    
    Generalizzando la dimostrazione dell'ultimo esempio, in  possiamo definire il seguente \textbf{teorema}:
    
    \begin{framedthm}{Integrazione di funzioni pari e dispari (parte 1)}
        Sia $F(x)$ definita come
        \[ F(t) = \int_{0}^{t} f(x) \; dx \]
        
        \begin{itemize}
            \item Se $f(x)$ è una \textbf{funzione pari}, allora $F(t)$ è una \textbf{funzione dispari}.
            
            \item Se $f(x)$ è una \textbf{funzione dispari}, allora $F(t)$ è una \textbf{funzione pari}.
            
        \end{itemize}
        
        \textit{Attenzione}: affinché valga tale teorema è strettamente necessario che l'intervallo di integrazione sia $[0, t]$
    \end{framedthm}
    
    Inoltre, tramite il teorema appena stipulato, possiamo affermare un secondo teorema:
    
    \begin{framedthm}{Integrazione di funzioni pari e dispari (parte 2)}
        Se viene integrata una funzione $f(x)$ in un intervallo $[-t, t]$, allora
        \begin{itemize}
            \item Se $f(x)$ è \textbf{dispari} si ha che
            \[ \int_{-t}^{t} f(x) \; dx = \int_{0}^{t} f(x) \; dx - \int_{0}^{-t} f(x) \; dx = 0\]
            \item Se $f(x)$ è \textbf{pari} si ha che
            \[ \int_{-t}^{t} f(x) \; dx = \int_{0}^{t} f(x) \; dx + \int_{-t}^{0} f(x) \; dx =2\int_{0}^{t} f(x) \; dx \;\]
        \end{itemize}
    \end{framedthm}
    
    Tuttavia, alcune volte è necessario fare \textbf{attenzione} nell'utilizzo di questi teoremi:
    
    \begin{itemize}
        \item Consideriamo l'integrale
        
        \[ \int_{-1}^{1} \frac{1}{x} \; dx\]
        
        \item Poiché $f(x)$ è dispari e l'intervallo di integrazione è $[-1, 1]$, applicando il teorema precedente otterremmo che $F(1) - F(-1) = 0$.
        
        \item Tuttavia, tale conclusione è errata, poiché la funzione \textbf{non è integrabile secondo Rienmann} nell'intervallo $[-1, 1]$, vista la presenza di una \textbf{discontinuità illimitata in } $x = 0$.
        
        \item Dunque, nonostante il teorema, abbiamo che
        
        \[ \int_{-1}^{1} \frac{1}{x} \; dx = \text{ Non Integrabile secondo Rienmann}\]
    \end{itemize}
    
    \newpage
    
    \section{Integrali impropri}
    
    Riprendiamo l'integrale visto alla fine della sezione precedente, il quale abbiamo già decretato \textbf{non integrabile}.
    
    \[ \int_{-1}^{1} \frac{1}{x} \; dx = \text{ Non Integrabile secondo Rienmann}\]
    
    Abbiamo visto come il motivo di ciò sia la presenza di una \textbf{discontinuità illimitata nel punto }$x = 0$, rendendo quindi inapplicabile per definizione stessa l'integrazione secondo Rienmann.
    
    Proviamo a \textbf{spezzare l'integrale} su due intervalli:
    
    \[ \int_{-1}^{1} \frac{1}{x} \; dx = \int_{-1}^{0} \frac{1}{x} \; dx + \int_{0}^{1} \frac{1}{x} \; dx\]
    
    Notiamo come anche i due integrali ottenuti \textbf{non siano integrabili secondo Rienmann}, sempre per via della presenza della discontinuità in $x = 0$.
    
    Tuttavia, come ci insegna l'analisi matematica, quando una cosa \textbf{non è possibile in modo strettamente matematico}, possiamo andare a studiare il comportamento di una funzione nel momento in cui ci \textbf{avviciniamo al punto problematico} (ossia andando ad effettuare il \textbf{limite}).
    
    Consideriamo solo il primo dei due integrali individuati, il quale sappiamo già non essere integrabile
    \[ \int_{-1}^{0} \frac{1}{x} \; dx = \left . ln(\abs{x}) \right |_{-1}^{0} = ln(0) - ln(1) = ln(0) = \text{Non esiste}\]
    
    Aggiungiamo all'estremo superiore in $0$ una \textbf{costante molto piccola $\varepsilon$}
    
    \[ \int_{-1}^{0+\varepsilon} \frac{1}{x} \; dx = \int_{-1}^{\varepsilon} \frac{1}{x} \; dx \]
    
    L'integrale risulta definito nell'intervallo $[-1, \varepsilon]$, rendendolo quindi \textbf{perfettamente integrabile secondo le condizioni di Rienmann}.
    
    A questo punto, facciamo tendere $\varepsilon \to 0^+$
    
    \[ \lim_{\varepsilon \to 0^+} \int_{-1}^{\varepsilon} \frac{1}{x} \; dx = \lim_{\varepsilon \to 0^+} \left . ln(\abs{x}) \right |_{-1}^{\varepsilon} = \lim_{\varepsilon \to 0^+} ln(\varepsilon) - ln(1) = \lim_{\varepsilon \to 0^+} ln(\varepsilon) = -\infty\]
    
    Abbiamo ottenuto, quindi, che l'area sotto la funzione $f(x)$ in $[-1, 0]$ è \textbf{divergente}, ossia \textbf{non ammette limite finito}. A questo punto, possiamo anche non studiare il comportamento del secondo integrale, poiché otterremmo una situazione $\infty - \infty$, che risulta comunque essere \textbf{divergente}, indicando comunque che si tratta di un \textbf{integrale di area illimitata}.
    
    Diamo quindi una definizione di \textbf{integrazione in senso improprio}:
    
    \begin{frameddefn}{Integrazione in senso improprio}
        Sia $f(x)$ una funzione con una \textbf{discontinuità illimitata} nel punto $x=b$.
        
        Di conseguenza, essa non è integrabile secondo Rienmann nell'intervallo $[a, b]$:
        
        \[ \int_{a}^{b} f(x) \; dx = \text{ Non integrabile secondo Rienmann}\]
        
        Allora, si dice che $f(x)$ \textbf{è integrabile in senso improprio} in $[a, b]$ se l'integrale per $[a, b-\varepsilon]$ dove $\varepsilon \to 0^+$ \textbf{ammette limite finito}:
        \[ \lim_{\varepsilon \to 0^+} \int_{a}^{b-\varepsilon} f(x) \; dx = \lim_{\varepsilon \to 0^+} F(x) |_{a}^{b-\varepsilon} = \ell\]
        
        \textit{Si ricorda che con limite finito si intende un limite diverso da $+\infty, - \infty $ e $\nexists$}
        
    \end{frameddefn}
    
    \quad
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Si stabilisca se il seguente integrale è integrabile secondo Rienmann o in senso improprio o se non sia integrabile:
        
        \[\int_{0}^{1} \frac{1}{\sqrt{x}} \; dx\]
        
        \begin{itemize}
            \item Poiché è presente una discontinuità illimitata in $x=0$, sappiamo che $f(x)$ non è integrabile secondo Rienmann in $[0, 1]$
            \item Proviamo quindi a vedere se sia integrabile in senso improprio
            
            \[\lim_{a \to 0^+}\int_{a}^{1} \frac{1}{\sqrt{x}} \; dx = \lim_{a \to 0^+}  \left . 2\sqrt{x} \right |_{a}^{1} = \lim_{a \to 0^+} 2 - 2\sqrt{a} = 2\]
            
            \item Siccome ammette limite finito, allora $f(x)$ \textbf{è integrabile in senso improprio in }$[0, 1]$ 
        \end{itemize}
        
        
        \item Si stabilisca se il seguente integrale è integrabile secondo Rienmann o in senso improprio o se non sia integrabile:
        
        \[\int_{0}^{\infty} e^{-3x} \; dx\]
        
        \begin{itemize}
            \item Poiché stiamo integrando un intervallo illimitato, sappiamo già che $f(x)$ non è integrabile secondo Rienmann in $[0, +\infty]$
            
            \item Vediamo quindi se sia integrabile in senso improprio
            
            \[\lim_{b \to +\infty} \int_{0}^{b} e^{-3x} \; dx = \lim_{b \to +\infty} \left . \frac{e^{-3x}}{-3} \right |_{0}^{b} = \lim_{b \to +\infty} \frac{1}{-3e^{3b}} + \frac{1}{3} = \frac{1}{3}\]
            
            \item Siccome ammette limite finito, allora $f(x)$ \textbf{è integrabile in senso improprio in} $[0, +\infty]$
            
        \end{itemize}\item Si stabilisca se il seguente integrale è integrabile secondo Rienmann o in senso improprio o se non sia integrabile:
        
        \[\int_{-1}^{1} \frac{1}{x^2+2x+1} \; dx\]
        
        \begin{itemize}
            \item Notiamo come l'integrale sia riscrivibile come
            
            \[\int_{-1}^{1} \frac{1}{x^2+2x+1} \; dx = \int_{-1}^{1} \frac{1}{(x+1)^2} \; dx\]
            
            \item Dunque, essendo $x=-1$ una radice del polinomio al denominatore, $f(x)$ non è integrabile secondo Rienmann in $[-1, 1]$
            
            \item Vediamo quindi se sia integrabile in senso improprio
            
            \[\lim_{a \to -1^+} \int_{a}^{1} \frac{1}{x^2+2x+1} \; dx = \lim_{a \to -1^+} \int_{a}^{1} \frac{1}{(x+1)^2} \; dx = \lim_{a \to -1^+} \left . -\frac{1}{x+1} \right |_{a}^{1} =\]
            \[ = \lim_{a \to -1^+} -\frac{1}{2}+\frac{1}{a+1} = -\frac{1}{2} + \infty = +\infty\]
            
            \item Siccome non ammette limite finito, allora $f(x)$ \textbf{non è integrabile in senso improprio in} $[-1, +1]$, dunque $f(x)$ non è integrabile in alcun modo (diverge).
        \end{itemize}
    \end{itemize}
    
    \newpage
    
    \subsection{Convergenza degli integrali impropri}
    
    Nella sezione precedente, abbiamo visto alcuni integrali riconducibili alla seguente \textbf{forma generica}:
    
    \[ \int_{0}^{t} \frac{1}{x^{\alpha}} \; dx = \int_{0}^{t} x^{-\alpha} \; dx\]
    
    Essendo $x=0$ un punto di discontinuità, sappiamo già che tale integrale \textbf{non è integrabile secondo Rienmann}. Vediamo quindi il suo comportamento in \textbf{senso improprio}:
    
    \[ \lim_{\varepsilon \to 0^+} \int_{\varepsilon}^{t} x^{-\alpha} \; dx\]
    
    A questo punto, in base al valore assunto da $\alpha$, integrale può svilupparsi in due modi:
    
    \begin{itemize}
        \item Se $\alpha = 1$, allora:
        \[\lim_{\varepsilon \to 0^+} \int_{\varepsilon}^{t} x^{-\alpha} \; dx =
        \lim_{\varepsilon \to 0^+} \left . ln(\abs{x}) \right |_{\varepsilon}^{t} = \lim_{\varepsilon \to 0^+} ln(t) - ln(\varepsilon) = +\infty\]
        
        \item Se $\alpha \neq 1$, allora:
        \[\lim_{\varepsilon \to 0^+} \int_{\varepsilon}^{t} x^{-\alpha} \; dx =
        \lim_{\varepsilon \to 0^+} \left . \frac{x^{-\alpha+1}}{-\alpha+1} \right |_{\varepsilon}^{t} = \lim_{\varepsilon \to 0^+} \frac{t^{-\alpha+1}}{-\alpha+1} - \frac{\varepsilon^{-\alpha+1}}{-\alpha+1} = \lim_{\varepsilon \to 0^+} \frac{t^{1-\alpha}-\varepsilon^{1-\alpha}}{1-\alpha} \]
        
        \begin{itemize}
            \item Se $\alpha < 1$, allora ne segue che che $1-\alpha > 0$. Dunque, poiché
            \[ \lim_{\varepsilon \to 0^+} \varepsilon^{1-\alpha} = 0\]
            avremo che
            \[\lim_{\varepsilon \to 0^+} \frac{t^{1-\alpha}-\varepsilon^{1-\alpha}}{1-\alpha} = \frac{t^{1-\alpha}+0}{1-\alpha} = \frac{t^{1-\alpha}}{1-\alpha}\]
            
            \item Se $\alpha > 1$, allora ne segue che che $1-\alpha < 0$. Dunque, poiché
            \[ \lim_{\varepsilon \to 0^+} \varepsilon^{1-\alpha} = +\infty\]
            avremo che
            \[\lim_{\varepsilon \to 0^+} \frac{t^{1-\alpha}-\varepsilon^{1-\alpha}}{1-\alpha} = \frac{t^{1-\alpha}+\infty}{1-\alpha} = +\infty\]
        \end{itemize}
    \end{itemize}
    
    Infine, possiamo concludere che 
    
    $$
    \lim_{\varepsilon \to 0^+} \int_{\varepsilon}^{t} \frac{1}{x^{\alpha}} \; dx = \left \{ \begin{array}{l l}
            \frac{t^{1-\alpha}}{1-\alpha} & \text{ se } \alpha < 1 \\
            +\infty & \text{ se } \alpha \geq 1
    \end{array} \right .
    $$
    
    dunque, l'integrale \textbf{converge} ad un valore finito se $\alpha < 1$ e \textbf{diverge} se $\alpha \geq 1$.
    
    Consideriamo invece la stessa funzione generica ma con un \textbf{intervallo di integrazione illimitato}, risultante di conseguenza nel seguente integrale improprio:
    
    \[ \int_{t}^{\infty} \frac{1}{x^{\alpha}} \; dx = \lim_{\varepsilon \to +\infty}  \int_{t}^{\varepsilon} \frac{1}{x^{\alpha}} \; dx\]
    
    A questo punto, analogamente al caso precedente, in base al valore assunto da $\alpha$ l'integrale può svilupparsi in due modi:
    
        \begin{itemize}
        \item Se $\alpha = 1$, allora:
        \[\lim_{\varepsilon \to +\infty} \int_{t}^{\varepsilon} x^{-\alpha} \; dx =
        \lim_{\varepsilon \to +\infty} \left . ln(\abs{x}) \right |_{t}^{\varepsilon} = \lim_{\varepsilon \to +\infty} ln(\varepsilon) - ln(t) = +\infty\]
        
        \item Se $\alpha \neq 1$, allora:
        \[\lim_{\varepsilon \to +\infty} \int_{t}^{\varepsilon} x^{-\alpha} \; dx =
        \lim_{\varepsilon \to +\infty} \left . \frac{x^{-\alpha+1}}{-\alpha+1} \right |_{t}^{\varepsilon} = \lim_{\varepsilon \to +\infty}  \frac{\varepsilon^{-\alpha+1}}{-\alpha+1} - \frac{t^{-\alpha+1}}{-\alpha+1} = \lim_{\varepsilon \to +\infty} \frac{\varepsilon^{1-\alpha}- t^{1-\alpha}}{1-\alpha} \]
        
        \begin{itemize}
            \item Se $\alpha < 1$, allora ne segue che che $1-\alpha > 0$. Dunque, poiché
            \[ \lim_{\varepsilon \to +\infty} \varepsilon^{1-\alpha} = +\infty\]
            avremo che
            \[\lim_{\varepsilon \to +\infty} \frac{\varepsilon^{1-\alpha}-t^{1-\alpha}}{1-\alpha} = \frac{+\infty-t^{1-\alpha}}{1-\alpha} = +\infty\]
            
            \item Se $\alpha > 1$, allora ne segue che che $1-\alpha < 0$. Dunque, poiché
            \[ \lim_{\varepsilon \to +\infty} \varepsilon^{1-\alpha} = 0\]
            avremo che
            \[\lim_{\varepsilon \to +\infty} \frac{\varepsilon^{1-\alpha}-t^{1-\alpha}}{1-\alpha} = \frac{0-t^{1-\alpha}}{1-\alpha} = \frac{t^{1-\alpha}}{\alpha-1}\]
        \end{itemize}
    \end{itemize}
    
    
    Quindi, possiamo concludere che 
    
    $$
    \lim_{\varepsilon \to +\infty} \int_{t}^{\varepsilon} \frac{1}{x^{\alpha}} \; dx = \left \{ \begin{array}{l l}
            \frac{t^{1-\alpha}}{\alpha-1} & \text{ se } \alpha > 1 \\
            +\infty & \text{ se } \alpha \leq 1
    \end{array} \right .
    $$
    
    dunque, inversamente al caso precedente, l'integrale \textbf{converge} ad un valore finito se $\alpha > 1$ e \textbf{diverge} se $\alpha \leq 1$.
    
    \newpage
    
    \subsection{Criteri di convergenza degli integrali}
    
    Consideriamo il seguente integrale:
    
    \[ \int_{1}^{3} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx\]
    
    Notiamo come la funzione $f(x)$ risulta essere \textbf{illimitata in entrambi gli estremi} dell'intervallo $[1, 3]$. \textbf{Spezziamo} quindi l'integrale per poterne studiare separatamente il comportamento in entrambi i punti illimitati:
    
    \[ \int_{1}^{3} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx = \int_{1}^{2} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx + \int_{2}^{3} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx\]
    
    A questo punto, vogliamo \textbf{determinare} se entrambi gli integrali ottenuti siano \textbf{convergenti} o \textbf{divergenti}, senza andare a calcolare il valore specifico dell'area sotto la funzione.
    
    Analizziamo quindi uno per volta entrambi gli integrali:
    
    \begin{itemize}
        \item Nell'intervallo $[1, 2]$, la radice $\sqrt[3]{3-x}$ presente all'interno della funzione non dà problemi in nessuno dei due estremi. Poiché esso non influenza l'illimitatezza dell'intervallo, esso \textbf{non influenzerà neanche la convergenza dell'integrale}.
        
        Possiamo quindi dire che i seguenti due integrali \textbf{si comportano allo stesso modo nell'intervallo} $[1, 2]$:
        
        \[ \int_{1}^{2} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx \approx \int_{1}^{2} \frac{1}{\sqrt{x-1}} \; dx\]
        
        Dunque, possiamo dire che se l'\textbf{integrale di destra} è \textbf{convergente}, allora lo è anche l'\textbf{integrale di sinistra}.
        
        \[ \int_{1}^{2} \frac{1}{\sqrt{x-1}} \; dx = \int_{0}^{1} \frac{1}{\sqrt{y}} \; dy < +\infty \text{ poiché $\alpha < 0$}\]
        
        Quindi concludiamo che:
        \[ \int_{1}^{2} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx < +\infty\]
        
        
        \item Nell'intervallo $[2, 3]$, la radice $\sqrt{x-1}$ presente all'interno della funzione non dà problemi in nessuno dei due estremi. Poiché esso non influenza l'illimitatezza dell'intervallo, esso \textbf{non influenzerà neanche la convergenza dell'integrale}.
        
        Possiamo quindi dire che i seguenti due integrali \textbf{si comportano allo stesso modo nell'intervallo} $[2, 3]$:
        \[ \int_{2}^{3} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx \approx \int_{2}^{3} \frac{1}{\sqrt[3]{3-x}} \; dx\]
        
        Dunque, possiamo dire che se l'\textbf{integrale di destra} è \textbf{convergente}, allora lo è anche l'\textbf{integrale di sinistra}.
        \[ \int_{2}^{3} \frac{1}{\sqrt[3]{3-x}} \; dx = \int_{2}^{3} \frac{1}{\sqrt[3]{y}} \; dy < +\infty \text{ poiché $\alpha < 0$}\]
        
        Quindi concludiamo che:
        \[ \int_{2}^{3} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx < +\infty\]
        
        \item Poiché \textbf{entrambi gli integrali sono convergenti}, possiamo affermare che
        \[ \int_{1}^{3} \frac{1}{\sqrt{x-1} \sqrt[3]{3-x}} \; dx < +\infty\]
        
    \end{itemize}
    
    \quad
    
    \begin{framedthm}{Criterio del confronto asintotico}

        Siano $f(x)$ e $g(x)$ due funzioni tali che in $x_0$ vi sia un punto illimitato. Dato $\ell \in (0, +\infty)$, si ha che
        \[ \lim_{x \to x_0} \frac{f(x)}{g(x)} = \ell \implies \int_{a}^{x_0} f(x) \; dx \approx \int_{a}^{x_0} g(x) \; dx\]
        
        allora l'integrale di $g(x)$ \textbf{converge} se e solo se l'integrale $f(x)$ \textbf{converge}
    \end{framedthm}
        
    \begin{framedthm}{Criterio del confronto diretto}
        Siano $f(x)$ e $g(x)$ due funzioni tali che in $x_0$ vi sia un punto illimitato. Se si verifica che
        \[ 0 \leq \int_{a}^{x_0} f(x) \; dx \leq \int_{a}^{x_0} g(x) \; dx\]
        
        allora se l'integrale di $g(x)$ \textbf{converge}, allora anche quello di $f(x)$ \textbf{converge}
    \end{framedthm}
    
    \begin{framedthm}{Criterio di convergenza assoluta}
        Sia $f(x)$ una funzioni tale che in $x_0$ vi sia un punto illimitato. Si ha che:
        \[ \int_{a}^{x_0} \abs{f(x)} \; dx < +\infty \implies \int_{a}^{x_0} f(x) \; dx < +\infty\]
    \end{framedthm}

    \newpage

    \textbf{Esempio:}
    
    Consideriamo il seguente integrale
        
    \[ \int_{0}^{\infty} \frac{1}{\sqrt{x}+x^2} \; dx\]
    
    \begin{itemize}
        \item La funzione è illimitata in entrambi gli estremi, quindi spezziamo l'integrale
        \[ \int_{0}^{\infty} \frac{1}{\sqrt{x}+x^2} \; dx = \int_{0}^{1} \frac{1}{\sqrt{x}+x^2} \; dx + \int_{1}^{\infty} \frac{1}{\sqrt{x}+x^2} \; dx\]
        
        \item Siccome per $x \to 0^+$ la funzione $\frac{1}{\sqrt{x}}$ \textbf{tende a $0$ meno velocemente rispetto a }$\frac{1}{x^2}$, la mettiamo in evidenza nel primo integrale. Nel secondo integrale, invece, mettiamo in evidenza la funzione $\frac{1}{x^2}$ siccome per $x \to +\infty$ \textbf{tende a 0 più velocemente rispetto a} $\frac{1}{\sqrt{x}}$ (non è fondamentale il modo in cui vengono messi in evidenza).
    \end{itemize}
    \[ \int_{0}^{1} \frac{1}{\sqrt{x}+x^2} \; dx + \int_{1}^{\infty} \frac{1}{\sqrt{x}+x^2} \; dx = \int_{0}^{1} \frac{1}{\sqrt{x}} \left ( \frac{1}{1+x^{\frac{3}{2}}} \right ) \; dx + \int_{1}^{\infty} \frac{1}{x^2} \left ( \frac{1}{x^{-\frac{3}{2}} + 1} \right ) \; dx\]
    
    \begin{itemize}
        \item A questo punto, studiamo separatamente i due integrali applicando il confronto asintotico:
        
        \begin{itemize}
            \item Sapendo che
            \[ \lim_{x \to 0} \frac{\frac{1}{\sqrt{x}} \left ( \frac{1}{1+x^{\frac{3}{2}}} \right )}{\frac{1}{\sqrt{x}}} = \lim_{x \to 0} \frac{1}{1+x^{\frac{3}{2}}} = \lim_{x \to 0} \frac{1}{1+0} = 1\]
            
            allora possiamo dire che
            
            \[ \int_{0}^{1} \frac{1}{\sqrt{x}} \left ( \frac{1}{1+x^{\frac{3}{2}}} \right ) \; dx \approx \int_{0}^{1} \frac{1}{\sqrt{x}} \; dx\]
            
            che sappiamo \textbf{convergere} poiché $\alpha < 0$
            
            \item Sapendo che
            \[ \lim_{x \to +\infty} \frac{\frac{1}{x^2} \left ( \frac{1}{x^{-\frac{3}{2}} + 1} \right )}{\frac{1}{x^2}} = \lim_{x \to +\infty} \frac{1}{x^{-\frac{3}{2}} + 1} = \lim_{x \to +\infty} \frac{1}{0 + 1} = 1\]
            
            allora possiamo dire che
            
            \[ \int_{1}^{\infty} \frac{1}{x^2} \left ( \frac{1}{x^{-\frac{3}{2}} + 1} \right ) \; dx \approx \int_{1}^{\infty} \frac{1}{x^2} \; dx\]
            
            che sappiamo \textbf{convergere} poiché $\alpha > 0$
        \end{itemize}
        
        \item Siccome entrambi gli integrali convergono, allora ne deriva che
        \[ \int_{0}^{\infty} \frac{1}{\sqrt{x}+x^2} \; dx < +\infty\]
    \end{itemize}
    
        \section{Cheatsheet riassuntivo per integrali rapidi}
        
        \begin{tabular}{m{0.45 \textwidth} | m{0.45 \textwidth}}
        \[\textbf{Funzione}: f(x)\] & \[\textbf{Primitiva}: F(x)\] \\
        \hline
        \[  \int x^{\alpha} \; dx \quad \text{ se } \alpha \neq -1\] & \[\frac{x^{\alpha +1}}{\alpha+1} + c\]\\
        \[ \int \sin(\alpha x)\; dx \] & \[\frac{-\cos(\alpha x)}{\alpha} + c\] \\
        \[ \int \cos(\alpha x)\; dx \] & \[\frac{\sin(\alpha x)}{\alpha} + c\] \\
        \[ \int \frac{1}{\cos^2(x)}\; dx \] & \[tg(x) + c\]\\
        \[ \int \frac{1}{1+x^2}\; dx \] & \[arctg(x)+c\]\\
        \[ \int  e^{\alpha x}\; dx \] & \[ \frac{e^{\alpha x}}{\alpha} + c\] \\
        \[ \int \alpha^x\; dx \] & \[\frac{\alpha^x}{ln(\alpha)}+c\] \\
        \[ \int \frac{1}{\alpha x + \beta} \; dx \] & \[\frac{ln(\abs{\alpha x + \beta})}{\alpha}+c\] \\
        \[ \int  \frac{g'(x)}{g(x)} \; dx \] & \[ ln(\abs{g(x)})+c\] \\
        \[ \int  e^{\alpha x} \cos(\beta x)\; dx \] & \[ \frac{e^{\alpha x}(\alpha \cos(\beta x) + \beta \sin(\beta x))}{\beta^2 + \alpha^2}+c\] \\
        \[ \int  e^{\alpha x} \sin(\beta x)\; dx \] & \[ \frac{e^{\alpha x}(\alpha \sin(\beta x) - \beta \cos(\beta x))}{\beta^2 + \alpha^2}+c\] \\
        
        \end{tabular}
        
        \begin{tabular}{m{0.45 \textwidth} | m{0.45 \textwidth}}
        $$\textbf{Funzione}: f(x)$$ & $$\textbf{Primitiva}: F(x)$$\\
        \hline
        
        $$
        \begin{array}{c}
            \int P_n(x) e^{\alpha x} \; dx \\
            \text{dove } P_n(x) = \alpha Q_n(x) + Q'_n(x)
        \end{array}
        $$ &
        
        $$
        \begin{array}{c}
            Q_n(x) e^{\alpha x}+c
        \end{array}
        $$\\
        
        $$
        \begin{array}{c}
            \int P(x) \cos(\alpha x) \; dx\\
            \text{oppure} \\
            \int P(x) \sin(\alpha x) \; dx
        \end{array}
        $$ &
        
        \[ \text{Derivare } P(x) \text{ e integrare $\cos$ o $\sin$} \] \\
        
        $$
        \begin{array}{c}
            \int P(x) ln(\alpha x)\; dx \\
            \text{oppure} \\
            \int P(x) arctg(\alpha x)\; dx
        \end{array}
        $$ &
        
        \[ \text{Integrare } P(x) \text{ e derivare $ln$ o $arctg$} \]
        \\
        
            \[ \int  \frac{\alpha x + \beta}{\gamma x + \delta}\; dx \quad \text{ se } \alpha \neq 0\]
            & \[\frac{\alpha \gamma x + (\beta \gamma - \alpha \delta) \cdot ln(\abs{\gamma x + \delta})}{\gamma^2}+c\]
            
            \\
            
        \[ \int \frac{1}{x^2 + \beta x + \gamma}\; dx
        \quad \text{ se } \Delta = 0\] &
        \[ -\frac{1}{x-x_1}+c\]
        \\
        
            
        \[ \int \frac{1}{x^2 + \beta x + \gamma}\; dx
        \quad \text{  se } \Delta > 0\] &
        \[ \frac{1}{x_1-x_2} \cdot ln \left ( \abs{\frac{x-x_1}{x-x_2}} \right )+c \]
        \\
        
            
        \[ \int \frac{1}{x^2 + \beta x + \gamma}\; dx
        \quad \text{ se } \Delta < 0\] &
        \[ \frac{2}{\sqrt{-\Delta}} \cdot arctg \left ( \frac{2x+ \beta}{\sqrt{-\Delta}}\right )+c \] \\
        
        \[\int \frac{P(x)}{Q(x)}\; dx
        \quad \text{ se grado } P(x) \geq 2\] &
        \[ \int S(x) \; dx + \int \frac{R(x)}{Q(x)} \; dx \] \\
        
        \[ \lim_{\varepsilon \to 0^+} \int_{\varepsilon}^{t} \frac{1}{x^{\alpha}} \; dx \] &
        $$
        \left . \begin{array}{l l}
                \frac{t^{1-\alpha}}{1-\alpha} & \text{ se } \alpha < 1 \\
                +\infty & \text{ se } \alpha \geq 1
        \end{array} \right .
        $$ \\
        
        \[ \lim_{\varepsilon \to +\infty} \int_{t}^{\varepsilon} \frac{1}{x^{\alpha}} \; dx \] &
        $$
        \left . \begin{array}{l l}
                \frac{t^{1-\alpha}}{\alpha-1} & \text{ se } \alpha > 1 \\
                +\infty & \text{ se } \alpha \leq 1
        \end{array} \right .
        $$
        
        \end{tabular}
        
        \newpage
        
        \chapter{Equazioni differenziali}
        
        \section{Equazioni differenziali ordinarie}
        
        Giunti a questo punto dello studio dell'Analisi Matematica, siamo finalmente pronti ad affrontare quelle che sono le \textbf{\textit{vere}} equazioni descriventi il mondo reale, ossia le \textbf{equazioni differenziali}.
        
        Fino a questo giorno, abbiamo parlato di \textbf{equazioni algebriche}, dove veniva richiesto di trovare uno o più \textbf{valori incogniti} che potessero soddisfare l'equazione (ossia le $x$ di un'equazione). Nell'ambito delle equazioni differenziali, invece, viene richiesto di trovare una \textbf{funzione incognita} che possa soddisfare l'equazione.
        
        Per capire cosa intendiamo con funzione incognita, vediamo la seguente equazione:
        
        \[ f^2(x)+x^2-1 = 0\]
        
        Procediamo in maniera algebrica, riscrivendo l'equazione come:
        
        \[ f^2(x) = 1-x^2\]
        \[ f(x) = \pm \sqrt{1-x^2}\]
        
        Abbiamo quindi trovato \textbf{due funzioni} (ossia $f_1(x) = \sqrt{1-x^2}$ e $f_2(x) = -\sqrt{1-x^2}$) in grado di soddisfare l'equazione. Entrambe le funzioni sono quindi \textbf{soluzioni dell'equazione}.
        
        \quad
        
        Vediamo ora un altro esempio di equazione con funzione incognita:
        
        \begin{itemize}
            \item Data $f(x) = \cos(x)$, trovare una funzione $g(x)$ tale che:
            
            \[ g'(x) = f(x)\]
            
            \item Notiamo come il problema dato non sia altro che un modo alternativo per descrivere un problema già affrontato numerose volte: trovare una \textbf{primitiva di $f(x)$}, ossia $g(x)$
            
            \item Per risolvere il problema, quindi, procediamo integrando entrambi i lati dell'equazione:
            
            \[ g'(x) = f(x)\]
            \[ g'(x) = \cos(x)\]
            \[ \int g'(x) \; dx = \int \cos(x) \; dx\]
            \[ g(x) = \int \cos(x) \; dx\]
            \[ g(x) = \sin(x)+c\]
            
            \item Abbiamo ottenuto quindi che $g(x) = \sin(x)+c$, dove $c \in \mathbb{R}$. Poiché $c$ può essere una costante qualsiasi, tale equazione possiede \textbf{infinite soluzioni}.
            
        \end{itemize}
        
        \quad
        
        L'equazione appena svolta è l'esempio più banale di \textbf{equazione differenziale}, ossia un'equazione in cui uno dei membri contiene una \textbf{derivata di qualsiasi ordine della funzione incognita}.
        
        \textbf{Esempi:}
        \begin{itemize}
            \item La funzione
            \[ x^2+3x-1 = 0\]
            
            \textbf{non è un'equazione differenziale}, poiché in essa non compare né una funzione incognita né una sua derivata.
            
            \item La funzione
            \[ f^2(x)+x^2-1 = 0\]
            
            \textbf{non è un'equazione differenziale}, poiché in essa non compare una derivata della funzione incognita
            
            \item La funzione
            \[ f(x)-f'(x)+f''(x)-3 = 0\]
            
            \textbf{è un'equazione differenziale}.
        \end{itemize}
        
        \quad
        
        \begin{frameddefn}{Equazione differenziale ordinaria (EDO)}
        Un'\textbf{equazione differenziale ordinaria} è un'equazione in cui l'\textbf{incognita} è una \textbf{funzione} e uno dei termini dell'equazione è una \textbf{derivata di qualsiasi ordine della funzione incognita stessa}.
        
        L'\textbf{ordine dell'EDO} corrisponde all'ordine della \textbf{derivata} della funzione incognita di \textbf{ordine più alto}.
        \end{frameddefn}
        
        Consideriamo la seguente \textbf{EDO di secondo ordine}:
        
        \[ y''(t) = 2\]
        
        Poiché $y''(t)$ è una derivata di secondo ordine, per trovare la funzione incognita $y(t)$ è necessario \textbf{integrare due volte} entrambi i membri dell'equazione:
        
        \[ y''(t) = 2\]
        \[ \int \int y''(t) \; dt \; dt = \int \int 2 \; dt \; dt\]
        \[ \int y'(t) \; dt = \int (2t+c_1) \; dt\]
        \[ y(t) = t^2+c_1t+c_2\]
        
        Otteniamo quindi che $y(t) = t^2+c_1t+c_2$, dove $c_1, c_2 \in \mathbb{R}$. Poiché sia $c_1$ che $c_2$ possono essere una qualsiasi costante, tale EDO di secondo ordine ha \textbf{infinite soluzioni} dipendenti da due parametri.
        
        \begin{framedprop}{}
        Un'equazione differenziale di ordine $N$ ha infinite soluzioni dipendenti da $N$ parametri reali.
        \end{framedprop}
            
    \newpage
    
    \section{Problema di Cauchy}
    
    Lo studio delle equazioni differenziali trova applicazione in numerosi campi scientifici, in particolare nella Fisica e nella Chimica.
    
    Consideriamo ad esempio il seguente \textbf{problema}:
    
    \textit{Ci troviamo su un dirupo e lasciamo cadere un sasso. Considerando la nostra posizione come l'origine del piano cartesiano e sapendo che l'accelerazione gravitazionale è pari a $g = 9.8 m/s^2$, vogliamo trovare un equazione in grado di descrivere la distanza $s$ percorsa dal sasso in base al tempo $t$ trascorso per qualsiasi valore di $t$.}
    
    Prima di procedere, è necessario effettuare prima un piccolo ripassino di fisica di base per assicurarci di essere tutti sullo stesso piano:
    
    \begin{itemize}
        \item La \textbf{velocità} di un corpo corrisponde al \textbf{cambiamento del suo spostamento} in rapporto al tempo.
        \[ v = \frac{\Delta s}{\Delta t}\]
        
        \item L'\textbf{accelerazione} di un corpo corrisponde al \textbf{cambiamento della sua velocità} in rapporto al tempo.
        \[ a = \frac{\Delta v}{\Delta t} = \frac{\Delta s}{\Delta t^2}\]
        
        \item Lo \textbf{spostamento} di un corpo in un qualsiasi tempo $t$ corrisponde alla funzione $s(t)$
        
        \item La \textbf{velocità istantanea} di un corpo in un qualsiasi tempo $t$ corrisponde alla derivata prima della funzione dello spostamento
        \[ v(t) = s'(t)\]
        
        \item L'\textbf{accelerazione istantanea} di un corpo in un qualsiasi tempo $t$ corrisponde alla derivata prima della funzione della velocità istantanea.
        \[ a(t) = v'(t) = s''(t)\]
    \end{itemize}
    
    \quad
    
    Poiché vogliamo trovare la \textbf{funzione} $s(t)$ in grado di descrivere la \textbf{distanza} percorsa dal sasso in base tempo trascorso e sappiamo che l'\textbf{accelerazione} del sasso è pari a $g = 9.8 m/s^2$, allora possiamo dire che
    \[ a(t) = s''(t) = g m/s^2\]
    
    Possiamo quindi ricavare la funzione dello spostamento integrando due volte $s''(t)$:
    
    \[ \int \int s''(t) \; dt \; dt= \int \int g \; dt \; dt\]
    \[ s(t) = \int (gt+c_1) \; dt\]
    \[ s(t) = \frac{1}{2}gt^2 + c_1t +c_2\]
    
    dove $c_1, c_2 \in \mathbb{R}$ sono due costanti che danno vita ad \textbf{infinite soluzioni}.
    
    Tuttavia, sapendo che \textbf{nell'origine} (ossia in $t = 0$), il \textbf{corpo non si è ancora mosso} (dunque sia la sua velocità istantanea sia il suo spostamento sono pari a 0), possiamo \textbf{restringere il problema} imponendo dei \textbf{vincoli}:
    \begin{itemize}
        \item Siccome $v(t) = gt+c_1$ e in $t=0$ la \textbf{velocità è pari a zero}, allora 
        \[ v(0) = 0 \]
        \[ g \cdot 0 +c_1 = 0\]
        \[ c_1 = 0 \]
        
        \item Siccome $s(t) = \frac{1}{2}gt^2 + c_1t +c_2$ e in $t=0$ lo \textbf{spostamento è pari a zero}, allora 
        \[ s(0) = 0 \]
        \[ \frac{1}{2}g \cdot 0^2 + c_1 \cdot 0 +c_2 = 0\]
        \[ c_2 = 0 \]
        
        \item Affinché i \textbf{vincoli} imposti siano \textbf{rispettati}, quindi, è necessario che $c_1 = 0$ e $c_2 = 0$, rendendo quindi possibile \textbf{un'unica soluzione all'equazione} rispetto alle infinite precedentemente trovate
        \[ s(t) = \frac{1}{2}gt^2 \]
        
        \item \textit{\textbf{Approfondimento}}: l'equazione trovata, difatti, corrisponde alla formula generica del \textit{ moto rettilineo uniformemente accelerato} che viene normalmente studiato in fisica. Nel nostro caso, abbiamo semplicemente posto $a = g$, $v_i = 0$ (ossia la velocità iniziale) e $s_i = 0$ (ossia lo spostamento iniziale):
        \[ s(t) = \frac{1}{2}at^2+v_it+s_i \quad \Longrightarrow \quad s(t) = \frac{1}{2}gt^2\]
    \end{itemize}
    
    \quad
    
    Applicando i vincoli imposti dalle \textbf{condizioni iniziali}, quindi, siamo riusciti a ricondurre il problema ad un'\textbf{unica soluzione}. Tale tipologia di problema viene detto \textbf{problema di Cauchy}:
    
    \begin{frameddefn}{Problema di Cauchy}
        Viene definito come \textbf{Problema di Cauchy} un problema matematico in cui data un'\textbf{equazione} \textbf{differenziale} di ordine N e una \textbf{quantità N di condizioni iniziali \underline{sufficienti}}, si è in grado di ricondurre tale problema ad un'\textbf{unica soluzione}.
        
        $$
        (c) = \left \{ \begin{array}{l}
            \text{EDO di ordine N per l'incognita } y(t) \\
            + \text{ N condizioni iniziali}
        \end{array}\right .
        $$
        
        dove $(c)$ è l'unica soluzione del problema.
    \end{frameddefn}
    
    Il problema precedentemente svolto, quindi, può essere riformulato nel seguente problema di Cauchy:
    
    $$
        (c) = \left \{ \begin{array}{l}
            y''(t) = g \\
            y(0) = 0 \\
            y'(0) = 0
        \end{array}\right .
    $$
    
    \textbf{ATTENZIONE}: è necessario sottolineare che affinché si possa ottenere una sola soluzione è necessario che il problema di Cauchy sia \textbf{ben formulato}, ossia le sue condizioni iniziali devono essere sufficienti a poter ottenere una sola soluzione.
    
    Ad esempio, il seguente problema non è un problema di Cauchy, poiché le condizioni iniziali fornite non sono sufficienti a poter ottenere una sola soluzione:
    $$
        (c) = \left \{ \begin{array}{l}
            y''(t) = g \\
            y(0) = 0 \\
            y(1) = 3
        \end{array}\right .
    $$
    
    \textbf{Esempi:}
    
    \begin{enumerate}
        \item Consideriamo il seguente problema:
        
        $$
            (c) = \left \{ \begin{array}{l}
                y'(t) = \alpha y(t)  [1-y(t)] \\
                y(0) = k \\
            \end{array}\right .
        $$
        dove $k$ è una costante
        
        \begin{itemize}
            \item La \textbf{prima soluzione all'EDO} data che riusciamo a trovare è $y_1(t) = 1$, poiché:
            
            \begin{itemize}
                \item La derivata della funzione $y_1(t) = 1$ deve essere $y_1'(t) = [y_1(t)]' = 0$
                \item Sostituendo $y_1(t)$ a $y(t)$ nell'EDO data, otteniamo effettivamente che $y'(t) = 0$:
                \[ y'(t) = \alpha \cdot 1 \cdot [1-1] = \alpha \cdot 1 \cdot 0 = 0\]
            \end{itemize}
            
            \item La \textbf{seconda soluzione all'EDO} data che riusciamo a trovare è $y_2(t) = 0$, poiché:
            
            \begin{itemize}
                \item La derivata della funzione $y_2(t) = 0$ deve essere $y_2'(t) = [y_2(t)]' = 0$
                \item Sostituendo $y_2(t)$ a $y(t)$ nell'EDO data, otteniamo effettivamente che $y'(t) = 0$:
                \[ y'(t) = \alpha \cdot 0 \cdot [1-0] = \alpha \cdot 0 \cdot 1 = 0\]
            \end{itemize}
            
            \item A questo punto, sarà la \textbf{condizione iniziale} a determinare quale delle due equazioni trovate sia l'unica soluzione del problema:
            
            \begin{itemize}
                \item Se $k = 1$, allora solo $y_1(t) = 1$ è in grado di soddisfare la condizione iniziale $y(0) = k$
                \item Se $k = 0$, allora solo $y_2(t) = 0$ è in grado di soddisfare la condizione iniziale $y(0) = k$
            \end{itemize}
        \end{itemize}
        
        \quad
        
        \item Consideriamo il seguente problema:
        
        $$
        (c) = \left \{ \begin{array}{l}
            y'(t) = 2y(t)  \\
            y(0) = 1 
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Sapendo che $y(0) = 1$, possiamo affermare che
            \[ y'(0) = 2y(0) = 2 \cdot 1 = 2\]
            
            \item Derivando l'equazione $y'(0) = 2y(0)$, otteniamo che
            
            \[ [y'(0)]' = [2y(0)]'\]
            \[ y''(0) = 2y'(0) \]
            
            dove sappiamo già che $y'(0) = 2$, dunque $ y''(0) = 2y'(0) = 2 \cdot 2 = 4$
            
            \item Se derivassimo nuovamente, otterremmo che
            
            \[ y'''(0) = 2 \cdot y''(0) = 2 \cdot 4 = 8\]
            
            \item A questo punto, notiamo che il pattern corrisponde a
            \[ y^{(n)}(0) = 2y^{(n-1)}(0) = 2^n\]
            
            \item Ricordando che il \textbf{Polinomio di Taylor} di una funzione corrisponde a
            
            \[ T_n(f(x), x_0) = \sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\]
            
            possiamo definire il polinomio di Taylor di $y(t)$ in $x_0 = 0$ come:
            
            \[ T_n(y(t), 0) = \sum_{k=0}^{n} \frac{y^{(k)}(0)}{k!} (t-0)^k \]
            
            \item Poiché abbiamo già dimostrato che
            \[ y^{(n)}(0) = 2^n\]
            possiamo riscrivere il polinomio come
            
            \[ T_n(y(t), 0) = \sum_{k=0}^{n} \frac{y^{(k)}(0)}{k!} t^k = \sum_{k=0}^{n} \frac{2^k}{k!} t^k =  \sum_{k=0}^{n} \frac{(2t)^k}{k!}\]
            
            \item A questo punto, facciamo tendere $n \to +\infty$, ottenendo la seguente serie notevole di Taylor
            
            \[ \lim_{n \to +\infty} \sum_{k=0}^{n} \frac{(2t)^k}{k!} = \sum_{k=0}^{\infty} \frac{(2t)^k}{k!} = e^{2t}\]
            
            \item Notiamo quindi che $y(t) = e^{2t}$ è \textbf{l'unica soluzione valida del problema di Cauchy}, poiché essa è sia una soluzione dell'EDO data sia rispetta la condizione iniziale data.
        \end{itemize}
    \end{enumerate}
    
    \quad
    
    Lo stesso procedimento utilizzato nell'esercizio precedente è applicabile con \textbf{qualsiasi altro valore costante}, ossia per qualsiasi equazione differenziale del tipo $y'(t)=\lambda y(t)$, dove $\lambda \in \mathbb{R}$:
    
    \[ y'(t) = \lambda y(t) \quad \Longrightarrow \quad y(t) = e^{\lambda x}\]
    
    \newpage
    
    \section{Equazioni lineari del primo ordine}
    
    \subsection{EDO lineari omogenee del primo ordine}
    
    Vediamo ora il \textbf{caso più generico} di quanto appena espresso, ossia un'equazione differenziale nella forma:
    
    $$
    \left \{ \begin{array}{l}
        y'(t) = a(t) \cdot y(t)\\
        y(t_0) = y_0
    \end{array}\right .
    $$
    
    Ricordando come la derivata della \textbf{funzione composta }$f(x) = e^{g(x)}$ sia
    
    \[ f(x) = e^{g(x)} \quad \Longrightarrow \quad f'(x) = g'(x) \cdot e^{g(x)}\]
    
    possiamo facilmente concludere che la \textbf{funzione} $a(t)$ dell'equazione differenziale corrisponda esattamente alla \textbf{derivata} $g'(t)$. Dunque, possiamo affermare che:
    
    \[ y'(t) = a(t) \cdot y(t) \quad \Longrightarrow \quad y(t) = e^{A(t)}\]
    
    dove $A(t)$ è l'\textbf{integrale} di $a(t)$ nell'intervallo $[t_0, t]$.
    
    \[ A(t) = \int_{t_0}^{t} a(t) \; dt\]
    
    Inoltre, possiamo osservare come se $y_0(t) = e^{A(t)}$ è \textbf{una soluzione dell'equazione}, allora anche $y_1(t) = C \cdot e^{A(t)}$ ($\forall C \in \mathbb{R}$) \textbf{è una soluzione dell'equazione}, poiché derivando $y_1(t)$ si andrebbe ad ottenere
    \[ y_1'(t) = [C \cdot e^{A(t)}]' = a(t) \cdot \underbrace{C \cdot e^{\lambda t}}_{y_1(t)} = a(t) \cdot y_1(t)\]
    
    la cui soluzione sappiamo già essere $y_1(t) = e^{A(t)}$.
    
    \begin{frameddefn}{EDO lineare omogenea}
        Data un'\textbf{EDO lineare}, ossia dove sia $y'(t)$ sia $y(t)$ compaiono con termini di grado 0 o 1, ed \textbf{omogenea}, ossia dove la derivata non contiene  termini costanti aggiuntivi indipendenti dalla funzione $y(t)$.
        
        Se $y_0(t) = e^{A(t)}$ è una soluzione dell'EDO, allora lo è anche $y_1(t) = C \cdot y_0(t)$ ($\forall C \in \mathbb{R})$:
        \[ y'(t) = a(t) y(t) \quad \Longrightarrow \quad y(t) = C e^{A(t)}\]
        
    \end{frameddefn}
    
    \newpage
    
    \subsection{EDO lineari non omogenee del primo ordine}
    
    Consideriamo ora un'\textbf{EDO lineare non omogenea}, espressa nella seguente forma:
    
    $$
    \left \{ \begin{array}{l}
        y'(t) = a(t) y(t) + b(t)\\
        y(t_0) = y_0
    \end{array}\right .
    $$
    
    La \textbf{soluzione} di tale equazione può essere scritta come
    
    \[ y(t) = y_0(t) + \overline{y}(t)\]
    
    dove:
    
    \begin{enumerate}
        \item $y_0(t)$ corrisponde alla \textbf{soluzione dell'EDO lineare omogenea associata} (e \underline{non} \underline{ al valore $y_0$ imposto dalla condizione del problema}), ossia
        \[ y_0'(t) = a(t)y_0(t)\]
        
        \item $\overline{y}(t)$ corrisponde ad \textbf{una soluzione particolare dell'equazione}, ottenuta tramite
        \[ \overline{y}'(t) = a(t)\overline{y}(t)+b(t)\]
    \end{enumerate}
    
    Sappiamo già che la \textbf{soluzione del punto 1} corrisponde a
    
    \[ y_0(t) = Ce^{A(t)}\]
    
    Quanto al \textbf{punto 2}, dobbiamo procedere con un ragionamento intuitivo:
    
    \begin{itemize}
        \item Ipotizziamo che 
        \[ \overline{y} (t) = h(t) e^{A(t)}\]
        
        \item Derivando tale funzione, otteniamo che
        \[ [\overline{y} (t)]' = [h(t) e^{A(t)}]'\]
        
        \[ \overline{y} \, ' (t) = h'(t) e^{A(t)}+a(t)\underbrace{h(t)e^{A(t)}}_{\overline{y}(t)}\]
        
        \[ \overline{y} \,'(t) =  h'(t) e^{A(t)}+ a(t) \overline{y}(t)\]
        
        \item A questo punto, confrontiamo l'equazione derivata con l'equazione precedentemente definita, ottenendo che
        
        \[ \overline{y} \,'(t) =  \underbrace{h'(t) e^{A(t)}}_{b(t)}+ a(t) \overline{y}(t)\]
        
        \[ \overline{y}'(t) = a(t)\overline{y}(t)+b(t) \]
        
        \item Quindi, possiamo affermare che
        
        \[ h'(t) e^{A(t)} = b(t)\]
        
        \[ h'(t) = \frac{b(t)}{e^{A(t)}}\]
        
        \[ h'(t) = b(t)e^{-A(t)}\]
        
        \[ h(t) = \int_{t_0}^{t} b(t)e^{-A(t)} \; dt\]
        
        \item A questo punto, possiamo riscrivere la funzione $\overline{y}(t)$ come
        
        \[ \overline{y}(t) = h(t)e^{A(t)}\]
        
        \[ \overline{y}(t) = e^{A(t)} \left [ \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ]\]
        
    \end{itemize}
    
    Una volta trovate $y_0(t)$ e $\overline{y}(t)$, possiamo dire che \textbf{le soluzioni dell'EDO lineare non-omogenea} equivalgono a:
    
    \[ y(t) = y_0(t) + \overline{y}(t) = Ce^{A(t)} + e^{A(t)} \left [ \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ] = \]
    \[ e^{A(t)} \left [ C + \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ]\]
    
    valida $\forall C \in \mathbb{R}$.
    
    \quad
    
    Tuttavia, considerando anche \textbf{il vincolo $y(t_0) = y_0$ imposto dal problema di Cauchy}, le infinite soluzioni vengono limitate ad \textbf{una ed una sola}:
    
    Ricordando che:
    \begin{itemize}
        \item Il vincolo imposto dal problema è:
        \[ y(t_0) = y_0 \]
        
        \item La primitiva $A(t)$ è calcolata come
        
        \[ A(t) = \int_{t_0}^{t} a(t) \; dt\]
        
        \item L'insieme di soluzioni dell'equazione è:
        \[ y(t) = e^{A(t)} \left [ C + \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ] \]
        
    \end{itemize}
    
    Ponendo $t = t_0$ ne segue che:
    \begin{itemize}
        \item Il vincolo imposto dal problema è:
        \[ y(t_0) = y_0 \]
        
        \item L'integrale di $A(t)$ nell'intervallo $[t_0, t_0]$ vale 0
        
        \[ A(t_0) = \int_{t_0}^{t_0} a(t_0) \; dt = 0\]
        
        \item L'integrale di $b(t)e^{-A(t)}$ nell'intervallo $[t_0, t_0]$ vale 0
        
        \[ y(t_0) = e^{A(t_0)} \left [ C + \int_{t_0}^{t_0} b(t)e^{-A(t_0)} \; dt \right ] \]
        
        \[ y(t_0) = e^{0} [ C + 0] \]
        
        
        \[ y(t_0) = C \]
        
    \end{itemize}
    
    Dunque, poiché $y(t_0) = y_0 = C$, ne segue che l'\textbf{unica soluzione possibile del problema di Cauchy} è:
    
    \[ y(t) = e^{A(t)} \left [ y_0 + \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ] \]
    
    \quad
    
    \begin{frameddefn}{Problema di Cauchy con EDOLNO}
        Dato un problema di Cauchy relativo ad un'\textbf{EDO lineare non omogenea} della forma
        
        $$
        \left \{ \begin{array}{l}
            y'(t) = a(t) y(t) + b(t)\\
            y(t_0) = y_0
        \end{array}\right .
        $$
        
        l'\textbf{unica soluzione} di tale problema corrisponde a:
        
        \[ y(t) = e^{A(t)} \left [ y_0 + \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ] \]
    \end{frameddefn}
    
    \newpage
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Si consideri il seguente problema di Cauchy:
        
        $$
        \left \{ \begin{array}{l}
            y'(t) = \frac{y(t)}{t+1} + t\\
            y(0) = 1
        \end{array}\right .
        $$
        
        \item Trattandosi di un EDO lineare non omogenea, sappiamo che l'unica soluzione al problema corrisponde a
        
        \[ y(t) = e^{A(t)} \left [ y_0 + \int_{t_0}^{t} b(t)e^{-A(t)} \; dt \right ] \]
        
        dove:
        
        \begin{itemize}
            \item La funzione $A(t)$ corrisponde a:
            
            \[ A(t) = \int_{t_0}^{t} a(t) \; dt = \int_{0}^{t} \frac{1}{t+1} \; dt = \left . ln(\abs{t+1}) \right |_{0}^{t} = ln(t+1) - ln(1+0) = ln(t+1)\]
            
            \item L'integrale della soluzione (considerando $t \geq 0$) corrisponde a :
            
            \[ \int_{t_0}^{t} b(t)e^{-A(t)} \; dt = \int_{0}^{t} te^{-ln(t+1)} \; dt = \int_{0}^{t} t \left (e^{ln(t+1)} \right )^{-1} \; dt = \]
            
            \[ = \int_{0}^{t} t (t+1)^{-1} \; dt = \int_{0}^{t} \frac{t}{t+1}\; dt = \int_{0}^{t} \frac{t+1-1}{t+1}\; dt = \int_{0}^{t} 1 \; dt - \int_{0}^{t} \frac{1}{t+1}\; dt = \]
            
            \[ = \left . t - ln(t+1) \right |_{0}^{t} = t - ln(t+1)\]
            
            \textit{Attenzione: non viene usato il valore assoluto nel logaritmo ottenuto dall'integrale poiché stiamo considerando $t \geq 0$}
            
            \item Dunque, la soluzione unica del problema corrisponde a:
            
            \[ y(t) = e^{ln(t+1)} \left [ 1 + \int_{0}^{t} te^{-ln(t+1)} \; dt \right ] \]
            
            \[ y(t) = (t+1) \left [ 1 + t - ln(t+1) \right ] \]
            
            \[ y(t) = (t+1) + t(t+1) - (t+1) \cdot ln(t+1)\]
            
            \[ y(t) = (t+1)^2 - (t+1) \cdot ln(t+1)\]
        \end{itemize}
    \end{itemize}
    
    \section{Equazioni a variabili separabili}
    
    Consideriamo il seguente problema di Cauchy e la sua \textbf{EDO non lineare} associata:
    
    $$
    \left \{ \begin{array}{l}
        y'(t) = f(y(t))g(t) \\
        y(t_0) = y_0
    \end{array}\right .
    $$
    
    Trattandosi di un \textbf{prodotto tra funzioni}, possiamo suddividere il problema in due casistiche:
    
    \begin{itemize}
        \item \textbf{Se $f(y(t_0)) = 0$, allora $y(t) = y_0$ è una soluzione dell'equazione}, poiché
        
        \begin{itemize}
            \item Derivando $y(t) = y_0$ otteniamo $y'(t) = 0$
            \item Dato che $y(t) = y_0$ e $f(y(t_0)) = 0$, ne segue che $y'(t) = f(y(t))g(t) = 0$
            \item Dunque, $y(t) = y_0$ è una soluzione valida dell'equazione e, poiché ogni problema di Cauchy ha una sola soluzione, essa è l'\textbf{unica soluzione del problema}.
        \end{itemize}
        
        \quad
        
        \item \textbf{Se $f(y(t_0)) \neq 0$, allora ciò è valido anche $\forall t \in \mathbb{R}$}.
        
        \begin{itemize}
            \item Data tale condizione (poiché altrimenti rischieremmo di effettuare una divisione per 0), possiamo \textbf{riscrivere l'equazione} come
        
            \[ \frac{y'(t)}{f(y(t))} = g(t)\]
        
            \item A questo punto, possiamo \textbf{integrare} entrambi i membri dell'equazione
            
            \[ \int_{t_0}^{s} \frac{y'(t)}{f(y(t))} \; dt = \int_{t_0}^{s} g(t) \; dt\]
            
            \item Procedendo per sostituzione con $z = y(t)$ otteniamo che
            
            \[ z = y(t) \quad \Longrightarrow \quad dz = y'(t) \, dt\]
            
            \[ \int_{y(t_0)}^{y(s)} \frac{1}{f(z)} \; dz = \int_{t_0}^{s} g(t) \; dt\]
            
            \[ F(y(s)) - F(y(t_0)) = G(s) - G(t_0)\]
            
            \[ F(y(s)) = F(y(t_0)) + G(s) - G(t_0)\]
            
            \newpage
            
            \item Se $F(t)$ è una \textbf{funzione invertibile} (ossia esiste una funzione $F^{-1}(t)$ tale che $F^{-1}(F(t)) = t$, ad esempio la funzione esponenziale e il logaritmo naturale), allora
            
            \[ F^{-1}(F(y(s)) = F^{-1}(F(y(t_0)) + G(s) - G(t_0))\]
            
            \[ y(s) = F^{-1}(F(y(t_0)) + G(s) - G(t_0))\]
            
            dove $y(s)$ è la \textbf{soluzione al problema di Cauchy}.
        \end{itemize}
    \end{itemize}
    
    \quad
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item Si consideri il seguente problema di Cauchy:
        
        $$
        \left \{ \begin{array}{l}
            y'(t) = \frac{t}{y(t)+1} \\
            y(0) = 0
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Siamo nel secondo caso delle EDO a variabili separabili, poiché 
            \[f(y(t_0)) = \frac{1}{y(0)+1} = \frac{1}{0+1} = 1\]
            
            \item Dunque, riscriviamo l'equazione come
            
            \[ (y(t)+1)y'(t) = t\]
            
            \item Integrando entrambi i membri otteniamo che
            
            \[ \int_{0}^{s} (y(t)+1)y'(t) \; dt = \int_{0}^{s} t \; dt\]
            
            \[ z = y(t) \quad \Longrightarrow \quad dz = y'(t) \, dt\]
            
            \[ \int_{y(0)}^{y(s)} (z+1) \; dz = \int_{0}^{s} t \; dt\]
            
            \[ \left . \frac{1}{2}z^2+z \right |_{y(0)}^{y(s)} = \left . \frac{1}{2}t^2 \right |_{0}^{s}\]
            
            \[ \frac{1}{2}y^2(s)+y(s) - 0 = \frac{1}{2} s^2 - 0\]
            
            \[ y^2(s) + 2y(s) - s^2 = 0 \]
            
            \item Abbiamo ottenuto un'\textbf{equazione di secondo grado} in variabile $y(s)$, le cui soluzioni sono:
            
            \[ y(s)_{1,2} = \frac{-2 \pm \sqrt{4+4s^2}}{2} = -1 \pm \sqrt{1+s^2}\]
            
            \item Tuttavia, sappiamo che il problema di Cauchy può avere \textbf{una sola soluzione}. Difatti, una delle due soluzioni trovate \textbf{non verifica la condizione iniziale del problema}, ossia $y(0) = 1$
            
            \[ y_1(0)=-1 + \sqrt{1+0} = -1 + 1 = 0 \quad \Longrightarrow \text{Soddisfa la condizione}\]
            \[ y_2(0)=-1 - \sqrt{1+0} = -1 - 1 = -2 \quad \Longrightarrow \text{Non soddisfa a condizione}\]
            
            \item Dunque, la soluzione del problema è:
            
            \[ y(t) = -1 + \sqrt{1+s^2}\]
        \end{itemize}
        
        \quad
        
        \item Si consideri il seguente problema di Cauchy
        
        $$
        \left \{ \begin{array}{l}
            y'(t) = y(t)[y(t)-2]\cos(t) \\
            y(0) = 2
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Siamo nel primo caso delle EDO a variabili separabili, poiché
            
            \[ f(y(t_0)) =  y(0)[y(0)-2] = 2 \cdot 0 = 0\]
            
            \item Dunque, la soluzione del problema è
            
            \[ y(t) = 2\]
        \end{itemize}
        
        \item Consideriamo un problema con la stessa EDO precedente, ma con una condizione iniziale diversa:
        
        $$
        \left \{ \begin{array}{l}
            y'(t) = y(t)[y(t)-2]\cos(t) \\
            y(0) = 1
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Siamo nel secondo caso delle EDO a variabili separabili, poiché
            
            \[ f(y(t_0)) =  y(0)[y(0)-2] = -1 \cdot (-1) = 1\]
            
            \item Dunque, riscriviamo l'equazione come
            
            \[ \frac{y'(t)}{y(t)[y(t)-2]} = \cos(t)\]
            
            \item Integrando entrambi i membri otteniamo che
            
            \[ \int_{0}^{s} \frac{y'(t)}{y(t)[y(t)-2]} \; dt = \int_{0}^{s} \cos(t) \; dt\]
            
            \[ z = y(t) \quad \Longrightarrow \quad dz = y'(t) \, dt\]
            
            \[ \int_{y(0)}^{y(s)} \frac{1}{z[z-2]} \; dz = \int_{0}^{s} \cos(t) \; dt\]
            
            \[ \left . \frac{1}{2}ln \left (\abs{\frac{z-2}{z}} \right ) \right |_{y(0)}^{y(s)} = \left . \sin(t) \right |_{0}^{s}\]
            
            \[ \frac{1}{2}ln \left (\abs{\frac{y(s)-2}{y(s)}} \right ) = \sin(s)\]
            
            \item Riscriviamo l'equazione ottenuta
            
            \[ ln \left (\abs{\frac{y(s)-2}{y(s)}} \right ) = 2sin(s)\]
            
            \[ \abs{\frac{y(s)-2}{y(s)}}  = e^{2sin(s)}\]
            
            \item Data la \textbf{condizione iniziale}, sappiamo se $s = 0$ allora $y(s) = y(0) = 1$. Considerando dei \textbf{valori di $s$ molto vicini a $t_0 = 0$}, quindi $s \approx 0$, possiamo dire che
            
            \[ \abs{\frac{y(s)-2}{y(s)}} \approx \abs{\frac{1-2}{1}} \approx \abs{-1} \approx -(-1) \]
            
            dunque, possiamo rimuovere il valore assoluto invertendo il segno del suo contenuto
            
            \[ -\frac{y(s)-2}{y(s)}  = e^{2sin(s)}\]
            
            \[ \frac{2-y(s)}{y(s)}  = e^{2sin(s)}\]
            
            \item A questo punto, semplifichiamo l'equazione
            
            \[ \frac{2}{y(s)}-1  = e^{2sin(s)}\]
            
            \[ y(s) = \frac{2}{e^{2sin(s)}+1}\]
            
        \end{itemize}
        
    \end{itemize}
    
    \quad

    \section{Equazioni lineari del secondo ordine}
    
    \subsection{EDO lineari omogenee del secondo ordine}
    
    Dopo aver visto come svolgere le EDO lineari del primo ordine, vediamo ora il caso in cui l\textbf{'EDO sia lineare, omogenea e del secondo ordine}:
    
    $$
    \left \{ \begin{array}{l}
    y''(t) + Ay'(t) + By(t) = 0 \\
    y(t_0) = y_0; \; y'(t_0) = y_1
    \end{array}\right .
    $$
    
    Analogamente al caso delle \textbf{EDO lineari omogenee del primo ordine}, la \textbf{funzione di partenza} che viene presa in considerazione è $f(x) = e^{\lambda x}$, poiché:
    
    \begin{itemize}
        \item $f(x) = e^{\lambda x}$
        \item $f'(x) = \lambda e^{\lambda x} = \lambda f(x)$
        \item $f''(x) = \lambda^2 e^{\lambda x} = \lambda f'(x) = \lambda^2 f(x)$
    \end{itemize}
    
    \quad
    
    Sostituendo $y(t) = e^{\lambda}$ nell'EDO, otteniamo che
    
    \[ y''(t) + Ay'(t) + By(t) = 0 \quad \Longrightarrow \quad \lambda^2 e^{\lambda t} + A\lambda e^{\lambda t} + Be^{\lambda t} = 0\]
    
    Dividendo entrambi i membri per $e^{\lambda t}$, riusciamo a ricondurre l'EDO ad un polinomio di secondo grado, detto \textbf{polinomio caratteristico associato all'EDO di secondo grado}:
    
    \[  \lambda^2e^{\lambda t} + A\lambda e^{\lambda t} + Be^{\lambda t} = 0 \quad \Longrightarrow \quad P(\lambda) = \lambda^2  + A\lambda + B\]
    
    Possiamo quindi ricavare il valore di $\lambda$ trovando le due radici del polinomio:
    
    \[ \lambda_{1, 2} = \frac{-A \pm \sqrt{A^2-4B}}{2}\]
    
    A questo punto, a seconda del $\Delta$ dell'equazione, il problema si suddivide in \textbf{tre casistiche}:
    
    \begin{itemize}
        \item \textbf{Se $\lambda_1 \neq \lambda_2$, allora} sia $y_1(t) = Ce^{\lambda_1 t}$ sia $y_2(t) = De^{\lambda_2 t}$ sono soluzioni dell'equazione. Siccome anche $y(t) = y_1(t) + y_2(t)$ è \textbf{soluzione dell'equazione}, possiamo unirle in un'unica soluzione nella forma generica:
        
        \[ y(t) = Ce^{\lambda_1 t} + De^{\lambda_2 t}\]
        
        dove $C, D \in \mathbb{R}$.
        
        \item \textbf{Se $\lambda_1 = \lambda_2$, allora} la soluzione dell'equazione è
        
        \[ y(t) = (C + Dt) e^{\lambda t}\]
        
        \item \textbf{Se $\lambda_1 = a + bi$ e $\lambda_2 = a - bi$}, (ossia sono due numeri complessi), \textbf{allora} la soluzione dell'equazione è
        
        \[ y(t) = Ce^{(a + bi) t} + De^{(a - bi) t}\]
        
        Ricordando la \textbf{Formula di Eulero} (sezione \ref{eulero}), possiamo riscrivere la soluzione come:
        
        \[ y(t) = Ce^{at} e^{bti} + De^{at} e^{-bti} = e^{at} (C[\cos(bt) + isin(bt)] + D[\cos(-bt)+isin(-bt)])\]
        
        Utilizzando alcune proprietà delle funzioni trigonometriche, possiamo ricondurre con alcuni passaggi (che ometteremo) la \textbf{soluzione} a:
        
        \[ y(t) = e^{at}[C \cdot \cos(bt) + D \cdot \sin(bt)]\]
        
        dove $C, D \in \mathbb{R}$.
        
    \end{itemize}
    
    \quad
    
    \quad
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \label{edolno_1}
        \item Si consideri il seguente problema di Cauchy:
        
        \[\left \{ \begin{array}{l}
            y''(t)+6y'(t)+5y(t)=0\\
            y(0)=0; \; y'(0)=2
        \end{array} \right .\]
        
        \begin{itemize}
            \item Il \textbf{polinomio caratteristico} associato all'EDO risulta essere:
            
            \[ P(\lambda) = \lambda^2+6\lambda+5\]
            
            \item Le radici del polinomio sono:
            
            \[ \lambda_{1,2} = \frac{-6 \pm  \sqrt{36-4 \cdot 5}}{2} = \frac{-6 \pm 4}{2} = -3 \pm 2\]
            
            \[ \lambda_1 = -1 \qquad\qquad \lambda_2 = -5\]
            
            \item Le soluzioni dell'equazione, quindi, corrispondono a:
            
            \[ y(t) = Ce^{-t} + De^{-5t}\]
            
            dove $C, D \in \mathbb{R}$.
            
            \item Per ottenere l'\textbf{unica soluzione del problema} in grado di verificare le condizioni iniziali, poniamo il seguente sistema:
            
            $$ 
            \left \{ \begin{array}{l}
                y(t) = Ce^{-t} + De^{-5t} \\
                y'(t) = -Ce^{-t} -5De^{-5t}
            \end{array}\right .
            $$
            
            \item Sostituendo nel sistema i valori imposti dalle condizioni iniziali, otteniamo che
            
            $$ 
            \left \{ \begin{array}{l}
                y(t_0) = Ce^{-t_0} + De^{-5t_0} \\
                y'(t_0) = -Ce^{-t_0} -5De^{-5t_0}
            \end{array}\right .
            \qquad \Longrightarrow \qquad
            \left \{ \begin{array}{l}
                0 = Ce^{0} + De^{0} = C+D \\
                2 = -Ce^{0} -5De^{0} = -C-5D
            \end{array}\right .
            $$
            
            $$ 
            \left \{ \begin{array}{l}
                C = -D \\
                2 = D-5D = -4D
            \end{array}\right .
            \qquad \Longrightarrow \qquad
            \left \{ \begin{array}{l}
                C = +\frac{1}{2} \\
                D = -\frac{1}{2}
            \end{array}\right .
            $$
            
            \item Dunque, l'unica soluzione del problema risulta essere:
            
            \[ y(t) = \frac{1}{2}e^{-t}-\frac{1}{2}e^{-5t}\]
        \end{itemize}
        
        \quad
        
        \item Si consideri il seguente problema di Cauchy:
        
        $$ 
        \left \{ \begin{array}{l}
            y''(t)-6y'(t)+9y(t)=0 \\
            y(0)=2; \; y'(0)=0
        \end{array}\right .
        $$
            
        \[ y''(t)-6y'(t)+9y(t)=0\]
        
        \begin{itemize}
            \item Il \textbf{polinomio caratteristico} associato all'EDO risulta essere:
            
            \[ P(\lambda) = \lambda^2-6\lambda+9\]
            
            \item Le radici del polinomio sono:
            
            \[ \lambda_{1,2} = \frac{6 \pm  \sqrt{36-4 \cdot 9}}{2} = \frac{6 \pm 0}{2} = 3\]
            
            \[ \lambda_1 = 3 \qquad\qquad \lambda_2 = 3\]
            
            \item Le soluzioni dell'equazione, quindi, corrispondono a:
            
            \[ y(t) = (C+Dt)e^{3t}\]
            
            dove $C, D \in \mathbb{R}$.
            
            \item Per ottenere l'\textbf{unica soluzione del problema} in grado di verificare le condizioni iniziali, poniamo il seguente sistema:
            
            $$ 
            \left \{ \begin{array}{l}
                y(t) = (C+Dt)e^{3t} \\
                y'(t) = De^{3t}+3(C+Dt)e^{3t}
            \end{array}\right .
            $$
            
            \item Sostituendo nel sistema i valori imposti dalle condizioni iniziali, otteniamo che
            
            $$ 
            \left \{ \begin{array}{l}
                y(t_0) = (C+Dt_0)e^{3t_0} \\
                y'(t_0) = De^{3t_0}+3(C+Dt_0)e^{3t_0}
            \end{array}\right .
            \quad \Longrightarrow \quad
            \left \{ \begin{array}{l}
                2 = (C)e^{0} = C\\
                0 = De^{0}+3(C+0)e^{0} = D+3C
            \end{array}\right .
            $$
            
            $$ 
            \left \{ \begin{array}{l}
                C = 2 \\
                0 = D+3 \cdot 2
            \end{array}\right .
            \qquad \Longrightarrow \qquad
            \left \{ \begin{array}{l}
                C = 2 \\
                D = -6
            \end{array}\right .
            $$
            
            \item Dunque, l'\textbf{unica soluzione del problema} risulta essere:
            
            \[ y(t) = (2+-6t)e^{3t} \]
        \end{itemize}
        
        \quad
        
        \item Si consideri il seguente problema di Cauchy:
        
        $$ 
        \left \{ \begin{array}{l}
            y''(t)+6y'(t)+13y(t)=0 \\
            y(0) = 0; \; y'(0) = 5
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Il \textbf{polinomio caratteristico} associato all'EDO risulta essere:
            
            \[ P(\lambda) = \lambda^2+6\lambda+13\]
            
            \item Le radici del polinomio sono:
            
            \[ \lambda_{1,2} = \frac{-6 \pm  \sqrt{36-4 \cdot 13}}{2} = \frac{-6 \pm \sqrt{-16}}{2} = \frac{-6 \pm 4i}{2} = -3 \pm 2i\]
            
            \[ \lambda_1 = -3 + 2i \qquad\qquad \lambda_2 = -3 - 2i\]
            
            \item Le soluzioni dell'equazione, quindi, corrispondono a:
            
            \[ y(t) = e^{-3t} [C \cdot \cos(2t) + D \cdot \sin(2t)]\]
            
            dove $C, D \in \mathbb{R}$.
            
            
            \item Per ottenere l'\textbf{unica soluzione del problema} in grado di verificare le condizioni iniziali, poniamo il seguente sistema:
            
            $$ 
            \left \{ \begin{array}{l}
                y(t) = e^{-3t} [C \cdot \cos(2t) + D \cdot \sin(2t)] \\
                y'(t) = -3e^{-3t} [C \cdot \cos(2t) + D \cdot \sin(2t)] + e^{-3t} [-2C \cdot \sin(2t) + 2D \cdot \cos(2t)]
            \end{array}\right .
            $$
            
            \item Sostituendo nel sistema i valori imposti dalle condizioni iniziali, otteniamo che
            
            $$ 
            \left \{ \begin{array}{l}
                y(t_0) = e^{-3t_0} [C \cdot \cos(2t_0) + D \cdot \sin(2t_0)] \\
                y'(t_0) = -3e^{-3t_0} [C \cdot \cos(2t_0) + D \cdot \sin(2t_0)] + e^{-3t_0} [-2C \cdot \sin(2t_0) + 2D \cdot \cos(2t_0)]
            \end{array}\right .
            $$
            
            $$
            \left \{ \begin{array}{l}
                0 = e^{0} [C \cdot \cos(0) + D \cdot \sin(0)] = C\\
                5 = -3e^{0} [C \cdot \cos(0) + D \cdot \sin(0)] + e^{0} [-2C \cdot \sin(0) + 2D \cdot \cos(0)] = -3C+2D
            \end{array}\right .
            $$
            
            $$ 
            \left \{ \begin{array}{l}
                C = 0 \\
                5 = -3 \cdot 0 + 2D
            \end{array}\right .
            \qquad \Longrightarrow \qquad
            \left \{ \begin{array}{l}
                C = 0 \\
                D = \frac{5}{2}
            \end{array}\right .
            $$
            
            \item Dunque, l'\textbf{unica soluzione del problema} risulta essere:
            
            \[ y(t) = \frac{5}{2} \sin(2t) e^{-3t}\]
            
            
        \end{itemize}
    \end{itemize}
    
    \quad
    
    \subsection{EDO lineari non omogenee del secondo ordine}
    
    Consideriamo ora un'\textbf{EDO lineare non omogenea del secondo ordine}, espressa nella forma:
    
    $$ 
    \left \{ \begin{array}{l}
        y''(t)+Ay'(t)+By(t)=g(t) \\
        y(t_0) = y_0; \; y'(t_0) = y_1
    \end{array}\right .
    $$
    
    Analogamente alle EDO lineari non omogenee del primo ordine, anche \textbf{le soluzioni di una EDOLNO del secondo ordine possono essere scritte come}
    
    \[ y(t) = y_0(t) + \overline{y}(t)\]
    
    dove:
    
    \begin{enumerate}
        \item $y_0(t)$ corrisponde alla \textbf{soluzione dell'EDO lineare omogenea associata}, ossia
        \[ y_0''(t)+Ay_0'(t)+By_0(t) = 0\]
        
        \item $\overline{y}(t)$ corrisponde ad \textbf{una soluzione particolare dell'equazione}
    \end{enumerate}
    
    A differenza delle EDOLNO di primo ordine, tuttavia, la \textbf{soluzione particolare} del problema si divide in \textbf{tre casistiche}:
    
    \begin{itemize}
        \item Se $g(t)$ è un \textbf{polinomio di grado n}, allora cercheremo la soluzione particolare sotto forma di polinomio di grado n
        \[ \overline{y}(t) = \sum_{k=0}^{n} a_k t^k = a_0 + a_1 t^1 + ... + a_n t^n \]
        
        \item Se $g(t)$ è una \textbf{funzione esponenziale}, allora cercheremo la soluzione particolare sotto forma di funzione esponenziale
        \[ \overline{y}(t) = Qe^{t} \]
        
        \item Se $g(t)$ è una \textbf{funzione trigonometrica}, allora cercheremo la soluzione particolare sotto forma di funzione trigonometrica
        \[ \overline{y}(t) = C \cdot \cos(t) + D \cdot \sin(t)\]

        \textbf{ATTENZIONE:} In ognuna delle tre casistiche, se la \textbf{soluzione particolare trovata è anche soluzione dell'EDO associata}, allora è necessario \textbf{riprovare} ricercando la soluzione particolare nella \textbf{forma precedente moltiplicata per $t$}.
        
    \end{itemize}
    
    \newpage
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Consideriamo il seguente problema di Cauchy
        
        $$ 
        \left \{ \begin{array}{l}
            y''(t)+6y'(t)+5y(t)=3t+2 \\
            y(0) = 0; \; y'(0) = 0
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Sappiamo che la soluzione dell'EDO corrisponde a
            
            \[ y(t) = y_0(t) + \overline{y}(t)\]
            
            \item  L'\textbf{EDO omogenea associata}, ossia $y_0(t)$, corrisponde a:
            
            \[y_0''(t)+6y_0'(t)+5y_0(t)=0 \]
            
            Le cui soluzioni sappiamo già essere (vedere sezione \ref{edolno_1}) :
            
            \[y_0(t) = Ce^{-t}+De^{-5t} \]
            
            \item Poiché $g(x)$ corrisponde ad un \textbf{polinomio di primo grado}, cerchiamo la \textbf{soluzione particolare} sotto forma di:
            
            \[ \overline{y}(t) = Qt+R\]
            
            \item Avendo posto $\overline{y} = Qt + R$, ne segue che:
            
            \[ \overline{y}'(t) = Q\]
            
            \[ \overline{y}''(t) = 0\]
            
            \item \textbf{Sostituendo} $\overline{y}(t)$, $\overline{y}'(t)$ e $\overline{y}''(t)$ nell'EDO, otteniamo che:
            
            \[ \overline{y}''(t)+6\overline{y}'(t)+5\overline{y}(t) = 3t+2\]
            
            \[ 0 + 6Q + 5(Qt+R) = 3t+2\]
            
            \[  5Qt + 6Q + 5R = 3t +2\]
            
            \item A questo punto, compariamo i due membri dell'equazione imponendo il seguente \textbf{sistema}:
            
            $$ 
            \left \{ \begin{array}{l}
                6Q+5R = 2 \\
                5Qt = 3t
            \end{array}\right .
            \quad \Longrightarrow \quad
            \left \{ \begin{array}{l}
                6Q+5R = 2 \\
                Q = \frac{3}{5}
            \end{array}\right .
            \quad \Longrightarrow \quad
            \left \{ \begin{array}{l}
                R = -\frac{8}{25} \\
                Q = \frac{3}{5}
            \end{array}\right .
            $$
            
            \item Una volta trovati $Q$ e $R$, concludiamo che
            
            \[ \overline{y}(t) = \frac{3}{5}t - \frac{8}{25}\]
            
            e di conseguenza che
            
            \[ y(t) = y_0(t) + \overline{y}(t) = Ce^{-t}+De^{-5t} + \frac{3}{5}t - \frac{8}{25}\]
            
            \item Per ottenere l'\textbf{unica soluzione del problema} in grado di verificare le condizioni iniziali, poniamo il seguente sistema:
            
            $$ 
            \left \{ \begin{array}{l}
                y(t) = Ce^{-t}+De^{-5t} + \frac{3}{5}t - \frac{8}{25} \\
                y'(t) = -Ce^{-t}-5De^{-5t}+\frac{3}{5}
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                y(t_0) = Ce^{-t_0}+De^{-5t_0} + \frac{3}{5}t_0 - \frac{8}{25} \\
                y'(t_0) = -Ce^{-t_0}-5De^{-5t_0}+\frac{3}{5}
            \end{array}\right .
            $$
            
            $$ 
            \left \{ \begin{array}{l}
                0 = Ce^{0}+De^{0} + \frac{3}{5} \cdot 0 - \frac{8}{25} \\
                0 = -Ce^{0}-5De^{0}+\frac{3}{5}
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                0 = C+D - \frac{8}{25} \\
                0 = -C-5D+\frac{3}{5}
            \end{array}\right .
            $$
            
            $$ 
            \left \{ \begin{array}{l}
                C = -D + \frac{8}{25} \\
                0 = -(D + \frac{8}{25})-5D+\frac{3}{5}
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                C = \frac{1}{4} \\
                D = \frac{7}{100}
            \end{array}\right .
            $$
            
            \item L'\textbf{unica soluzione del problema}, quindi, corrisponde a:
            
            \[ y(t) = \frac{1}{4}e^{-t}+\frac{7}{100}e^{-5t} + \frac{3}{5}t - \frac{8}{25}\]
        \end{itemize}
        
        \quad
        
        \item Consideriamo il seguente problema di Cauchy
        
        $$ 
        \left \{ \begin{array}{l}
            y''(t)+4y'(t)+4y(t)=9e^{t} \\
            y(0) = 0; \; y'(0) = 0
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Sappiamo che la soluzione dell'EDO corrisponde a
            
            \[ y(t) = y_0(t) + \overline{y}(t)\]
            
            \item L'\textbf{EDO omogenea associata}, ossia $y_0(t)$, corrisponde a:
            
            \[ y_0''(t)+4y_0'(t)+4y_0(t)=0\]
            
            Siccome il suo \textbf{polinomio caratteristico associato} corrisponde a:
            
            \[ P(\lambda) = \lambda^2+4\lambda+4 = (\lambda+2)^2\]
        
            possiamo affermare che le soluzioni dell'EDO sono:
            
            \[ y(t) = (C+Dt)e^{-2t}\]
            
            dove $C, D \in \mathbb{R}$.
            
            \item Poiché $g(x)$ corrisponde ad una \textbf{funzione esponenziale}, cerchiamo la \textbf{soluzione particolare} sotto forma di:
            
            \[ \overline{y}'(t) = Qe^{t}\]
            
            le cui derivate sono:
            
            \[ \overline{y}'(t) = Qe^{t}\]
            
            \[ \overline{y}''(t) = Qe^{t}\]
            
            \item \textbf{Sostituendo} $\overline{y}(t)$, $\overline{y}'(t)$ e $\overline{y}''(t)$ nell'EDO, otteniamo che:
            
            \[ \overline{y}''(t)+4\overline{y}'(t)+4\overline{y}(t)=9e^t\]
            
            \[ Qe^{t}+4Qe^{t}+4Qe^{t}=9e^t\]
            
            \item \textbf{Dividendo}  entrambi i membri per $e^t$, otteniamo che
            \[ Q+4Q+4Q=9 \quad \Longrightarrow \quad 9Q = 9 \quad \Longrightarrow \quad Q = 1\]
            
            dunque la soluzione particolare sarà
            
            \[ \overline{y}(t) = e^t\]
            
            e quindi la soluzione dell'EDO corrisponde a
            
            \[ y(t) = y_0(t) + \overline{y}(t) = (C+Dt)e^{-2t}+e^t\]
            
            \item Per ottenere l'\textbf{unica soluzione del problema} in grado di verificare le condizioni iniziali, poniamo il seguente sistema:
            
            $$ 
            \left \{ \begin{array}{l}
                y(t) = (C+Dt)e^{-2t}+e^t\\
                y'(t) = De^{-2t}-2(C+Dt)e^{-2t}+e^t
            \end{array}\right .
            $$ 
            
            $$ 
            \left \{ \begin{array}{l}
                y(t_0) = (C+Dt_0)e^{-2t_0}+e^t_0\\
                y'(t_0) = De^{-2t_0}-2(C+Dt_0)e^{-2t_0}+e^t_0
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                0 = Ce^{0}+e^0\\
                0 = De^{0}-2Ce^{0}+e^0
            \end{array}\right .
            $$
            
            $$
            \left \{ \begin{array}{l}
                0 = C+1\\
                0 = D-2C+1
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                C = -1\\
                0 = D+2+1
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                C = -1 \\
                D = -3
            \end{array}\right .
            $$
            
            \item L'\textbf{unica soluzione del problema}, quindi, corrisponde a:
            
            \[ y(t) = (-1+-3t)e^{-2t}+e^t = -3e^{-2t}-e^{-2t}+e^t\]
        \end{itemize}
        
        \quad
        
        \item Consideriamo il seguente problema di Cauchy
        
        $$ 
        \left \{ \begin{array}{l}
            y''(t)+y(t)=\sin(t) \\
            y(0) = 0; \; y'(0) = 1
        \end{array}\right .
        $$
        
        \begin{itemize}
            \item Sappiamo che la soluzione dell'EDO corrisponde a
            
            \[ y(t) = y_0(t) + \overline{y}(t)\]
            
            \item  L'\textbf{EDO omogenea associata}, ossia $y_0(t)$, corrisponde a:
            
            \[y''(t)+y(t)=0 \]
            
            Siccome il suo \textbf{polinomio caratteristico associato} corrisponde a:
            
            \[P(\lambda) = \lambda^2 + 1 \]
            
            le cui radici sono
            
            \[ \lambda^2 + 1 = 0\]
            \[ \lambda^2 = -1\]
            \[ \lambda = 0 \pm i\]
            
            possiamo affermare che le \textbf{soluzioni dell'EDO sono}:
            
            \[ y_0(t) = e^{0t}[C \cdot \cos(t) + D \cdot \sin(t)] = C \cdot \cos(t) + D \cdot \sin(t)\]
            
            \item Poiché $g(x)$ corrisponde ad una \textbf{funzione trigonometrica}, cerchiamo la \textbf{soluzione particolare} sotto forma di:
            
            \[ \overline{y}(t) = Q \cdot \cos(t) + R \cdot \sin(t)\]
            
            Tuttavia, poiché tale soluzione particolare è \textbf{anche soluzione dell'EDO omogenea associata}, cerchiamo \textbf{un'altra soluzione particolare} sotto forma di:
            
            \[ \overline{y}(t) = t[Q \cdot \cos(t) + R \cdot \sin(t)]\]
            
            le cui derivate sono:
            
            \[ \overline{y}'(t)  = [Q+Rt]\cos(t)+[R-Qt]\sin(t)\]
            
            \[ \overline{y}''(t) = [2R-Qt]\cos(t)-[2Q+Rt]\sin(t)\]
            
            \item \textbf{Sostituendo} $\overline{y}(t)$ e $\overline{y}''(t)$ nell'EDO, otteniamo che:
            
            \[ \overline{y}''(t)+\overline{y}(t)=\sin(t)\]
            
            \[ [2R-Qt]\cos(t)-[2Q+Rt]\sin(t) +  t[Q \cdot \cos(t) + R \cdot \sin(t)] = \sin(t)\]
            
            \[ 2R \cdot \cos(t) - 2Q \cdot \sin(t) = \sin(t)\]
            
            \item A questo punto, compariamo i due membri dell'equazione imponendo il seguente \textbf{sistema}:
            
            $$ 
            \left \{ \begin{array}{l}
                2R \cdot \cos(t) = 0 \cdot \cos(t) \\
                -2Q \cdot \sin(t) = 1 \cdot \sin(t)
            \end{array}\right .
            \quad \Longrightarrow \quad
            \left \{ \begin{array}{l}
                2R = 0 \\
                -2Q = 1
            \end{array}\right .
            \quad \Longrightarrow \quad
            \left \{ \begin{array}{l}
                R = 0 \\
                Q = -\frac{1}{2}
            \end{array}\right .
            $$
            
            \item Una volta trovati Q e R, concludiamo che
            
            \[ \overline{y}(t) = -\frac{1}{2}t \cdot \cos(t)\]
            
            e quindi che la soluzione dell'EDO corrisponde a
            
            \[ y(t) = y_0(t) + \overline{y}(t) = C \cdot \cos(t) + D \cdot \sin(t) -\frac{1}{2}t \cdot \cos(t) \]
            
            \item Per ottenere l'\textbf{unica soluzione del problema} in grado di verificare le condizioni iniziali, poniamo il seguente sistema:
            
            $$ 
            \left \{ \begin{array}{l}
                y(t) = C \cdot \cos(t) + D \cdot \sin(t) -\frac{1}{2}t \cdot \cos(t)\\
                y'(t) = -C \cdot \sin(t) + D \cdot \cos(t) -\frac{1}{2} \cdot \cos(t) + \frac{1}{2}t \cdot \sin(t)
            \end{array}\right .
            $$ 
            
            $$ 
            \left \{ \begin{array}{l}
                y(t_0) = C \cdot \cos(t_0) + D \cdot \sin(t_0) -\frac{1}{2}\cos(t_0)\\
                y'(t_0) = -C \cdot \sin(t_0) + D \cdot \cos(t_0) -\frac{1}{2} \cdot \cos(t_0) + \frac{1}{2}t_0 \cdot \sin(t_0)
            \end{array}\right .
            $$ 
            
            $$ 
            \left \{ \begin{array}{l}
                0 = C \cdot \cos(0) + D \cdot \sin(0) -\frac{1}{2}\cos(0)\\
                0 = -C \cdot \sin(0) + D \cdot \cos(0) -\frac{1}{2} \cdot \cos(0) + \frac{1}{2} \cdot 0 \cdot \sin(t)
            \end{array}\right .
            $$ 
            
            $$ 
            \left \{ \begin{array}{l}
                0 = C\\
                0 = D - \frac{1}{2}
            \end{array}\right .
            \quad \Longrightarrow \quad 
            \left \{ \begin{array}{l}
                C = 0 \\
                D = \frac{1}{2}
            \end{array}\right .
            $$ 
            
            \item Dunque, l'\textbf{unica soluzione del problema} corrisponde a:
            
            \[ y(t) = \frac{1}{2} \sin(t)-\frac{1}{2}t \cdot \cos(t) \]
        \end{itemize}
        
    \end{itemize}

\end{document}
