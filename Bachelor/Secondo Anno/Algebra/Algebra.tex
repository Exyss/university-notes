\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%

\def\useItalian{1}  % 1 = Italian, 0 = English

\def\courseName{Algebra}

\def\coursePrerequisites{Apprendimento del materiale relativo al corso \textit{Metodi Matematici per l'Informatica}.}

\def\book{\curlyquotes{Geometria analitica con elementi di Algebra lineare}, M. Abate, C. De Fabritiis}

\def\authorName{Simone Bianco}
\def\email{bianco.simone@outlook.it}
\def\github{https://github.com/Exyss/university-notes}
\def\linkedin{https://www.linkedin.com/in/simone-bianco}


%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../../packages/Nyx/nyx-packages}
\usepackage{../../../packages/Nyx/nyx-styles}
\usepackage{../../../packages/Nyx/nyx-frames}
\usepackage{../../../packages/Nyx/nyx-macros}
\usepackage{../../../packages/Nyx/nyx-title}
\usepackage{../../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../../packages/Nyx/logo.png}

\if\useItalian1
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} Università di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \ifdefined\book
        \subtitle{Appunti integrati con il libro \book}
    \fi
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \ifdefined\book
        \subtitle{Lecture notes integrated with the book \book}
    \fi
    \author{\textit{Author}\\\authorName}
\fi


\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%

    \chapter{Strutture algebriche principali}
    
    \section{Operazioni binarie e loro proprietà}

    \quad
    
    \begin{frameddefn}{Operazione binaria}
    Dato un insieme $S$, definiamo \textbf{operazione binaria} una funzione che manda ogni coppia di elementi appartenenti ad $S$ in $S$ stesso.
    \[\funcmap{m}{S \times S}{S}{(x,y)}{m(x,y)}\]

    Tale proprietà viene anche detta \textbf{proprietà di chiusura}.
    \end{frameddefn}

    \begin{framedobs}{}
        Per leggibilità, d'ora in poi indicheremo l'applicazione di un'operazione binaria generica $m(x,y)$ come $xy$.
    
        Tuttavia, tale notazione \underline{non corrisponde all'operazione prodotto} (a meno che non sia specificato), bensì corrisponde ad un semplice \textbf{"segnaposto"} per una qualsiasi operazione binaria.
    \end{framedobs}

    \textbf{Esempio:}
    \begin{itemize}
        \item Sull'insieme $\R$, l'\textbf{operazione additiva} e l'\textbf{operazione moltiplicativa}, ossia:
        \[\funcmap{+}{\R \times \R}{\R}{(x,y)}{x + y}\]
        \[\funcmap{\cdot}{\R \times \R}{\R}{(x,y)}{x \cdot y}\]
        sono entrambe operazioni binarie.
        \item Sull'insieme $X = \{f \mid \funcmap{f}{S}{S}{x}{y}\}$ (dove $S$ è un insieme qualsiasi) la \textbf{composizione tra funzioni}, ossia:
        \[ \funcmap{\circ}{X \times X}{X}{(g, f)}{g \circ f}\]
        è un'operazione binaria.
        \item Sull'insieme $\N$, l'\textbf{operazione sottrazione}, ossia:
        \[\funcmap{-}{\N \times \N}{\N}{(x,y)}{x - y}\]
        non è un'operazione binaria
    \end{itemize}
    
    \begin{frameddefn}{Proprietà associatività}
    Data un'operazione binaria $\func{m}{S \times S}{S}$, tale operazione gode di \textbf{proprietà associativa} se l'ordine di applicazione di tale operazione binaria non influenza il risultato:
    \[\forall x,y,z \in S \quad x(yz) = (xy)z = xyz\]
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item L'operazione additiva $\func{+}{\R \times \R}{\R}$ gode della proprietà associativa:
        \[\forall x,y,z \in \R \quad (x+y)+z = x+(y+z) = x+y+z\]
        \item L'operazione moltiplicativa $\func{\cdot}{\R \times \R}{\R}$ gode della proprietà associativa:
        \[\forall x,y,z \in \R \quad (x\cdot y)\cdot z = x\cdot(y\cdot z) = x\cdot y\cdot z\]
    \end{itemize}
    
    \begin{frameddefn}{Esistenza dell'elemento neutro}
    Data un'operazione binaria $\func{m}{S \times S}{S}$, tale operazione gode dell'\textbf{esistenza dell'elemento neutro} se
    \[ \exists! e \in S \mid \forall x \in S \quad xe = ex = x\]
    dove $e$ viene detto \textbf{elemento neutro}.
    \end{frameddefn}

    \textit{Dimostrazione unicità:}

    \begin{itemize}
        \item Supponiamo che 
        \[\exists e_1, e_2 \in S \mid e_1x = xe_1 = x \land e_2x=xe_2=x, \forall x \in S\]
        
        \item Di conseguenza, si ha che
        \[e_1 = e_1e_2 = e_2e_1 = e_2 \iff e_1 = e_2\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item Nell'operazione additiva $\func{+}{\R \times \R}{\R}$, l'elemento neutro è 0 poiché
        \[\forall x \in \R \quad x+0 = 0+x = x\]
        \item Nell'operazione moltiplicativa $\func{\cdot}{\R \times \R}{\R}$, l'elemento neutro è 1 poiché
        \[\forall x \in \R \quad x \cdot 1 = 1 \cdot x = x\]
    \end{itemize}
    
    \begin{frameddefn}{Esistenza dell'elemento inverso}
    Data un'operazione binaria $\func{m}{S \times S}{S}$, tale operazione gode dell'\textbf{esistenza dell'elemento inverso} se
    \[\forall x \in S \quad \exists! x^{-1} \in S \mid xx^{-1} = x^{-1}x = e\]
    
    \textit{Attenzione}: con la scrittura $x^{-1}$ indichiamo l'elemento inverso di $x$ rispetto all'operazione binaria definita, \underline{non} il "classico" inverso del prodotto (ossia $\frac{1}{x}$)
    \end{frameddefn}
    
    \textit{Dimostrazione unicità:}
    
    \begin{itemize}
        \item Supponiamo che 
        \[\exists x^{-1}_1, x^{-1}_2 \in S \mid xx^{-1}_1 = x^{-1}_1x = e \land xx^{-1}_2 = x^{-1}_2x = e, \forall x \in S\]
        
        \item Di conseguenza, si ha che
        \[x^{-1}_1x = e = x^{-1}_2x \iff x^{-1}_1x = x^{-1}_2x \iff x^{-1}_1 = x^{-1}_2\]
        $\hfill\qed$
    \end{itemize}
    
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item L'operazione additiva $\func{+}{\R \times \R}{\R}$ gode dell'esistenza dell'elemento inverso poiché
        \[\forall x \in \R \quad \exists! -x \in \R \mid x+(-x) = (-x)+x = 0\]
        \item L'operazione moltiplicativa $\func{\cdot}{\R \times \R}{\R}$ gode dell'esistenza dell'elemento inverso poiché
        \[\forall x \in \R \quad \exists! \frac{1}{x} \in \R \mid x \cdot \frac{1}{x} = \frac{1}{x} \cdot x = 1\]
    \end{itemize}
    
    \begin{framedobs}{}
        Se un'operazione binaria gode dell'\textbf{esistenza dell'elemento inverso}, allora essa gode necessariamente anche  dell'\textbf{esistenza dell'elemento neutro}
    \end{framedobs}

    \begin{frameddefn}{Proprietà commutativa}
    Data un'operazione binaria $m : S \times S \to S$, tale operazione gode di \textbf{proprietà commutativa} se l'ordine degli elementi su cui viene applicata tale operazione non influenza il risultato:
    \[\forall x,y\in S \quad xy = yx\]
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item L'operazione additiva $\func{+}{\R \times \R}{\R}$ gode della proprietà commutativa:
        \[\forall x \in \R \quad x+y = y+x\]
        \item L'operazione moltiplicativa $\func{+}{\R \times \R}{\R}$ gode della proprietà commutativa:
        \[\forall x \in \R \quad x \cdot y = y \cdot x\]
    \end{itemize}

    \quad
    
    \section{Gruppi, Anelli e Campi}
    
    \begin{frameddefn}{Strutture algebriche semplici}
        Data la coppia $(S, m)$ dove $S$ è un \textbf{insieme} e $m$ l'\textbf{operazione binaria} applicata su di esso, definiamo tale \textbf{struttura algebrica} come:
        
        \begin{itemize}
            \item Un \textbf{semigruppo} se in esso vale l'assioma d'associatività
            \item Un \textbf{monoide} se in esso valgono gli assiomi di associatività ed elemento neutro 
            \item Un \textbf{gruppo} se in esso valgono gli assiomi di associatività, elemento neutro ed elemento inverso 
            \item Un \textbf{gruppo abeliano} (o commutativo) se in esso valgono gli assiomi di associatività, elemento neutro, elemento inverso e commutatività
        \end{itemize}
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item $(\N-\{0\}, +)$ è un \textbf{semigruppo}
        \item $(\N, +)$ è un \textbf{monoide} commutativo
        \item $(\R, \cdot)$ è un \textbf{gruppo abeliano}
        \item $(\Z, \cdot)$ è un \textbf{monoide} commutativo
        
        \newpage

        \item Dato l'insieme $X$, definiamo $X^X$ come l'insieme di tutte le funzioni da $X$ ad $X$, ossia $X^X:= \{f \mid \func{f}{X}{X}\}$.
        
        La struttura algebrica $(X^X, \circ)$ è un monoide, poiché gode di:
        \begin{itemize}
            \item \textbf{Associatività}:
            \[f, g, h \in X^X \implies h \circ (g \circ f) = (h \circ g) \circ f = h \circ g \circ f\]
            \item \textbf{Elemento neutro}:
            \[\exists! \id \in X^X \mid \forall f \in X^X, f \circ \id = \id \circ f = f\]
            dove $\id$ è la \textbf{funzione identità}, ossia tale che $\forall x \in X$ valga $\id(x) = x$.
        \end{itemize}
    \end{itemize}
    
    \begin{frameddefn}{Anello}
        Definiamo una struttura algebrica $(A, +, \cdot)$ come \textbf{anello} se:
        \begin{itemize}
            \item $(A, +)$ è un \textbf{gruppo abeliano}
            \item $(A, \cdot)$ è un \textbf{monoide}
            \item Gode della \textbf{proprietà distributiva}, definita come:
            \[\forall a,b,c \in A \quad a(b+c) = ab+ac\]
            \[\forall a,b,c \in A \quad (b+c)a = ba+ca\]
        \end{itemize}
    
        In particolare, definiamo un anello come \textbf{anello commutativo} se in $(A, \cdot)$ vale anche l'\textbf{assioma di commutatività} (dunque se $(A, \cdot)$ è un monoide commutativo)
    \end{frameddefn}
    
    \begin{framedprop}{}
        Sia $A$ un anello. Dato l'elemento neutro della somma $0 \in A$, si ha che:
        \[\forall a \in A \quad a \cdot 0 = 0\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dato un elemento $a \in A$ si ha che:
        \[ a = a \cdot 1 = a \cdot (0 + 1) = a \cdot 0 + a \cdot 1 = a \cdot 0 + a \iff\]
        \[\iff a = a \cdot 0 + a \iff a+(-a) = a \cdot 0 + a + (-a) \iff 0 = a \cdot 0\]
    $\hfill\qed$
    \end{itemize}

    \newpage
    
    \begin{framedprop}{}
        Sia $A$ un anello. Dati due elementi $x,y \in A$, si ha che:
        \[\forall a \in A \quad (xy)^{-1} = y^{-1}x^{-1} \]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dati due elementi $x,y \in A$ si ha che:
        \[(xy)^{-1}(xy) = 1 \iff (xy)^{-1}xy = 1 \iff (xy)^{-1}xyy^{-1} = y^{-1} \iff \]
        \[\iff (xy)^{-1}x = y^{-1} \iff (xy)^{-1}xx^{-1} = y^{-1}x^{-1} \iff (xy)^{-1} =y^{-1}x^{-1}\]
    $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Sia $A$ un anello commutativo. Dati due elementi $x,y \in A$, si ha che:
        \[\forall a \in A \quad (xy)^{-1} = y^{-1}x^{-1} = x^{-1}y^{-1}\]
    \end{framedcor}
    
    \begin{frameddefn}{Campo}
    Definiamo una struttura algebrica $(K, +, \cdot)$ come \textbf{campo} se:
    \begin{itemize}
        \item $(K, +, \cdot )$ è un \textbf{anello commutativo}
        \item $\forall x \in K-\{0\} \quad \exists! x^{-1} \in K-\{0\} \mid xx^{-1} = x^{-1}x = e$
    \end{itemize}
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item $(\Z, +, \cdot)$ è un \textbf{anello commutativo}
        \item $(\Q, +, \cdot)$ è un \textbf{campo}
        \item $(\R, +, \cdot)$ è un \textbf{campo}
    \end{itemize}
    
    \newpage
    
    \section{Sottogruppi ed Ideali}
    \label{subgroups}
    
    \quad
    
    \begin{frameddefn}{Sottogruppo}
    Dato un gruppo $(G, \cdot)$, definiamo $(H,\cdot)$ come \textbf{sottogruppo} di $G$, indicato come $H \subgrp G$, se:
    \begin{itemize}
        \item $H \subseteq G$
        \item $e \in H$, dove $e$ è l'elemento neutro di $G$
        \item $x, y \in H \implies xy \in H$ 
        \item $x \in H \implies x^{-1} \in H$ 
    \end{itemize}
    
    \textit{\textbf{Attenzione}}: ricordiamo che con $\cdot$ intendiamo una qualsiasi operazione binaria
    \end{frameddefn}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item $(\Z, +) \subgrp (\Q, +) \subgrp (\R, +) \subgrp (\C, +)$
        \item $(\Z-\{0\}, \cdot) \not\subgrp (\Q-\{0\}, \cdot) \subgrp (\R-\{0\}, \cdot)$
    \end{itemize}
    
    \begin{frameddefn}{Ideale}
    Dato un anello $(A,+,\cdot)$, definiamo $(I,+,\cdot)$ come \textbf{ideale} di $A$, indicato come $I \ideal A$, se:
    \begin{itemize}
        \item $I \subseteq A$
        \item $(I, +) \subgrp (A, +)$
        \item $AI \subseteq I$, dove $AI := \{ ax \mid  x \in I, a \in A\}$
        \item $IA \subseteq I$, dove $IA := \{ yb \mid  y \in I, b \in A\}$
    \end{itemize}
    
    \textbf{\textit{Attenzione:}} generalmente, si ha che $AI \neq IA$
    \end{frameddefn}
    
    \begin{framedobs}{}
        Sia $A$ un anello e sia $I \ideal A$. Se $A$ è \textbf{commutativo}, allora $I$ è \textbf{commutativo}
    \end{framedobs}
    
    \begin{frameddefn}{Ideale generato da elementi}
    Sia $A$ un anello commutativo e siano $a_1, \ldots, a_n \in A$. Definiamo $I(a_1, \ldots, a_n) \ideal A$ come \textbf{ideale generato da $a_1, \ldots, a_n$}, dove
    \[I(a_1, \ldots, a_n) := \{a_1b_1+\ldots+a_nb_n \mid  b_1, \ldots, b_n \in A\}\]
    \end{frameddefn}
    
    \newpage

    \textit{Dimostrazione:}
        
    \begin{itemize}
        \item Verifichiamo che l'elemento neutro della somma sia in $I(a_1, \ldots, a_n)$:
        \[0 = a_1 \cdot 0 + \ldots +a_n \cdot 0 \in I(a_1, \ldots, a_n)\]
    
        \item Verifichiamo che $I(a_1, \ldots, a_n)$ sia chiuso rispetto alla somma:
        \[x, y \in I(a_1, \ldots, a_2) \iff x = a_1b_1+\ldots+a_nb_n, y = a_1c_1+\ldots+a_nc_n \iff\]
        \[\iff x+y = a_1(b_1+c_1)+\ldots+a_n(b_n+c_n) \implies x+y \in I(a_1, \ldots, a_n)\]
        
        \item Verifichiamo che $I(a_1, \ldots, a_n)$ sia chiuso rispetto agli inversi della somma:
        \[x \in I(a_1, \ldots,a_2) \iff x = a_1b_1+\ldots+a_n+b_n \iff\]
        \[\iff -x = -a_1b_1-\ldots-a_nb_n = a_1(-b_1)- \ldots- a_n(-b_n) \implies -x \in I(a_1, \ldots, a_n)\]
        
        \item Verifichiamo che $I(a_1, \ldots, a_n)$ sia chiuso rispetto al prodotto:
        \[x \in I(a_1, \ldots, a_2) \iff x = a_1b_1+\ldots+a_nb_n \implies c \in A \mid cx = c(a_1b_1+\ldots+a_nb_n) \]
        \[\implies cx = c(a_1b_1+\ldots+a_nb_n) = a_1(b_1c)+\ldots+a_n(b_nc) \implies cx \in I(a_1, \ldots, a_n)\]
        $\hfill\qed$
    \end{itemize}

    \begin{frameddefn}{Ideale principale}
    Sia $A$ un anello commutativo. Definiamo $I(a) \ideal A$ come \textbf{ideale principale di $A$ generato da $a$}, dove $I(a)$ ricordiamo essere definito come:
    \[I(a) = \{ax \mid x \in A\}\]
    \end{frameddefn}
    
    \begin{framedprop}{Somma tra ideali}
        Dato un anello commutativo $A$ e due suoi ideali $I, J \ideal A$, si ha che $I+J \ideal A$, dove:
        \[ I+J:= \{ i+j \mid  i \in I, j \in J\}\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $I+J \subgrp A$ poiché:
        \begin{itemize}
            \item $0 \in I, 0 \in J \implies 0 = 0+0 \in I+J$
            \item $x,y \in I+J \iff x+y = (i_1 + j_1) +(i_2 + j_2) = (i_1 + i_2)+(j_1 + j_2) \implies x+y \in I+J$
            \item $x = i+j \in I+J \iff -x = -(i+y) = (-i)+(-j), -i \in I, -j \in J \implies -x \in I+J$
        \end{itemize}
        
        \item $a \in A, x \in I + J \implies ax \in I + J$, poiché:
        \[a \in A \mid  ai \in I, aj \in J \implies ai+aj = a(i+j) \in I+J\]
        $\hfill\qed$
    \end{itemize}
        
    \begin{framedprop}{Intersezione tra ideali}
        Dato un anello commutativo $A$ e due suoi ideali $I, J \ideal A$, si ha che $I \cap J \ideal A$, dove:
        \[ I \cap J:= \{ h \mid  h \in I \land h \in J\}\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $I\cap J \subgrp A$ poiché:
        \begin{itemize}
            \item $0 \in I \land 0 \in J \implies 0 \in I \cap J$
            \item $x,y \in I \cap J \implies x,y \in I \land x,y \in J \implies x+y \in I \land x+y \in J \implies x+y \in I \cap J$
            \item $x \in I \land x \in J \implies -x \in I \land -x \in J \implies -x \in I \cap J$
        \end{itemize}
        
        \item $a \in A, x \in I \cap J \implies ax \in I \cap J$, poiché:
        \[a \in A, x \in I \cap J  \implies ax \in I \land ax \in J \implies ax \in I \cap J \]
        $\hfill\qed$
    \end{itemize}
    
    
    \begin{framedprop}{Prodotto tra ideali}
        Dato un anello commutativo $A$ e due suoi ideali $I, J \ideal A$, si ha che $I \cdot J \ideal A$, dove:
        \[ I \cdot J:= \{ i_1j_1 + i_2j_2 + \ldots + i_nj_n \mid i_k \in I, j_h \in J\}\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $I \cdot J \subgrp A$, poiché:
        \begin{itemize}
            \item $0 \in I \land 0 \in J \implies 0 = 0+0 \in I \cdot J$
            \item $x = i_1j_1 + i_2j_2  + \ldots + i_nj_n \in I\cdot J,, y = i_1'j_1' + i_2'j_2'  + \ldots + i_n'j_n'   \in I\cdot J \implies x+y = i_1j_1 + i_1'j_1' + \ldots + i_nj_n + i_n'j_n' \in I\cdot J$
            \item $x \in I \cdot J \implies -x = x = (-i_1)j_1 + (-i_2)j_2  + \ldots + (-i_n)j_n \mid  -i_k \in I, j_h \in J \implies -x \in I \cdot J $
        \end{itemize}
        
        \item $a \in A, x \in I \cdot J \implies ax \in I \cdot J$, poiché:
        \[a \in A, x \in I \cdot J  \implies ax = (ai_1)j_1 + (ai_2)j_2  + \ldots + (ai_n)j_n \implies ax \in I \cdot J\]
        $\hfill\qed$
    \end{itemize}
    
    
    \chapter{Numeri Complessi}

    \section{Il campo dei numeri complessi}
    
    \quad

    \begin{frameddefn}{Numeri complessi}
        Definiamo l'insieme dei \textbf{numeri complessi} come:
        \[ \C := \{ a+ib \mid  a,b \in \R\}\]

        dove il simbolo $i$ è l'\textbf{unità immaginaria}, per cui si ha che $i^2 = -1$.

        Inoltre, si ha che $\R \subseteq \C$.
    \end{frameddefn}
    
    \begin{frameddefn}{Parte reale e Parte immaginaria}
        Dato $z := a+ib \in \C$, definiamo:
        \begin{itemize}
            \item $Re(z)$ come \textbf{parte reale di $z$}, dove $Re(z) = a$
            \item $Im(z)$ come \textbf{parte immaginaria di $z$}, dove $Im(z) = b$
        \end{itemize}
        
        Inoltre, definiamo $z$ come \textbf{numero immaginario puro} se $Re(z) = 0$ e $Im(z) \neq 0$
    \end{frameddefn}

    \begin{frameddefn}{Coniugato di un numero complesso}
        Dato $z := a+ib\in \C$, definiamo $\overline{z} \in \C$ come \textbf{coniugato di $z$} se
        \[\overline{z} = a-ib\]
        ossia se $Re(\overline{z}) = Re(z)$ e $Im(\overline{z}) = -Im(z)$
    \end{frameddefn}
    
    \newpage

    Poiché un numero complesso è determinato da una \textbf{coppia di valori} $a, b \in \R \mid  z \in \C, z = a+ib$, possiamo rappresentare tale numero graficamente attraverso il \textbf{piano di Gauss}, avente come ascisse la \textbf{parte reale} dei numeri complessi e come ordinate la \textbf{parte immaginaria}.
    
    \begin{center}
    \includegraphics[scale=0.6]{images/complex.png}
    \end{center}
    
    Per tale motivo, dato un elemento $z \in \C$, definiamo come suo \textbf{valore assoluto} il numero reale corrispondente alla distanza di $z$ stesso dall'origine, facilmente ricavabile attraverso la \textbf{distanza euclidea}:
    \[ \abs{z} = \sqrt{a^2+b^2}\]
    
    \label{complex_conj}
    \begin{framedprop}{}

        Dati $z,w \in \C$, si ha che:
        \begin{enumerate}
            \item $\overline{z} + \overline{w} = \overline{z+w}$
            \item $\overline{z} \cdot \overline{w} = \overline{zw}$
            \item $z \cdot \overline{z} = \abs{z}^2$
        \end{enumerate}
    \end{framedprop}
        
    \textit{Dimostrazione:}

    \begin{itemize}
        \item Dati $z := a+ib \in \C$ e $w := c+id \in C$, si ha che
        \begin{enumerate}
            \item $\overline{z} + \overline{w} = a-ib + c-id = (a-c)-i(b+d) = \overline{z+w}$
            \item $\overline{z} \cdot \overline{w} = (a-ib)(c-id) = (ac-bd)-i(ad+bc) = \overline{zw}$
            \item $z \cdot \overline{z} = (a+ib)(a-ib) = a^2-(ib)^2 = a^2+b^2 = \abs{z}^2$
        \end{enumerate}
    \end{itemize}
    
    \begin{framedprop}{}
        La struttura algebrica $(C, +, \cdot)$ è un \textbf{campo}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Le operazioni binarie di somma e prodotto sono ben definite:
        \[ z, w \in C \implies z+w = a+ib+c+id = (a+c) + i(b+d) \implies z+w \in \C\]
        \[ z, w \in C \implies zw = (a+ib)(c+id) = (ac-bd) + i(ad+bc) \implies zw \in \C\]
        
        
        \item Per costruzione di $\cdot$ e $+$, vale la \textbf{relazione distributiva}:
        \[ \forall z,w,q \in \C \mid  z(w+q) = zw+zq\]
            
        \item Dimostriamo quindi che $(\C, +, \cdot)$ sia un anello commutativo:
        \begin{itemize}
            \item \textbf{Associatività della somma}
            \[z := a+bi,w := c+di, q := e+fi \in \C \implies (z+w)+q = (a+bi + c+di)+e+fi\]
            \[= a+bi+c+di+e+fi = a+bi+(c+di+e+fi) = z+(w+q)\]
            
            \item \textbf{Elemento neutro della somma}
            \[ \forall z \in \C \quad \exists! 0 \in \C \mid  z + 0 = a+bi+0 = a+bi = z \]
            
            \item \textbf{Elemento inverso della somma}
            \[ \forall z\in \C \quad \exists! -z \in \C \mid  z+(-z) = a+bi+(-a-bi) = 0 \]

            \item \textbf{Commutatività della somma}
            \[ \forall z,w \in \C \quad z+w = a+bi+c+di = c+di+a+bi = w+z\]

            \item \textbf{Associatività del prodotto}
            \[\forall z := a+bi,w := c+di, q := e+fi \in \C \quad (zw)q = [(a+bi) \cdot (c+di)] \cdot (e+fi) =\]
            \[=(a+bi) \cdot (c+di) \cdot (e+fi) = (a+bi) \cdot [(c+di) \cdot (e+fi)] = z(wq)\]
            
            \item \textbf{Elemento neutro del prodotto}
            \[ \forall z \in \C \quad \exists! 1 \in \C \mid  z \cdot 1 = (a+bi) \cdot 1 = a+bi = z \]
            
            \item \textbf{Commutatività del prodotto}
            \[ \forall z,w \in \C \quad z \cdot w = (a+bi)(c+di)= (c+di)(a+bi) = w \cdot z\]
        \end{itemize}

        \item Dato $z := a+ib \in \C$, consideriamo il suo possibile inverso, ossia l'elemento $z^{-1} = \frac{1}{a+ib}$, il quale non appare nella forma $c+id \mid  c,d \in \R$ richiesta dalla definizione di insieme dei numeri complessi.
        
        \item Riscriviamo quindi $z^{-1}$ come:
        \[ z = a+ib \implies z^{-1} = \frac{1}{z} = \frac{\overline{z}}{z\cdot \overline{z}} = \frac{\overline{z}}{\abs{z}^2} = \frac{a-ib}{a^2+b^2} = \frac{a}{a^2+b^2}+i\cdot \frac{-b}{a^2+b^2}\]
        
        \item A questo punto, ponendo $c := \frac{a}{a^2+b^2} \in \R$ e $d := \frac{-b}{a^2+b^2} \in \R$, otteniamo che $z^{-1} = c+id \in \C$, rientrando quindi nella definizione corretta di insieme dei numeri complessi
        
        \item Per tanto, si ha che:
        
        \[\forall z \in \C -\{0\} \quad \exists! z^{-1} := \frac{a}{a^2+b^2}+i\cdot \frac{-b}{a^2+b^2} \mid  z \cdot z^{-1} = 1\]

        implicando dunque che valga anche l'assioma di elemento inverso del prodotto e di conseguenza che $\C$ sia un campo.
        
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Forma polare dei numeri complessi}
    
    Abbiamo già trattato di come un numero complesso possa essere espresso come un punto sul piano gaussiano tramite una \textbf{coppia di valori}, descrivendo la distanza di tale punto dall'origine del piano $(0,0)$ come $\abs{z}$.
    
    Possiamo quindi identificare una \textbf{circonferenza di raggio $r = \abs{z}$} rappresentante tutti i numeri complessi aventi la stessa distanza dall'origine, dove $\theta$ corrisponde all'\textbf{arco in radianti} descritto dal \textbf{vettore} costruito attraverso le due coordinate gaussiane rappresentate da $z$.

    \begin{center}
        \includegraphics[scale=0.7]{images/complex2.png}
    \end{center}

    Di conseguenza, dato $r = \abs{z}$ abbiamo che:
    \[
    r = \abs{z} \implies
    \soe{cc}{
        a = r \cdot \cos(\theta)&\\
        b = r \cdot \sin(\theta)&
    }
    \implies
    \soe{cc}{
        \cos(\theta) = \frac{a}{r} = \frac{a}{\abs{z}}&\\
        \sin(\theta) = \frac{b}{r} = \frac{b}{\abs{z}}& 
    }
    \]
    
    \begin{frameddefn}{Argomento di un numero complesso}
        Dato $z := a+ib \in \C$, definiamo come \textbf{argomento di $z$}, indicato come $\arg(z)$, una qualsiasi soluzione valida al seguente sistema:
        \[
        \soe{c}{
            \cos(\arg(z)) = \frac{a}{\abs{z}}\\
            \sin(\arg(z)) = \frac{b}{\abs{z}} 
        }
        \]
    \end{frameddefn}

    \begin{framedobs}{}
        Dato $z := a+ib \in \C$, esistono \textbf{infiniti argomenti di $z$}.
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sia $\theta \in \R$ un argomento di $z$, ossia tale che:
        \[\soe{c}{
            \cos(\theta) = \frac{a}{\abs{z}}\\
            \sin(\theta) = \frac{b}{\abs{z}} 
        }\]
        \item Essendo le funzioni seno e coseno periodiche, $\forall k \in \Z$ si ha che:
        \[\soe{c}{
            \cos(\theta) = \cos(\theta + 2k\pi)\\
            \cos(\theta) = \cos(\theta + 2k\pi)
        }
        \implies 
        \soe{c}{
            \cos(\theta + 2k\pi) = \frac{a}{\abs{z}}\\
            \cos(\theta + 2k\pi) = \frac{b}{\abs{z}} 
        }\]

        $\hfill\qed$
    \end{itemize}

    \begin{frameddefn}{Argomento principale di un numero complesso}
        Dato $z := a+ib \in \C$, definiamo come \textbf{argomento principale di $z$}, indicato come $\mathrm{Arg}(z)$, l'unico argomento di $z$ tale che $\mathrm{Arg}(z) \in [0, 2\pi]$
    \end{frameddefn}
    
    \begin{framedthm}{Formula di Eulero}
        Dato un angolo $\theta \in \R$, si ha che
        \[e^{i\theta} = \cos(\theta) + i \cdot \sin(\theta)\]
        \textit{(dimostrazione omessa)}
    \end{framedthm}

    \begin{framedcor}{Formula di De Moivre}
        Dato un angolo $\theta \in \R$ e $n \in \Z$, si ha che
        \[ (\cos(\theta) + i \cdot \sin(\theta))^n = \cos(n \theta) + i \cdot \sin(n \theta)\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Tramite la formula di Eulero, si ha che:
        \[(\cos(\theta) + i \cdot \sin(\theta))^n = (e^{i\theta})^n = e^{i(n \theta)} = \cos(n \theta) + i \cdot \sin(n \theta)\]

        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Forma polare di un numero complesso}
        Dato $z \in \C$ e posti $r := \abs{z}$ e $\theta := \mathrm{Arg}(z)$, si ha che
        \[z = r(\cos(\theta) + i \cdot \sin(\theta)) = re^{i\theta}\]
        Inoltre, definiamo tali altre due rappresentazioni di $z$ come \textbf{forma polare di $z$}
    \end{framedprop}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Rappresentando $z$ sul piano di Gauss, si ha che:
        \[r = \abs{z} \implies
        \soe{cc}{
            a = r \cos(\theta)&\\
            b = r \sin(\theta)&
        } \implies z = r\cos(\theta) + ir\sin(\theta) = r(\cos(\theta) + i \cdot \sin(\theta))\]
        \item Infine, per la formula di Eulero si ha che:
        \[z = r(\cos(\theta) + i \cdot \sin(\theta)) =  re^{i\theta}\]

        $\hfill\qed$
    \end{itemize}


    \begin{framedobs}{Operazioni in forma polare}
        Dati $z,w \in \C$ e posti $r_1 := \abs{z}, \theta_1 := \mathrm{Arg}(z)$ e $r_2 := \abs{w}, \theta_2 := \mathrm{Arg}(w)$, si ha che:
        \begin{enumerate}
            \item $z \cdot w = r_1r_2e^{i(\theta_1+\theta_2)}$
            \item $\frac{z}{w} = \frac{r_1}{r_2}e^{i(\theta_1-\theta_2)}$
            \item $z^n = r_1e^{in\theta_1}$, dove $n \in \Q$
        \end{enumerate}

        \textbf{Attenzione:} eccetto rari casi, per calcolare prodotti o divisioni tra numeri complessi risulta essere comunque più comodo calcolare il prodotto tramite la proprietà distributiva e la divisione riscrivendo $\frac{1}{w}$ come $\frac{a}{a^2+b^2}+i\cdot \frac{-b}{a^2+b^2}$, dove $w := a+ib$
    \end{framedobs}

    \textbf{Esempi:}
    \begin{enumerate}
        \item Dato $z = -i$, calcolare $z^4$.
        
        \begin{itemize}
            \item Calcoliamo l'argomento principale di $z$:
            \[r := \abs{z} = \sqrt{0^2+(-1)^2} = 1 \implies
            \left \{ \begin{array}{l}
                \cos(\theta) = \frac{0}{1} = 0   \\
                \sin(\theta) = \frac{-1}{1} = -1  \\
            \end{array}
            \right .
            \implies \mathrm{Arg}(z) = \frac{3}{2}\pi
            \]
            
            \item Di conseguenza, otteniamo che
            \[ z = r e^{\mathrm{Arg}(z) \cdot i} = e^{\frac{3}{2}\pi \cdot i} \implies z^{4} = e^{4\cdot \frac{3}{2}\pi \cdot i} = e^{6\pi \cdot i} = e^{0 \cdot i} = 1\]
        \end{itemize}
        
        \item Dato $z = 1-i$, calcolare $z^{10}$.
        
        \begin{itemize}
            \item Calcoliamo l'argomento principale di $z$:
            \[r:= \abs{z} = \sqrt{1^2+(-1)^2} = \sqrt{2} \implies 
            \left \{ \begin{array}{l}
                \cos(\theta) = \frac{1}{\sqrt{2}}   \\
                \sin(\theta) = \frac{-1}{\sqrt{2}}  \\
            \end{array}
            \right .
            \implies \mathrm{Arg}(z) = \frac{7}{4}\pi\]
            
            \item Di conseguenza, otteniamo che
            \[ z = r e^{\mathrm{Arg}(z) \cdot i} = \sqrt{2} e^{\frac{7}{4}\pi \cdot i} \implies z^{10} = (\sqrt{2})^{10} e^{10 \cdot \frac{7}{4}\pi \cdot i} = 2^5 e^{\frac{35}{2}\pi i} = 2^5e^{\frac{3}{2}\pi i}\]
            
            \item Avendo già visto precedentemente che $e^{\frac{3}{2}\pi i} = -i$,  otteniamo che:
            \[ z^{10} = 2^5e^{\frac{3}{2}\pi i} = -2^5i\]
        \end{itemize}
        
    \end{enumerate}
    
    \quad
    
    \section{Teorema fondamentale dell'algebra}
    \label{complex_radix}
    
    Dati $z \in \C$ e $n \in \N \mid n \geq 2$, ci chiediamo quante siano le \textbf{soluzioni complesse all'equazione $x^n = z$}:
    \begin{itemize}
        \item Se $z = 0$, l'unica soluzione risulta essere $x=0$
        \item Consideriamo quindi il caso in cui $z \neq 0$. Tramite la formula di De Moivre, possiamo riscrivere tale equazione come:
        \[ x^n = z \iff x = \sqrt[n]{z} \iff x = z^{\frac{1}{n}} \iff x = r^{\frac{1}{n}}e^{\frac{1}{n}\theta i}\]
        ottenendo quindi una soluzione valida all'equazione.
        
        \item Tuttavia, ricordando che un numero complesso $z$ possiede \textbf{infiniti argomenti}, riscriviamo $x$ nella forma più generica:
        \[ x = r^{\frac{1}{n}} e^{i(\frac{\theta}{n} + \frac{2k\pi}{n})}\]
    
        \item A questo punto, al variare di $k = 0, 1, \ldots, n-1$ è possibile trovare le \textbf{uniche $n$ soluzioni distinte all'equazione}. Difatti, quando $k=n$, riotteniamo la prima soluzione dell'equazione, mentre quando $k=n+1$ otteniamo la seconda, e così via. 
    \end{itemize}

    \textbf{Esempio:}
    
    \begin{itemize}
        \item Dato $z = i$, vogliamo sapere le soluzioni dell'equazione $x^3 = z$.
        \[ x^3 = i \iff x^3 = e^{\frac{1}{2}\pi i} \iff x = e^{i(\frac{1}{2\cdot 3}\pi + \frac{2k\pi}{3})}\]
    
        \begin{itemize}
            \item Se $k=0$
            \[ x_1 = e^{i(\frac{1}{2\cdot 3}\pi)} = e^{\frac{1}{6}\pi i}\]
            \item Se $k=1$
            \[ x_2 = e^{i(\frac{1}{2\cdot 3}\pi + \frac{2\pi}{3})} = e^{\frac{5}{6}\pi i}\]
            \item Se $k=2$
            \[ x_3 = e^{i(\frac{1}{2\cdot 3}\pi + \frac{4\pi}{3})} = e^{\frac{9}{6}\pi i} =  e^{\frac{3}{2}\pi i}\]
            
            \item Se $k=3$
            \[ x_4 = e^{i(\frac{1}{2\cdot 3}\pi + \frac{6\pi}{3})} = e^{i(\frac{1}{6}\pi + 2\pi)} =  e^{\frac{1}{6}\pi i} \implies x_4 = x_1\]
            \item Se $k=4$
            \[ x_5 = e^{i(\frac{1}{2\cdot 3}\pi + \frac{8\pi}{3})} = e^{i(\frac{1}{6}\pi + \frac{2\pi}{3} + 2\pi)} =  e^{\frac{5}{6}\pi i} \implies x_5 = x_2\]
            \item \ldots
        \end{itemize}

        \item Notiamo quindi che nonostante esistano \textbf{infiniti argomenti di z}, le soluzioni risultano essere cicliche tra di loro, risultando in solo \textbf{$3$ soluzioni valide}.
    

        \item Inoltre, graficando sul piano di Gauss le tre radici soluzioni dell'equazione, notiamo come ognuna di esse corrisponda al vertice di un triangolo equilatero iscritto in una circonferenza di raggio 1:
        
        \begin{center}
            \includegraphics[scale=0.55]{images/complex3.png}
        \end{center}
    \end{itemize}
        
    \begin{framedobs}{}
        Le $n$ radici n-esime di un numero complesso corrispondono ai vertici di un poligono regolare di $n$ lati inscritto in una circonferenza di raggio $\abs{z}^{\frac{1}{n}}$.
    \end{framedobs}
    
    \begin{framedthm}{Teorema fondamentale dell'algebra}
    Dato un polinomio  $p(x) := a_0 + a_1x + \ldots + a_nx^n = 0$ dove $a_i \in \C, n \geq 1, a_n \neq 0$, \textbf{esistono sempre $n$ radici complesse} di $p(x)$:
    \[\forall i \in [1,n] \quad \exists x_1, \ldots, x_n \in \C \mid p(x_i) = 0\]
    \end{framedthm}
    
    \chapter{Relazioni e Induzione}
    
    \begin{frameddefn}{Relazione}
    Dato un insieme $X$, definiamo come \textbf{relazione} $R$ su $X$ un \textbf{sottoinsieme del prodotto cartesiano} $X\times X$:
    \[ R \subseteq X \times X \iff R \subseteq \{(x,y) \mid  x, y \in X\}\]
    
    Data una coppia $(x,y)$, se essa appartiene alla relazione $R$ allora affermiamo ciò con la notazione $x \sim y$ (oppure con $R(x,y)$), altrimenti affermiamo che essa non appartiene alla relazione con la notazione $x\not\sim y$ (oppure con $\not\mathrel{R}(x,y)$).
    \[ x \sim y \iff (x,y) \in R \qquad\qquad x\not\sim y \iff (x,y) \notin R\]
    \end{frameddefn}
    
    \begin{frameddefn}{Relazione di equivalenza}
         Una relazione $\sim$ viene detta \textbf{relazione di equivalenza} se su di essa valgono le seguenti proprietà:
        \begin{itemize}
            \item \textbf{Riflessività}: $\forall x \in X \quad x \sim x$
            \item \textbf{Simmetria}: $\forall x,y \in X \quad x \sim y \implies y \sim x$
            \item \textbf{Transitività}: $\forall x,y,z \in X \quad x \sim y, y \sim z \implies x \sim z$
        \end{itemize}
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item La relazione di eguaglianza $a \sim b \iff a = b$ è una relazione di equivalenza
        \item Dato l'insieme $X$ corrispondente ad un insieme di automobili, la relazione $a \sim b \iff a \texttt{ ha lo stesso colore di } b$ è una relazione di equivalenza
    \end{itemize}
    
    \begin{frameddefn}{Relazione d'ordine totale e parziale}
         Una relazione $\prec$ viene detta \textbf{relazione d'ordine totale} se su di essa valgono le seguenti proprietà:
        \begin{itemize}
            \item \textbf{Riflessività}: $\forall x \in X \quad x \prec x$
            \item \textbf{Anti-simmetria}: $\forall x,y \in X \quad x \prec y, y \prec x \implies x = y$
            \item \textbf{Transitività}: $\forall x,y,z \in X \quad x \prec y, y \prec z \implies x \prec z$
            \item \textbf{Totalità}: $\forall x,y \in X \quad x \prec y \lor y \prec x$
        \end{itemize}
        
        Se $\prec$ è una relazione che soddisfa la riflessività, l'anti-simmetria, la transitività ma non la totalità, allora tale relazione viene detta \textbf{relazione d'ordine parziale}
    \end{frameddefn}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item La relazione di minor-eguaglianza $a \leq b$ è una relazione d'ordine
        
        \item Dato un insieme $X$, definiamo come $\mathcal{P}(X)$ l'insieme contenente tutte le parti di $X$ (ossia i suoi sottoinsiemi) 
        \[ \mathcal{P}(X) = \{ X' \mid X' \subseteq X\}\]
        
        La relazione $\subseteq$ su $\mathcal{P}(X)$ risulta essere una relazione d'ordine parziale, poiché:
        \begin{itemize}
            \item Ogni sottoinsieme $A$ è sottoinsieme di se stesso (riflessività):
            \[ A \subseteq A, \forall A \in \mathcal{P}(X)\]
            \item Se un sottoinsieme $A$ è sottoinsieme di $B$ e $B$ è sottoinsieme di $A$, allora ciò è possibile solo se $A$ e $B$ sono lo stesso sottoinsieme (anti-simmetria):
            \[ A \subseteq B \land B \subseteq A \implies A=B\]
            \item Se un sottoinsieme $A$ è sottoinsieme di $B$ e $B$ è sottoinsieme di $C$, allora anche $A$ è sottoinsieme di $C$ (transitività):
            \[ A \subseteq B \land B \subseteq C \implies A \subseteq C\]
            \item Non tutti i sottoinsiemi sono confrontabili tra loro (ordine non totale). Ad esempio, se $X = \{a,b,c\}$ si ha che:
            \[\{a\}, \{b,c\} \in \mathcal{P}(X) \implies \{a\} \not\subseteq \{b,c\} \land \{b,c\} \not\subseteq \{a\}\]
        \end{itemize}
    \end{itemize}
    
    \newpage
    
    \section{Classi di equivalenza}
    
    \quad
    
    \begin{frameddefn}{Classe di equivalenza}
        Sia $\sim$ relazione d'equivalenza definita su un insieme $X$. Dato un elemento $x \in X$, denotiamo come $[x]$ la sua \textbf{classe di equivalenza su $\sim$}, ossia l'insieme di tutti gli elementi in relazione con $x$:
        \[ [x] = \{ y \in X \mid x \sim y\}\]
    \end{frameddefn}
    
    \begin{framedobs}{}
        Sia $\sim$ relazione d'equivalenza definita su un insieme $X$. Dato un elemento $x \in X$, per riflessività della relazione $\sim$ si ha che:
        \[x \sim x \iff x \in [x]\]
    \end{framedobs}
    
    \begin{frameddefn}{Insieme quoziente}
        Sia $\sim$ relazione d'equivalenza definita su un insieme $X$. Definiamo come \textbf{insieme quoziente di $X$ su $\sim$} l'insieme di tutte le classi di equivalenza indotte dalla relazione:
        \[ X/\sim := \{ [x] \mid  x \in X\}\]
    \end{frameddefn}
    
    \begin{frameddefn}{Partizione di un insieme}
    Dato un insieme $X$, definiamo come \textbf{partizione di $X$} l'insieme $\{X_1, \ldots, X_n\}$ delle sue \textbf{parti}, ossia i suoi sottoinsiemi disgiunti tra loro la cui unione corrisponde ad $X$:
    \[X = \bigsqcup_{i=1}^n X_i\]
    dove $\bigsqcup$ corrisponde al simbolo di \textbf{unione disgiunta}, equivalente a:
    \[X = \bigcup_{1 \leq i,j\leq n} X_i \text{ dove } X_i \cap X_j = \emptyset, \forall i \neq j\]
    \end{frameddefn}
    
    \begin{framedobs}{}
    Data una relazione d'equivalenza $\sim$ definita su un insieme $X$, si verifica che:
    \[x \sim y \iff [x] = [y] \qquad\qquad x \not\sim y \iff [x] \cap [y] = \emptyset\]
    Dunque, \textbf{tutte le classi di equivalenza} indotte da $\sim$ sono \textbf{disgiunte tra loro}.
    \end{framedobs}
    
    \newpage
    
    \textit{Dimostrazione:}
     
    \begin{itemize}
        \item $x \sim y \implies [x] = [y]$
        \begin{itemize}
            \item Se $x \sim y$, allora si ha che:
            \[z \in [x] \iff z \sim x \implies z \sim x, x \sim y \implies z \sim y \iff z \in [y] \implies [x] \subseteq [y]\]
            
            \item Viceversa, siccome $x \sim y \iff y \sim x$, si ha che:
            \[w \in [y] \iff w \sim y \implies w \sim y, y \sim x \implies w \sim x \iff w \in [x] \implies [y] \subseteq [x] \]
        \end{itemize}
        
        \item $[x] = [y] \implies x \sim y$
        
        \begin{itemize}
            \item Se $[x] = [y]$, allora si ha che:
            \[z \in [x] = [y] \iff z \sim x, z \sim y \iff x \sim z, z \sim y \implies x \sim y\]
        \end{itemize}
        
        \item $x \not\sim y \implies [x] \cap [y]  = \emptyset$
        
        \begin{itemize}
            \item Supponiamo per assurdo che $x \not\sim y$ e che $[x] \cap [y] \neq \emptyset$. Dunque, si ha che:
            \[[x] \cap [y] \neq \emptyset \iff \exists z \in [x] \cap [y] \iff z \in [x] \land z \in [y] \iff\]
            \[\iff z \sim x, z \sim y \iff x \sim z, z \sim y \implies x \sim y\]
            contraddicendo l'ipotesi iniziale, dunque $x \not \sim y \implies [x] \cap [y] = \emptyset$
        \end{itemize}
        
        \item $[x] \cap [y]  = \emptyset \implies x \not\sim y$
        \begin{itemize}
            \item Supponiamo per assurdo che $[x] \cap [y] = \emptyset$ e che $x \sim y$. Dunque, si ha che:
            \[x \sim y \iff x \in [x] = [y] \implies [x] \cap [y] = [x] = [y] \neq \emptyset\]
            contraddicendo l'ipotesi iniziale, dunque $[x] \cap [y]  = \emptyset \implies x \not\sim y$
        \end{itemize}
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Data una relazione d'equivalenza $\sim$ definita su un insieme $X$, l'\textbf{insieme quoziente} $X/\sim$ è una \textbf{partizione} di $X$:
        \[X = \bigsqcup_{[x] \in X/\sim} [x]\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché tutte le classi di equivalenza appartenenti a $X/\sim$ sono disgiunte tra loro, si ha che:
        \[\bigcup_{[x] \in X/\sim} [x] = \bigsqcup_{[x] \in X/\sim} [x]\]
        \item Dato $x \in X$, si ha che:
        \[x \in X \iff x \sim x \iff x \in [x] \implies x \in \bigsqcup_{[x] \in X/\sim} [x]\]
        \item Viceversa, si ha che:
        \[z \in \bigsqcup_{[x] \in X/\sim} [x] \implies \exists [x] \in X/\sim \mid z \in [x] \iff z \sim x \implies z \in X\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Dato un insieme $X$, una partizione $P := \{X_1, \ldots, X_n\}$ di $X$ \textbf{indce una relazione di equivalenza} sull'insieme $X$
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Definiamo la relazione $x \sim y \iff x,y \in X_i$, dove $X_i \in P$, indicante che due elementi sono in relazione se e solo se appartengono alla stessa parte della partizione.
        \item Verifichiamo che si tratti di una relazione di equivalenza:
        \begin{itemize}
            \item Riflessività:
            \[\forall x \in X, \exists X_i \in P \mid x \in X_i \implies x \sim x\]
            \item Simmetria:
            \[x \sim y \iff x,y \in X_i, \exists X_i \in X \implies y \sim x\]
            \item Transitività:
            \[x \sim y, y \sim z \iff x,y \in X_i \land y,z \in X_j, \exists X_i, X_j \in P \implies y \in X_i \cap X_j\]
            Poiché tutte le parti sono per definizione disgiunte tra loro, abbiamo che $y \in X_i \cap X_j \iff X_i = X_j$, dunque si ha che:
            \[x \sim y, y \sim z \implies x,y \in X_i \land y,z \cap X_j \iff x,y,z \in X_i = X_j \implies x \sim z\]
        $\hfill\qed$
        \end{itemize}
    \end{itemize}
    
    \begin{framedprop}{Proiezione canonica al quoziente}
        Una relazione di equivalenza $\sim$ su un insieme $X$ induce una funzione suriettiva detta \textbf{proiezione canonica al quoziente} la quale mappa ogni elemento $x \in X$ alla propria classe di equivalenza su $\sim$:
        \[ \pi : X \to X/\sim \,: x \mapsto [x]\]
    \end{framedprop}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché per riflessività si ha che $x \sim x \iff x \in [x]$, la funzione di proiezione $\pi$ risulta essere evidentemente suriettiva:
        \[x \in [x] \implies \forall [x] \in X/\sim, \exists x \in X \mid \pi(x) = [x]\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Relazione di Divisore}
    
    \quad
    
    \begin{frameddefn}{Relazione di divisore}
        Dati due numeri naturali $m, n \in \Z$, definiamo la relazione "$m$ è divisore di $n$", indicato come $m \mid n$, se esiste un elemento $q \in \Z \mid  n = mq$:
        \[ m \mid n \iff \exists q \in \Z \mid n = mq \]
        
        \textit{\textbf{Attenzione}}: $m \mid n$ non è il simbolo matematico "tale che"
    \end{frameddefn}
    
    \begin{framedobs}{}
        Dati $m,n \in \Z$, la relazione di divisore $m \mid n$ è una relazione \textbf{riflessiva} e \textbf{transitiva}
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Soddisfa la \textbf{riflessività}:
        \[ \forall n \in Z, n = n \cdot 1 \iff n \mid n \cdot 1 \iff n \mid n\]
        
        \item Soddisfa la \textbf{transitività}:
        \[m \mid n, n \mid d \iff \exists p,q \in Z \mid n = mp, d = nq \implies d = (mp)q = m(pq) \implies m \mid d\]
        
        \item Non soddisfa l'\textbf{anti-simmetria}:
        \[m \mid n, n \mid m \iff \exists p,q \in Z \mid n = mp, m = nq \implies n = mp = (np)q = n(pq)\]
        
        A questo punto, si verificano due casi:
        \begin{itemize}
            \item Se $n = 0$ allora
            \[n = 0 \implies m = nq = 0 \cdot q = 0 \implies m = 0 \implies n = m = 0\]
            \item Se $n \neq 0$ allora
            \[n \neq 0 \implies n = n(pq) \implies qp = 1 \implies p = q = \pm 1 \implies \]
            \[\implies \left \{ \begin{array}{ll}
                n = m & \text{ se } p = q = 1\\
                n = -m & \text{ se } p = q = -1
            \end{array} \right .\]
        \end{itemize}
        
        Dunque, non in tutti i casi la relazione è anti-simmetrica.
        
        $\hfill\qed$
    \end{itemize}
    
    \label{antisim}
    \begin{framedcor}{}
        Dati $m,n \in \N$, la relazione di divisore $m \mid n$ è una \textbf{relazione d'ordine}
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Ovviamente, poiché $m,n \in \N \subseteq \Z$, se segue che la relazione di divisore sia riflessiva e transitiva
        \item Procedendo analogamente alla dimostrazione precedente, il caso in cui $p = q = -1$ verrebbe scartato poiché $-1 \notin \N$, rendendo quindi $m=n$ l'unica possibilità
        \[m \mid n, n \mid m, m,n \in \N \implies m = n\]
        $\hfill\qed$
    \end{itemize}
    
    \section{Relazione di Congruenza}
    
    \quad
    
    \begin{frameddefn}{Relazione di congruenza}
        Dato $a,b \in \Z$ e dato $n \geq 2 \in \N$, definiamo la relazione "$a$ è congruente a $b$ in modulo $n$", denotata come con $a \equiv b (\texttt{mod } n)$, se e solo se $n \mid  b-a$
        \[ a \equiv b (\texttt{mod } n) \iff n \mid (b-a)\]
    \end{frameddefn}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item $7 \equiv 22 (\texttt{mod } 5) \implies n \mid b-a  \implies 5 \mid (22-7) \implies 5 \mid 15$
        \item $7 \equiv 2 (\texttt{mod } 5) \implies n \mid b-a  \implies 5 \mid (2-7) \implies 5 \mid -5$
    \end{itemize}

    \quad
    
    \begin{framedobs}{}
        La relazione di congruenza $a \equiv b (\texttt{mod }n)$ è una \textbf{relazione di equivalenza}.
    \end{framedobs}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item \textbf{Riflessiva}:
        \[ a = a \iff a = n\cdot 0 + a \iff a-a = n \cdot 0 \iff n \mid a-a \iff a \equiv a (\texttt{mod } n)\]
        
        \item \textbf{Simmetrica}:
        
        \[ a \equiv b (\texttt{mod } n) \iff n \mid  b-a \iff \exists p \in \Z \mid b-a = n p \iff \]
        \[ \iff a-b = n(-p) \iff n \mid  a-b \iff b \equiv a (\texttt{mod } n)\]
        
        \item \textbf{Transitiva}:
        
        \[ a \equiv b (\texttt{mod } n), b \equiv c (\texttt{mod n}) \iff \exists p,q \in \Z \mid b-a = np, c-b = nq \implies \]
        \[ \implies c-a = (c-b)+(b-a) = qn + pn = n(q+p) \iff n \mid  c-a \iff a \equiv c (\texttt{mod } n)\]
    \end{itemize}
    
    \quad
    
    \section{Teorema della divisione con resto euclidea}
    
    \quad
    
    \begin{framedthm}{Teorema della divisione con resto euclidea}
    Dati due interi $m, n \in \Z$ dove $n > 0$, si ha che:
    \[ \exists! q,r \in \Z, 0 \leq r < n \mid  m = nq+r\]
    
    dove $q$ viene definito come \textbf{quoziente} e $r$ come \textbf{resto} della divisione
    \end{framedthm}
    
    \textit{Dimostrazione dell'esistenza:}
    
    \begin{itemize}
        \item Dato $[m] := \{ a \in \Z \mid  a \equiv m (\texttt{mod } n)\}$, si ha che:
        \[a \in [m] \iff a \equiv m(\texttt{mod } n) \iff n \mid  m-a \iff \exists p \in \Z \mid m-a = np \iff a = m-np\]
    
        \item Consideriamo quindi $[m]_{\geq 0} := \{a \in [m] \mid a \in \N\}$. Poiché $[m]_{\geq 0} \subseteq \N$, per il \href{https://it.wikipedia.org/wiki/Principio_del_buon_ordinamento}{\textbf{principio del buon ordinamento}} si ha che:
        \[\exists r \in [m]_{\geq 0} \mid r = \min([m]_{\geq 0}) \implies \exists q \in \Z \mid r = m - nq\]
    
        \item Supponiamo per assurdo che $r \geq n$, da cui ne segue che $r-n \geq 0 \implies r-n \in \N$. Dunque, abbiamo che:
        \[ r-n = (m-nq)-n \iff r-n = m-n(q+1) \iff r-n \in [m]_{\geq 0}\]
        
        \item Poiché $r-n \leq r$, l'ipotesi per cui $r$ sia il minimo di $m_{\geq 0}$ viene contraddetta, dunque l'unica possibilità è che $r < n$
        
        \item Dunque, concludiamo che
        \[\exists q, r \in \Z, 0 \leq r < n \mid r = m-nq \implies m = nq + r\]
        $\hfill\qed$
    \end{itemize}
    
    \textit{Dimostrazione dell'unicità:}
    
    \begin{itemize}
        \item Supponiamo che $q$ ed $r$ non siano unici. Allora, ne segue che:
        \[\exists q_1, q_2, r_1, r_2 \in \Z, 0 \leq r_1, r_2 < n \mid nq_1+r_1 = m = nq_2 + r_2 \implies \]
        \[\implies nq_1+r_1 = nq_2+r_2 \implies r_2 - r_1 = n (q_1-q_2) \iff n \mid  r_2-r_1\]
    
        \item Siccome $0 \leq r_1, r_2 < n \implies -n < r_2 - r_1 < n$ e siccome $n \mid  r_2-r_1$, allora $r_2-r_1$ deve necessariamente essere un multiplo di $n$ compreso tra $-n$ ed $n$ stesso.

        \item Poiché l'unico numero rispettante tali caratteristiche è $0$, ne segue che:
        \[\left \{ \begin{array}{l}
            -n < r_2 - r_1 < n \\
            n \mid r_2-r_1 \iff \exists b \in \Z \mid r_2-r_1 = nb
        \end{array}\right . \iff r_2-r_1 = 0 \iff r_2 = r_1\]
    
        \item A questo punto, poiché $n>0$,si ha che:
        \[nq_1+r_1 = nq_2 + r_2 \iff nq_1+0 = nq_2 + 0 \iff n(q_1-nq_2)=0 \iff q_1 = q_2\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Relazione di Coniugio}
    \label{conjiugate}
    
    \quad
    
    \begin{frameddefn}{Relazione di coniugio}
        Dato un gruppo $G$ e dati $g,h \in G$, definiamo la relazione "$g$ è coniugato di $h$" se si verifica che:
        \[ g \sim h \iff \exists a \in G \mid  h = aga^{-1}\]
    \end{frameddefn}
    
    \begin{framedobs}{}
        Se $G$ è un gruppo abeliano, allora si ha che:
        \[ g \sim h \iff h = aga^{-1} = aa^{-1}g = g\]
    \end{framedobs}
    
    \begin{framedobs}{}
        La relazione di coniugio è una \textbf{relazione di equivalenza}.
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item \textbf{Riflessività}:
        \[ g = 1 \cdot g \cdot 1^{-1} \implies g \sim g\]
        
        \item \textbf{Simmetria}:
        \[ g \sim h \implies h = aga^{-1} \implies a^{-1}ha = a^{-1}aga^{-1}a \implies a^{-1}h{a} = g\]
        ponendo $b := a^{-1}$, si ha che:
        \[ bhb^{-1} = g \implies h \sim g\]
        
        \item \textbf{Transitività}:
        \[g \sim h \land h \sim k \implies h = aga^{-1}, k = bhb^{-1} \implies k = b(aga^{-1})b^{-1} = (ba)g(a^{-1}b^{-1})\]
        
        ponendo $c := ba$, si ha che:
        \[ k = cgc^{-1} \implies g \sim k\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Induzione matematica}
    \label{induction}
        
    Vogliamo dimostrare una successione di $n$ proposizioni, etichettate come $p_1), p_2), \ldots, p_n)$. Supponiamo di aver dimostrato la proposizione $p_1)$, che denominiamo come \textbf{caso base}. Se le prime $p_1), \ldots, p_n)$ sono vere, allora anche la proposizione $p_{n+1})$ è vera (\textbf{passo induttivo}).
    
    Per esprimere tale concetto matematicamente, possiamo dire che:
    
    \begin{framedthm}{Principio di induzione}
    Data una successione di proposizioni $p_1), \ldots, p_n)$, si ha che:
    \[ p_1) \implies p_2)\]
    \[ p_1), p_2) \implies p_3)\]
    \[\ldots\]
    \[ p_1), \ldots, p_n) \implies p_{n+1})\]
    \end{framedthm}
    
    \newpage

    \textbf{Esempi:}
    \begin{enumerate}
        \item \begin{itemize}
            \item Vogliamo verificare che la proposizione seguente proposizione sia vera $\forall n \geq 1$:
            \[ 1+2+3+\ldots+(n-1)+n = \frac{n(n+1)}{2}\]
            \item Verifichiamo quindi il \textbf{caso base $p_1)$}, ossia $n = 1$
            \[ 1 = \frac{1(1+1)}{2} = \frac{2}{2}\]
            che risulta essere vero
            \item A questo punto, assumiamo per \textbf{ipotesi induttiva} che $p_n)$ sia vera.

            \item Impostiamo quindi il \textbf{passo induttivo}, ossia $p_{n+1})$:
            \[1+2+3+\ldots+n+(n+1) = \frac{(n+1)(n+1+1)}{2}\]
            
            \item Notiamo come il \textbf{passo induttivo contenga al suo interno l'ipotesi induttiva stessa}, che abbiamo affermato essere vera:
            
            \[\underbrace{1+2+3+\ldots+n}_{\text{Ipotesi induttiva}}+(n+1) = \frac{(n+1)(n+1+1)}{2} \iff\]
            \[\frac{n(n+1)}{2}+(n+1) = \frac{(n+1)(n+2)}{2} \iff \frac{n(n+1)+2(n+1)}{2} = \frac{(n+1)(n+2)}{2}\]\[\iff \frac{(n+1)(n+2)}{2} = \frac{(n+1)(n+2)}{2}\]
            Dunque, anche il passo induttivo risulta essere vero, concludendo che \textbf{la proposizione $p_n)$ sia valida $\forall n \geq 1$}

            $\hfill\qed$
        \end{itemize}
        
        \quad
        
        \item \begin{itemize}
            \item La funzione di Fibonacci è definita come:
            $$
            \left \{
            \begin{array}{l r}
                F_0 = 0 & \text{se } n=0\\
                F_1 = 1 & \text{se } n=1\\
                F_n = F_{n-1} + F_{n-2} & \text{se } n\geq 2\\
            \end{array}
            \right .
            $$
            
            \item Le costanti $\varphi$ e $\psi$, corrispondenti alle soluzioni dell'equazione $x^2 = x+1$, sono definite come:
            \[ \varphi = \frac{1+\sqrt{5}}{2} \qquad \psi = \frac{1-\sqrt{5}}{2}\]
            
            \item Vogliamo verificare per induzione che la seguente proposizione sia vera $\forall n$:
            \[ F_n = \frac{\varphi^n-\psi^n}{\varphi-\psi}\]
            
            \item Verifichiamo quindi $p_0)$ e $p_1)$:
            \[ F_0 = \frac{\varphi^0-\psi^0}{\varphi-\psi} = \frac{1-1}{\varphi-\psi} = 0\]
            \[ F_1 = \frac{\varphi^1-\psi^1}{\varphi-\psi} = \frac{\varphi-\psi}{\varphi-\psi} = 1\]
            \item Assumiamo quindi per ipotesi induttiva che $p_{n-1})$ sia vera e verifichiamo il passo induttivo $p_{n}$, utilizzando però la definizione originale di $F_n$:
            \[ F_{n} = F_{n-1} + F_{n-2} =  \frac{\varphi^{n-1}-\psi^{n-1}}{\varphi-\psi} +  \frac{\varphi^{n-2}-\psi^{n-2}}{\varphi-\psi} =  \frac{\varphi^{n-2}(\varphi+1)-\psi^{n-2}(\psi+1)}{\varphi-\psi}\]
            \item Siccome per definizione stessa $\varphi^2 = \varphi+1$ e $\psi^2 = \psi+1$, allora abbiamo che:
            
            \[ F_{n} = F_{n-1} + F_{n-2} =  \frac{\varphi^{n-2}\varphi^2-\psi^{n-2}\psi^2}{\varphi-\psi} = \frac{\varphi^{n}-\psi^{n}}{\varphi-\psi}\]
            verificando quindi la validità del passo induttivo
            
            $\hfill\qed$
        \end{itemize}
        
    \end{enumerate}

    \begin{framedprop}{Identità binomiale di Newton}
        Dati $a, b \in \R$ e $n \in \N$, si ha che
        \[ (a+b)^n = \sum_{k=0}^n \binom{n}{k}a^kb^{n-k}\]
        dove il \textbf{coefficiente binomiale} è definito come:
        \[ \binom{n}{k} = \frac{n!}{(n-k)! \cdot k!}\]
    \end{framedprop}

    \textit{Dimostrazione:}
            
    \begin{itemize}
        \item \textbf{Caso base:}
        \[ 1 = (a+b)^0 = \sum_{k=0}^0 \binom{0}{k}a^kb^{0-k} = \binom{0}{0}a^0b^{0-0} = 1\]
        
        \item \textbf{Passo induttivo}:
        \[\sum_{k=0}^{n+1} \binom{n+1}{k}a^kb^{n+1-k} = (a+b)^{n+1} = (a+b)(a+b)^n = \]
        \[ =(a+b)\sum_{k=0}^n \binom{n}{k}a^kb^{n-k} = \sum_{k=0}^n \binom{n}{k}a^{k+1} b^{n-k}+ \sum_{k=0}^n \binom{n}{k}a^k b^{n-k+1}\]
        
        \item Trasliamo di -1 l'indice della prima sommatoria e portiamo fuori il suo ultimo termine:
        \[\sum_{k=1}^{n+1} \binom{n}{k-1}a^{k} b^{n+1-k}+ \sum_{k=0}^n \binom{n}{k}a^k b^{n-k+1} = \]
        \[= \binom{n}{n+1-1} a^{n+1}b^{n+1-(n+1)} + \sum_{k=1}^{n} \binom{n}{k-1}a^{k} b^{n+1-k}+ \sum_{k=0}^n \binom{n}{k}a^k b^{n-k+1} = \]
        \[= a^{n+1} + \sum_{k=1}^{n} \binom{n}{k-1}a^{k} b^{n+1-k}+ \sum_{k=0}^n \binom{n}{k}a^k b^{n-k+1} \]
        
        \item Nella seconda sommatoria, invece, portiamo fuori il primo termine, in modo che gli indici di entrambe le sommatorie coincidano:
        \[a^{n+1} + \sum_{k=1}^{n} \binom{n}{k-1}a^{k} b^{n+1-k}+ \sum_{k=1}^n \binom{n}{k}a^k b^{n-k+1} + \binom{n}{0}a^0 b^{n-0+1} = \]
        \[ = a^{n+1} + \sum_{k=1}^{n} \binom{n}{k-1}a^{k} b^{n+1-k}+ \sum_{k=1}^n \binom{n}{k}a^k b^{n-k+1} + b^{n+1}\]
        
        \item A questo punto uniamo nuovamente le due sommatorie:
        \[ a^{n+1} + b^{n+1} + \sum_{k=1}^n \left [ \binom{n}{k-1} + \binom{n}{k} \right ] a^k b^{n-k+1}\]
        
        \item Per le proprietà dei coefficienti binomiali (facilmente verificabili) si ha che $\binom{n+1}{k} = \binom{n}{k-1} + \binom{n}{k}$, dunque riscriviamo la sommatoria come:
        \[ = a^{n+1} + b^{n+1} + \sum_{k=1}^n \binom{n+1}{k} a^k b^{n-k+1}=\]
        
        \item A questo punto, poiché $\binom{n}{0} = \binom{n}{n+1} = 1$, riscriviamo i due termini esterni alla sommatoria in modo da poterli reinserire in essa, ottenendo il risultato cercato:
        \[\binom{n+1}{n+1}a^{n+1} + \binom{n+1}{0}b^{n+1} + \sum_{k=1}^n \binom{n+1}{k} a^k b^{n-k+1}= \sum_{k=0}^{n+1} \binom{n+1}{k} a^k b^{n+1-k}\]
        
        $\hfill\qed$
    \end{itemize}
        
    \chapter{Elementi di Teoria degli Anelli}
    
    \section{Classi laterali sinistre}
    \label{class_sx}
    
    \quad
    
    \begin{framedprop}{}
    Sia $G$ un gruppo e sia $H \subgrp G$ sottogruppo. La seguente relazione è una \textbf{relazione di equivalenza}:
    \[x \sim y \iff x^{-1}y \in H\]
    \end{framedprop}
    \textit{Dimostrazione:}
    \begin{itemize}
        \item \textbf{Riflessività}:
        \[ x \sim x \implies x^{-1}x = 1\in H \]
        \item \textbf{Simmetria}:
        \[ x \sim y \implies h := x^{-1}y \in H \implies h^{-1} := y^{-1}x \in H \implies y \sim x\]
        \item \textbf{Transitività}:
        \[ x \sim y, y \sim z \implies h := x^{-1}y, k := y^{-1}z \in H \implies\]
        \[ \implies hk = x^{-1}yy^{-1}z = x^{-1}z \implies x^{-1}z \in H\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Classi laterali sinistre}
        Sia $G$ un gruppo e sia $H \subgrp G$. Definiamo come \textbf{classi laterali sinistre di H di G} le classi d'equivalenza generate dalla relazione d'equivalenza $x \sim y \iff x^{-1}y \in H$:
        \[ x \in G \quad [x] = \{ y \in G \mid  x \sim y\}\]
        
        Denotiamo come $G/H$ (letto "$G$ modulo $H$") l'\textbf{insieme quoziente di tutte le classi laterali sinistre} di $H$ in $G$.
    \end{frameddefn}

    \begin{framedprop}{Insieme quoziente $\Z_n$}
        Dato $I(n) \subgrp \Z$, si ha che:
        \[ \Z_n := \{ [0], \ldots, [n-1]\} = \Z / I(n) = \Z / \equiv\]
    \end{framedprop}  
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Considerando la relazione $a \sim b \iff (-a)+b \in I(n)$ (poiché $a^{-1}$ nell'operazione somma corrisponde a $-a$), otteniamo che:
        \[ a \sim b \iff -a+b = b-a \in I(n) \iff -a+b = nk, \exists k \in \Z \iff\]
        \[\iff n \mid  b-a \iff a \equiv b (\texttt{mod } n)\]
        dunque si ha che $ \Z_n := \Z / I(n) = \Z / \equiv$
        
        $\hfill\qed$
    \end{itemize}

    \textbf{Esempio:}
    \begin{itemize}
        \item Dato $I(3) \ideal \Z$, si ha che: $\Z/I(3) = \{ [0], [1], [2]\}$. Difatti, notiamo che:
        \begin{itemize}
            \item $0 \in [0]$, poiché $0 \sim 0 \iff -0+0=0 = 3 \cdot 0 \in I(3)$
            \item $1 \in [1]$, poiché $1 \sim 1 \iff -1+1=0 = 3 \cdot 0 \in I(3)$
            \item $2 \in [2]$, poiché $2 \sim 2 \iff -2+2=0 = 3 \cdot 0 \in I(3)$
            \item $3 \in [0]$, poiché $3 \sim 0 \iff -3+0=-3 = 3 \cdot (-1) \in I(3)$
            \item $4 \in [1]$, poiché $4 \sim 1 \iff -4+1=-3 = 3 \cdot (-1) \in I(3)$
            \item $5 \in [2]$, poiché $5 \sim 2 \iff -5+2=-3 = 3 \cdot (-1) \in I(3)$
            \item $6 \in [0]$, poiché $6 \sim 0 \iff -6+0=-6 = 3 \cdot (-2) \in I(3)$
            \item $7 \in [1]$, poiché $7 \sim 1 \iff -7+1=-6 = 3 \cdot (-2) \in I(3)$
            \item $8 \in [2]$, poiché $8 \sim 2 \iff -8+2=-6 = 3 \cdot (-2) \in I(3)$
            \item $9 \in [0]$, poiché $9 \sim 0 \iff -9+0=-9 = 3 \cdot (-3) \in I(3)$
            \item $\ldots$
        \end{itemize}
    \end{itemize}
    
    \begin{framedobs}{}
        Dato un gruppo $G$ e $H \subgrp G$, per ogni $[x] \in G/H$ si ha che:
        \[ [x] = xH := \{ xh \mid  h \in H\}\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dimostriamo che $[x] = xH$:
        \[y \in [x] \iff x \sim y \iff h := x^{-1}y \in H \iff h = x^{-1}y \iff\]
        \[\iff xh = xx^{-1}y \iff xh = y \in xH \]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \subsection{Teorema di Lagrange}
    
    \quad
    
    \begin{framedobs}{}
    Dato un gruppo $G$ e $H \subgrp G$, per ogni $[x] = xH\in G/H$ si ha che:
    \[\abs{[x]} = \abs{xH} = \abs{H}\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dato $x \in G$ consideriamo la funzione $\varphi : H \to xH : h \mapsto xh$
        \item La funzione risulta essere iniettiva poiché:
        \[h, k \in H \mid \varphi(h) \neq \varphi (k) \iff xh \neq xk \iff h \neq k\]
        
        \item La funzione risulta essere suriettiva poiché per costruzione di $xH$ si ha che:
        \[\forall xh \in xH, \exists h \in H \mid \varphi(h) = xh\]
        
        \item Poiché esiste una funzione biettiva $\varphi : H \to xH$, ne segue che $\abs{H} = \abs{xH}$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Teorema di Lagrange}
    
    Sia $G$ un \textbf{gruppo finito} e sia $H \subgrp G$. In tal caso, si ha che:
    \[ \abs{G} = \abs{H} \cdot \abs{G/H}\]
    
    Inoltre, definiamo $[G : H] := \abs{G/H}$ come l'\textbf{indice di $H$ in $G$}
    
    \end{framedthm}
    
    \newpage
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $G /H$ è una partizione di $G$ e poiché $\forall [x] \in G/H, \abs{[x]} = \abs{H}$:
        \[ G = \bigsqcup_{[x] \in G/H} [x] \implies \abs{G} = \abs{H} \cdot \abs{G/H}\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{L'anello commutativo $\Z_n$}
    
    \quad
    
    \begin{framedprop}{Gruppo quoziente $G/H$}
        Dato il gruppo abeliano $(G, +)$ e $H \subgrp G$, si ha che $(G/H, +)$ è un \textbf{gruppo abeliano}.
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dimostriamo prima che l'operazione somma intesa come $[x]+[y]=[x+y]$ sia ben definita, ossia che $[x]=[x'], [y]=[y'] \implies [x+y]=[x'+y']$:
        \[[x]=[x'], [y]=[y'] \iff x \sim x', y \sim y' \iff x'-x, y'-y \in H\]
        
        Poiché $h_1 := x'-x, h_2 := y'-y \in H$, per chiusura nella somma di $H$ si ha che:
        \[h_1 + h_2 \in H \implies (x'-x)+(y'-y) = x'-x+y'-y = x'+y'-(x+y) \in H \iff\]
        \[x'+y' \sim x+y \iff [x+y]=[x'+y']\]
        
        \item Successivamente, verifichiamo gli assiomi di gruppo abeliano:
        
        \begin{itemize}
            \item \textbf{Associatività}:
            \[ ([x]+[y])+[z] = [x+y]+[z] = [x+y+z] = [x] + [y+z] = [x] + ([y]+[z]) \]
            \item \textbf{Elemento neutro}:
            \[ [x]+[0] = [x+0] = [x]\]
            \item \textbf{Elemento inverso}:
            \[ [x]+[-x] = [x+(-x)] = [0]\]
            \item \textbf{Commutatività}:
            \[ [x]+[y] = [x+y] = [y+x] = [y]+[x]\]
            $\hfill\qed$
        \end{itemize}
    \end{itemize}
        
    \begin{framedcor}{Gruppo quoziente $\Z_n$}
        Poiché $\Z$ è un anello commutativo, ($\Z_n, +$) è un \textbf{gruppo abeliano}
    \end{framedcor}
    
    \textbf{Esempi:}
    
    Operando nel gruppo $\Z_{11}$ si avrà che:
    
    \begin{itemize}
        \item $[9]+[8]=[17]=[6]$, poiché $17 \equiv 6 (\texttt{mod } 11)$
        \item $[4]+[3]=[7]$
        \item $[5]-[6]=[-1]=[10]$, poiché $-1 \equiv 10 (\texttt{mod } 11)$
    \end{itemize}
    
    \quad
    
    \begin{framedprop}{Anello quoziente $G/H$}
        Dato l'anello commutativo $A$ e $I \ideal A$, si ha che $(A/I, +, \cdot)$ è un \textbf{anello commutativo}.
    \end{framedprop}
        
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $I \subgrp A$, per dimostrazione precedente si ha che $(A/I, +)$ è gruppo abeliano
        
        \item Dimostriamo prima che l'operazione prodotto intesa come $[x][y]=[xy]$ sia ben definita, ossia che $[x]=[x'], [y]=[y'] \implies [xy]=[x'y']$:
        \[[x]=[x'], [y]=[y'] \iff x \sim x', y \sim y' \iff x'-x, y'-y \in I\]
        
        Poiché $i_1 := x'-x, i_2 := y'-y \in I$, per chiusura nel prodotto di $I$ si ha che:
        \[i_1, i_2 \in I \implies i_1y', xi_2 \in I \implies i_1y'+xi_2 \in I \implies\]
        \[i_1y'+xi_2 = (x'-x)y'+x(y'-y) = x'y'-xy'+xy'-xy = x'y'-xy \in I \iff\]
        \[\iff x'y' \sim xy \iff [x'y']=[xy]\]
        
        \item Successivamente, verifichiamo i rimanenti assiomi di anello commutativo
        \begin{itemize}
            \item \textbf{Associatività nel prodotto}:
            \[ ([x][y])[z] = [xy][z] = [xyz] = [x][yz] = [x]([y][z])\]
            \item \textbf{Elemento neutro nel prodotto}:
            \[ [x][1] = [x \cdot 1] = [x]\]
            \item \textbf{Commutatività nel prodotto}:
            \[ [x][y] = [xy] = [yx] = [y][x]\]
            \item \textbf{Distributività}:
            \[[x]([y]+[z]) = [x][y+z] = [x(y+z)] = [xy+xz] = [xy]+[xz] = [x][y]+[x][z]\]
            $\hfill\qed$
        \end{itemize}
    \end{itemize}
    
        
    \begin{framedcor}{Anello quoziente $\Z_n$}
        Poiché $\Z$ è un anello commutativo, ($\Z_n, +, \cdot$) è un \textbf{anello commutativo}
    \end{framedcor}
    
    \textbf{Esempi:}
    
    Operando nell'anello $\Z_{4}$ avremo che:
    \begin{itemize}
        \item $[2][3] = [6] = [2]$, poiché $6 \equiv 2 (\texttt{mod }4)$
        \item $[2][3]^{-1} = [2][3] = [4]$, poiché $[3]$ è l'inverso di $[3]$ in $\Z_4$ in quanto $[3][3] = [9] = [1]$
    \end{itemize}
    
    \quad
    
    \section{Invertibili e Divisori dello zero}
    \label{inv}
    
    \quad
    
    \begin{frameddefn}{Invertibile e Divisore dello zero}
        Dato un anello commutativo $A$ e un elemento $a \in A$, definiamo $a$ come \textbf{elemento invertibile} se e solo se
        \[\exists a^{-1} \in A \mid aa^{-1} = a^{-1}a = 1\]
        
        Definiamo invece $a$ come \textbf{divisore dello zero} se e solo se:
        \[a \mid 0 \iff \exists c \neq 0 \in A \mid 0 = ac\]
    \end{frameddefn}
    
    \begin{frameddefn}{Gruppo degli invertibili}
        Dato un anello commutativo $(A, +, \cdot)$, definiamo l'\textbf{insieme degli invertibili di $A$} come:
        \[A^* := \{a \in A \mid \exists a^{-1} \in A\}\]
        
        Inoltre, $(A^*, \cdot)$ è un \textbf{gruppo}
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item \textbf{Chiusura:}
        \[x,y \in A^* \implies (xy)^{-1} = y^{-1}x^{-1} \implies xy \in A^*\]
        \item \textbf{Associatività:}
        \[x,y,z \in A^* \implies x(yz) = xyz = (xy)z\]
        \item \textbf{Elemento neutro:}
        \[1 = 1^{-1} \in A \mid 1 \cdot 1^{-1} = 1 \cdot 1 = 1 \implies 1 \in A^* \mid a \cdot 1 = a, \forall a \in A^*\]
        \item \textbf{Elemento inverso:}
        \[x \in A^* \implies x = (x^{-1})^{-1} \implies x^{-1} \in A^*\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
         Dato un anello commutativo $A$ e un elemento $a \in A$, se $a$ è un \textbf{divisore dello zero} allora esso \textbf{non è invertibile}:
         \[a \mid 0 \implies a \notin A^*\]
         
         Dunque, per contronominale di tale implicazione, se $a$ è \textbf{invertibile} allora esso \textbf{non è un divisore dello zero}:
         \[a \in A^* \implies a \nmid 0\]
    \end{framedobs}
    
    \textit{Dimostrazione per assurdo:}
    
    \begin{itemize}
        \item Supponiamo che per assurdo che $a \mid 0$ e che $a \in A^*$. Allora, si ha che:
        \[ a \mid 0 \iff \exists b\neq 0 \in A \mid 0 = ab \implies a^{-1} \cdot 0 = a^{-1}ab \implies 0 = b\]
        contraddicendo quindi l'ipotesi iniziale $b \neq 0$, dunque l'unica possibilità è che $a \notin A^*$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Dominio di integrità}
    Sia $A$ un anello commutativo. Definiamo $A$ come \textbf{dominio di integità} se $0 \in A$ è l'unico divisore dello zero:
    \[\nexists a \neq 0 \in A \text{ t.c. } a \mid 0 \iff a \nmid 0, \forall a \neq 0 \in A\]
    \end{frameddefn}
    
    \begin{framedobs}{}
        Un anello commutativo $A$ è un dominio di integrità se e solo se vale la \textbf{legge di annullamento del prodotto}:
        \[\forall x,y \in A \mid xy = 0 \implies x=0 \lor y=0\]
    \end{framedobs}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo per assurdo che $A$ sia un dominio di integrità e che $\exists x,y \neq 0 \in A \mid xy = 0$, implicando che non valga la legge di annullamento del prodotto. Dunque, si ha che:
        \[xy = 0 \implies x^{-1}xy=x^{-1}0 \implies y = 0\]
        contraddicendo l'ipotesi per cui $y\neq0$, dunque l'unica possibilità è che valga la legge di annullamento del prodotto
        \item Supponiamo ora per assurdo che valga la legge di annullamento del prodotto e che $A$ non sia un dominio di integrità. Dunque, si ha che:
        \[\exists a \neq 0 \in A \text{ t.c } a \mid 0 \implies ab = 0, \exists b \neq 0\]
        Poiché vale la legge di annullamento del prodotto, si ha che
        \[ab=0 \implies a = 0 \lor b = 0\]
        Tuttavia, poiché $b \neq 0$, l'unica possibilità è che $a = 0$, contraddicendo l'ipotesi per cui $a \neq 0$. Di conseguenza, $A$ è un dominio di integrità
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        L'anello commutativo $\Z$ è un \textbf{dominio di integrità} poiché in esso vale la \textbf{legge di annullamento del prodotto}
    \end{framedcor}
    
    \begin{framedobs}{}
        Se $K$ è un \textbf{campo}, allora esso è un \textbf{dominio di integrità} poiché $K^* = K-\{0\}$
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Se $K$ è un campo, allora
        \[\forall a \neq 0 \in K, \exists a^{-1} \in K \mid aa^{-1} = 1 \iff K^* = K-\{0\}\]
        \item Inoltre, siccome tutti gli elementi di $K$ escluso zero sono invertibili, si ha che:
        \[\forall a \neq 0 \in K, a \in K^* \implies a \nmid 0, \forall a \neq 0 \in K\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Dato un \textbf{dominio di integrità} $A$ e dati $a,b \in A$, si ha che:
        \[I(a) = I(b) \iff a=bc, \exists c \in A^*\]
    \end{framedprop}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item $a = bc, \exists c \in A^* \implies I(a)=I(b)$
        $$
        a = bc, \exists c \in A^* \implies ac^{-1} = b \implies
        \left \{
        \begin{array}{l}
            a=bc \implies a \in I(b) \implies I(a) \subseteq I(b)\\
            b=ac^{-1} \implies b \in I(a) \implies I(b) \subseteq I(a)\\
        \end{array}
        \right .
        $$
        \item $I(a)=I(b) \implies a = bc, \exists c \in A^*$
        $$
        I(a)=I(b) \implies
        \left \{
        \begin{array}{l}
            a \in I(a) = I(b) \implies a=bc, \exists c \in A\\
            b \in I(b) = I(a) \implies b=ad, \exists d \in A
        \end{array}
        \right .
        $$
        Dunque si verifica che:
        \[ a = bc = adc \implies a = adc \implies a(1-dc)=0 \implies\]
        $$
        \implies \left \{
        \begin{array}{l}
            a = 0 \implies b = ad = 0 \implies a = bc = 0\\
            1-dc=0 \implies dc = 1 \implies c = d^{-1} \implies c \in A^*
        \end{array}
        \right .
        $$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Dato il dominio di integrità $\Z$ e dati $a,b \in \Z$, si verifica che:
        \[ I(a)=I(b) \iff a = \pm b\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\Z$ è un dominio di integrità, dati $a,b \in \Z$ si ha che:
        \[I(a) = I(b) \iff a=bc, \exists c \in \Z^*\]
        \item Tuttavia, siccome $\Z^* = \{1, -1\}$, si ha che:
        \[I(a) = I(b) \iff a=bc, c = 1 \lor c = -1 \iff a =\pm b \]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Elementi irriducibili e primi}
    
    \quad
    
    \begin{frameddefn}{Elementi irriducibili e primi}
        Dato un anello commutativo $A$ e un elemento $a \in A$, definiamo $a$ come \textbf{elemento irriducibile} se e solo se:
        \[a \neq 0, a \notin A^*, a = bc \implies b \in A^* \lor c \in A^*\]
        
        Definiamo invece $a$ come \textbf{elemento primo} se e solo se:
        \[a \neq 0, a \notin A^*, a \mid bc \implies a \mid b \lor a \mid c\]

        \textbf{\textit{Attenzione:}} la definizione di elemento primo \underline{non coincide} con la "normale" definizione di numero primo
    \end{frameddefn}

    \label{primi}
    
    \begin{frameddefn}{Insieme dei numeri interi primi}
        Definiamo come \textbf{insieme dei numeri interi primi} l'insieme:
        \[\mathbb{P} := \{p \in \N_{>1} \mid \nexists y \in \N-\{1,p\} \text{ t.c } y \mid a\}\]

        \textbf{\textit{Attenzione:}} gli elementi appartenenti a tale insieme \underline{coincidono} con la "normale" definizione di \textbf{numero primo}
    \end{frameddefn}

    \begin{framedobs}{}
        Dati un elemento $p \in \mathbb{P}$, si verifica che
        \[p \in \mathbb{P} \implies p \text{ elemento primo}\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\mathbb{P} \subseteq \N \subseteq \Z$, allora $p \in \mathbb{P} \implies p \in \Z$
        \item Supponiamo che $p \mid ab$, dove $ab \in \Z$, dunque necessariamente $p$ apparterrà alla fattorizzazione di $ab$, implicando che $p \mid a \lor p \mid b$
        
        $\hfill\qed$
    \end{itemize}

    \begin{framedobs}{}
        Dato un dominio di integrità $A$ ed un elemento $a \in A$, si verifica che
        \[a \text{ elemento primo} \implies a \text{ elemento irriducibile}\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se $a \in A$ è primo, allora per definizione si ha che $a \neq 0, a \notin A^*$.
        \item Se $a = bc$, allora si ha che $a \mid a \implies a \mid bc \implies a \mid b \lor a \mid c$
        \item A questo punto, si ha che:
        \[ a \mid b \implies b = ad, \exists d \in A \implies a = bc = adc \implies a = adc \implies a(1-cd)=0\]
        \item Siccome $a \neq 0$, allora:
        \[ a(1-cd)=0, a \neq 0 \implies 1-cd=0 \implies cd = 1 \implies c = d^{-1} \implies c \in A^*\]
        \item Analogamente, dimostriamo che $a \mid c \implies b \in A^*$
        \item Dunque, concludiamo che se $a$ è primo allora esso è anche irriducibile:
        \[ a \text{ primo} \mid a = bc \implies a \mid b \lor a \mid c \implies b \in A^* \lor c \in A^*\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{}
        Dato il dominio di integrità $\Z$ e un elemento $p \in \Z \mid p \geq 2$, le seguenti condizioni sono \textbf{equivalenti}:
        \begin{itemize}
            \item $p \in \mathbb{P}$
            \item $p$ è un elemento primo
            \item $p$ è un elemento irriducibile
        \end{itemize}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per dimostrazione precedente, sappiamo che
        \[p \in \mathbb{P} \implies p \text{ elemento primo} \implies p \text{ elemento irriducibile}\]
        \item Supponiamo che $p \geq 2 \in \Z$ sia irriducibile e che esistano $a,b \in \N$, tali che:
        \[p = ab \in \N \subseteq \Z \implies a \in \Z^* \lor b \in \Z^*\]
        \item Poiché $\Z^* = \{-1, 1\}$ e poiché $a,b \in \N$, allora ne segue che:
        \[a \in \Z^* \lor b \in \Z^* \implies a = 1 \lor b = 1\]
        
        \begin{itemize}
            \item Se $a \in \Z^*, b \in \Z^*$, allora
            \[a \in \Z^*, b \in \Z^* \implies a = 1, b = 1 \implies  p = 1\]
            contraddicendo l'ipotesi per cui $p \geq 2$, dunque si tratta di un caso impossibile
            
            \item Se $a \in \Z^*, b \notin \Z^*$, allora
            \[a \in \Z^* \implies a = 1 \implies p = b \implies b \mid p \lor 1 \mid p, b = p\]
            \item Se $a \notin \Z^*, b \in \Z^*$, allora
            \[b \in \Z^* \implies b = 1 \implies p = a \implies a \mid p \lor 1 \mid p, a = p\]
        \end{itemize}
        
        \item Dunque, in entrambi i casi possibili si ottiene che
        \[p \text{ elemento irriducibile} \implies p \in \mathbb{P}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Dominio di integrità $\Z_p$}
        Dato l'anello commutativo $\Z_n$, si ha che
        \[\Z_n \text{ dominio di integrità } \iff n \in \mathbb{P}\]
        
        Nel caso in cui $n \in \mathbb{P}$, per comodità utilizziamo la \textbf{notazione} $\Z_p$.
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
            \item Supponiamo per assurdo che $\Z_n$ sia dominio di integrità e che $n \notin \mathbb{P}$, implicando che:
            \[n \notin \mathbb{P} \implies ab = n, \exists a,b \notin \Z^*, 0 <a,b< n \implies\]
            \[\implies [ab] = [n] = [0] \in \Z_n \implies [a][b] = [0] \implies [a] = [0] \lor [b]=[0]\]

            Tuttavia, per ipotesi si ha che $a,b > 0 \implies [a] \neq 0, [b] \neq 0$, creando una contraddizione, dunque l'unica possibilità è che $n \in \mathbb{P}$

            \item Supponiamo per assurdo che $n \in \mathbb{P}$ e che $\Z_n$ non sia dominio di integrità, implicando che:
            \[\exists [a] \neq [0] \in \Z_n \text{ t.c. } [a] \mid [0] \implies [0] = [a][b], \exists [b] \neq [0] \in \Z_n \implies \]
            \[\implies [0]=[ab] \iff ab \equiv 0 (\text{mod }n) \iff n \mid ab-0\]

            Poiché $n \in \mathbb{P}$, si ha che:
            \[n \mid ab \implies n \mid a \lor n \mid b \implies a \equiv 0 (\texttt{mod }n) \lor b \equiv 0 (\texttt{mod }n) \implies \]
            \[\implies [a] = [n] = [0] \lor [b] = [n] = [0]\]

            Tuttavia, per ipotesi si ha che $a,b \neq 0 \implies [a],[b] \neq [0]$, creando una contraddizione, dunque l'unica possibilità è che $\Z_n$ sia dominio di integrità
            
            $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Massimo comun divisore}
    \label{ideali_principali}
    
    \quad
    
    \begin{frameddefn}{Dominio ad ideali principali}
        Dato un dominio di integrità $A$, definiamo $A$ come \textbf{dominio ad ideali principali} se e solo se considerato un qualsiasi $I \ideal A$ si ha che:
        \[\exists d \in I \mid I = I(d)\]
        
        In altre parole, ogni ideale coincide esattamente con un ideale principale
    \end{frameddefn}
    
    \begin{framedprop}{}
        Il dominio di integrità $\Z$ è un \textbf{dominio ad ideali principali}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Supponiamo che esista $I \ideal \Z$ tale che $I = \{0\}$. In tal caso, si ha che $I = I(0)$
        \item Supponiamo quindi che $I \neq \{0\}$, implicando che per definizione stessa di ideale si abbia che:
        \[\forall n \in I \implies -n \in I\]
        Dunque, possiamo considerare direttamente il sottoinsieme $I_{>0}$, poiché per i numeri negativi basterebbe considerare il loro opposto.
        \item Siccome $I_{>0} \subseteq \N$, per il principio del buon ordinamento si ha che
        \[\exists d \in I_{>0} \mid d = \min(I_{>0})\]
        
        \item Dimostriamo quindi che $I = I(d)$:
        \begin{itemize}
            \item Dato $x \in I(d)$, si ha che:
            \[x \in I(d) \implies \exists y \in \Z \mid x = dy\]
            \item Siccome $d \in I_{>0} \subseteq I$, allora $x = dy \in I$
        \end{itemize}
        \begin{itemize}
            \item Dato $x \in I$, per il teorema della divisione con resto euclidea si ha che:
            \[\exists!q,r \in \Z, 0 \leq r, d \mid x = dq+r \implies r = x - dq \in I\]
            \item Assumiamo per assurdo che $r \neq 0$, implicando che $r > 0$ e dunque che $r \in I_{>0}$. Tuttavia,poiché $r < d$, allora ne seguirebbe che $d$ non sia il minimo di $I_{>0}$.
            \item Dunque, l'unica possibilità è che $r = 0$, implicando che:
            \[x = dq + r = dq + 0 = dq \implies x = dq \implies x \in I(d)\]
            $\hfill\qed$
        \end{itemize}
    \end{itemize}

    \begin{frameddefn}{Massimo Comun Divisore (MCD)}
        Dati $a_1, \ldots, a_n \in \Z$, definiamo $d \in \N$ come \textbf{massimo comun divisore di $a_1, \ldots, a_n$}, indicato come $d := \MCD(a_1, \ldots, a_n)$, se dato $k \in \Z$ si verifica che
        \[k \mid a_1 \land \ldots \land k \mid a_n \iff k \mid d\]
    \end{frameddefn}
    
    \begin{framedprop}{Identità di Bézout}
    Dati $a_1, \ldots, a_n \in \Z$ e dato $d := \MCD(a_1, \ldots, a_n)$, si ha che:
    \[I(a_1, \ldots, a_n) = I(d)\]
    
    In altre parole, si ha che:
    \[ \exists x_1, \ldots, x_n \in \Z \mid  a_1x_1 + \ldots + a_nx_n = d\]
    che definiamo come \textbf{identità di Bézout}.
    \end{framedprop}
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Essendo $\Z$ un dominio ad ideali principali, si ha che 
        \[\exists n \in I(a_1, \ldots, a_n) \mid I(a_1, \ldots, a_n) = I(b)\]
        \item Dato $d := \MCD(a_1, \ldots, a_n)$, ovviamente, si ha che
        \[d := \MCD(a_1, \ldots, a_n) \implies d \mid a_1 \land \ldots \land d \mid a_n \iff \forall i \in [1,n], \exists k_i \in \Z \mid a_i = dk_i\]
        \item Di conseguenza, otteniamo che
        \[b \in I(b) = I(a_1, \ldots, a_n) \iff \exists x_1, \ldots, x_n \in \Z \mid  a_1x_1 + \ldots + a_nx_n = b \iff\]
        \[(dk_1)x_1+\ldots+(dk_n)x_n = b \iff d(k_1x_1+\ldots+k_nx_n) = b \iff d \mid b\]
        \item Inoltre, $\forall i \in [1,n]$ si ha che
        \[a_i \in I(a_i) \subseteq I(a_1, \ldots, a_n) = I(b) \iff \exists k \in \Z \mid a_i = bk \iff b \mid a_i\]
        \item Dunque, per definizione stessa di massimo comun divisore si ha che
        \[b \mid a_1 \land \ldots b \mid a_n \iff b \mid d\]
        \item Poiché $d \mid b$, $b \mid d$ e $d \in \N$ necessariamente si ha che $d = b$

        $\hfill\qed$
    \end{itemize}
    
    \label{inv_mcd}
    \begin{framedprop}{}
    Dato l'anello commutativo $\Z_n$ e dato $0 < a < n$ si ha che:
    \[[a] \in \Z_n^* \iff \MCD(a,n) = 1\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Se $[a] \in \Z_n^*$ si ha che:
        \[\exists 0 < b <n \mid  [a][b] = 1 \iff ab \equiv 1 (\texttt{mod } n) \iff \exists k \in \Z \mid 1 =ab + nk\]
        
        Posto $d := \MCD(a,n) > 0$, si ha che:
        \[1 = ab + nk \in I(a, n) = I(d) \implies 1 \in I(d) \implies \exists p \in \Z \mid  1 = dp \implies d=p= \pm 1\]
        
        Poiché $d > 0$, l'unico caso possibile è $d = 1$
        
        \item Viceversa, supponendo che $\MCD(a, n) = 1$ si ha che:
        \[ I(d) = I(a, n) \implies d \in I(a, n) \implies \exists b,k \in \Z \mid d = ab+nk \implies \]
        \[ \implies [1] = [ab+nk] \in \Z_n \implies [a][b]+[n][k] = [a][b]+[0][k] = [a][b]\]
        \[\implies [b] = [a]^{-1} \implies [a] \in \Z_n^*\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{Campo $\Z_p$}
        Dato $p \in \mathbb{P}$, il dominio di integrità $\Z_p$ è un \textbf{campo}
    \end{framedcor}
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\nexists y \in \Z-\{1,p\}$ tale che $y \mid p$, allora:
        \[\MCD(a, p) = 1, \forall 0< a<p \iff [a] \in \Z_n^*, \forall 0<a<p \implies  \Z_p^* = \Z_p -\{0\}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{}
        Dato l'anello commutativo $\Z_n$ e data la seguente equazione:
        \[ ax \equiv b (\texttt{mod }n) \]
        
        Posto $d := \MCD(a,n)$ si verifica che:
        \begin{itemize}
            \item Se $d \nmid b$, allora $\nexists x \in \Z_n \mid ax = b (\texttt{mod }n)$ (\textbf{non esistono soluzioni})
            \item Se $d \mid b$, allora posti $p := \frac{a}{d}, q := \frac{b}{d}, m := \frac{n}{d}$, l'equazione è \textbf{equivalente} a:
            \[ax \equiv b (\texttt{mod }n) \iff px \equiv q (\texttt{mod }m)\]
        \end{itemize}
    \end{framedthm}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Prima di tutto, affermiamo che se l'equazione ammette una soluzione $x \in \Z$, allora
        \[ax \equiv b (\texttt{mod }n) \iff ax = b+nk, \exists k \in \Z \iff ax-nk = b\]
        
        \item Poiché $d := \MCD(a,n)$, si ha che
        \[d \mid a, d \mid n \implies d \mid ax, d \mid nk \implies d \mid ax-nk = b\]
        
        \item Viceversa, ciò dimostra che se $d \nmid b$, allora tale equazione non potrebbe ammettere soluzioni.
        
        \item Supponiamo quindi che $d \mid b$ e poniamo $p := \frac{a}{d}, q := \frac{b}{d}, m := \frac{n}{d}$ (implicando quindi che $a = pd, b = qd, n = md$. Dunque si verifica che:
        \[ ax \equiv b (\texttt{mod }n) \iff pdx \equiv qd (\texttt{mod } md) \iff \]
        \[\iff dpx = dq + dmk, \exists k \in \Z \iff px = q+mk \iff px \equiv q (\texttt{mod }m)\]
        $\hfill\qed$
    \end{itemize}
    
    \subsection{Algoritmo di Euclide}
    
    \quad
    
    \begin{framedlem}{}
        Dati tre elementi $a,b,c \in \Z$, si ha che:
        \[a\mid b, a \mid c \implies a \mid z, \forall z \in I(b,c) \]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se $a \mid b$ e che $a \mid c$, si ha che:
        \[z \in I(b,c) \iff z = bx+cy, \exists x,y \in \Z \implies z = (ak)x+(ah)c, \exists k,h \in \Z \implies \]
        \[ \implies z = a(kx)+a(hc) = a(kx+hc) \implies a \mid z\]
        $\hfill\qed$   
    \end{itemize}
    
    \begin{framedmeth}{Algoritmo di Euclide}
    Siano $a,b \in \Z$ e sia $d := \MCD(a,b)$. Il seguente \textbf{algoritmo di Euclide} permette di calcolare $d$:
    \begin{enumerate}
        \item Assumiamo $0 < a \leq b$ poniamo $r_0 := b$ e $r_1 := a$
        \item Poniamo $r_{i+1} := r_{i-1} (\texttt{mod } r_{i})$ ad ogni iterazione, da cui ne segue che $r_{i-1} = r_iq_i+r_{i+1}$, ripetendo tale operazione fino a quando $r_{i+1} = 0$
        \item All'n-esima iterazione, ossia quando $r_{n+1} = 0$, si ha che $\MCD(a,b) = r_n$
    \end{enumerate}
    \end{framedmeth}

    \textit{Dimostrazione correttezza algoritmo:}
    \begin{itemize}
        \item Poiché $I(a,b) = I(-a,b) = I(a, -b) = I(-a, -b)$, assumiamo che $0 < a,b$.
        \item Inoltre, poiché $I(a,b) = I(b,a)$, $\MCD(0, b) = 0$ e $\MCD(a,0) = 0$, assumiamo che $0 < a \leq b$.
        \item Siccome $r_0 := b, r_1 := a \in I(a,b)$, si ha che:
        \[r_2 \equiv r_0 (\texttt{mod } r_1) \iff r_0 = r_1q_1+r_2 \iff r_2 = r_0-r_1q_1 \in I(a,b) = I(d)\]
        
        \item Supponiamo per ipotesi induttiva che $r_i \in I(a,b) = I(d), \forall i \in [0,n]$. 
        
        Dimostriamo quindi il passo induttivo:
        \[r_{i+1} \equiv r_{i-1} (\texttt{mod } r_i) \iff r_{i-1} = r_iq_1+r_{i+1} \iff r_{i+1} = r_{i-1}-r_iq_i \in I(a,b) = I(d)\]
        
        \item Di conseguenza, $\forall i \in [0,n]$ si ha che
        \[r_i \in I(a,b)=I(d) \iff r_i = dp, \exists p \in \Z \iff d \mid r_i \forall i \in [0,n] \implies d \mid r_n\]
        
        \item Poiché l'algoritmo termina quando $r_{n+1} = 0$, ne segue che:
        \[r_{n+1} \equiv r_{n-1} (\texttt{mod } r_n) \iff 0 \equiv r_{n-1} (\texttt{mod } r_n) \iff r_{n-1} = r_nq_n \iff r_n \mid r_{n-1}\]
        
        \item Siccome $r_n \mid r_n$ e $r_n \mid r_{n-1}$, per il lemma precedente si ha che:
        \[r_{n} \equiv r_{n-2} (\texttt{mod } r_{n-1}) \iff r_{n-2} = r_{n-1}q_{n-1}+r_n \in I(r_{n-1}, r_n)\]

        \item Dunque, poiché $r_n, r_{n-1}, r_{n-2} \in I(r_{n-1}, r_n) \subseteq \Z$, per il lemma precedente si ha che:
        \[r_n \mid r_{n}, r_n \mid r_{n-1} \implies r_n \mid r_{n-2}\]
        
        \item A questo punto, procedendo analogamente si ha che
        \[ r_n \mid  r_n, r_n \mid  r_{n-1} \implies r_n \mid  r_{n-2}\]
        \[ r_n \mid  r_{n-1}, r_n \mid  r_{n-2} \implies r_n \mid  r_{n-3}\]
        \[\ldots\]
        \[ r_n \mid  r_{2}, r_n \mid  r_{1} \implies r_n \mid r_0\]
        
        \item Dunque, poiché $d:= \MCD(a, b)$ e dati $r_1 := a, r_0 := b$, si ha che:
        \[r_n \mid a, r_n \mid b \implies r_n \mid d \]
        
        \item Infine, siccome $d,r_n \in \N$ (sezione \ref{antisim}) si ha che:
        \[d \mid r_n, r_n \mid d \implies d = r_n\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item Vogliamo calcolare \textbf{$\MCD(448, 216)$}. Poniamo quindi inizialmente $r_0 = 448$ e $r_1 = 216$. Applicando l'algoritmo abbiamo quindi che:
        \[ r_0 = r_1 \cdot q_1 + r_2\]
        \[ 448 = 216 \cdot 2 + 16\]
        \[ 216 = 16 \cdot 13 + 8\]
        \[ 16 = 8 \cdot 2 + 0\]
        
        Dunque, otteniamo che $\MCD(448, 216) = 8$
        
        \item Vogliamo calcolare l'\textbf{identità di Bézout} per $b = 216$ e $a = 448$ ossia i due valori $x$ e $y$ tali che:
        \[ x, y \in \Z \mid  \MCD(488, 216) = 216x+448y\]
        
        Tramite l'\textbf{algoritmo di Euclide} utilizzato nell'esercizio precedente, sappiamo che $\MCD(488, 216) = 8$. Poniamo quindi:
        \[ 216x+448y = 8\]
        
        A questo punto, riscriviamo il risultato secondo la forma espressa dell'identità di Bézout utilizzando i calcoli dell'algoritmo di Euclide:

        \begin{itemize}
            \item $448 = 216 \cdot 2 + 16 \implies 16 = 448-216 \cdot 2$
            \item $216 = 16 \cdot 13 + 8 \implies 8 = 216 - 16 \cdot 13 = 216 - (448-216 \cdot 2) \cdot 13 = 216 \cdot 27 + 448 \cdot (-13)$
        \end{itemize}

        Dunque otteniamo che
        \[ 216x+448y = 8 = 216(27) + 448(-13) \implies x = 27, y = -13\]
        
        \item Vogliamo calcolare l'identità di Bézout e MCD per $a = 1470, b = 8316$ e $c = 12600$:
        \[ \MCD(a,b,c) = \MCD(a, \MCD(b,c)) = \MCD(\MCD(a,b),c)\]
        
        \begin{itemize}
            \item $d := \MCD(b,c) = \MCD(8316, 12600)$
            \[ 12600 = 8316 \cdot 1 + 4284\]
            \[ 8316 = 4284 \cdot 1 + 4032\]
            \[ 4284 = 4032 \cdot 1 + 252\]
            \[ 4032 = 252 \cdot 16 + 0\]
            
            dunque $d = 252$
            
            \item A questo punto, riscriviamo il risultato secondo la forma espressa dell'identità di Bézout utilizzando i calcoli dell'algoritmo di Euclide:
            
            \begin{itemize}
                \item $12600 = 8316 \cdot 1 + 4284 \implies 4284 = 12600 - 8316 \cdot 1$
                \item $8316 = 4284 \cdot 1 + 4032 \implies 4032 = 8316 - 4284 \cdot 1 = 8316 - (12600 - 8316 \cdot 1) = 8316 \cdot 2 - 12600$
                \item $4284 = 4032 \cdot 1 + 252 \implies 252 = 4284 - 4032 \cdot 1 = (12600 - 8316 \cdot 1) - (8316 \cdot 2 - 12600) = 12600 \cdot 2 - 8316 \cdot 3$
            \end{itemize}

            Dunque otteniamo che
            \[ 8316x + 12600y = 12600(2) + 8316(-3) \implies x = -3, y = 2\]
            
            \item $p := \MCD(a,d) = \MCD(1470, 252)$
            \[1470 = 252 \cdot 5 + 210\]
            \[252 = 210 \cdot 1 + 42\]
            \[210 = 42 \cdot 5 + 0\]
            
            dunque $p = 42$
            
            \item A questo punto, riscriviamo il risultato secondo la forma espressa dell'identità di Bézout utilizzando i calcoli dell'algoritmo di Euclide:
            
            \begin{itemize}
                \item $1470 = 252 \cdot 5 + 210 \implies 210 = 1470 - 252 \cdot 5$
                \item $252 = 210 \cdot 1 + 42 \implies 42 = 252 - 210 \cdot 1 = 252 - (1470 - 252 \cdot 5) = 1470 \cdot (-1)+252 \cdot 6$
            \end{itemize}

            Dunque otteniamo che

            \[ 1470z+252w = 1470(-1) + 252(6) \implies x = -1, y = 6\]
            
            \item Infine, ricostruiamo l'identità di Bézout richiesta tramite le due precedentemente calcolate:
            \[ 1470x + 8316y + 12600z = 42\]
            \[ 1470x + 8316y + 12600z = 1470 \cdot (-1)+252 \cdot 6\]
            \[ 1470x + 8316y + 12600z = 1470 \cdot (-1)+(12600 \cdot 2 - 8316 \cdot (-3)) \cdot 6\]
            \[ 1470x + 8316y + 12600z = 1470 \cdot (-1)+12600 \cdot (12) + 8316 \cdot (-18)\]
            
            dunque $x = -1, y = -18, z = 12$
        \end{itemize}
    \end{itemize}
    
    \quad
    
    \subsection{Criteri di divisibilità}
    
    Sia $a \in \Z$ con la sua \textbf{rappresentazione decimale}:
    \[ a=a_k\cdot 10^k + \ldots + a_0\cdot 10^0 = \sum_{i=0}^k a_i \cdot 10^i \text{ dove } a_i \in \{0, \ldots, 9\}\]
    
    Osserviamo che:
    \begin{itemize}
        \item In $\Z_3$ si ha che
        \[ a = \sum_{i=0}^k a_i \cdot 10^i \equiv \left [ \sum_{i=0}^k a_i \cdot (1)^i \right ] (\texttt{mod }3)\]
        \item In $\Z_9$ si ha che
        \[ a = \sum_{i=0}^k a_i \cdot 10^i \equiv \left [ \sum_{i=0}^k a_i \cdot (1)^i \right ] (\texttt{mod }9)\]
        \item In $\Z_{11}$ si ha che
        \[ a = \sum_{i=0}^k a_i \cdot 10^i \equiv \left [ \sum_{i=0}^k a_i \cdot (-1)^i \right ] (\texttt{mod }11)\]
    \end{itemize}

    Di conseguenza, possiamo stabilire dei criteri di divisibilità utilizzando le precedenti riduzioni e il fatto che $n \mid m \iff m \equiv 0 (\ttt{mod }n)$.
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Vogliamo sapere se $3 \mid  129383716$. Siccome siamo in $\Z_3$ abbiamo che:
        \[ 129383716 \equiv [6+1+7+3+8+3+9+2+1] \equiv 40 \not\equiv 0(\texttt{mod } 3)\]
        dunque $3 \nmid 129383716$
        
        \item Vogliamo sapere se $11 \mid  129383716$. Siccome siamo in $\Z_{11}$ abbiamo che:
        \[ 129383716 \equiv [6-1+7-3+8-3+9-2+1] \equiv 22 \equiv 0(\texttt{mod } 11)\]
        dunque $11 \mid  129383716$
    \end{itemize}la definizione stessa di congruenza a 0 in modulo
    
    \quad
    
    \section{Minimo comune multiplo}
    \label{mcm}
    
    \quad

    \begin{frameddefn}{Minimo Comune Multiplo (mcm)}
        Dati $a_1, \ldots, a_n \in \Z$, definiamo $m \in \N$ come \textbf{minimo comune multiplo di $a$ e $b$}, indicato come $m := \mcm(a_1, \ldots , a_n)$, se dato $k \in \Z$ si verifica che
        \[a_1 \mid k \land a_n \mid k \iff m \mid k\]
    \end{frameddefn}
    
    \begin{framedprop}{}
    Dati $a_1, \ldots, a_n \in \Z$ e dato $m := \mcm(a_1, \ldots, a_n)$, si ha che:
    \[I(a_1) \cap \ldots \cap I(a_n) = I(m)\]
    \end{framedprop}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Essendo $\Z$ un dominio ad ideali principali, si ha che 
        \[\exists b \in I(a_1, \ldots, a_n) \mid I(a_1) \cap \ldots \cap I(a_n) = I(b)\]
        \item Dato $m := \mcm(a_1, \ldots, a_n)$, ovviamente, si ha che
        \[m := \mcm(a_1, \ldots, a_n) \implies a_1 \mid m \land a_n \mid m \iff \forall i \in [1,n], \exists k_i \in \Z \mid m = a_ik_i\]
        \[\iff m \in I(a_i) \iff m \in I(a_1) \cap \ldots \cap I(a_n) = I(b) \iff b \mid m\]
        \item Inoltre, si ha che
        \[b \in I(b) = I(a_1) \cap I(a_2) \cap \ldots \cap I(a_n) \iff b = a_ik_1, \exists k_1 \in \Z, \forall i \in [0,n] \implies a_i \mid b\]
        \item Dunque, per definizione stessa di minimo comune multiplo di ha che
        \[a_i \mid b \land \ldots \land a_n \mid b \iff m \mid b\]
        \item Poiché $m \mid b$, $b \mid m$ e $m \in \N$ necessariamente si ha che $m = b$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Dato il dominio ad ideali principali $\Z$ e dati $I_1, \ldots I_n \ideal A$ ideali, $\exists! a_1, \ldots, a_n \in \Z$ tali che :
        \begin{itemize}
            \item $I_1+ \ldots +I_n = I(a_1, \ldots, a_n) = I(d)$, dove $d:= \MCD(a_1, \ldots, a_n)$
            \item $I_1 \cdot \ldots \cdot I_n = I(a_1) \cdot \ldots \cdot I(a_n) = I(a_1 \cdot \ldots \cdot a_n)$
            \item $I_1 \cap \ldots \cap I_n = I(a_1) \cap \ldots \cap I(a_n) = I(m)$, dove $m:= \mcm(a_1, \ldots, a_n)$
        \end{itemize}
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\Z$ è un dominio ad ideali principali si ha che
        \[I_i = I(a_i), \exists! a_i \in \Z, \forall i \in [i,n]\]
        
        \item Di conseguenza, si ha che:
        \[I_1 + \ldots + I_n =  I(a_1)+\ldots+I(a_n) = \{b_1+\ldots+b_n \mid b_i \in I(a_i), \forall i \in [0,1]\} = \]
        \[ =  \{b_1+\ldots+b_n \mid b_i = a_ix_i, \exists x_i \in \Z, \forall i \in [0,1]\} =  \{a_ix_i+\ldots+a_nx_n \mid x_i \in \Z, \forall i \in [0,1]\} =\]
        \[ = I(a_1, \ldots, a_n) = I(d)\]
        dove $d: \MCD(a_1, \ldots, a_n)$
        
        \item Preso $x \in I_1 \cdot \ldots \cdot I_n$, si ha che:
        \[x \in I_1 \cdot \ldots \cdot I_n = I(a_1)\cdot \ldots \cdot I(a_n) \iff\]
        \[\iff \exists b_{j_{a_i}} \in I(a_i), \forall i,j \in [1,0] \mid x = a_1b_{1_{a_1}} \cdot \ldots \cdot a_nb_{1_{a_n}} + \ldots + a_1b_{n_{a_1}}\cdot \ldots \cdot a_nb_{n_{a_n}} \iff \]
        \[\iff x = a_1 \cdot \ldots a_n(b_{1_{a_1}} \cdot \ldots b_{1_{a_n}}+ \ldots + b_{n_{a_1}}\cdot \ldots \cdot b_{n_{a_n}}) \iff x \in I(a_1 \cdot \ldots \cdot a_n) \]
        dunque, si ha che $I_1 \cdot \ldots \cdot I_n = I(a_1\cdot \ldots \cdot a_n)$
        
        \item Infine, per dimostrazione precedente si ha che:
        \[I_1 \cap \ldots \cap I_n = I(a_1) \cap \ldots \cap I(a_n) = I(m)\]
        dove $m:= \mcm(a_1, \ldots, a_n)$
        
        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{Infinite soluzioni all'identità di Bézout}
        Dati $a,n \in \Z$ siano $x_0$ e $y_0$ due soluzioni particolari della loro identità di Bézeout (es: trovate tramite l'algoritmo di Euclide).
        
        Posti $d := \MCD(a,b)$ e $m:= \mcm(a,b)$, tale identità ammette \textbf{infinite soluzioni} nella seguente forma:
        \[ x = x_0 + \frac{m}{a} k, \forall k \in \Z \qquad y = y_0 - \frac{m}{b} k, \forall k \in \Z\]
    \end{framedprop}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Prima di tutto, verifichiamo che le soluzioni possibili siano effettivamente valide:
        \[ a(x_0 + \frac{m}{a} k) + b(y_0 - \frac{m}{b} k) = d \iff ax_0 + mk + by_0 -mk = d \iff ax_0 + by_0 = d\]
        
        \item A questo punto, verifichiamo che tali soluzioni appaiano solo nella forma indicata:
        
        $$
        \left \{
        \begin{array}{cc}
        ax_0+by_0=d \\
        ax_1+by_1=d
        \end{array}
        \right .
        \implies (ax_1+by_1)-(ax_0+by_0)=d-d \implies
        $$
        \[a(x_1-x_0)+b(y_1 +  y_0) = 0 \implies  a(x_1-x_0) = -b(y_1 - y_0) \implies \]
        \[\implies a(x_1-x_0) = b(y_0 + y_1)\]
        
        \item Posto $N := a(x_1-x_0) = b(y_0 - y_1)$, si ha che $a \mid  N$ e $b \mid  N$, implicando che $N$ sia un multiplo di $m := \mcm(a,b)$. Dunque si ha che $\exists k \in \Z \mid  N = mk$:
        
        $$
        \left \{
            \begin{array}{cc}
            a(x_1-x_0) = N = mk\\
            b(y_0 - y_1) = N = mk
            \end{array}
        \right .
        \implies
        \left \{
            \begin{array}{cc}
            x_1 - x_0 = \frac{m}{a}k\\
            y_0 - y_1 = \frac{m}{b}k
            \end{array}
        \right .
        \implies
        \left \{
            \begin{array}{cc}
            x_1 = x_0 + \frac{m}{a}k\\
            y_1 = y_0 - \frac{m}{b}k
            \end{array}
        \right .
        $$
        
        $\hfill\qed$
    \end{itemize}
   
    \quad
    
    \section{Teorema fondamentale dell'aritmetica}
    
    \quad

    \begin{framedthm}{Teorema fondamentale dell'aritmetica}
        Dato $n \geq 2 \in \N$, tale $n$ può essere espresso come un'unica (a meno di riordinamento) \textbf{fattorizzazione in numeri interi primi}, ossia:
        \[\exists! p_1, \ldots, p_k \in \Primes, a_1, \ldots, a_k \in \N_{>0} \mid n = p_1^{a_1} \cdot \ldots \cdot p_k^{a_k}\]
    \end{framedthm}
    
    \textit{Dimostrazione esistenza:}

    \begin{itemize}
        \item Definiamo $S := \{n \in \N \mid n \geq 2 \land n\text{ non fattorizzabile in numeri primi}\}$ e supponiamo per assurdo che $\abs{S} \neq 0$. Poiché $S \subseteq \N$, per il principio del buon ordinamento ne segue che $\exists m \in S \mid m = \min(S)$
        \item Supponiamo quindi che $\exists a,b \neq 0 \in \N - \Primes \mid m = ab$, da cui traiamo che:
        \[\soe{l}{
            a \mid m \implies a \leq m\\
            b \mid m \implies b \leq m\\
            m = \min(S)
        } \implies a,b \notin S \implies \soe{l}{
            a = 1 \lor a \text{ fatt. in primi}\\
            b = 1 \lor b \text{ fatt. in primi}\\
        }\]

        \newpage
        
        \item Consideriamo quindi i vari casi:
        \begin{itemize}
            \item Se $a = 1 \land b = 1$, allora $m = ab = 1$, contraddicendo l'ipotesi per cui $m \in S \implies m \geq 2$
            \item Se $a \text{ fatt. in primi } \land b = 1$, allora $m = a = p_1^{a_1} \cdot \ldots \cdot p_k^{a_k}$, contraddicendo l'ipotesi per cui $m \in S \implies m \text{ non fatt. in primi}$
            \item Se $a = 1 \;\land \; b \text{ fatt. in primi}$, allora $m = b = q_1^{b_1} \cdot \ldots \cdot q_j^{b_j}$, contraddicendo l'ipotesi per cui $m \in S \implies m \text{ non fatt. in primi}$
            \item Se $a \text{ fatt. in primi } \land \;b \text{ fatt. in primi}$, allora
            \[m = ab = p_1^{a_1} \cdot \ldots \cdot p_k^{a_k} \cdot  q_1^{b_1} \cdot \ldots \cdot q_j^{b_k}\]
            contraddicendo l'ipotesi per cui $m \in S \implies m \text{ non fatt. in primi}$
        \end{itemize}
        \item Di conseguenza, ne segue necessariamente che $\abs{S} = 0$ e dunque che tale fattorizzazione valga $\forall n \geq 2 \in \N$
        
        $\hfill\qed$
    \end{itemize}

    \textit{Dimostrazione unicità:}

    \begin{itemize}
        \item Dato $n \geq 2 \in \N$, consideriamo le sue due fattorizzazioni:
        \[p_1^{a_1} \cdot \ldots \cdot p_k^{a_k} = n = q_1^{b_1} \cdot \ldots \cdot q_j^{b_j}\]
        \item Dato che $p_i^{a_i} \mid p_1^{a_1} \cdot \ldots \cdot p_k^{a_k}, \forall i \in [1,k]$ e che $p_i \in \Primes$, ne segue automaticamente che:
        \[p_i^{a_i} \mid p_1^{a_1} \cdot \ldots \cdot p_k^{a_k} \iff p_i^{a_i} \mid q_1^{b_1} \cdot \ldots \cdot q_j^{b_j} \implies \]
        \[\implies p_i \mid p_i^{a_1} \land p_i^{a_i} \mid q_1^{b_1} \cdot \ldots \cdot q_j^{b_j} \implies p_i \mid q_1^{b_1} \cdot \ldots \cdot q_j^{b_j} \implies p_i \mid q_1^{b_1} \lor \ldots \lor p_i \mid q_j^{b_j}\]
        \item Sia quindi $q_h \mid h \in [1,j]$ tale che $p_i \mid q_h^{b_h}$. Poiché $q_h \in \Primes$, ne segue necessariamente che:
        \[p_i \mid q_h^{b_h} \implies p_i \mid q_h \implies p_i = q_h\]
        dunque concludiamo che $\forall i \in [1,k], \exists h \in [1,j]$ tale che $p_i \mid q_h \implies p_i = q_h \land k = j$

        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{Prodotto tra mcm e MCD}
        Dati $a,b \in \N$, si ha che:
        \[ \mcm(a,b) \cdot \MCD(a,b) = ab\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Se $a=0 \lor b = 0$, allora:
        \[\mcm(a,b) = 0 \implies \mcm(a,b) \cdot \MCD(a,b) = 0 = ab\]
        
        \item Se invece $a = b = 1$, allora:
        \[\mcm(a,b) \cdot \MCD(a,b) = 1 \cdot 1 = 1 = ab\]

        \item Supponiamo quindi che $a,b \geq 2$. Per il teorema fondamentale dell'aritmetica, $a$ e $b$ sono esprimibili come una fattorizzazione in numeri interi primi. Poniamo quindi:
        \[ a := \prod_{p \in \mathbb{P}} p^{a_p} \qquad\qquad b := \prod_{p \in \mathbb{P}} p^{b_p}\]

        dove $p \nmid a \implies a_p = 0$ e $p \nmid b \implies b_p = 0$ (ossia tale numero intero primo non compare nella loro fattorizzazione)
        
        \item Poniamo inoltre $d := \MCD(a,b)$ e $m := \mcm(a,b)$, che per loro definizione corrispondono a:
        \[ d = \prod_{p \in \mathbb{P}} p^{\min(a_p, b_p)} \qquad\qquad m = \prod_{p \in \mathbb{P}} p^{\max(a_p, b_p)}\]
        
        \item A questo punto, osserviamo che:
        \[\min(a_p, b_p) = a_p \iff \max(a_p,b_p) = b_p\]
        
        \item Quindi, il prodotto tra $d$ e $m$ corrisponde a:
        
        \[ dm = \prod_{p \in \mathbb{P}} p^{\min(a_p, b_p)} \cdot \prod_{p \in \mathbb{P}} p^{\max(a_p, b_p)} = \prod_{p \in \mathbb{P}} p^{a_p+b_p} = \prod_{p \in \mathbb{P}} p^{a_p}\cdot \prod_{p \in \mathbb{P}} p^{b_p} = ab\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Dati $a_1, \ldots, a_n \in \N$ dove $n > 2 \in \N$, \underline{non è detto che}:
        \[\mcm(a_1, .., a_n) \cdot \MCD(a_1,\ldots,a_n) = a_1 \cdot \ldots \cdot a_n\]
    \end{framedobs}
    
    \newpage
    
    \section{Teorema cinese dei resti}
    
    \quad
    
    \begin{framedlem}{Numeri coprimi ed mcm}
    Dati $a_1, \ldots, a_n \geq 2 \in \N$, si ha che:
    \[\MCD(a_i, a_j) = 1, \forall i \neq j \in \N \implies \mcm(a_1, \ldots, a_n) = a_1 \cdot \ldots \cdot a_n\]
    
    Inoltre, due elementi $a,b \in \N \mid \MCD(a,b) = 1$, definiamo tali elementi come \textbf{coprimi}
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dati $a_1, \ldots, a_n \geq 2 \in \N$, per il teorema fondamentale dell'aritmetica si ha che:
        \[ a_1 = \prod_{p \in \mathbb{P}}p^{a_{1,p}}, \quad a_2 = \prod_{p \in \mathbb{P}}p^{a_{2,p}}, \quad \ldots, \quad a_n = \prod_{p \in \mathbb{P}}p^{a_{n,p}}\]
        
        \item Poiché $a_1, \ldots, a_n$ sono coprimi tra loro, si ha che
        \[ \MCD(a_i,a_j)=1, \forall i \neq j \in \N \implies \forall p \in \mathbb{P}, p \mid  a_i \implies p \nmid a_j, \forall i \neq j \in \N\]
        
        \item Di conseguenza, per ogni esponente $a_{i,p} > 0$ si ha che $a_{j,p} = 0, \forall j \neq i$, da cui ne segue che:
        \[\forall p \in \mathbb{P}, \exists! a_{k,p} > 0 \mid a_{1,p}+\ldots+a_{n,p} = a_{k,p} = \max(a_{1,p}, \ldots, a_{n,p})\]
        
        \item Ponendo $m := \mcm(a_1, \ldots, a_n)$ abbiamo che:
        \[ m = \prod_{p\in \mathbb{P}} p^{\max(a_{1,p}, \ldots, a_{n,p})} =\prod_{p\in \mathbb{P}} p^{a_{1,p}+ \ldots+ a_{n,p}} = \prod_{p\in \mathbb{P}} p^{a_1} \cdot \ldots \prod_{p\in \mathbb{P}} p^{a_n} = a_1 \cdot \ldots \cdot a_n \]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
    Consideriamo la notazione $x (\texttt{mod } q)$, indicante la classe $[x] \in \Z_q$, dove $q \in \N$.
    
    Dati $a_1, \ldots, a_n \geq 2$ e posto $m := \mcm(a_1, \ldots, a_n)$, la seguente funzione 
    è \textbf{ben definita} ed \textbf{iniettiva}
    \[ \varphi : \Z_n \to  \Z_{a_1} \times \ldots \times \Z_{a_m} : x (\texttt{mod } m) \mapsto (x (\texttt{mod }a_1),\ldots,x (\texttt{mod }a_n))\]
    
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \[x \equiv x' (\texttt{mod } m) \iff x'-x \in I(m) = I(a_1) \cap \ldots \cap I(a_n) \iff \]
    $$
    \iff x'-x \in I(a_i), \forall i \in [1,n] \iff
    \left \{
    \begin{array}{c}
        x'-x \in I(a_1)\\
        x'-x \in I(a_2)\\
        \vdots\\
        x'-x \in I(a_n)\\
    \end{array}
    \right .
    \iff
    \left \{
    \begin{array}{c}
        x \equiv x' (\texttt{mod } a_1)\\
        x \equiv x' (\texttt{mod } a_2)\\
        \vdots\\
        x \equiv x' (\texttt{mod } a_n)\\
    \end{array}
    \right .
    $$
    $\hfill\qed$
    
    \begin{framedthm}{Teorema cinese dei resti}
    Dati $a_1, \ldots, a_n \geq 2 \in \N$ \textbf{coprimi tra loro}, dunque tali che $\MCD(a_i, a_j) = 1, \forall i \neq j$ e dati $0 \leq b_i < a_i \in \N, \forall i \in [1,n]$, il seguente sistema di congruenze (\underline{se compatibile}) ammette un'\textbf{unica soluzione}
    
    $$
    \exists! x (\texttt{mod }m) \mid \left \{
    \begin{array}{c}
        x \equiv b_1 (\texttt{mod } a_1)\\
        x \equiv b_2 (\texttt{mod } a_2)\\
        \vdots\\
        x \equiv b_3 (\texttt{mod } a_n)\\
    \end{array}
    \right .
    $$
    
    dove $m: = \mcm(a_1, \ldots, a_n) = a_1 \cdot \ldots \cdot a_n$ e dove $x (\texttt{mod }m)$ indica la classe $[x] \in \Z_m$
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Per il lemma precedente, la seguente funzione è ben definita ed iniettiva
        \[ \varphi : \Z_n \to  \Z_{a_1} \times \ldots \times \Z_{a_m} : x (\texttt{mod } m) \mapsto (x (\texttt{mod }a_1),\ldots,x (\texttt{mod }a_n))\]
        \item Inoltre, posto $m := \mcm(a_1, \ldots, a_n)$, per il lemma precedente si ha che:
        \[\MCD(a_i, a_j) = 1, \forall i \neq j \implies m = a_1 \cdot \ldots \cdot a_n\]
        
        \item A questo punto, notiamo che:
        \[ \abs{\Z_{a_1} \times \ldots \times \Z_{a_n}} = \abs{\Z_{a_1}} \cdot \ldots \cdot \abs{\Z_{a_n}} = a_1 \cdot \ldots \cdot a_n = m = \abs{\Z_m}\]
        
        \item Di conseguenza, poiché $\abs{\Z_{a_1} \times \ldots \times \Z_{a_n}} = \abs{\Z_m}$ e poiché $\varphi$ è iniettiva, ne segue che $\varphi$ possa essere iniettiva se e solo se è suriettiva.
        
        \item Poiché $\varphi$ è biettiva, concludiamo quindi che $\exists! x \texttt{ mod m}$ tale che
        \[\varphi(x \texttt{ mod m}) = (b_1 \texttt{ mod }a_1, \ldots, b_n \texttt{ mod }a_n)\]
        implicando che $x \texttt{ mod m}$ sia l'unica soluzione del sistema.
        
        $\hfill\qed$
    \end{itemize}
    
    \newpage

    \textbf{Esempi:}
    
    \begin{enumerate}
        \item \begin{itemize}
            \item Consideriamo il seguente sistema:
            $$
            \left \{
            \begin{array}{l}
                x \equiv 2 (\texttt{mod } 3)\\
                x \equiv 3 (\texttt{mod } 5)\\
                x \equiv 2 (\texttt{mod } 7)\\
            \end{array}
            \right .
            $$
            
            \item Poiché $x \equiv 2 (\texttt{mod } 3) \iff x=2+3a, \exists a \in \Z$, sostituendo $x=2+3a$ dentro $x \equiv 3 (\texttt{mod } 5)$ otteniamo che:
            \[ 2+3a \equiv 3 (\texttt{mod } 5)\]
            
            \item Impostiamo la seguente equazione, dove le seguenti classi di congruenza appartengono tutte a $\Z_5$:
            \[ [2+3a] = [3] \iff [2]+[3][a] = [3] \iff \]
            \[ \iff [3][a] = [3]-[2] \iff [a] = [1][3]^{-1} \iff \]
            \[ \iff [a] = [1][2] \iff [a] = [2]\]
            
            \item Quindi si ha che $[a] = [2] \in \Z_5 \iff a \equiv 2 (\texttt{mod } 5) \iff a = 2+5b, \exists b \in \Z$
            
            \item Sostituendo $x = 2+3(2+5b) = 8+15b$ dentro $x \equiv 2 (\texttt{mod } 7)$, otteniamo che:
            \[ 8+15b \equiv 2 (\texttt{mod } 5)\]
            
            \item Ripetiamo quindi i passaggi analoghi a prima, stavolta lavorando in $\Z_7$:
            \[ [8+15b] = [2] \iff [8]+[15][b] = [2] \iff \]
            \[ \iff [15][b] = [2]-[8] \iff [1][b] = [2]-[1] \iff [b] = [1]\]
            
            \item Quindi si ha che $[b] = [1] \in \Z_7 \iff b \equiv 1 (\texttt{mod } 7) \iff b = 1+7c, \exists c \in \Z$
            
            \item Infine, otteniamo che 
            \[x = 8+15(1+7c) = 23+105c, \exists c \in \Z \iff x \equiv 23 (\texttt{mod } 105)\]
            
            \item Notiamo come $\mcm(3, 5, 7) = 105$. Difatti, $x \equiv 23 (\texttt{mod } 105)$ è l'unica soluzione del sistema:
            $$
            \left \{
            \begin{array}{l}
                23 \equiv 2 (\texttt{mod } 3)\\
                23 \equiv 3 (\texttt{mod } 5)\\
                23 \equiv 2 (\texttt{mod } 7)\\
            \end{array}
            \right .
            $$
            
        \end{itemize}
    
        \newpage
        
        \item \begin{itemize}
            \item Consideriamo il seguente sistema:
            
            $$
            \left \{
            \begin{array}{l}
                x \equiv 6 (\texttt{mod } 15)\\
                x \equiv 9 (\texttt{mod } 20)\\
            \end{array}
            \right .
            $$
            
            \item Poiché $15$ e $20$ non sono fattori primi, scomponiamo le due congruenze utilizzando il \textbf{teorema cinese dei resti}, in particolare la funzione $\varphi$:
            
            $$
            x \equiv 6 (\texttt{mod } 15)
            \iff 
            \left \{
            \begin{array}{l}
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 1 (\texttt{mod } 5)\\
            \end{array}
            \right .
            $$
            
            $$
            x \equiv 9 (\texttt{mod } 20)
            \iff 
            \left \{
            \begin{array}{l}
                x \equiv 1 (\texttt{mod } 4)\\
                x \equiv 4 (\texttt{mod } 5)\\
            \end{array}
            \right .
            $$
            
            \item Il sistema iniziale, quindi, è equivalente a:
            
            $$
            \left \{
            \begin{array}{l}
                x \equiv 6 (\texttt{mod } 15)\\
                x \equiv 9 (\texttt{mod } 20)\\
            \end{array}
            \right .
            \iff
            \left \{
            \begin{array}{l}
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 1 (\texttt{mod } 5)\\
                x \equiv 1 (\texttt{mod } 4)\\
                x \equiv 4 (\texttt{mod } 5)\\
            \end{array}
            \right .
            $$
            
            \item Notiamo come il sistema sia \textbf{incompatibile}, poiché 
            \[x \equiv 1 (\texttt{mod } 5) \iff x \not\equiv 4 (\texttt{mod } 5)\]
            dunque il sistema \textbf{non ammette alcuna soluzione}
            
        \end{itemize}
        
        \quad
        
        \item \begin{itemize}
            \item Consideriamo il seguente sistema:
            
            $$
            \left \{
            \begin{array}{l}
                x \equiv 6 (\texttt{mod } 15)\\
                x \equiv 11 (\texttt{mod } 20)\\
                x \equiv 15 (\texttt{mod } 21)\\
            \end{array}
            \right .
            $$
            
            \item Scomponendo in fattori primi si ha che:
            
            $$
            x \equiv 6 (\texttt{mod } 15)
            \iff
            \left \{
            \begin{array}{l}
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 1 (\texttt{mod } 5)\\
            \end{array}
            \right .
            $$
            
            $$
            x \equiv 11 (\texttt{mod } 20)
            \iff
            \left \{
            \begin{array}{l}
                x \equiv 3 (\texttt{mod } 4)\\
                x \equiv 1 (\texttt{mod } 5)\\
            \end{array}
            \right .
            $$
            
            $$
            x \equiv 15 (\texttt{mod } 21)
            \iff
            \left \{
            \begin{array}{l}
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 1 (\texttt{mod } 7)\\
            \end{array}
            \right .
            $$
            
            \item Il sistema iniziale, quindi, è equivalente a:
            
            $$
            \left \{
            \begin{array}{l}
                x \equiv 6 (\texttt{mod } 15)\\
                x \equiv 11 (\texttt{mod } 20)\\
                x \equiv 15 (\texttt{mod } 21)\\
            \end{array}
            \right .
            \implies
            \left \{
            \begin{array}{l}
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 1 (\texttt{mod } 5)\\
                x \equiv 3 (\texttt{mod } 4)\\
                x \equiv 1 (\texttt{mod } 7)\\
            \end{array}
            \right .
            $$
            
            \item Poiché $x \equiv 0 (\texttt{mod } 3) \iff x = 0 + 3a, \exists a \in \Z$, sostituendo nella seconda congruenza otteniamo che $3a \equiv 1 (\texttt{mod } 5)$. Lavorando in $\Z_5$ quindi si ha che:
            \[ [3a] = [1] \iff [3][a] = [1] \iff \]
            \[ \iff [a] = [1][3]^{-1} \iff [a] = [2]\]
            
            \item Dunque $[a] = [2] \in \Z_5 \iff a \equiv 2 (\texttt{mod } 5) \iff a = 2 +5b, \exists b \in \Z$.
            
            
            \item Sostituendo nella terza congruenza otteniamo $x = 3(2+5b) = 6+15b \iff 6+15b \equiv 3 (\texttt{mod } 4)$. Lavorando in $\Z_4$ si ha che:
            \[ [6+15b] = [3] \iff [6]+[15][b] = [3] \iff \]
            \[ \iff [2]+[3][b] = [3] \iff [3][b] = [3]-[2] \iff\]
            \[ \iff [b] = [1][3]^{-1} \iff  [b] = [3]\]
            
            \item Dunque $[b] = [3] \in \Z_4 \iff b \equiv 3 (\texttt{mod } 4) \iff b = 3+4c, \exists c \in \Z$
            \item Sostituendo nella quarta congruenza otteniamo $x = 6+15(3+4c) = 51+60c \iff 51+60c \equiv 1 (\texttt{mod } 7)$. Lavorando in $\Z_7$ quindi si ha che:
            \[ [51+60c] = [1] \iff [2]+[4][c] = [1] \iff [2]+[4][c] = [1] \iff \]
            \[ \iff [4][c] = [1]-[2] \iff [4][c] = [-1] \iff [c] = [6][4]^{-1} \iff \]
            \[ \iff [c] = [6][2] \iff [c] = [12] \iff [c] = [5]\]
            \item Dunque $[c] = [2] \iff c \equiv 5 (\texttt{mod }7) \implies c = 5+7d, \exists d \in \Z$.
            \item Infine, otteniamo che
            \[x = 51+60(5+7d) = 351+420d \implies x \equiv 351 (\texttt{mod } 420)\]
            che risulta essere l'unica soluzione del sistema. Difatti verifichiamo che:
            
            $$
            \left \{
            \begin{array}{l}
                351 \equiv 6 (\texttt{mod } 15)\\
                351 \equiv 11 (\texttt{mod } 20)\\
                351 \equiv 15 (\texttt{mod } 21)\\
            \end{array}
            \right .
            \implies
            \left \{
            \begin{array}{l}
                351 \equiv 0 (\texttt{mod } 3)\\
                351 \equiv 1 (\texttt{mod } 5)\\
                351 \equiv 3 (\texttt{mod } 4)\\
                351 \equiv 1 (\texttt{mod } 7)\\
            \end{array}
            \right .
            $$
        \end{itemize}
        
        \quad
        
        \item \begin{itemize}
            \item Vogliamo calcolare le ultime due cifre di $37^{37}$. Poniamo quindi $x := 37^{37}$ e calcoliamo la classe di equivalenza $x \texttt{ mod } 100$.
            
            \item Scomponiamo quindi $100 = 4 \cdot 25$ in modo da poter applicare il teorema cinese dei resti:
            
            \begin{itemize}
                \item Calcoliamo la classe di equivalenza di $x$ in $\Z_4$
                \[ [x] = [37^{37}] = [37]^{37} = [1]^{37} = [1]\]
                
                \item Calcoliamo la classe di equivalenza di $x$ in $\Z_{25}$
                \[ [x] = [37^{37}] = [37]^{37} = [12]^{37} = [12][12]^{36} = [12][(12)^2]^{18} = [12][144]^{18} = \]
                \[= [12][19]^{18} = [12][-6]^{18} = [12][(-6)^2]^9 = [12][36]^9 = [12][11]^9 =\]
                \[ = [12][11][(11)^2]^4 = [12][11][121]^4 = [12][11][-4]^4 = [12][11][6] = [792] = [17]\]
                
            \end{itemize}
            
            \item Impostiamo quindi il seguente sistema e procediamo applicando il teorema cinese:
            $$
            \left \{
            \begin{array}{l}
                x \equiv 1 (\texttt{mod } 4)\\
                x \equiv 17 (\texttt{mod } 25)\\
            \end{array}
            \right .
            $$
            
            \item Abbiamo quindi che $x = 1 +4k \implies 1+4k \equiv 17 (\texttt{mod } 25)$:
            
            \[ [1]+[4][k] = [17] \iff  [4][k]=[16] \iff [k] = [16][4]^{-1} \iff \]
            \[ \iff [k] = [16][19] \iff [k] = [304] \iff [k] = [4]\]
            
            \item Dunque $k \equiv 4 (\texttt{mod } 25) \implies k = 4+25j \implies x= 1+4(4+25j) = 17+100j$
            \item Quindi concludiamo che $x \equiv 17 (\texttt{mod } 100)$ e quindi che le ultime cifre di $37^{37}$ corrispondono a $17$
        \end{itemize}
        
        \quad
        
        \item \begin{itemize}
            \item Vogliamo calcolare l'inverso di 193 in $\Z_{240}$. Per definizione, ciò equivale a calcolare $193x \equiv 1(\texttt{mod }240)$
            \item Dato che $240 = 3 \cdot 5 \cdot 16$, applichiamo il teorema cinese dei resti
            $$
            193x \equiv 1(\texttt{mod }240) \iff 
            \left \{
            \begin{array}{l}
                193x \equiv 1 (\texttt{mod } 3)\\
                193x \equiv 1 (\texttt{mod } 5)\\
                193x \equiv 1 (\texttt{mod } 16)\\
            \end{array}
            \right .
            $$
            \item Riduciamo le classi di equivalenza del sistema:
            \begin{itemize}
                \item Riduciamo $193x \equiv 1 (\texttt{mod } 3)$ in:
                \[ [193][x] = [1] \implies [1][x] = [1] \implies [x] = [1]\]
                \item Riduciamo $193x \equiv 1 (\texttt{mod } 5)$ in:
                \[ [193][x] = [1] \implies [1][x] = [1] \implies [x] = [3]^{-1} \implies [x] = [2]\]
                \item Riduciamo $193x \equiv 1 (\texttt{mod } 16)$ in:
                \[ [193][x] = [1] \implies [1][x] = [1] \implies [x] = [1]\]
            \end{itemize}
            
            \item Riconduciamo quindi il sistema iniziale ad una versione semplificata sulla quale possiamo applicare il teorema cinese dei resti:
            $$
            \left \{
            \begin{array}{l}
                193x \equiv 1 (\texttt{mod } 3)\\
                193x \equiv 1 (\texttt{mod } 5)\\
                193x \equiv 1 (\texttt{mod } 16)\\
            \end{array}
            \right .
            \implies
            \left \{
            \begin{array}{l}
                x \equiv 1 (\texttt{mod } 3)\\
                x \equiv 2 (\texttt{mod } 5)\\
                x \equiv 1 (\texttt{mod } 16)\\
            \end{array}
            \right .
            $$
            
            \item Quindi si ha che $x = 1 +16k \implies 1+16k \equiv 1 (\texttt{mod } 3)$:
            \[[1]+[16][k] = [1] \iff [k] = [0][16]^{-1} \iff [k] = [0]\]
            
            \item Dunque $k = 0 + 3j \implies x = 1+16(0+3j) = 1+48j \implies 1+48j \equiv 2 (\texttt{mod } 5)$:
            \[ [1]+[48][j] = [2] \iff [j] = [1][3]^{-1} \iff [j] = [2]\]
            
            \item Infine $j = 2+5h \implies x = 1+48(2+5h) = 97+240h \implies x \equiv 97 (\texttt{mod } 240)$
            \item Dunque $[197]^{-1} = [97] \in \Z_{240}$. Difatti, in $\Z_{240}$ si ha che $[193][97] = [1]$
        \end{itemize}
    \end{enumerate}

    \begin{framedprop}{}
        Dati $n,m \in \Z$ tali che $m \mid n$, si ha che:
        \[x \equiv y (\ttt{mod } n) \implies x \equiv y (\ttt{mod }m)\]
    \end{framedprop}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Poiché $m \mid n \iff n = mk, k \in \Z$, il risultato segue automaticamente dalla definizione di congruenza:
        \[x \equiv y (\ttt{mod } n) \iff x = y + nh, h \in \Z \implies x = y+mkh \iff x \equiv y (\ttt{mod }m)\]

        $\hfill\qed$
    \end{itemize}

    \textbf{Esempi:}

    \begin{enumerate}
        \item \begin{itemize}
            \item Consideriamo il seguente sistema:
            
            $$
            \soe{l}{
                x \equiv 2 (\texttt{mod } 12)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            $$

            \item Applicando il teorema cinese dei resti otteniamo che
            
            $$
            \soe{l}{
                x \equiv 2 (\texttt{mod } 12)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            \iff
            \soe{l}{
                x \equiv 2 (\texttt{mod } 3)\\
                x \equiv 2 (\texttt{mod } 4)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            $$

            \item Poiché $4 \mid 8$, per la proposizione precedente si ha che:
            \[x \equiv 4 (\texttt{mod } 8) \implies x \equiv 4 \equiv 0 (\texttt{mod } 4)\]

            \item Di conseguenza, il sistema risulta incompatibile poiché
            \[x \equiv 2 (\texttt{mod } 4) \iff x \not\equiv 0 (\texttt{mod } 4)\]
        \end{itemize}


        \item \begin{itemize}
            \item Consideriamo il seguente sistema:
            
            $$
            \soe{l}{
                x \equiv 0 (\texttt{mod } 12)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            $$

            \item Applicando il teorema cinese dei resti otteniamo che
            
            $$
            \soe{l}{
                x \equiv 0 (\texttt{mod } 12)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            \iff
            \soe{l}{
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 0 (\texttt{mod } 4)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            $$

            \item Poiché $4 \mid 8$, per la proposizione precedente si ha che:
            \[x \equiv 4 (\texttt{mod } 8) \implies x \equiv 4 \equiv 0 (\texttt{mod } 4)\]

            \item Di conseguenza, il sistema risulta compatibile, implicando che esso possa essere ridotto in 
            
            $$
            \soe{l}{
                x \equiv 0 (\texttt{mod } 12)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            \iff
            \soe{l}{
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 0 (\texttt{mod } 4)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            \iff
            \soe{l}{
                x \equiv 0 (\texttt{mod } 3)\\
                x \equiv 4 (\texttt{mod } 8)\\
            }
            $$
        \end{itemize}
    \end{enumerate}
    
    \quad
    
    \section{Piccolo teorema di Fermat}
    
    \quad
    
    \begin{framedlem}{}
        Dato $p \in \mathbb{P}$ e dato $0 < k < p$ si ha che:
        \[ p \,\left | \, \binom{p}{k}\right .\]
    \end{framedlem}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché $p \in \mathbb{P}$, allora esso non potrà essere semplificato dal denominatore, dunque si ha che:
        \[\binom{p}{k} = \frac{p!}{k! \cdot (p-k)!} = p \cdot \frac{(p-1)!}{k! \cdot (p-k)!} = ph \iff p \,\left | \,\binom{p}{k} \right .\]
        dove $\frac{(p-1)!}{k! \cdot (p-k)!} \in \Z$ poiché per definizione di coefficiente binomiale si ha che $\binom{p}{k} \in \N$
        
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    \[ \binom{7}{3} = \frac{7!}{3! \cdot 4!} = \frac{7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2}{3 \cdot 2 \cdot 4 \cdot 3 \cdot 2} = 7 \cdot 5 \implies 7 \,\left | \, \binom{7}{3}\right .\]
    
    \begin{framedcor}{}
        Dato $p \in \mathbb{P}$, dato  dato $0 < k < p$ e dato $[a] \in \Z_p$, si ha che:
        \[\binom{p}{k} \cdot [a] = [0]\]
    \end{framedcor}
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per il lemma precedente si ha che
        \[p \,\left | \, \binom{p}{k}\right . \iff \binom{p}{k} = ph, \exists h \in \Z\]
        \item Di conseguenza, si ha che:
        \[\binom{p}{k} \cdot [a] = ph \cdot [a] = [p][h][a] = [0][h][a] = [0], \exists h \in \Z\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Dato $p \in \mathbb{P}$ e dati $[a],[b] \in \Z_p$ si ha che:
        \[([a]+[b])^p = [a]^p+[b]^p\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dato il \textbf{binomio di Newton} (dimostrato nella sezione \ref{induction}), sappiamo che:
        \[ ([a]+[b])^p = \sum_{k=0}^p \binom{p}{k}[a]^k[b]^{p-k}\]
        \item Se $k = 0 \lor k=p$, si ha che:
        \[ \binom{p}{0} = \binom{p}{p} = 1\]
        \item Se invece $0 < k < p$, per il corollario precedente sappiamo dato $[x] \in \Z_p$ si ha che:
        \[ p \, \left | \,\binom{p}{k} \right . \implies \binom{p}{k} \cdot [x] = 0\]
        \item Di conseguenza, ogni termine della sommatoria, escluso il primo e l'ultimo, può essere ricondotto alla classe $[0]$:
        \[ ([a]+[b])^p = \sum_{k=0}^p \binom{p}{k} [a]^k[b]^{p-k} = \binom{p}{0}[b]^p+ \binom{p}{p}[a]^p + \sum_{k=1}^{p-1} \binom{p}{k} [a]^k[b]^{p-k}=\]
        \[ = [b]^p+ [a]^p + \sum_{k=1}^{p-1} [0] = [b]^p + [a]^p\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Dato $p \in \mathbb{P}$ e dati $[a_1], \ldots, [a_n] \in \Z_p \mid n \in \N$ si ha che:
        \[ ([a_1] + \ldots + [a_n])^p = [a_1]^p + \ldots + [a_n]^p\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Caso base (n=1):
        \[[a_1]^p = [a_1]^p\]
        \item Caso base (n=2):
        \[ ([a_1]+[a_2])^p = [a_1]^p+[a_2]^p\]
        \item Ipotesi induttiva:
        \[ ([a_1] + \ldots + [a_n])^p = [a_1]^p + \ldots + [a_n]^p \mid n \in \N\]
        \item Passo induttivo:
        \[ ([a_1] + \ldots + [a_n] + [a_{n+1}])^p = (([a_1] + \ldots + [a_n]) + [a_{n+1}])^p = \]
        \[= ([a_1] + \ldots + [a_n])^p + [a_{n+1}]^p = [a_1]^p + \ldots + [a_n]^p+ [a_{n+1}]^p\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Piccolo teorema di Fermat}
    Dato il campo $\Z_p$ dove $p \in \mathbb{P}$, dato $[a] \in \Z_p$ si ha che:
    \[ a^p \equiv a (\texttt{mod }p)\]
    \end{framedthm}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Caso base (a=0):
        \[ [0]^p = [0]\]
        \item Ipotesi induttiva:
        \[ [a]^p = [a]\]
        \item Passo induttivo:
        \[ [a+1]^p = ([a]+[1])^p = [a]^p + [1]^p = [a]^p + [1] = [a]+[1] = [a+1] \]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
    Dato il campo $\Z_p$ dove $p \in \mathbb{P}$, dato $[a] \in \Z_p$ si ha che:
    \[ a^{p+k} \equiv a^{k+1} (\texttt{mod }p)\]
    
    In particolare, se $k = -2$, si ha che:
    \[ a^{p-2} \equiv a^{-1} (\texttt{mod }p)\]
    dunque è sempre possibile calcolare comodamente $a^{-1} \in \Z_p$
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per il piccolo teorema di Fermat, si ha che:
        \[ [a]^p = [a] \iff [a]^{k}[a]^p = [a][a]^{k} \iff [a]^{p+k} = [a][a]^{k+1-1}\iff \]
        \[ \iff [a]^{p+k} = [a][a]^{-1}[a]^{k+1} \iff [a]^{p+k} = [1][a]^{k+1} \iff [a]^{p+k} = [a]^{k+1}\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Vogliamo trovare $[4]^{-1} \in \Z_{13}$. Per il corollario appena mostrato, si ha che:
        \[4^{-1} \equiv 4^{13-2} (\texttt{mod }13) \iff [4]^{-1} = [4]^{11} = [4][4]^{10} = [4][4^2]^5 = [4][16]^5=\]
        \[= [4][3]^5 = [4][3]^2[3]^3 = [4][9][27] = [4][9][1] = [36] = [10]\]
    \end{itemize}

    \quad

    \section{Funzione totiente di Eulero}

    \quad

    \begin{frameddefn}{Funzione totiente di Eulero}
        Dato $n \in \N$, definiamo come $\varphi(n)$ come \textbf{funzione totiente di Eulero}, dove:
        \[\varphi : \N \to \N : n \mapsto \abs{\Z_n}\]
    \end{frameddefn}

    \begin{framedlem}{}
        Dati i due anelli commutativi $\Z_m$ e $\Z_n$, dove $\MCD(m,n)=1$, si ha che:
        \[[a] \in \Z_{mn}^* \iff [a] \in \Z_m^*, [a] \in \Z_n^*\]
    \end{framedlem}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo che $[a] \in \Z_{mn}^*$, da cui ricaviamo che:
        \[[a] \in \Z_{mn}^* \iff \exists x \in \Z_{mn} \mid ax\equiv 1 (\texttt{mod }mn)\]
        \item Poiché $\MCD(m,n) = 1\implies \mcm(m,n) = mn$, per il teorema cinese dei resti si ha che:
        \[ax\equiv 1 (\texttt{mod }mn) \iff \left \{ \begin{array}{l}
             ax\equiv 1 (\texttt{mod }m)\\
             ax\equiv 1 (\texttt{mod }n)
        \end{array}\right . \iff [a] \in \Z_m^*, [a] \in \Z_n^*\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedobs}{}
        Dati $m,n \in \N$ dove $\MCD(m,n)=1$, si ha che:
        \[\varphi(mn)=\varphi(m)\varphi(n)\]
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per il lemma precedente, la seguente funzione $f: \Z_{mn}^* \to \Z_{m}^* \times \Z_n^*$ risulta essere biettiva, implicando che:
        \[\varphi(mn) = \abs{\Z_{mn}^*} = \abs{\Z_{m}^* \times \Z_n^*} = \abs{\Z_{m}^*} \cdot \abs{\Z_n^*} =\varphi(m)\varphi(n)\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedobs}{}
        Dati $p \in \mathbb{P}$ e $k \neq 0 \in \N$, si ha che:
        \[\varphi(p^k) = p^{k-1}(p-1)\]
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per dimostrazione precedente, per ogni $0<a<p^k$, si ha che:
        \[[a] \in \Z_{p^k}^* \iff \MCD(a,p^k) = 1 \]
        \item Inoltre, poiché $p \in \mathbb{P}$, si ha che:
        \[\MCD(a,p^k) = 1 \iff p \nmid a \iff \nexists n \in \Z \mid a = np\]
        \item Simmetricamente, quindi, si ha che:
        \[[a] \notin \Z_{p^k}^* \iff \MCD(a,p^k) \neq 1 \iff p \mid a \iff \exists n \in \Z \mid a = np\]
        \item Consideriamo quindi i multipli di $p$ compresi tra $0$ e $p^k$ (escluso):
        \[0\leq np<p^k \implies 0\leq n \leq p^{k-1}\]
        da cui traiamo che la cardinalità degli elementi non invertibili in $\Z_{p^k}$ corrisponde a:
        \[H := \{[a] \in \Z_{p^k} \mid [a] \notin \Z_{p^k}^*\} \implies \abs{H} = p^{k-1}\]
        \item Infine, quindi, concludiamo che:
        \[\varphi(p^l) = \abs{\Z_{p^k}} = \abs{\Z_{p^k}^*}-\abs{H} = p^k-p^{k-1} = p^{k-1}(p-1)\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{}
        Dato $n \geq 2 \in \N$, si ha che:
        \[\varphi(n) = n \prod_{\substack{p \in \Primes \\ \text{t.c. } p\mid n}}\left ( 1-\frac{1}{p}\right )\]
    \end{framedprop}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per il teorema fondamentale dell'aritmetica si ha che:
        \[\exists! p_1, \ldots, p_k \in \mathbb{P}, i_1, \ldots, i_k \in \N_{>0} \mid n = p_1^{i_1} \cdot \ldots \cdot p_k^{i_k}\]
        \item Poiché tali fattori sono tutti coprimi tra loro, si ha che:
        \[\varphi(n) = \varphi(p_1^{i_1})\cdot \ldots \cdot \varphi(p_k^{i_k}) = p_1^{i_1-1}(p_i-1) \cdot \ldots \cdot p_k^{i_k-1}(p_k-1) =\]
        \[= \frac{p_1^{i_1}(p_1-1)}{p_1} \cdot \ldots \cdot \frac{p_k^{k_1}(p_k-1)}{p_k} =n \prod_{\substack{p \in \Primes \\ \text{t.c. } p\mid n}}\left ( 1-\frac{1}{p}\right )\]
        $\hfill\qed$
    \end{itemize}
    
    \section{Ordine di un elemento di un gruppo}
    \label{order}
    
    \qquad
    
    \begin{frameddefn}{Sottogruppo ciclico ed Ideale d'ordine}
        
    Sia $G$ un gruppo. Dato $g \in G$, definiamo il \textbf{sottogruppo ciclico} $\abk{g} \subgrp G$ e l'\textbf{ideale d'ordine} $I(g) \ideal \Z$ come:
    \[\abk{g} := \{g^n \mid  n \in \Z\}\]
    \[I(g) := \{n \in \Z \mid  g^n = e\}\]
    \end{frameddefn}
    
    \textit{Dimostrazioni:}
    \begin{itemize}
        \item $H \subgrp G$
        \begin{itemize}
            \item $g^0 = e \implies e \in \abk{g}$
            \item $g^n, g^m \in \abk{g} \implies g^n \cdot g^m = g^{n+m} \implies g^{n+m} \in \abk{g}$
            \item $g^n \in \abk{g} \implies (g^n)^{-1} = g^{-n} \implies g^{-n} \in \abk{g}$
        \end{itemize}
        
        \item $I(g) \ideal \Z$
        \begin{itemize}
            \item $g^0 = e \implies 0 \in \abk{g}$
            \item $n, m \in I(g) \implies g^n = g^m = e \implies g^{n+m} = g^{n}\cdot g^m = e \implies n+m \in I(g)$
            \item $n \in I(g) \implies g^{-n} = (g^n)^{-1} = e^{-1} = e \implies -n \in I(g)$
            \item $n \in I(g), k \in \Z \implies g^{nk} = (g^n)^k = e^k = e \implies kn \in I(g)$

            $\hfill\qed$
        \end{itemize}
    \end{itemize}
    
    \begin{frameddefn}{Ordine di un elemento}
        Sia $G$ un gruppo. Dato $g \in G$, definiamo l'\textbf{ordine di $g$} come:
        \[ o(g) := \abs{\abk{g}}\]
    \end{frameddefn}
    
    \begin{framedprop}{}
    Sia $G$ un gruppo. Dato $g \in G$, si ha che $\exists! d \in \N \mid  I(g) = I(d)$ tale che
    \begin{itemize}
        \item $d = 0 \implies o(g) = +\infty$
        \item $d > 0 \implies o(g) = d$
    \end{itemize}
    Dunque, $o(g)$ corrisponde al \textbf{più piccolo esponente} $d$ tale che $g^d = e$ 
    \end{framedprop}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $I(g) \ideal \Z$ e poiché $\Z$ è un dominio ad ideali principali, si ha che $\exists! d \in \N \mid I(g) = I(d)$.

        \item Di conseguenza, si ha che:
        \[ n, m \in I(g) \iff g^n = g^m \iff g^{-n}\cdot g^{n} = g^m \cdot g^{-n} \iff e = g^{m-n} \iff\]
        \[\iff m-n \in I(g) = I(d) \iff m-n = dh, \exists h \in \Z \iff d \mid m-n\]
        \item Consideriamo la funzione $f : \Z \to \abk{g} : n \mapsto g^n$, la quale risulta essere suriettiva per definizione stessa di $\abk{g}$
        
        \item Nel caso in cui $d = 0$, si ha che:
        \[g^n = g^m \iff d \mid m-n \iff 0 \mid  m-n \iff\]
        \[\iff m-n = 0k, \exists k \in \Z \iff m-n = 0 \iff m = n\]
        di conseguenza, si ha che $f(n)=f(m) \iff m = n, \forall m,n \in \Z$, implicando che $f$ sia anche iniettiva.  Dunque, siccome $f$ sarebbe una funzione biettiva, ne segue che
        \[o(g) := \abs{\abk{g}} = \abs{Z} = +\infty\]

        \item Nel caso in cui $d > 0$, invece, si ha che $d \in I(d) = I(g) \iff g^d = e$
        \[\forall n \in \Z, \exists! q,r \in \Z, 0 \leq r < d \mid n = dq+r \implies \]
        \[\implies g^n = g^{dq+r} = g^{dq}g^r = (g^d)^qg^r = e^qg^r = g^r\]
        \item Poiché $\forall n \in \Z, \exists r \in [0,d) \mid g^n = g^r$ ne segue che possano esistere al massimo $d$ potenze di $g$, implicando che $\abs{\abk{g}} \leq d$
        
        \item Consideriamo ora invece la seguente restrizione di $f$, ossia $g := \{0, \ldots, d-1\} \to \abk{g} : n \mapsto g^n$
        \item Considerando ancora il caso in cui $d>0$ e presi $0 \leq m,n < d$, da cui traiamo che $-d < m-n < d$, si ha che:
        \[g^n = g^m \iff g^ng^{-m} = g^{m}g^{-n} \iff g^{m-n} = e \iff\]
        \[\iff m-n \in I(g) = I(d) \iff m-n = dp, \exists p \in \Z\]

        \item Tuttavia, poiché $-d < m-n < d$, l'unica possibilità è $m-n = 0$, implicando che $m = n$. Di conseguenza, si ha che $g(n)=g(m) \iff n=m, \forall 0 \leq m,n < d$, implicando che $g$ sia iniettiva, implicando a sua volta che:
        \[o(g) := \abs{\abk{g}} \leq \abs{\{0, \ldots, d-1\}} = d\]
        \item Infine, quindi, otteniamo che $d \geq \abs{\abk{g}} \leq d \implies \abs{\abk{g}} = d$, implicando quindi che $g$ possa essere iniettiva se e solo se è suriettiva, da cui concludiamo che $g(x) = g^x \in \abk{g}, \forall 0 \leq x < d$:
        \[\abk{g} = \{g^0, \ldots, g^{d-1}\}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Sia $G$ un gruppo. Dato $g \in G \mid o(g) < +\infty$ e dato $k \in \Z$, si ha che:
        \[g^k = e \implies o(g) \mid k\]
    \end{framedobs}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Sia $d:=o(g)$. Dalla definizione stessa di $I(d)$, si ha che:
        \[g^k = e \implies k \in I(d) \implies k = dn, n \in \Z \implies d \mid k\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedcor}{}
        Sia $G$ un gruppo con cardinalità finita. Dato $g \in G$ si ha che 
        \[o(g) := \abs{\abk{g}} \leq \abs{G} < +\infty \implies o(g) \mid  \abs{G} \implies g^{\abs{G}} = e\]
        
        \textit{\textbf{Attenzione}}: se $o(g) = +\infty$ allora $o(g) \not\leq \abs{G}$
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dato $d := o(g) := \abs{\abk{g}} \leq \abs{G} < +\infty $, per il teorema di Lagrange si ha che:
        \[ o(g) \mid  \abs{G} \implies d \mid  \abs{G} \implies \abs{G} = dk, \exists k \in \Z \implies g^{\abs{G}} = g^{dk} = (g^d)^k = e^k = e\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{Piccolo teorema di Fermat (II dim.)}
    Dato il campo $\Z_p$ dove $p \in \mathbb{P}$, dato $[a] \in \Z_p$ si ha che:
    \[ a^p \equiv a (\texttt{mod }p)\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Se $[a] = [0]$, allora abbiamo che $[a]^p = [0]$
        \item Poiché $\Z_p$ è un campo, si ha che $\Z_p^* = \Z_p - \{0\}$, implicando che $\abs{\Z_p^*} = p-1$.
        \item Di conseguenza, dato $[a] \neq [0] \in \Z_p^*$ si ha che:
        \[ o(a) \mid \abs{\Z_p^*} \implies [a]^{\abs{\Z_p^*}} = [1] \implies [a]^{p-1} = [1] \implies [a]^p [a]^{-1} = [1] \implies [a]^p = [a]\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedthm}{Teorema di Eulero}
        Dati $a,n \in \N$ tali che $\MCD(a,n)=1$, si ha che:
        \[a^{\varphi(n)}\equiv 1 (\texttt{mod } n)\]
        dove $\varphi(n)$ è la funzione totiente di Eulero
    \end{framedthm}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per dimostrazione precedente, si ha che:
        \[\MCD(a,n) = 1 \iff [a] \in \Z_n^*\]
        \item Di conseguenza, si ha che:
        \[ o(a) \mid \abs{\Z_n^*} \implies [a]^{\abs{\Z_n^*}} = [1] \implies [a]^{\varphi(n)} = [1] \]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Gruppo ciclico}
        Sia $G$ un gruppo con cardinalità finita. Dato $g \in G$ si ha che 
        \[o(g) := \abs{\abk{g}} = \abs{G} \iff \abk{g} = G\]
        In tal caso definiamo $G$ come \textbf{gruppo ciclico}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\abk{g} \subgrp G \implies \abk{g} \subseteq G$, per definizione stessa di insieme improprio si ha
        \[\abs{\abk{g}} = \abs{G} \iff \abk{g} = G\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Sia $G$ un gruppo. Dato $g \in G$, si ha che:
        \[ g \in G^* \implies g^{o(g)-1} = g^{-1} \]
    \end{framedcor}

    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Siccome $g \in G^* \iff \exists g^{-1} \in \mathbb{G}$, allora:
        \[g^{o(g)} = 1 \iff g^{o(g)}g^{-1} = g^{-1} \iff g^{o(g)-1} = g^{-1}\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedlem}{}
        Sia $G$ un gruppo. Dato $g \in G$, si ha che:
        \[g \in G^* \implies o(g) = o(g^{-1})\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Siccome $g \in \mathbb{G}^* \iff \exists g^{-1} \in \mathbb{G}$, allora:
        \[(g^{-1})^n \in \abk{g^{-1}} \implies (g^{-1})^n = g^{-n} \in \abk{g}\]
        \item Analogamente, si ha che:
        \[g^n \in \abk{g} \implies g^n = (g^-1)^{-n} \in \abk{g^{-1}}\]
        \item Di conseguenza, si verifica che $\abk{g} = \abk{g^{-1}} \implies o(g) = o(g^{-1})$
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Sia $G$ un gruppo finito e  $k \in \Z$, per ogni $g \in G$ si verifica che:
        \[ o(g^k) \mid o(g)\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dimostriamo che $\abk{g^k} \subgrp \abk{g}$
        \begin{itemize}
            \item $(g^k)^n \in \abk{g^k} \implies (g^k)^n = g^{kn} \in \abk{g} \implies \abk{g^k} \subseteq \abk{g}$
            \item $(g^k)^0 = g^0 = e \in \abk{g^k}$
            \item $(g^k)^n, (g^k)^m \in \abk{g} \implies (g^k)^n(g^k)^m = g^{kn}g^{km} = g^{kn+km} = (g^k)^{n+m} \in \abk{g}$
            \item $(g^k)^{n} \in \abk{g^k} \implies ((g^k)^{n})^{-1} = (g^k)^{-n} \in \abk{g^k}$
        \end{itemize}
        \item Di conseguenza, per il teorema di Lagrange si ha che
        \[\abs{\abk{g^k}} \mid \abs{\abk{g}} \iff o(g^k) \mid o(g)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Sia $G$ un gruppo finito. Dati $g, h \in G \mid gh=hg$, si ha che:
        \[ \frac{m}{d} \mid o(gh) \qquad \text{ e } \qquad o(gh) \mid m\]
        dove $m:= \mcm(o(g), o(h))$ e $d:= \MCD(o(g), o(h))$.
        
        In particolare, se $d = 1$, allora $o(gh)=o(g)o(h)$.
    \end{framedlem}
    
    \label{mcd_order}
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Per definizione stessa di $m:= \mcm(o(g), o(h))$, si ha che
        \[ o(g) \mid m, o(h) \mid m \iff  o(g) \cdot p = m = o(h) \cdot q, \exists p,q \in \Z\]
        
        \item Siccome per ipotesi $gh = hg$, si ha che:
        \[ (gh)^m = \underbrace{gh \cdot \ldots \cdot gh}_{\text{$m$ volte}} = g^m h^m = g^{o(g) \cdot p}h^{o(h) \cdot q} = (g^{o(g)})^p(h^{o(h)})^q = e^pe^q = e \implies\]
        \[\implies m \in I(gh) = I(o(gh)) \implies o(gh) \mid m\]
        \item Inoltre, abbiamo che
        \[ e = (gh)^{o(gh)} = \underbrace{gh \cdot \ldots \cdot gh}_{\text{$o(gh)$ volte}} = g^{o(gh)}h^{o(gh)} \implies e = g^{o(gh)} h^{o(gh)} \iff g^{o(gh)} = h^{-{o(gh)}}\]
        \item Per il lemma precedente, abbiamo che
        \[o(g^{o(gh)}) \mid o(g), o(h^{-{o(gh)}}) \mid o(h)\]
        e dato che  $g^{o(gh)} = h^{-{o(gh)}}$, otteniamo che 
        \[o(g^{o(gh)}) \mid o(g), o(h^{-{o(gh)}}) \mid o(h) \iff o(g^{o(gh)}) \mid o(g), o(g^{o(gh)}) \mid o(h) \implies o(g^{o(gh)}) \mid d\]
        dove $d = \MCD(o(g),o(h))$
        \item A questo punto, notiamo che:
        \[ \frac{m}{d} \cdot \frac{d}{o(g^{o(gh)})} = \frac{m}{o(g^{o(gh)})} \implies \frac{m}{d} \mid \frac{m}{o(g^{o(gh)})}\]
        \item Inoltre, ponendo $k := g^{o(gh)}$ abbiamo che
        \[ g^{o(g^{o(gh)}){o(gh)}} = g^{k \cdot o(gh)} = (g^{o(gh)})^k = k^{o(k)} = e \implies o(g) \mid o(g^{o(gh)}){o(gh)}\]
        e analogamente che:
        \[ h^{-o(g^{o(gh)}){o(gh)}} = (h^{-{o(gh)}})^{o(g^{o(gh)})} = (g^{o(gh)})^{o(g^{o(gh)})} = k^{o(k)} = e \implies\]
        \[\implies o(h) \mid -o(g^{o(gh)}){o(gh)} \implies o(h) \mid o(g^{o(gh)}){o(gh)}\]
        
        di conseguenza si ha che $m \mid o(g^{o(gh)}){o(gh)}$
        
        \item Quindi, $\exists j \in \Z$ tale che:
        \[ o(g^{o(gh)}){o(gh)} = mj \implies {o(gh)} = \frac{m}{o(g^{o(gh)})} \cdot j \implies \frac{m}{o(g^{o(gh)})} \mid {o(gh)}\]
        
        \item Infine, per transitività si ha che:
        \[ \frac{m}{d} \mid \frac{m}{o(g^{o(gh)})}, \frac{m}{o(g^{o(gh)})} \mid o(gh) \implies \frac{m}{d} \mid o(gh)\]
        
        \item Per l'ultima affermazione notiamo che se $d = 1$, allora:
        \[\frac{m}{d} \mid o(gh) \implies m \mid o(gh)\]
        di conseguenza, poiché $m,d \in \N$, per anti-simmetria (sezione \ref{antisim}) si ha che:
        \[ m \mid o(gh), o(gh) \mid m \iff m = o(gh)\]
        
        \item Dunque, per il teorema fondamentale dell'algebra, se $d = 1$ si ha che:
        \[ o(gh) = m = o(g)o(h)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Siano $n_1, \ldots, n_k \neq 0 \in \N \mid \MCD(a_i, a_j) \iff i \neq j$ e sia $N := \mcm(n_1, \ldots, n_k) = n_1 \cdot \ldots \cdots n_k$.
        
        Dato $[a] \in \Z_m^*$, dove $m := \mcm(o_1, \ldots, o_k)$ e dove $o_h := o([a])$ nel gruppo $\Z_{n_h}^*, \forall 0<h<k$, posto $o := o([a])$ nel gruppo $\Z_m^*$ si ha che:
        \[ o = m := \mcm(o_1, \ldots, o_k)\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Per il teorema cinese dei resti, abbiamo che:
        $$
        a^o \equiv 1 (\texttt{mod }N) \iff
        \left \{ \begin{array}{l}
            a^o \equiv 1 (\texttt{mod }n_1)\\
            \vdots\\
            a^o \equiv 1 (\texttt{mod }n_k)\\
        \end{array}
        \right .
        \iff
        $$
        $$
        \iff
        \left \{ \begin{array}{l}
            o_1 \mid o\\
            \vdots\\
            o_k \mid o\\
        \end{array}\right .
        \iff
        m := \mcm(o_1, \ldots, o_k) \mid o
        $$
        
        \item Inoltre, poiché $m := \mcm(o_1, \ldots, o_k)$, abbiamo che:
        $$
        \left \{ \begin{array}{l}
            o_1 \mid m\\
            \vdots\\
            o_k \mid m\\
        \end{array}\right .
        \iff
        \left \{ \begin{array}{l}
            a^m \equiv 1 (\texttt{mod }n_1)\\
            \vdots\\
            a^m \equiv 1 (\texttt{mod }n_k)\\
        \end{array}\right .
        \iff
        a^m \equiv 1 (\texttt{mod } N)
        \implies o \mid m
        $$
        \item Poiché $o,m \in \N$, per anti-simmetria (sezione \ref{antisim}) si ha che:
        \[ o \mid m, m \mid o \iff o = m\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{}
        Dato l'anello commutativo $\Z_n$, dove $n >2$, si ha che $o(n-1) = 2$
    \end{framedprop}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Dalle proprietà dei moduli ne segue direttamente che:
        \[(n-1)^2 \equiv n^2-2n+1 \equiv 0^2-2 \cdot 0+1 \equiv 1 (\texttt{mod }n)\]
        \item Di conseguenza, otteniamo che:
        \[2 = o(n-1)k, k \in \Z \implies o(n-1) \mid 2 \implies o(n-1) \in \{1,2\}\]
        \item Poiché $(n-1)^1 \equiv n-1 (\texttt{mod }n)$ e $n > 2 \implies n-1 > 1$, ne segue necessariamente che $n-1 \not\equiv (\texttt{mod }n)$ e di conseguenza che $o(n-1) \neq 1$.
        \item Dunque, l'unica possibilità è che $o(n-1) = 2$

        $\hfill\qed$
    \end{itemize}

    \begin{framedcor}{}
        Dato l'anello commutativo $\Z_n$, dove $n >2$, si ha che $\abs{\Z_n^*} = 2k, k \in \N$, ossia che $\abs{\Z_n^*}$ è sempre pari
    \end{framedcor}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Per la proposizione precedente e il teorema di Lagrange, ne segue automaticamente che
        \[o(n-1) = 2 \land o(n-1) \mid \abs{\Z_n^*} \implies 2 \mid \abs{\Z_n^*}  \implies \abs{\Z_n^*} = 2k, k \in \Z\]
        \item Poiché la cardinalità di un gruppo non può essere negativa, ne segue necessariamente che $\abs{\Z_n^*} = 2k, k \in \N$
        
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Vogliamo trovare tutti gli inversi di $\Z_{21}$ e il loro ordine, determinando se $\Z_{21}$ sia un gruppo ciclico
        \item Dato $g \in \Z_{21}$, sappiamo che $g \in \Z_{21}^* \iff \MCD(a, 21) = 1$ (sezione \ref{inv_mcd}). Dunque abbiamo che:
        \[ \Z_{21}^* := \{ [1], [2], [4], [5], [8], [10], [11], [13], [16], [17], [19], [20]\} \implies \abs{\Z_{21}^*} = 12\]
        
        \item Dato $g \in \Z_{21}^*$, per Lagrange abbiamo che $o(g)$ può essere solo un divisore di $\abs{\Z_{21}^*}$, riducendo i tentativi necessari a trovare l'ordine di ogni elemento da $21$ a $6$:
        \[
        o(g) \mid \abs{\Z_{21}^*} \implies o(g) \mid 12 \implies o(g) \in \{1,2,3,4,6,12\} 
        \]
        
        \item Ricordando che $g^{-1} = g^{o(g)-1}$ e che $o(g) = o(g^{-1})$, calcoliamo gli ordini dei vari invertibili in $\Z_{21}$  trovati:
        
        \begin{itemize}
            \item 
            $
            [1]^1 = 1 \implies \left \{
            \begin{array}{l}
                o([1])= 1 \\
                {[1]}^{-1} = [1]^{0} = [1]
            \end{array}\right.
            $
            
            \item 
            $
            [2]^{6} = [64] = [1] \implies \left \{
            \begin{array}{l}
                o([2])= 6 \\
                {[2]}^{-1} = [2]^{5} = [11]
            \end{array}\right.
            \implies \left \{
            \begin{array}{l}
                o([11])= 6 \\
                {[11]}^{-1} =  [2]
            \end{array}\right.
            $
            
            \item 
            $
            [4]^{3} = [2]^6 = [64] = [1]  \implies \left \{
            \begin{array}{l}
                o([4])= 3 \\
                {[4]}^{-1} = [4]^{2} = [16]
            \end{array}\right.
            \implies \left \{
            \begin{array}{l}
                o([16])= 3 \\
                {[16]}^{-1} =  [4]
            \end{array}\right.
            $
            
            \item 
            $
            [5]^{6} = [5^2]^3 = [4]^3 = [1]  \implies \left \{
            \begin{array}{l}
                o([5])= 6 \\
                {[5]}^{-1} = [5]^{5} = [17]
            \end{array}\right.
            \implies \left \{
            \begin{array}{l}
                o([17])= 6 \\
                {[17]}^{-1} = [5]
            \end{array}\right.
            $
            
            \item 
            $
            [8]^{2} = [2]^6 = [1]  \implies \left \{
            \begin{array}{l}
                o([8])= 2 \\
                {[8]}^{-1} = [8]
            \end{array}\right.
            $
            
            \item 
            $
            [10]^{6} = [10^3]^2 = [13]^2 = [1]  \implies \left \{
            \begin{array}{l}
                o([10])= 6 \\
                {[10]}^{-1} = [10]^{5} = [19]
            \end{array}\right.
            \implies \left \{
            \begin{array}{l}
                o([19])= 6 \\
                {[19]}^{-1} = [10]
            \end{array}\right.
            $
            
            \item 
            $
            [13]^{2} = [1]  \implies \left \{
            \begin{array}{l}
                o([13])= 2 \\
                {[13]}^{-1} =  [13]^{1} = [13]
            \end{array}\right.
            $
            
            \item 
            $
            [20]^{2} = [1]  \implies \left \{
            \begin{array}{l}
                o([20])= 2 \\
                {[20]}^{-1} = [20]^{1} = [20]
            \end{array}\right.
            $
            
        \end{itemize}
        
        \item Poiché $\nexists g \in \Z_{21}^* \mid o(g) = \abs{\Z_{21}^*} = 12$, concludiamo che $\Z_{21}^*$ non è un gruppo ciclico
    \end{itemize}
    
            
    \chapter{Gruppo Simmetrico}
    \label{permutations}
    
    \begin{framedobs}{}
        Una funzione $f : X \to Y : x \to f(x)$ è invertibile se e solo se $f$ è biettiva.
        \[ f \texttt{ invertibile} \iff f \texttt{ biettiva}\]
        dove l'essere invertibile equivale a dire che $\exists f^{-1} : Y \to X : f(x) \mapsto x$
    \end{framedobs}
    \textit{Dimostrazione:}

    \begin{itemize}
        \item Sia $f : X \to Y : x \mapsto f(x)$
        \item Se $\exists f^{-1} : Y \to X : f(x) \mapsto x$, ossia $f$ è invertibile, allora
        \[f(x) = f(y) \implies  f^{-1}(f(x)) = f^{-1}(f(g)) \implies x = y\]
        dunque $f$ è iniettiva
        \item Analogamente, si ha che
        \[\forall y \in Y, \exists x \in X \mid y = f(x) = f(f^{-1}(y))\]
        dunque $f$ è anche suriettiva, implicando che essa sia biettiva
        \item Inoltre, poiché $f = (f^{-1})^{-1}$, anche $f^{-1}$ è invertibile e di conseguenza biettiva
        \item Se invece $f$ è biettiva, allora
        \[\forall x \in X, \exists! y \in Y \mid f(x)=y \implies f(f^{-1}(y)) = y \]
        di conseguenza, $f$ è invertibile
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Gruppo simmetrico}
    Dato un insieme $X$, denotiamo come $\mathcal{S}_X$ l'insieme:
    \[ \mathcal{S}_X := \{f: X \to X \mid  f \texttt{ è biettiva}\}\]
    
    Inoltre, si ha che $(\mathcal{S}_X, \circ)$ è un gruppo.
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Per natura stessa della composizione tra funzione si ha che
        \[f, g, h \in \mathcal{S}_X \implies h \circ (g \circ f) = h \circ g \circ f = (h \circ g) \circ f\]        
        \item La funzione identità $\id : X \to X : x \mapsto x$ è biettiva, dunque
        \[\exists \id \in \mathcal{S}_X \mid \forall f \in \mathcal{S}_X, f \circ \id = \id \circ f = f\]
        
        \item Poiché una funzione è biettiva se e solo se è invertibile, ne segue che
        \[\forall f \in \mathcal{S}_X, \exists f^{-1}\in \mathcal{S}_X \mid f \circ f^{-1} = f^{-1} \circ f = \id\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedobs}{}
        Dato il gruppo simmetrico $\mathcal{S}_X$, ogni $f \in \mathcal{S}_X$ corrisponde ad una \textbf{permutazione} del dominio $X$, poiché $f : X \to X$ è biettiva. Dunque, è possibile definire impropriamente $\mathcal{S}_X$ come il "\textbf{gruppo delle permutazioni di $X$}".

        In particolare, se $\abs{X} = n$ dove $n \in \N$, ogni $f \in \mathcal{S}_X$ corrisponderà ad una permutazione di $n$ elementi. In tal caso, denotiamo come $\mathcal{S}_n$ il \textbf{gruppo simmetrico di ordine $n$}, la cui cardinalità corrisponde a $\abs{\mathcal{S}_n} = n!$
    \end{framedobs}

    \textbf{Esempio:}
    \begin{itemize}
        \item Data la permutazione $\sigma \in \mathcal{S}_5$, possiamo utilizzare due \textbf{notazioni} per poterne descrivere il comportamento:
        
        \begin{center}
            \begin{tabular}{m{0.4 \textwidth}m{0.3 \textwidth}}
                \begin{tabular}{c}
                    \textbf{Tramite grafo}\\
                    \includegraphics[scale=0.5]{images/permutation.png}
                \end{tabular}
                &
                \begin{tabular}{c}
                    \textbf{Tramite matrice}\\
                    \\
                    $
                    \sigma = \left (
                    \begin{array}{c c c c c}
                        1 & 2 & 3 & 4 & 5\\
                        5 & 1 & 2 & 4 & 3
                    \end{array}\right )
                    $
                \end{tabular}
            \end{tabular}
        \end{center}
    \end{itemize}
    
    \newpage

    \begin{framedobs}{}
        Per comodità di scrittura, definiamo l'operazione binaria \textbf{prodotto tra permutazioni} come:
        \[\cdot : \mathcal{S}_n \times \mathcal{S}_n \to \mathcal{S}_n : (\sigma, \tau) \mapsto \tau \circ \sigma\]

        In altre parole, si ha che $\sigma\tau := \sigma \circ \tau = \sigma(\tau(x)), \forall x$.
        
        Ovviamente, $(\mathcal{S}_n, \cdot)$ risulta essere un \textbf{gruppo} (non abeliano poiché per natura stessa della composizione si ha che $\sigma\tau \neq \tau\sigma)$)
    \end{framedobs}

    \textbf{Esempio:}
    \begin{itemize}
        \item Siano $\sigma, \tau \in \mathcal{S}_5$ tali che:
        \[ \sigma = \left (
    \begin{array}{c c c c c}
        1 & 2 & 3 & 4 & 5\\
        5 & 1 & 2 & 4 & 3
    \end{array}\right )
    \qquad\qquad
    \tau = \left (
    \begin{array}{c c c c c}
        1 & 2 & 3 & 4 & 5\\
        5 & 3 & 1 & 2 & 4
    \end{array}\right )\]

    \item Per calcolare il prodotto tra le due permutazioni (dunque la loro composizione), utilizziamo due metodi:
        \begin{itemize}
            \item Tramite \textbf{grafo}, considerando la composizione delle frecce rappresentanti le due permutazioni
        \end{itemize}
        \begin{center}
            \includegraphics[scale=0.5]{images/permutation2.png}
        \end{center}
        
        \begin{itemize}
            \item Tramite \textbf{matrici}, dove ci basta "allineare" gli elementi in input della seconda permutazione con gli elementi in output della seconda. Il risultato del prodotto sarà costituito dagli \textbf{elementi in input della prima} e gli \textbf{elementi in output della seconda}.
        \end{itemize}
    \end{itemize}
    
        $$
        \begin{array}{ccc}
        \sigma = \left (
        \begin{array}{c c c c c}
            1 & 2 & 3 & 4 & 5\\
            5 & 1 & 2 & 4 & 3
        \end{array}\right )
        \\
        \tau = \left (
        \begin{array}{c c c c c}
            1 & 2 & 3 & 4 & 5\\
            5 & 3 & 1 & 2 & 4
        \end{array}\right )
        \end{array}
        \implies
        \begin{array}{ccc}
        \sigma = \left (
        \begin{array}{c c c c c}
            1 & 2 & 3 & 4 & 5\\
            5 & 1 & 2 & 4 & 3
        \end{array}\right )
        \\
        \tau = \left (
        \begin{array}{c c c c c}
            5 & 1 & 2 & 4 & 3\\
            4 & 5 & 3 & 2 & 1
        \end{array}\right )
        \end{array}
        \implies
        \tau\sigma = \left (
        \begin{array}{c c c c c}
            1 & 2 & 3 & 4 & 5\\
            4 & 5 & 3 & 2 & 1
        \end{array}\right )
        $$
    
    \newpage
    
    \section{Ordine di una permutazione}

    \quad
    
    \begin{frameddefn}{Ciclo di una permutazione}
    
    Sia $\sigma \in  \mathcal{S}_n $. Definiamo come \textbf{ciclo di $\sigma$} una sequenza di interi $1 \leq i_1, \ldots, i_n \leq n$ tutti distinti tra loro tali che:
    \[ \sigma(i_1) = i_2, \sigma(i_2) = i_3, \ldots, \sigma(i_n) = i_1\]
    Definiamo come \textbf{lunghezza del ciclo} il numero di elementi appartenenti al ciclo.
    \end{frameddefn}

    \textbf{Esempio:}

    \begin{itemize}
        \item Consideriamo la seguente permutazione in $\sigma \in \mathcal{S}_9$:
        $$
        \sigma = \left (
        \begin{array}{ccccccccc}
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
            5 & 7 & 6 & 9 & 2 & 3 & 1 & 4 & 8
        \end{array}
        \right )
        $$
    
        \item Notiamo la presenza di tre cicli all'interno di tale permutazione:
        \begin{itemize}
            \item $1 \to 5 \to 2 \to 7 \to 1$ che abbreviamo come $(1527)$
            \item $3 \to 6 \to 3$ che abbreviamo come $(36)$
            \item $4 \to 9 \to 8 \to 4$ che abbreviamo come $(498)$
        \end{itemize}
    \end{itemize}
    
     \begin{frameddefn}{Decomposizione in cicli}
        Data $\sigma \in \mathcal{S}_n$ composta da $k$ cicli, definiamo la sua \textbf{decomposizione in cicli} come:
        \[ \sigma = \gamma_1\gamma_2\ldots\gamma_k\]
        dove $\gamma_i$ è un ciclo di $\sigma$
     \end{frameddefn}

    \textbf{Esempio:}
     \begin{itemize}
        \item Considerando ancora l'esempio precedente, possiamo riscrivere $\sigma$ tramite la sua decomposizione in cicli:
        $$
        \sigma \in \mathcal{S}_9 \mid \sigma = \left (
        \begin{array}{ccccccccc}
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
            5 & 7 & 6 & 9 & 2 & 3 & 1 & 4 & 8
        \end{array}
        \right ) \implies
        \sigma = (1587)(36)(498)  
        $$

        \item  Ovviamente, tramite una decomposizione in cicli è possibile ricostruire la permutazione associata:
        $$
        \tau \in \mathcal{S}_8 \mid \tau = (235)(1874)(6) \implies \tau =
        \left (
        \begin{array}{cccccccc}
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
            8 & 3 & 5 & 1 & 2 & 6 & 4 & 7
        \end{array}
        \right ) 
        $$
     \end{itemize}
    
    \begin{frameddefn}{}
        Sia $\sigma \in  \mathcal{S}_n $. Dati $1 \leq i \leq n$, definiamo:
        \[ I(\sigma, i) := \{n \in \Z \mid  \sigma^n(i) = i\}\]
        \[ I(\sigma) := \{ n \in \Z \mid  \sigma^n = \id\}\]
        dove:
        \begin{itemize}
            \item $\id$ è la permutazione identica, dunque $\id = (1)(2)\ldots(n-1)(n)$
            \item $(I(\sigma(i)), +) \ideal (\Z, +)$
            \item $(I(\sigma), +) \ideal (\Z, +)$
        \end{itemize}
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $I(\sigma,i) \ideal \Z$:
        \begin{itemize}
            \item $\sigma^0(i) = \id(i) = i \implies 0 \in I(\sigma, i)$
            \item $m, n \in I(\sigma, i) \implies \sigma^n(i) = i = \sigma^m(i)\implies \sigma^{n+m}(i) = \sigma^n(\sigma^m(i)) = \sigma^n(i) = i \implies m+n \in I(\sigma, i)$
            \item $n \in I(\sigma, i) \implies \sigma^{-n}(i) = (\sigma^n)^{-1}(i) = i \implies -n \in I(\sigma, i)$
            \item $n \in I(\sigma, i) \implies \sigma^{nk}(i) = (\sigma^n)^k(i) = i, \forall k \in \Z \implies nk \in I(\sigma, i), \forall k \in \Z$
            \item Per gli ultimi due punti è necessario osservare che poiché $\sigma^n(i) = i$, allora $(\sigma^n)^k(i) = i, \forall k \in \Z$, poiché $i$ viene sempre mandato in se stesso
        \end{itemize}
        \item Viene omessa la dimostrazione di $I(\sigma) \ideal \Z$ poiché analoga a quella di $I(\sigma,i) \ideal \Z$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
       Sia $\sigma \in \mathcal{S}_n$ e sia $\gamma_1\ldots\gamma_k$ la sua decomposizione in cicli. Dato il dominio ad ideali principali $\Z$ e dato $i \in \gamma_j \mid j \in [1,k]$ si ha che:
       \[I(\sigma, i) = I(d_j)\]
       dove $d_j$ è la lunghezza di $\gamma_j$
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\Z$ è un dominio ad ideali principali e poiché $I(\sigma, i) \ideal \Z$, si ha che:
        \[\exists! h \in \N \mid I(\sigma, i) = I(h)\]
        dove $h := \min(I(\sigma, i)_{>0})$
        
        \item Sia $i \in (i_1 i_2 \ldots i_{d_j})$, dunque appartenente ad un ciclo di lunghezza $d_j$. Per comodità, supponiamo che $i = i_1$, poiché scorrere l'ordine degli elementi del ciclo non ne cambia le proprietà (ad esempio: $(2783) = (7832) = (8327) = \ldots$)
        \item Se $0<h<d_j$, si ha che:
        \[ 0<h<d_j \implies \sigma^h(i) = \sigma(\sigma^{h-1}(i)) = \sigma(i_h) = i_{h+1} \implies h \notin I(\sigma, i) \]
        
        \item Nel caso in cui invece $h = d$, si verifica che:
        \[ h = d_j \implies \sigma^h(i) = \sigma^{d_j}(i) =\sigma(\sigma^{d_j-1}(i))= \sigma(i_{d_j}) = i_1 = i \implies h \in I(\sigma, i)\]
        
        \item Di conseguenza, affinché $I(\sigma, i) = I(h)$, ne segue necessariamente che $h = d_j$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Ordine di una permutazione}
       Sia $\sigma \in \mathcal{S}_n$ e sia $\gamma_1\ldots\gamma_k$ la sua decomposizione in cicli. Dato il dominio ad ideali principali $\Z$, si ha che:
       \[I(\sigma) = I(m) \implies o(\sigma) = m\]
       dove $m := \mcm(d_1, \ldots, d_k)$ e dove $d_1,\ldots,d_k$ sono rispettivamente le lunghezze di $\gamma_1\ldots\gamma_k$.
       
       Dunque, $o(\sigma)$ corrisponde al \textbf{minimo comune multiplo delle lunghezze dei cicli di $\sigma$}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Per definizione stessa di $I(\sigma)$ e $I(\sigma, i)$, si ha che:
        \[ n \in I(\sigma) \iff \sigma^n = \id \iff \sigma^n(i) = i, \forall i \in [1,n] \iff\]
        \[ \iff n \in I(\sigma, i), \forall i \in [1,n] \iff n \in I(\sigma, 1) \cap \ldots \cap I(\sigma, n)\]
        implicando quindi che $I(\sigma) = I(\sigma, 1) \cap \ldots \cap I(\sigma, n)$
        \item Poiché $\Z$ è un dominio ad ideali principali e poiché $I(\sigma) \ideal \Z$, per il lemma precedente si ha che:
        \[I(\sigma) = I(\sigma, 1) \cap \ldots \cap I(\sigma, n) = I(d_1) \cap \ldots \cap I(d_k) = I(m)\]
        dove $m := \mcm(d_1, \ldots, d_k)$
        
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempi}:
    \begin{itemize}
        \item Data $\sigma \in S_7$ tale che:
        $$
        \sigma =
        \left (
        \begin{array}{ccccccc}
            1 & 2 & 3 & 4 & 5 & 6 & 7\\
            3 & 6 & 5 & 7 & 2 & 1 & 4
        \end{array}
        \right ) = (13526)(47)
        $$
        
        L'ordine di tale permutazione risulta essere:
        \[o(\sigma) = \mcm(5, 2) = 10\]
        
        \item Data $\sigma \in S_{15}$ tale che:
        \[ \sigma = (1 \; 2 \; 10 \; 8 \; 3)(11 \; 7)(4 \; 12 \; 14 \; 6)(13)(5 \; 15 \; 9) \]
        
        L'ordine di tale permutazione risulta essere:
        \[ o(\sigma) = \mcm(5, 2, 4, 1, 3) = 60\]
    \end{itemize}
    
    \quad
    
    \section{Segno delle permutazioni}
    
    \quad
    
    \begin{frameddefn}{Segno di una permutazione}
    Sia $\sigma \in  \mathcal{S}_n $. Definiamo il \textbf{segno di $\sigma$} come:
    
    $$
    \sgn(\sigma) = (-1)^{\abs{\inv(\sigma)}} = \left \{
    \begin{array}{l l}
        +1 & \text{ se } \abs{\inv(\sigma)} \text{ è pari}\\
        -1 & \text{ se } \abs{\inv(\sigma)} \text{ è dipari}\\
    \end{array}\right .
    $$
    
    Dove $\inv(\sigma)$ è l'\textbf{insieme delle sue inversioni}:
    \[ \inv(\sigma) := \{(i,j) \mid 1 \leq i<j \leq n, \sigma(i) > \sigma(j) \}\]
    
    Definiamo $\sigma$ come \textbf{pari} se $\sgn(\sigma)=+1$, mentre come \textbf{dispari} $\sgn(\sigma)=-1$
    \end{frameddefn}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Sia $\sigma \in \mathcal{S}_5$ tale che
        $$
        \sigma = \left (
        \begin{array}{ccccccc}
            1 & 2 & 3 & 4 & 5\\
            2 & 5 & 3 & 1 & 4
        \end{array}\right)
        $$
        
        \item L'insieme delle sue inversioni sarà:
        \[ \inv(\sigma) := \{ (1,4), (2,3), (2,4), (2,5), (3,4)\}\]
        
        da cui otteniamo che $\sgn(\sigma) = -1$
    \end{itemize}
    
    \begin{frameddefn}{Trasposizione e Trasposizione adiacente}
        Definiamo $\tau_{i,j} \in  \mathcal{S}_n$, dove $1 \leq i < j \leq n$, come \textbf{trasposizione} se:
        $$
        \tau_{i,j}(k) = \left \{
        \begin{array}{ll}
            j & \text{ se } k=i\\
            i & \text{ se } k=j\\
            k & \text{ se } k\neq i, k \neq j\\
        \end{array}
        \right .
        $$
    
    In particolare, definiamo come  $\tau_{i,j} \in  \mathcal{S}_n$ come \textbf{trasposizione adiacente} se $j = i+1$, dunque avente l'effetto di scambiare due elementi adiacenti tra loro.
    \end{frameddefn}
    
    \begin{framedlem}{}
        Data $\sigma \in \mathcal{S}_n $, si ha che:
        \[ \exists 1 \leq i_1, \ldots, i_k \leq n \mid  \sigma = \tau_{i_1, i_1+1} \cdot \ldots \cdot \tau_{i_k, i_k+1}\]
        In altre parole, $\sigma$ può essere espressa come il \textbf{prodotto di $k$ trasposizioni adiacenti}
    \end{framedlem}
    
    \textit{Dimostrazione tramite esempio:}
    
    \begin{itemize}
        \item Prima di tutto, osserviamo che dati $\sigma, \tau_{i,j} \in  \mathcal{S}_n $ tali che:
        
        $$
        \sigma = \left (
        \begin{array}{ccccccc}
            1 & \ldots & i & \ldots & j & \ldots & n\\
            \sigma(1) & \ldots & \sigma(i) & \ldots & \sigma(j) & \ldots & \sigma(n)\\
        \end{array}
        \right )
        $$
        
        $$
        \tau_{i, j} = \left (
        \begin{array}{ccccccc}
            1 & \ldots & i & \ldots & j & \ldots & n\\
            1 & \ldots & j & \ldots & i & \ldots & n\\
        \end{array}
        \right )
        $$
        
        si ha che:
        $$
        \sigma \tau_{i, j} = \left (
        \begin{array}{ccccccc}
            1 & \ldots & i & \ldots & j & \ldots & n\\
            \sigma(1) & \ldots & \sigma(j) & \ldots & \sigma(i) & \ldots & \sigma(n)\\
        \end{array}
        \right )
        $$
        
        \item Dunque, data $\sigma \in S_3$ tale che:
        
        $$
        \sigma = \left (
        \begin{array}{cccc}
            1 & 2 & 3 & 4\\
            2 & 4 & 3 & 1
        \end{array}
        \right )
        $$
        
        abbiamo che:
        
        $$
        \sigma \cdot \tau_{3,4} = \left (
        \begin{array}{cccc}
            1 & 2 & 3 & 4\\
            2 & 4 & 1 & 3
        \end{array}
        \right )
        \implies
        \sigma \cdot \tau_{3,4} \cdot \tau_{2,3} = \left (
        \begin{array}{cccc}
            1 & 2 & 3 & 4\\
            2 & 1 & 4 & 3
        \end{array}
        \right )
        \implies
        $$
        
        $$
        \implies
        \sigma \cdot \tau_{3,4} \cdot \tau_{2,3} \cdot \tau_{1,2} = \left (
        \begin{array}{cccc}
            1 & 2 & 3 & 4\\
            1 & 2 & 4 & 3
        \end{array}
        \right )
        \implies
        \sigma \cdot \tau_{3,4} \cdot \tau_{2,3} \cdot \tau_{1,2} \cdot \tau_{3,4} = \left (
        \begin{array}{cccc}
            1 & 2 & 3 & 4\\
            1 & 2 & 3 & 4
        \end{array}
        \right )
        = \id
        $$
        
        \item Di conseguenza, si ha che:
        \[ \sigma (\tau_{3,4} \tau_{2,3} \tau_{1,2} \tau_{3,4}) = \id \iff\]
        \[ \iff \sigma (\tau_{3,4} \tau_{2,3} \tau_{1,2} \tau_{3,4}) (\tau_{3,4} \tau_{2,3} \tau_{1,2} \tau_{3,4})^{-1} = \id (\tau_{3,4} \tau_{2,3} \tau_{1,2} \tau_{3,4})^{-1} \iff \]
        \[ \iff \sigma = (\tau_{3,4} \tau_{2,3} \tau_{1,2} \tau_{3,4})^{-1} \iff \sigma = \tau_{3,4} \tau_{1,2} \tau_{2,3} \tau_{3,4}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Data $\sigma \in  \mathcal{S}_n \mid \sigma = \tau_1 \cdot  \ldots \cdot \tau_k$, dove $\tau_i := \tau_{i,i+1} \in \mathcal{S}_n $, si ha che:
        \[ \sgn(\sigma) = (-1)^k\]
        dove $k$ è il numero di trasposizioni adiacenti che compongono $\sigma$
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia $\tau_i = \tau_{i,i+1}$. Allora si ha che:
        
        $$
        \sigma \tau_i = \left (
        \begin{array}{cccccc}
            1 & \ldots & i & i+1 & \ldots & n\\
            \sigma(1) & \ldots & \sigma(i+1) & \sigma(i) & \ldots & \sigma(n)\\
        \end{array}
        \right )
        $$
        
        \item Lo scambio effettuato genera una di due situazioni possibili: viene \textbf{creata una nuova inversione} oppure viene \textbf{risolta un'inversione pre-esistente}:
        
        $$
        \inv(\sigma \tau_i) = \left \{
        \begin{array}{ll}
        \inv(\sigma) \cup \{(i,i+1)\} & \text{ se } (i,i+1) \notin \inv(\sigma)\\
        \inv(\sigma) - \{(i,i+1)\} & \text{ se } (i,i+1) \in \inv(\sigma)\\
        \end{array}\right .
        $$
        
        \item Di conseguenza, si ha che
        \[\abs{\inv(\sigma \tau_i)} = \abs{\inv(\sigma)} \pm 1 \implies \sgn(\sigma\tau_i) = \left \{ \begin{array}{ll}
            -1 & \text{ se } \sgn(\sigma) = +1\\
            +1 & \text{ se } \sgn(\sigma) = -1
        \end{array}\right .\implies \]
        \[\implies \sgn(\sigma \tau) = -\sgn(\sigma)\]
        
        \item Di conseguenza, se $\sigma = \tau_1 \cdot \ldots \cdot \tau_k$, si ha che:
        \[ \sigma(\tau_i \cdot \ldots \cdot \tau_k)^{-1} = (\tau_i \cdot \ldots \cdot \tau_k)(\tau_i \cdot \ldots \cdot \tau_k)^{-1} = \id \]
        
        \item Poiché per definizione stessa di $\id$ si ha che $\abs{\inv(\id)} = 0 \implies \sgn(\id) = 1$, ne segue che:
        \[ 1 = \sgn(\id) = \sgn(\sigma (\tau_i \cdot \tau_2 \cdot \tau_3 \cdot \ldots \cdot \tau_k)^{-1}) = \sgn(\sigma \cdot \tau_k \cdot \ldots \cdot \tau_3 \cdot \tau_2 \cdot \tau_1)=\]
        \[= -\sgn(\sigma \cdot \tau_k \cdot \ldots \cdot \tau_3 \cdot \tau_2) = \sgn(\sigma \cdot \tau_k \cdot \ldots \cdot \tau_3) = \ldots = (-1)^k \cdot \sgn(\sigma)\]
        
        \item Quindi, otteniamo che:
        \[ 1 = (-1)^k \cdot \sgn(\sigma) \implies \sgn(\sigma) = (-1)^k\]
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Date $\sigma, \sigma' \in  \mathcal{S}_n $, si verifica che:
        \[ \sgn(\sigma \sigma') = \sgn(\sigma) \cdot \sgn(\sigma')\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Data $\sigma = \tau_1  \cdot \ldots \cdot \tau_k$ e $\sigma' = \tau_1' \cdot \ldots \cdot \tau_k'$, si ha che:
        \[ \sgn(\sigma \sigma') = \sgn(\tau_1 \cdot \ldots \cdot \tau_k \cdot \tau_1' \cdot \ldots \cdot \tau_k') = (-1)^{k+j} = (-1)^k (-1)^j = \sgn(\sigma) \cdot \sgn(\sigma')\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Data $\sigma \in  \mathcal{S}_n $, si verifica che:
        \[ \sgn(\sigma^{-1}) = \sgn(\sigma) \]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \[ 1 = \sgn(\id) = \sgn(\sigma \sigma^{-1}) = \sgn(\sigma) \cdot \sgn(\sigma^{-1}) \implies \]
    \[ \implies 1 = \sgn(\sigma) \cdot \sgn(\sigma^{-1}) \iff \sgn(\sigma) = \pm 1 = \sgn(\sigma^{-1})\]
    $\hfill\qed$
    
    \begin{frameddefn}{}
        Dato il gruppo $\mathcal{S}_n$, definiamo $\mathcal{A}_n \subgrp \mathcal{S}_n$ come il \textbf{sottogruppo alterno di ordine $n$}:
        \[\mathcal{A}_n := \{\sigma \in \mathcal{S}_n  \mid  \sgn(\sigma) = +1\}\]
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $\sgn(\id) = 1 \implies \id \in \mathcal{A}_n$
        \item $\sigma,\tau \in \mathcal{A}_n \implies \sgn(\sigma\tau) = \sgn(\sigma)\sgn(\tau) = 1 \cdot 1 = 1 \implies \sigma\tau \in \mathcal{A}_n$
        \item $\sigma \in \mathcal{A}_n \implies \sgn(\sigma^{-1}) = \sgn(\sigma) = 1 \implies \sigma^{-1} \in \mathcal{A}_n$
        
        $\hfill\qed$
    \end{itemize}
    
    \label{alterno}
    \begin{framedobs}{}
        Dato $\mathcal{S}_n$ e $\mathcal{A}_n \subgrp \mathcal{S}_n$, si ha che:
        \begin{itemize}
            \item $\abs{\mathcal{A}_n} = \frac{n!}{2}$
            \item $[\mathcal{S}_n : \mathcal{A}_n] = 2$
        \end{itemize}
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Date $\sigma, \sigma' \in \mathcal{S}_n$, si ha che:
        \[\sigma \sim \sigma' \iff \sigma^{-1}\sigma' \in \mathcal{A}_n \iff  \sgn(\sigma^{-1}\sigma') = 1 \iff\]
        \[\iff \sgn(\sigma^{-1})\sgn(\sigma') =1 \iff \sgn(\sigma) = \sgn(\sigma')\]
        
        \item Di conseguenza, poiché $\sgn(\sigma) = \pm 1, \forall \sigma \in \mathcal{S}_n$ e poiché $\sgn(\id) = +1$, esistono solo due classi laterali sinistre:
        \begin{itemize}
            \item La classe $[+1] := \{\sigma \in \mathcal{S}_n \mid \sigma \sim \id\}$
            \item La classe $[-1] := \{\sigma \in \mathcal{S}_n \mid \sigma \not\sim \id\}$
        \end{itemize}
        dunque si ha che $[\mathcal{S}_n : \mathcal{A}_n] = 2$
        
        \item Infine, per il teorema di Lagrange, concludiamo che:
        \[ \abs{A_n} = \frac{\abs{ \mathcal{S}_n }}{[\mathcal{S}_n : \mathcal{A}_n]} = \frac{n!}{2}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Sia $\sim$ la \textbf{relazione di coniugio} (sezione \ref{conjiugate}) e siano $\sigma, \sigma' \in \mathcal{S}_n$, tali che:
        \begin{itemize}
            \item $\gamma_1\ldots\gamma_k$ è la decomposizione in cicli di $\sigma$ e $d_1, \ldots d_k$ sono le lunghezze rispettive dei cicli
            \item $\gamma'_1\ldots\gamma'_k$ è la decomposizione in cicli di $\sigma'$ e $d'_1, \ldots d'_k$ sono le lunghezze rispettive dei cicli
        \end{itemize}
        
        In tal caso, si ha che:
        \[\sigma \sim \sigma' \iff \left \{
        \begin{array}{l}
        k = h\\
        d_1 = d_1'\\
        \ldots\\
        d_k = d_h'\\
        \end{array}
        \right .
        \]
    \end{framedprop}
    
    \textit{Dimostrazione:}

    \begin{itemize}
        \item Supponiamo che $\sigma \sim \sigma'$, dunque $\exists \alpha \in  \mathcal{S}_n  \mid  \sigma' = \alpha\sigma\alpha^{-1}$
        \item Sia $\gamma_j = (i_1 \ldots i_d) \mid j \in [1,k]$ un ciclo di $\sigma$. Allora, $\forall i_q \in \gamma_j \mid q \in [1,d]$ si ha che:
        \[\sigma'(i_q) = \alpha\sigma\alpha^{-1}(i_q) \iff \sigma'\alpha(i_q) = \alpha\sigma\alpha^{-1}\alpha(i_q) \iff \sigma'\alpha(iq) = \alpha\sigma(i_q) \implies\]
        \[\implies \sigma'\alpha(iq) = \alpha\sigma(i_q) = \left \{ \begin{array}{ll}
            \alpha(i_{q+1}) & \text{ se }  q < d\\
        \alpha(i_1) & \text{ se } q = d
        \end{array}\right .\]
        
        \item Di conseguenza, $\sigma'$ possiede un ciclo nella forma $(\alpha(i_1), \ldots, \alpha(i_d))$. Applicando lo stesso ragionamento con gli altri $\sigma$, possiamo creare una corrispondenza biunivoca tra i cicli di $\sigma$ e i cicli di $\sigma'$, da cui otteniamo che $h=k$ e che $d_p = d_p', \forall p \in [1,k]$
        
        \item Viceversa, supponiamo che $\sigma$ e $\sigma'$ abbiano lo stesso numero di cicli e le stesse lunghezze per ogni ciclo, dunque tali che:
        \[\sigma = (i_1\ldots i_{d_1})\ldots(j_1 \ldots j_{d_k}) \qquad\qquad \sigma' = (a_1\ldots a_{d_1})\ldots(b_1 \ldots b_{d_k})\]
        \item Presa $\alpha \in \mathcal{S}_n$ tale che:
        \[\alpha(i_1) = a_1, \ldots, \alpha(i_{d_1})=a_{d_1}, \alpha(j_1) = b_1, \ldots, \alpha(j_{d_k}) = b_{d_k}\]
        
        dunque tale che:
        \begin{center}
            \begin{tabular}{cccccccc}
                $\sigma =$ & ($i_1$ & \ldots & $i_{d_1}$) & \ldots& ($j_1$ & \ldots & $j_{d_k}$)\\
                           & \textdownarrow & \textdownarrow & \textdownarrow & \textdownarrow & \textdownarrow & \textdownarrow & \textdownarrow \\
                $\sigma' =$ & ($a_1$ & \ldots & $a_{d_1}$) & \ldots& ($b_1$ & \ldots & $j_{b_k}$)
            \end{tabular}
        \end{center}
        
        \item Dato $t \in [1,n]$ e dato $j \in [1,k]$, quindi, si ha che:
        $$
        \alpha\sigma\alpha^{-1}(a_t) = \alpha\sigma(i_t) = \left \{ \begin{array}{ll}
            \alpha(i_{k+1}) & \text{ se } t < d_j\\
            \alpha(i_1) & \text{ se } t = d_j\\
        \end{array}\right .
        =
        \left \{ \begin{array}{ll}
            a_{k+1} & \text{ se } k < d_j\\
            a_1 & \text{ se } k = d_j\\
        \end{array}\right . \implies
        $$
        \[\alpha\sigma\alpha^{-1}(a_t) = \sigma'(a_t) \implies \sigma \sim \sigma'\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempi:}
    
    \begin{enumerate}
        \item Date le seguenti permutazioni $\sigma, \sigma' \in S_6$, trovare $\alpha \in S_6 $ tale che $\sigma' = \alpha\sigma\alpha^{-1}$:
        $$
        \begin{array}{ccc}
            \sigma = (13)(254)(876)\\
            \sigma' = (25)(184)(376)
        \end{array}
        \implies
        \alpha = \left ( \begin{array}{cccccccc}
        1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
        2 & 1 & 5 & 4 & 8 & 6 & 7 & 3
        \end{array}\right )
        $$
        
        \item Date le seguenti permutazioni $\sigma, \sigma' \in S_7$, trovare $\alpha \in S_7 $ tale che $\sigma' = \alpha\sigma\alpha^{-1}$:
        $$
        \begin{array}{ccc}
            \sigma = \left (
            \begin{array}{cccccccc}
                1 & 2 & 3 & 4 & 5 & 6 & 7\\
                3 & 6 & 1 & 4 & 2 & 7 & 5
            \end{array}
            \right )
            = (4)(13)(2675)
            \\
            \sigma' = \left (
            \begin{array}{cccccccc}
                1 & 2 & 3 & 4 & 5 & 6 & 7\\
                2 & 3 & 4 & 1 & 6 & 5 & 7
            \end{array}
            \right )
            = (7)(56)(1234)
        \end{array}
        \implies
        \alpha = \left (
            \begin{array}{cccccccc}
                1 & 2 & 3 & 4 & 5 & 6 & 7\\
                5 & 1 & 6 & 7 & 4 & 2 & 3
            \end{array}
            \right )
        $$
    \end{enumerate}
    
    \begin{framedobs}{}
        Sia $\sim$ la relazione di coniugio. Date $\sigma, \sigma' \in \mathcal{S}_n$, si ha che:
        \[\sigma \sim \sigma' \implies \sgn(\sigma) = \sgn(\sigma)'\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se $\sigma \sim \sigma'$, allora 
        \[\exists \alpha \in \mathbb{S}_n \mid \sigma' = \alpha\sigma\alpha^{-1} \implies \sgn(\sigma') = \sgn(\alpha\sigma\alpha^{-1}) = \sgn(\alpha)\sgn(\sigma)\sgn(\alpha^{-1})\]
        \item Dunque, poiché $\sgn(\alpha) = \sgn(\alpha^{-1}) = \pm 1 \implies \sgn(\alpha)\sgn(\alpha^{-1}) = 1$, se segue che:
        \[\sgn(\sigma') = \sgn(\alpha)\sgn(\sigma)\sgn(\alpha^{-1}) \implies\]
        \[\implies \sgn(\sigma') = \left \{ \begin{array}{ll}
            1 \cdot \sgn(\sigma) \cdot 1 = \sgn(\sigma) & \text{ se } \sgn(\alpha) = 1\\
            (-1) \cdot \sgn(\sigma) \cdot (-1) = \sgn(\sigma) & \text{ se } \sgn(\alpha) = -1
        \end{array}\right . \implies \]
        \[\implies \sgn(\sigma') = \sgn(\sigma)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Data $\sigma \in  \mathcal{S}_n $ e data la sua scomposizione in cicli $\sigma = \gamma_1 \ldots \gamma_k$, si ha che:
        \[ \sgn(\sigma) = (-1)^{n-k}\]
        dove $k$ è il numero di cicli
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia $\sigma \in \mathbb{S}_n$ tale che
        \[ \sigma = (i_1 \; \ldots \; i_{d_1})(i_{d_1+1} \; \ldots \; i_{d_2}) \;\ldots \;(j_1 \;\ldots\; j_{d_k})\]
        \item Consideriamo una permutazione $\sigma' \in  \mathcal{S}_n$ definita come:
        \[ \sigma' = (1,\ldots\;,d_1)(d_1+1,\ldots, d_1+d_2)) \;\ldots\; (n-d_k+1, n-d_k+2,\; \ldots\;,n-1, n) = \]
        $$
        = \left (
         \begin{array}{cccccccccccc}
             1 & 2 & \ldots & d_1 & d_1+1 & \ldots & d_1+d_2 & \ldots & \ldots & n-d_k+1 & \ldots & n\\
             2 & 3 & \ldots & 1 & d_1+2 & \ldots & d_1+1 & \ldots & \ldots & n-d_k+2 & \ldots & n-d_k+1
         \end{array}
        \right )
        $$
        
        \item Poiché $\sigma$ e $\sigma'$ hanno la stessa quantità di cicli ognuno avente la stessa lunghezza del ciclo corrispondente, ne segue che $\sigma \sim \sigma'$. Di conseguenza, si ha che
        \[\sigma \sim \sigma' \implies \sgn(\sigma) = \sgn(\sigma')\]
        
        dunque, ci basterà trovare il segno di $\sigma'$ per ottenere il segno di $\sigma$
        
        \item A questo punto, le seguenti $d_1-1$ trasposizioni otteniamo che:
        \[\sigma \cdot \tau_{d_1-1, d_1} \cdot \tau_{d_1-2, d_1-1}\cdot \ldots \cdot \tau_{2,3} \cdot \tau_{1,2} =\]
        \[ = (1)(2)\ldots(d_1-1)(d_1)(d_1+1,\ldots, d_1+d_2)) \;\ldots\; (n-d_k+1, n-d_k+2,\; \ldots\;,n-1, n) = \]
        $$
        = \left (
         \begin{array}{cccccccccccc}
             1 & 2 & \ldots & d_1 & d_1+1 & \ldots & d_1+d_2 & \ldots & \ldots & n-(d_k+1) & \ldots & n\\
             1 & 2 & \ldots & d_1 & d_1+2 & \ldots & d_1+1 & \ldots & \ldots & n-(d_k+1)+1 & \ldots & n-(d_k+1)
         \end{array}
        \right )
        $$
        
        \item Ripetendo tale procedimento per ogni ciclo di $\sigma'$, otteniamo la permutazione identica.
        \item Di conseguenza, il numero di trasposizioni componenti $\sigma'$ corrisponde a:
        \[ \sum_{j=1}^k d_j-1 = \sum_{j=1}^k d_j - \sum_{j=1}^k 1 = n - k\]
        poiché la somma di tutte le lunghezze dei cicli corrisponde ad $n$.
        
        \item Dunque, poiché $\sigma'$ è composta da $n-k$ trasposizioni adiacenti, concludiamo che:
        \[ \sgn(\sigma) = \sgn(\sigma') = (-1)^{n-k}\]
        $\hfill\qed$
    \end{itemize}

    \chapter{Omomorfismi}
    
    \quad
    
    \begin{frameddefn}{Omomorfismo}
        Date due strutture algebriche $(G, \cdot)$ e $(H, \odot)$ dello stesso tipo (dunque entrambe monoidi, gruppi, anelli, \ldots), definiamo come \textbf{omomorfismo} una funzione $f : G \to H$ tale che:
        \[ f(g \cdot h) = f(g) \odot f(h), \forall g,h \in G\]
        
        \textbf{\textit{Attenzione}}: se le strutture algebriche presentano più operazioni binarie, è necessario che la condizione di omomorfismo sia valida per ognuna di esse
    \end{frameddefn}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Dati i due gruppi $(G, \cdot)$ e $(H, \cdot)$, la funzione $f : G \to H$ è un omomorfismo tra gruppi se e solo se:
        \[f(gh) = f(g)f(h), \forall g,h \in G\]
        \item Dati i due gruppi $(G, \cdot)$ e $(H, +)$, la funzione $f : G \to H$ è un omomorfismo tra gruppi se e solo se:
        \[f(gh) = f(g)+f(h), \forall g,h \in G\]
        \item Dati i due anelli $(A, +, \cdot)$ e $(B, +, \cdot)$, la funzione $f : A \to B$ è un omomorfismo tra anelli se e solo se:
        \[f(a+b) = f(a)+f(b), \forall a,b \in A\]
        \[f(ab) = f(a)f(b), \forall a,b \in A\]
    \end{itemize}
    
    \begin{framedobs}{}
        Dati due gruppi $G$ ed $H$ e un omomorfismo $f : G \to H$, si ha che:
        \begin{enumerate}
            \item $f(1_G) = 1_H$
            \item $f(g^{-1}) = f(g)^{-1}, \forall g \in G$
            \item $f(g^k) = f(g)^k, \forall g \in G, \forall k \in \Z$
        \end{enumerate}
        dove $1_G$ ed $1_H$ sono rispettivamente l'elemento neutro di $G$ ed $H$
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{enumerate}
        \item Dato $g \in G$, per le proprietà del omomorfismo $f$ ne segue che:
        \[ f(g) = f(1_G \cdot g) = f(1_G) f(g) \implies f(g) f(g)^{-1}  = f(1_G)f(g)f(g)^{-1}  \implies\]
        \[\implies 1_H = f(1_G) \cdot 1_H \implies f(1_G) = 1_H\]
    
        \item Dato $g \in G$, per le proprietà del omomorfismo $f$ ne segue che:
        \[ f(1_G) = 1_H \implies f(g \cdot g^{-1}) = 1_H \implies f(g)f(g^{-1}) = 1_H \implies \]
        \[\implies f(g^{-1}) = 1_H \cdot f(g)^{-1} \implies f(g^{-1}) =f(g)^{-1}\]

        \item Dato $g \in G$ e dato $n \in \N$, per le proprietà del omomorfismo $f$ ne segue che:
        \[f(g^n) = f(\underbrace{g \cdot g \cdot \ldots \cdot g\cdot g}_{\text{n volte}}) = \underbrace{f(g)f(g)\ldots f(g) f(g)}_{\text{n volte}} = f(g)^n\]

        A questo punto, dato $k := -n \in \Z$, tramite il punto 2) e il punto appena dimostrato ne segue automaticamente che:
        \[f(g^k) = f(g^{-n}) = f((g^{n})^{-1}) = f(g^n)^{-1} = (f(g)^n)^{-1} = f(g)^{-n} = f(g)^k\]
        
        $\hfill\qed$
    \end{enumerate}

    \begin{framedobs}{}
        Sia $\func{f}{G}{H}$ un omomorfismo tra gruppi. Dato $g \in G \mid o(g) < +\infty$, si ha che
        \[o(f(g)) \mid o(g)\]
    \end{framedobs}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Dalle proprietà degli omomorfismi, ne segue automaticamente che:
        \[f(g)^{o(g)} = f(g^{o(g)}) = f(1_G) = 1_H \implies o(f(g)) \mid o(g)\]
        $\hfill\qed$
    \end{itemize}

    \section{Isomorfismi, Endomorfismi ed Automorfismi}
    
    \quad
    
    \begin{frameddefn}{Isomorfismo, Endomorfismo ed Automorfismo}
        Date due strutture algebriche $G, H$ ed una funzione $f : G \to H$, definiamo $f$ come:
        \begin{itemize}
            \item \textbf{Isomorfismo} se è un \textbf{omomorfismo} ed è \textbf{biettiva}
            \item \textbf{Endomorfismo} se è un omomorfismo e $G = H$, ossia è un \textbf{omomorfismo sullo stesso gruppo}
            \item \textbf{Automorfismo} se è un \textbf{isomorfismo} e un \textbf{endomorfismo}
        \end{itemize}
    \end{frameddefn}

    \textbf{Esempi:}
    
    \begin{enumerate}
    \item \begin{itemize}
        \item Dato $n \in \Z$, definiamo come \textbf{radice n-esima dell'unità}, ossia il numero 1,  un elemento $z \in \C$ tale che $z^n = 1$.
        
        \item Come già visto nella sezione \ref{complex_radix}, l'equazione $z^n = 1$ dove $z \in \C$ ammette $n$ radici. Di conseguenza, esistono $n$ radici n-esime ($z_0, \ldots, z_{n-1}$) tali che $z_k^n = 1$, dove $z_k := e^{i \cdot \frac{2\pi k}{n}}$.
        
        \item Inoltre, poiché tutte le $z_k$ differiscono solo di $k$ all'esponente, denotiamo $\zeta := e^{i \cdot \frac{2\pi}{n}}$, ottenendo quindi che $\zeta^k = z_k$ (tale operazione risulta essere più comoda poiché ci permette di utilizzare le proprietà delle potenze)
        
        \item Definiamo quindi il seguente insieme:
        \[ H^n = \{ z \in \C \mid z^n = 1\} = \{ \zeta^0, \ldots, \zeta^{n-1}\}\]
        
        e dimostriamo che $(H^n, \cdot) \subgrp (\C^*, \cdot)$:
        
        \begin{itemize}
            \item $1 = \zeta^0 \implies 1 \in H^n$
            \item $z,w \in H^n \iff z^n = w^n = 1 \implies 1 = z^nw^n = (zw)^n \implies zw \in H^n$
            \item $z \in H^n \iff z^n = 1 \implies (z^{-1})^n = (z^n)^{-1} = 1^{-1} = 1 \implies z^{-1} \in H^n$
        \end{itemize}
        
        \item Definiamo inoltre la funzione $f : (\Z_n, +) \to (H^n, \cdot) : [k] \mapsto \zeta^k$, la quale risulta essere biettiva poiché $\abs{\Z_n} = \abs{H^n}$.
        
        \item Posto $[k] := [i]+[j]$ dove $[i],[j] \in \Z_n$, si ha che:
        \[[k] = [i]+[j] \iff i+j = k + nh, \exists h \in \Z \iff i+j-nh = k\]
        
        \item Verifichiamo quindi che $f$ sia anche un omomorfismo:
        \[f([i]+[j]) = f([k]) = \zeta^{k} = \zeta^{i+j-nh} = \zeta^{i}\zeta^{j}\zeta^{-nh} = \zeta^{i}\zeta^{j}(\zeta^{n})^{-h} = \zeta^{i}\zeta^{j} = f([i])f([j]\]
        \item Dunque, $f$ risulta essere un isomorfismo tra $\Z_n$ e $H^n$
    \end{itemize}
    
    \quad
    
    \item 
    \begin{itemize}
        \item Sia $G$ un gruppo e sia $g \in G$. La funzione $f : (\Z, +) \to (G, \cdot) : n \to g^n$ è un omomorfismo:
        \[f(n+m) = g^{n+m} = g^ng^m = f(n)f(m)\]
        
        \item Supponiamo per assurdo che $f$ non sia iniettiva e che $o(g) = +\infty$, implicando che:
        \[\exists n \neq m \mid  f(n)=f(m) \implies g^n = g^m \implies \]
        \[ \implies 1_G = g^0 = g^{n-n} = g^{m-n} \implies \exists m-n \neq 0 \mid  g^{m-n} = 1_G \implies m-n \neq 0 \in I(g)\]
        
        \item Tuttavia, come dimostrato nella sezione \ref{order}, abbiamo che $o(g) = +\infty \iff I(g) = I(0)$ e poiché $m-n\neq 0 \implies m-n \notin I(d) = I(0)$, siamo giunti ad una contraddizione. Dunque, l'unica possibilità è che $o(g) < +\infty $
        
        \item Dunque, concludiamo che $f \text{ non iniettiva} \implies o(g) = +\infty$ e di conseguenza che $o(g) < +\infty \implies f \text{ iniettiva}$
    \end{itemize}

    \item Dato il gruppo $(G, \cdot)$ e $g \in G$, la funzione $f_g : G \to G : h \mapsto ghg^{-1}$ è un endomorfismo:
    \[ f_g(h)f_g(h') = (ghg^{-1})(gh'g^{-1}) = ghh'g^{-1} = f_g(hh')\]
    \end{enumerate}
    
    \begin{framedobs}{}
        La proiezione canonica al quoziente $\pi : (\Z, +, \cdot) \to (\Z_n, +, \cdot) : x \to [x]$ è un \textbf{omomorfismo suriettivo} di anelli
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sappiamo già che $\pi$ sia suriettiva. Verifichiamo quindi che sia un omomorfismo di anelli:
        \[\pi(x+y) = [x+y] = [x]+[y] = \pi(x)+\pi(y)\]
        \[\pi(xy) = [xy] = [x][y] = \pi(x)\pi(y)\]
        $\hfill\qed$
    \end{itemize}

    \quad

    \begin{framedobs}{}
        Se $f : G \to H$ è un isomorfismo, esiste sempre l'isomorfismo inverso $f^{-1} : H \to G$
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Se $f : G \to H$ è un isomorfismo, dunque è biettiva, allora $\exists f^{-1} : H \to G$ poiché una funzione è biettiva se e solo se è invertibile. Inoltre, poiché $(f^{-1})^{-1} = f$, anche $f^{-1}$ è invertibile e dunque biettiva
        
        \item Dati $g,h \in H$, mostriamo che $f^{-1}$ sia anche un omomorfismo:
        \[gh = f(f^{-1}(g))f(f^{-1}(h)) \implies gh = f((f^{-1}(g)(f^{-1}(g)) \implies\]
        \[\implies f^{-1}(gh) = f^{-1}(f((f^{-1}(g)(f^{-1}(g))) \implies f^{-1}(gh) = f^{-1}(g)f^{-1}(h)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Se $f : G \to H$ e $g : H \to K$ sono due isomorfismi, la loro composta $g \circ f : G \to K$ è un isomorfismo
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché la composizione di due funzioni biettive è anch'essa biettiva, si ha che
        \[h := g \circ f : G \to K \text{ biettiva}\]
        \item Dati $x, y \in G$, mostriamo che $h$ sia anche un omomorfismo:
        \[h(xy) = g(f(xy)) = g(f(x)f(y)) =g(f(x))g(f(y)) = h(x)h(y)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Relazione di isomorfismo}
        Date due strutture algebriche $G$ ed $H$, definiamo la \textbf{relazione di equivalenza} "G è \textbf{isomorfo} ad H", indicato come $G \cong H$, se e solo se esiste un isomorfismo $f : G \to H$.
        \[G \cong H \iff \exists f : G \to H \text{ isomorfismo}\]
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per ogni gruppo $G$ esiste l'automorfismo $\id : G \to G : g \to g$
        \[\id(gh) = gh = \id(g)\id(h), \forall g,h \in G \implies G \cong G\]
        \item Se $G \cong H$ allora:
        \[G \cong H \implies \exists f:G \to H \text{ isomorfismo} \iff\]
        \[ \iff \exists f^{-1} : H \to G \text{ isomorfismo inverso} \implies H \cong G\]
        \item Se $G \cong H, H \cong K$ allora:
        \[G \cong H, H \cong K \implies \exists f : G \to H \text{ isomorfismo}, \exists g : H \to K \text{ isomorfismo} \implies \]
        \[ \implies g \circ f : G \to K \text{ isomorfismo} \implies G \cong K\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Sia $\func{f}{G}{G'}$ un isomorfismo tra gruppi (dunque $G \cong G'$). Dato $g \in G \mid o(g) < +\infty$, si ha che:
        \[o(g) = o(f(g))\]
    \end{framedlem}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Poiché $f$ è un omomorfismo, per dimostrazione precedente ne segue che $o(f(g)) \mid o(g)$
        \item Inoltre, poiché $f$ è un isomorfismo, sappiamo che esiste sempre l'isomorfismo inverso $\func{f^{-1}}{G'}{G}$.
        \item A questo punto, ne segue che:
        \[g^{o(f(g))} = f^{-1}(f(g^{o(f(g))})) = f^{-1}(f(g)^{o(f(g))}) = f^{-1}(1_H) = 1_G \implies o(g) \mid o(f(g))\]
        \item A questo punto, per anti-simmetria si ha che:
        \[o(f(g)) \mid o(g), o(g) \mid o(f(g)) \implies o(g) = o(f(g))\]

        $\hfill\qed$
    \end{itemize}

    \label{iso_subgrp}
    \begin{framedthm}{Isomorfismo tra sottogruppi ciclici}
        Sia $\func{f}{G}{G'}$ un isomorfismo tra gruppi (dunque $G \cong G'$). Dato $g \in G \mid o(g) < +\infty$, si ha che:
        \[\abk{g} \cong \abk{f(g)}\]
    \end{framedthm}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Sia $\funcmap{h}{\abk{g}}{\abk{f(g)}}{x}{f(x)}$
        \item Notiamo che:
        \[h(xx') = f(xx') = f(x)f(x')=h(x)h(x')\]
        dunque $h$ risulta essere un omomorfismo
        \item Essendo $f$ un isomorfismo (dunque iniettivo), ne segue che:
        \[\forall x,y \in \abk{f(g)} \quad h(x) = h(y) \implies f(x) = f(y) \implies x = y\]
        dunque $h$ risulta essere iniettiva
        \item Per il lemma precedente, si ha che 
        \[G \cong G' \implies o(g) = o(f(g)) \implies \abs{\abk{g}} = \abs{\abk{f(g)}}\]
        
        \item Di conseguenza, poiché $h$ è un omomorfismo iniettivo e poiché $ \abs{\abk{g}} = \abs{\abk{f(g)}}$, ne segue automaticamente che $h$ sia un isomorfismo e dunque che $\abk{g} \cong \abk{f(g)}$

        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Nucleo ed Immagine di un omomorfismo}
    
    \quad
    
    \begin{frameddefn}{Nucleo ed Immagine di un omomorfismo}
        Dato un omomorfismo $f : G \to H$ definiamo $\ker(f)$ come \textbf{nucleo di $f$} e $\im(f)$ come \textbf{immagine di $f$}, dove:
        \[ \ker(f) := \{ g \in G \mid  f(g) = 1_H\}\]
        \[ \im(f) := \{ y \in H \mid  f(x) = y, \exists x \in G\}\]
        Inoltre, si ha che $\ker(f) \subgrp G$ e $\im(f) \subgrp H$
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $\ker(f) \subgrp G$:
        \begin{itemize}
            \item $f(1_G) = 1_H \implies 1_G \in \ker(f)$
            \item $x,y \in \ker(f) \implies f(x) = f(y) = 1_H \implies f(xy) = f(x)f(y) = 1_H \cdot 1_H = 1_H \implies xy \in \ker(f)$
            \item $x \in \ker(f) \implies f(x) = 1_H \implies 1_H = 1_H^{-1} = f(x)^{-1} = f(x^{-1}) \implies x^{-1} \in \ker(f)$
        \end{itemize}
        
        \item $\im(f) \subgrp H$:
        \begin{itemize}
            \item $f(1_G) = 1_H \implies 1_H \in \im(f)$
            \item $x,y \in \im(f) \implies x = f(g), y = f(h) \implies xy = f(g)f(h) = f(gh) \implies xy \in \im(f)$
            \item $x \in \im(f) \implies x = f(g) \implies x^{-1} = f(g)^{-1} = f(g^{-1}) \implies x^{-1} \in \im(f)$
        \end{itemize}
        
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \begin{framedobs}{}
        Un omomorfismo è \textbf{iniettivo} se e solo se il nucleo è \textbf{semplice}, ossia $\ker(f) = \{1_G\}$
    \end{framedobs}

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché $\ker(f) \subgrp G \implies 1_G \in \ker(f)$, supponiamo per assurdo che $f$ iniettiva e che $\exists x \neq 1_G \in \ker(f)$:
        \[ g \neq 1_G \in \ker(f) \implies f(g) = 1_H = f(1_G)\]
        contraddicendo l'ipotesi per cui $f$ sia iniettiva, dunque l'unica possibilità è che
        \[\nexists g \neq 1_G \in \ker(f) \implies \ker(f) = \{1_G\}\]
        
        \item Supponiamo invece che $\ker(f) = \{ 1_G\}$. In tal caso, si ha che:
        \[\forall g,g' \in G, f(g) = f(g') \iff f(g)^{-1}f(g) = f(g)^{-1}f(g') \iff \]
        \[\iff 1_H = f(g^{-1})f(g') \iff 1_H = f(g^{-1}g')\]
        implicando che $g^{-1}g' \in \ker{f}$. Tuttavia, poiché $\ker(f) = \{1_G\}$, necessariamente si ha che $g^{-1}g' = 1_G$ e dunque che $g = g'$, concludendo che $f$ sia iniettiva.
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Se $f : A \to B$ è un omomorfismo di anelli, allora:
        \[ \ker(f) := \{ a \in A \mid  f(a) = 0_B\}\]
        \[ \im(f) := \{b \in B \mid  f(a) = b, \exists a \in A\}\]
    \end{framedobs}
    
    \quad

    \section{Primo teorema d'isomorfismo}
    \label{isomorphism}
    
    \quad
    
    \begin{frameddefn}{Sottoanello}
        Sia $A$ un anello. Definiamo $(B, +, \cdot) \subgrp (A, +, \cdot)$ come \textbf{sottoanello} se:
        \begin{itemize}
            \item $(B,+) \subgrp (A, +)$
            \item $x,y \in B \implies xy \in B$ 
        \end{itemize}
    \end{frameddefn}
    
    \begin{framedobs}{}
        Se $f : A \to B$ è un omomorfismo di anelli, allora
        \begin{itemize}
            \item $\ker(f) \ideal A$
            \item $\im(f) \subgrp B$ sottoanello
        \end{itemize}
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Abbiamo già dimostrato che $\ker(f) \subgrp G$ e $\im(f) \subgrp B$.
        \item $x \in \ker(f), y \in A \implies f(xy) = f(x)f(y) = 0_B \cdot f(y) = 0_B \implies xy \in \ker(f)$
        \item $x,y \in \im(f) \iff x = f(a), y = f(b), \exists a,b \in A \implies xy = f(a)f(b) = f(ab) \implies xy \in \im(f)$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Primo teorema d'isomorfismo}
        Se $f : A \to B$ è un omomorfismo tra anelli, allora
        \[ A / \ker(f) \cong \im(f)\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Mostriamo che esiste $\varphi : A/\ker(f) \to \im(f) : [a] \mapsto f(a)$ e che è \textbf{ben definita}, ossia che $[a],[b] \in A/\ker(f) \mid  [a] = [b] \implies f(a) = f(b)$:
        \[[a]=[b] \iff a \equiv b (\texttt{mod }\ker(f)) \iff b-a \in \ker(f) \iff \]
        \[\iff 0_B = f(b-a) = f(b)-f(a) \iff f(a) = f(b)\]
        
        \item Mostriamo che $\varphi$ è un \textbf{omomorfismo} sia un omomorfismo di anelli:
        \[ \varphi([a])+\varphi([b]) = f(a)+f(b) = f(a+b) = \varphi([a+b])\]
        \[ \varphi([a])\varphi([b]) = f(a)f(b) = f(ab) = \varphi([ab])\]
        
        \item Mostriamo che $\varphi$ è \textbf{iniettiva} poiché il suo nucleo è semplice:
        \[[x] \in \ker(\varphi) \iff \varphi([x]) = 0_B \iff f(x) = 0_B \iff \]
        \[\implies x \in \ker(f) \iff x \in [0_A] \iff [x] = [0_A] \implies \ker(\varphi) = \{[0_A]\}\]
        
        \item Mostriamo che $\varphi$ è \textbf{suriettiva} poiché il suo codominio, ossia $\im(f)$, coincide con la sua immagine, ossia $\im(\varphi)$:
        \[y \in \im(\varphi) \iff \exists [a] \in A/\ker(f) \mid \varphi([a]) = y \iff\]
        \[\iff \exists a \in A \mid f(a) = y \iff y \in \im(f)\]
        
        \item Concludiamo quindi che $\varphi : A/\ker(f) \to \im(f)$ è un \textbf{isomorfismo} e dunque che
        \[A/\ker(f) \cong \im(f)\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Sottogruppi normali}

    \quad
    
    \begin{frameddefn}{Classi laterali sinistre e destre}
    Dati $G$ gruppo e $H \subgrp G$, definiamo le seguenti due relazioni di equivalenza:
    \[ x \sim_{sx} y \iff x^{-1}y \in H \qquad\qquad x \sim_{dx} y \iff xy^{-1} \in H\]
    
    Definiamo come \textbf{classi laterali sinistre} le classi di equivalenza generate da $\sim_{sx}$ e come \textbf{classi laterali destre} le classi di equivalenza generate da $\sim_{dx}$.
    \[ [x]_{sx} := \{ y \in G \mid  x \sim_S y\} \qquad\qquad [x]_{dx} := \{ y \in G \mid  x \sim_D y\}\]

    (\textit{dimostrazione equivalenza analoga alla sezione} \ref{class_sx})
    \end{frameddefn}
    
    \begin{frameddefn}{Insieme quoziente sinistro e destro}
         Dati $G$ gruppo e $H \subgrp G$, denotiamo come $G/H_{sx}$ l'\textbf{insieme quoziente} generato da $\sim_{sx}$ e come $G/H_{dx}$ l'\textbf{insieme quoziente} generato da $\sim_{dx}$.

         Nel caso in cui non sia specificato il pedice, ossia $G/H$, verrà \underline{sottointeso} che si tratti di uno qualsiasi dei due insiemi quozienti, poiché irrilevante
    \end{frameddefn}

    \begin{framedobs}{}
        Dati $G$ gruppo, $H \subgrp G$ e le due relazioni $\sim_{sx}$ e $\sim_{dx}$, si ha che:
        \[ [x]_{sx} = xH = \{ xh \mid  h \in H\} \qquad\qquad [x]_{dx} = Hx = \{ hx \mid  h \in H\}\]

        Inoltre, si ha che:
        \[\abs{xH} = \abs{H} = \abs{Hx}\]
        
        (\textit{dimostrazioni analoghe alla sezione} \ref{class_sx})
    \end{framedobs}
    
    \begin{framedobs}{}
         Dati $G$ gruppo finito e $H \subgrp G$, si ha che:
         \[[G : H] := \abs{G/H_{sx}} = \abs{G/H_{dx}}\]
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché sia $G/H_{sx}$ sia $G/H_{dx}$ sono partizioni di $G$ le cui classi laterali possiedono tutte la stessa cardinalità di $H$, si ha che:
        \[\abs{G/H_{sx}} = \frac{\abs{G}}{\abs{H}} = \abs{G/H_{dx}}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        La \textbf{classe neutra}, ossia generata dall'elemento neutro di $G$, è sia una classe laterale \textbf{sinistra} sia una classe laterale \textbf{destra}:
        \[ [1_G]_{sx} = 1_G \cdot H = H = H \cdot 1_G = [1_G]_{dx}\]
    \end{framedobs}
    
    \begin{frameddefn}{Sottogruppo normale}
    Sia $G$ un gruppo. Definiamo $H$ come \textbf{sottogruppo normale} di $G$, indicato come $H \normsubgrp G$, se:
    \begin{itemize}
        \item $H \subgrp G$
        \item $\forall x \in G$ si ha che $xH = Hx$, ossia la classe laterale sinistra di ogni elemento coincide con la classe laterale destra dell'elemento stesso

        \textit{\textbf{Attenzione:}} tale condizione \underline{non implica} che valga la commutatività tra elementi di $G$ ed elementi di $H$ (ossia che valga $gh=hg, \forall g \in G, \forall h \in H$)
    \end{itemize}
    \end{frameddefn}
    
    \begin{framedprop}{}
        Sia $G$ un gruppo. Le seguenti condizioni sono \textbf{equivalenti}:
        \begin{enumerate}
            \item $H \normsubgrp G$
            \item $\forall g \in G, h \in H$ si ha che $ghg^{-1} \in H$
            \item $\forall g \in G, h \in H, \exists k \in H \mid  gh = kg$
        \end{enumerate}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item $1) \implies 3)$
        \[ g \in G, h \in G \implies gh \in gH = Hg = \{kg \mid  k \in H\} \implies \exists k \in H \mid gh = kg\]
        
        \item $3) \implies 2)$
        \[ g \in G, h \in H, \exists k \in H \mid  gh = kg \implies ghg^{-1} = kgg^{-1} \implies ghg^{-1} = k \in H\]
        
        \item $2) \implies 1)$
        \begin{itemize}
            \item Dato $g \in G$, si ha che:
            \[ gh \in gH \implies g \in G, h \in H \implies x := ghg^{-1} \in H \implies\]
            \[\implies xg = ghg^{-1}g \in Hg \implies xg = gh \implies gh \in Hg \implies gH \subseteq Hg\]
            \item Analogamente, si ha che:
            \[ kg \in Hg \implies g \in G, h \in H \implies \exists g^{-1} \in G \implies y := g^{-1}k(g^{-1})^{-1} = g^{-1}kg \in H\]
            \[\implies gy = gg^{-1}kg\in gH \implies gy = kg \in gH \implies kg \in gH \implies Hg \subseteq gH\]

            \item Dunque, poiché $\forall g \in G$ si ha che $gh = Hg$, ne segue che $H \normsubgrp G$

            $\hfill\qed$
        \end{itemize}
    \end{itemize}
    
    \begin{framedobs}{}
        Se $G$ un \textbf{gruppo abeliano} e $H \subgrp G$, allora $H \normsubgrp G$
    \end{framedobs}
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $G$ è abeliano e poiché $k \in H \implies k \in G$, ne segue che:
        \[\forall g \in G, h \in H, \exists h \in H \mid gh = hg\]
        dunque $H \normsubgrp G$

        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Se $(G, \cdot)$ è un gruppo e $H \normsubgrp G$, allora $(G/H, \cdot)$ è un \textbf{gruppo}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dimostriamo che il prodotto $[x][y] = [xy]$ sia ben definito, ossia che $[x] = [x'], [y]=[y'] \implies [xy]=[x'y']$
        
        \begin{itemize}
            \item Poiché $H \normsubgrp G$ e poiché  $[x] = [x'], [y]=[y']$, si ha che:
            \[ xH = Hx = [x] = [x'] = x'H = Hx'\]
            \[ yH = Hy = [y] = [y'] = y'H = Hy'\]
            
            \item Di conseguenza, otteniamo che:
            \[ x'y'h \in x'y'H = [x'y'] \implies \exists k,j \in H \mid x'y'h = x(ky') = \]
            \[=(xk)y' = (jx)y = jxy \implies x'y'h \in Hxy = [xy]\]
            e che:
            \[ xyb \in xyH = [xy] \implies \exists d,q \in H \mid xyb = x(dy') = (xd)y' =\]
            \[=(qx')y' = qx'y' \implies xyb \in Hx'y' = [x'y']\]
            dunque il prodotto è ben definito
        \end{itemize}
        
        \item Dimostriamo quindi che $(G/H, \cdot)$ sia un gruppo
        
        \begin{itemize}
            \item $([x][y])[z] = [xy][z] = [xyz] = [x][yz] = [x]([y][z])$
            \item $\forall [x] \in G/H, \exists [1_G] \in G/H \mid [x][1_G] = [x\cdot 1_G] = [x]$
            \item $\forall [x] \in G/H, \exists [x]^{-1} \in G/H \mid [x][x]^{-1} = [x][x^{-1}] = [xx^{-1}][1_G]$
        \end{itemize}

        $\hfill\qed$
    \end{itemize}

    \begin{framedcor}{}
        Se $(G, \cdot)$ è un gruppo abeliano e $H \normsubgrp G$, allora $(G/H, \cdot)$ è un \textbf{gruppo abeliano}
    \end{framedcor}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sappiamo già che $(G/H, \cdot)$ è un gruppo, dunque verifichiamo che valga la commutatività:
        \[[x],[y] \in G/H \implies [x][y] = [xy] = [yx] = [y][x]\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Dato $G$ gruppo e $H \subgrp G$, si ha che:
        \[[G : H] = 2 \implies H \normsubgrp G\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo che $[G:H] = 2$, implicando che esistano solo due classi laterali sinistre e due classi laterali destre. Poiché la classe neutra $[1_G]_{sx} = H = [1_G]_{dx}$ è condivisa da entrambi gli insiemi quozienti, ne segue che:
        \[G/H_{sx} = \{[1_G], [x]\}\]
        \[G/H_{dx} = \{[1_G], [y]\}\]
        \item Poiché $G/H$ è una partizione di $G$, ne segue che:
        \[z \in [x] \iff z \notin [1_G]_{sx} = [1_G]_{dx} \iff z \in [y] \]
        implicando quindi che $[x] = [y]$ e di conseguenza che $H \normsubgrp G$
        \item In particolare, si ha che:
        \[G/H_{sx} = \{H, G-H\}\]
        \[G/H_{dx} = \{H, G-H\}\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Dato $\mathcal{S}_n$, si ha che $\mathcal{A}_n \normsubgrp \mathcal{S}_n$
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\mathcal{A}_n \subgrp \mathcal{S}_n$ e poiché $[\mathcal{S}_n : \mathcal{A}_n] = 2$ (sezione \ref{alterno}), ne segue che $\mathcal{A}_n \normsubgrp \mathcal{S}_n$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Se $f : G \to H$ è un omomorfismo di gruppi, allora $\ker(f) \normsubgrp G$ e $\im(f) \subgrp H$
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sappiamo già che $\ker(f) \subgrp G$ e $\im(f) \subgrp H$
        \item Verifichiamo quindi che $\ker(f) \normsubgrp G$
        \[g \in G, h \in \ker(f) \implies f(ghg^{-1}) = f(g)f(h)f(g)^{-1} = f(g) \cdot 1_H \cdot f(g)^{-1} =\]
        \[=f(g)f(g)^{-1} = 1_H \implies ghg^{-1} \in \ker(f) \implies \ker(f) \normsubgrp G\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Primo teorema d'isomorfismo}
        Se $f : G \to H$ è un omomorfismo tra gruppi, allora:
        \[ G / \ker(f) \cong \im(f)\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\ker(f) \normsubgrp G$, sappiamo che $(G/\ker(f), \cdot)$ sia un gruppo con l'operazione di prodotto ben definita. A questo punto, la dimostrazione risulta essere analoga a quella vista nel caso degli anelli (sezione \ref{isomorphism})

        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{}
        Sia gruppo $G$ e sia $g \in G$. Posto $d := o(g)$, si ha che:
        \[\abk{g} \cong \left \{ \begin{array}{ll}
            \Z & \text{ se } d = 0\\
            \Z_d & \text{ se } d > 0\\
        \end{array}\right .\]
    \end{framedprop}

    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Riprendiamo il seguente omomorfismo di gruppi $f : \Z \to G : n \to g^n$, già visto negli esempi precedenti
        \[f(n+m) = g^{n+m} = g^ng^m = f(n)f(m)\]

        \item Di conseguenza, per il primo teorema d'isomorfismo si ha che:
        \[\Z/\ker(f) \cong \im(f)\]
        
        \item Tuttavia, notiamo che:
        \[ \im(f) = \{f(n) \mid n \in \Z\} = \{ g^n \mid  n \in \Z\} = \abk{g}\]
        \[ \ker(f) = \{ n \in \Z \mid  g^n = 1_G\} = I(g)\]

        \item Dunque, si ha che:
        \[\Z/I(g) = \Z/\ker(f)\cong \im(f) = \abk{g}\]
        
        \item Poiché in $\Z$ si ha che: $\exists! d \geq 0 \mid I(g) = I(d)$ tale che:
        \begin{itemize}
            \item $d = 0 \implies o(g) := \abs{\abk{g}} = +\infty$
            \item $d > 0 \implies o(g) := \abs{\abk{g}} = d$
        \end{itemize}

        ne segue che:
        \[ \Z_d := \Z/I(d) = \Z/I(g) = \Z/\ker(f)\cong \im(f) = \abk{g}\]

        \item In particolare, se $d = 0$, per ogni $[x] \in \Z_0$, si ha che:
        \[[y] = [x] \iff x \sim y \iff y-x \in I(0) \iff \]
        \[\iff x-y = 0k, \exists k \in \Z \iff x-y = 0 \iff x=y\]
        \item Di conseguenza, ne segue che proiezione canonica al quoziente $\pi : \Z \to \Z_0 : x \mapsto [x]$, la quale sappiamo già essere un omomorfismo suriettivo, sia anche iniettiva, risultando quindi in un isomorfismo
        \[\forall x, y \in \Z \mid x \neq y \implies [x] \neq [y] \implies \pi(x) \neq \pi(y)\]
        da cui concludiamo che:
        \[\abk{g} \cong \Z_0, \Z_0 \cong \Z \implies \abk{g} \cong \Z\]

        \item In definitiva, concludiamo che:
        \[\abk{g} = \im(f) \cong \Z/\ker(f) = \Z/I(g) = \Z/I(d) = \left \{ \begin{array}{ll}
            \Z_0 \cong \Z & \text{ se } d = 0\\
            \Z_d & \text{ se } d > 0\\
        \end{array}\right .\]
        $\hfill\qed$
    \end{itemize}
    
    
    \label{g_cong_zn}
    \begin{framedcor}{}
        Dato $G$ un gruppo finito dove $\abs{G} = n$, con $n \in \N$, si ha che:
        \[\exists g \in G \mid o(g) = n \implies G \cong \Z_n\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo che $\exists g \in G \mid o(g) = n$. In tal caso, per la proposizione precedente, si ha che:
        \[ o(g) = n \implies \abk{g} \cong  \Z_n\]
        \item Siccome $\abk{g} \subgrp G \implies \abk{g} \subseteq G$ e $\abs{H}(g) = \abs{G}$, allora ne segue che $G = \abk{g}$, implicando quindi che:
        \[G =  \abk{g} \cong  \Z_n\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        Se $G$ è un gruppo finito e $\abs{G} = p$ dove $p \in \mathbb{P}$, allora
        \[ G \cong  \Z_p\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché $\abs{G} = p$ ne segue che $\exists g \in G \mid g \neq 1_G$.
        \item Dato $\abk{g} \subgrp G$, per il teorema di Lagrange, si ha che
        $$
        \abs{\abk{g}} \mid  \abs{G} = p \implies \abs{\abk{g}} = \left \{ \begin{array}{l}
            1\\
            p
        \end{array}\right .
        $$
        \item Poiché $g \neq 1_G \implies \abs{\abk{g}} > 1$, ne segue che l'unica possibilità sia $o(p) := \abs{\abk{g}} = p$.
        \item Di conseguenza, per il corollario precedente, ne segue che:
        \[o(g) = p \implies G = \abk{g} \cong \Z_p\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Sia $G$ un gruppo. Se $H \normsubgrp G$ allora la proiezione canonica al quoziente $\pi : (G, \cdot) \to (G/H,\cdot) : x \to [x]$ è un omomorfismo suriettivo e $\ker(\pi) = H$
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sappiamo già che  $\pi : (G, \cdot) \to (G/H,\cdot) : x \to [x]$ sia un omomorfismo suriettivo
        \item Verifichiamo quindi che $\ker(f) = H$:
        \[ g \in \ker(\pi) \iff \pi(g) = [1_G] \iff [g] = [1_G] = H \iff g \in [g] = H\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item La funzione $\sgn : \mathcal{S}_n \to \{ +1, -1\}$ è un omomorfismo
        \[ \sgn(\sigma\sigma') = \sgn(\sigma)\sgn(\sigma')\]
        \item Dunque, per il primo teorema d'isomorfismo, si ha che:
        \[\mathcal{S}_n / \ker(\sgn) \cong \im(\sgn)\]
        \item Inoltre, si ha che:
        \[\ker(\sgn) = \{\sigma \in \mathcal{S}_n \mid \sgn(\sigma) = +1\} = \mathcal{A}_n \normsubgrp \mathcal{S}_n \implies \]
        \[\implies [\mathcal{S}_n : \ker(\sgn)] = [\mathcal{S}_n : \mathcal{A}_n] = 2 \implies \abs{\im(\sgn)} = 2 \implies \im(\sgn) = \{+1,-1\}\]
    \end{itemize}
    
    \quad
    
    \section{Gruppi diedrali}
    \label{dihedral}
    
    \quad
    
    \begin{frameddefn}{Gruppo diedrale}
        Definiamo come \textbf{gruppo diedrale} $\mathcal{D}_n$ il gruppo delle \textbf{simmetrie di un poligono regolare di $n$ lati}, dove con simmetrie intendiamo tutte le azioni che mantengono la figura simmetrica, ossia:
        \begin{itemize}
            \item Rotazioni di un angolo giro (ossia $\frac{2\pi}{n}$) in senso antiorario (o orario, vista come l'inverso di una rotazione antioraria)
             \[ \rho : \texttt{ rotazione antioraria di } \frac{2\pi}{n}\]
            \item Riflessioni a specchio rispetto agli assi di simmetria del poligono (ogni poligono regolare possiede $n$ assi di simmetria)
            \[ \sigma_i : \texttt{ riflessione rispetto all'asse di simmetria } r_i\]
        \end{itemize}
    \end{frameddefn}
    
    \textit{\textbf{Attenzione:}} il prodotto è dato dalla composizione (dunque viene trattato come nel caso delle permutazioni), ossia $\rho\sigma(i) = \rho(\sigma(i))$
    
    \begin{framedobs}{}
        Dato $\mathcal{D}_n$, effettuare $n$ volte una rotazione riporta il poligono allo stato iniziale (poiché $n \cdot \frac{2\pi}{n} = 2\pi$), dunque
        \[ \rho^n = \rho^0 = 1 \implies \rho^{n+k} = \rho^n \rho^k = \rho^0\rho^k = \rho^k\]
        
        Analogamente, poiché un poligono regolare di $n$ lati possiede solo $n$ assi di simmetria, si ha che:
        \[ \sigma_n = \sigma_0 \implies \sigma_{n+k} = \sigma_k\]
    \end{framedobs}
    
    \begin{framedobs}{}
        Per definizione stessa, ogni riflessione a specchio è uguale alla sua inversa.
        
        Dunque, riflettere due volte rispetto allo stesso asse corrisponde alla simmetria neutra, ossia $\rho^0 = 1$
        \[ \sigma_i = \sigma_i^{-1} \implies \sigma_i^2 = 1\]
    \end{framedobs}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Consideriamo il gruppo $\mathcal{D}_3$, corrispondente alle simmetrie di un triangolo equilatero.
        
        \begin{center}
            \includegraphics[scale=0.6]{images/triangle.png}
        \end{center}
        In tal caso, abbiamo che:
        \[ \mathcal{D}_3 := \{ 1, \rho, \rho^2, \sigma_0, \sigma_1, \sigma_2\}\]
        
        \item Consideriamo il gruppo $\mathcal{D}_4$, corrispondente alle simmetrie di un quadrato.
        
        \begin{center}
            \includegraphics[scale=0.6]{images/square.png}
        \end{center}
        In tal caso, abbiamo che:
        \[ \mathcal{D}_4 := \{ 1, \rho, \rho^2, \rho^3, \sigma_0, \sigma_1, \sigma_2, \sigma_3\}\]
        
        Notiamo inoltre come in $\mathcal{D}_4$ si ha: 
        \[ \rho^3\sigma_1 = \sigma_{0}\]
        
        \begin{center}
            \includegraphics[scale=0.55]{images/dihedral2.png}
        \end{center}
        
        E che:
        \[ \sigma_1\rho^3 = \sigma_{2}\]
        
        \begin{center}
            \includegraphics[scale=0.55]{images/dihedral1.png}
        \end{center}
        
        Dunque, concludiamo che il prodotto non sia commutativo:
        \[ \rho^3\sigma_1 \neq \sigma_1\rho^3\]
        
    \end{itemize}
    
    \begin{framedobs}{}
        Dato il gruppo $\mathcal{D}_n$, si ha che:
        \begin{itemize}
            \item $\rho^i \rho^j = \rho^{i+j (\texttt{mod } n)}$
            \item $\sigma_i \sigma_j = \rho^{i-j (\texttt{mod } n)}$
            \item $\rho^i \sigma_j = \sigma_{i+j (\texttt{mod } n)}$
            \item $\sigma_i \rho^j = \sigma_{i-j (\texttt{mod } n)}$
        \end{itemize}
    \end{framedobs}
    
    \begin{framedprop}{}
        Numerando i vertici del poligono, ogni simmetria \textbf{corrisponde} ad una permutazione dei vertici, dunque si verifica che $\mathcal{D}_n$ è \textbf{isomorfo} ad un sottogruppo \textbf{iniettivamente incluso} in $\mathcal{S}_n$.
        
        In generale, se $\alpha \in \mathcal{D}_n$, ovvero una simmetria del poligono regolare di $n$ lati, manda il vertice $i$ nel vertice $j$, allora la corrispondente permutazione $\sigma_{\alpha} \in \mathcal{S}_n$ manderà anch'essa $i$ in $j$
    \end{framedprop}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Consideriamo il gruppo $\mathcal{D}_3$, numerando i vertici del triangolo corrispondente
        \begin{center}
            \includegraphics[scale=0.6]{images/triangle.png}
        \end{center}
        
        In tal caso abbiamo che:
        \begin{center}
            \begin{tabular}{c c c c c}
            $1 = \left ( \begin{array}{ccc}
                 1 & 2 & 3\\
                 1 & 2 & 3
            \end{array}\right )$
            &\qquad&
            $\rho = \left ( \begin{array}{ccc}
                 1 & 2 & 3\\
                 2 & 3 & 1
            \end{array}\right )$
            &\qquad&
             $\rho^2 = \left ( \begin{array}{ccc}
                 1 & 2 & 3\\
                 3 & 1 & 2
            \end{array}\right )$\\
            $\sigma_0 = \left ( \begin{array}{ccc}
                     1 & 2 & 3\\
                     1 & 3 & 2
                \end{array}\right )$
            &\qquad&
            $\sigma_1 = \left ( \begin{array}{ccc}
                     1 & 2 & 3\\
                     2 & 1 & 3
                \end{array}\right )$
            &\qquad&
            $\sigma_2 = \left ( \begin{array}{ccc}
                     1 & 2 & 3\\
                     3 & 2 & 1
                \end{array}\right )$
            \end{tabular}
        \end{center}
    \end{itemize}
    
    \begin{framedcor}{}
        Dato il gruppo $\mathcal{D}_n$ e dato $H_n \subseteq \mathcal{S}_n$ dove:
        \[ H_n:= \{\sigma_{\alpha} \in \mathcal{S}_n \mid \sigma_{\alpha} = \alpha, \exists \alpha \in \mathcal{D}_n\}\]
        si ha che:
        \[ \mathcal{D}_n \cong H_n \subgrp \mathcal{S}_n\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Posto $H_n:= \{\sigma_{\alpha} \in \mathcal{S}_n \mid \sigma_{\alpha} = \alpha, \exists \alpha \in \mathcal{D}_n\}$, ogni simmetria in $\mathcal{D}_n$ corrisponde ad una permutazione in $H \subgrp \mathcal{S}_n$, dunque si ha che:
        $$
        \left \{
        \begin{array}{c}
            \alpha = \sigma_{\alpha} : i \mapsto j \\
            \beta = \sigma_{\beta} : j \mapsto k
        \end{array}
        \right .
        \implies
        \beta\alpha = \sigma_{\beta}\sigma_{\alpha} : i \mapsto k
        $$
        
        \item Inoltre, si ha che:
        \[\beta\alpha \in \mathcal{D}_n \implies \exists \sigma_{\beta\alpha} \in H_n \mid \sigma_{\beta\alpha} = \beta\alpha = \sigma_{\beta}\sigma_{\alpha}\]
        
        \item Definiamo quindi la funzione $f : (\mathcal{D}_n, \cdot) \to (H_n, \cdot) : \alpha \mapsto \sigma_{\alpha}$, la quale risulta essere un omomorfismo poiché:
        \[ f(\beta\alpha) = \sigma_{\beta\alpha} = \sigma_{\beta}\sigma_{\alpha} = f(\beta)f(\alpha)\]
        
        \item In particolare, $f$ risulta essere suriettiva poiché
        \[\forall \sigma_{\alpha} \in H_n, \exists \alpha \in \mathcal{D}_n \mid f(\alpha) = \sigma_{\alpha}\]
        \item Infine, poiché $\abs{\mathcal{D}_n} = \abs{H_n}$, ne segue che $f$ possa essere suriettiva se e solo se è anche iniettiva, dunque $f$ è un isomorfismo
        
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Gruppo di Klein e Teorema di Cauchy}
    
    \quad
    
    \begin{frameddefn}{Gruppo di Klein}
        Definiamo come \textbf{gruppo di Klein} (o gruppo quadrinomio) il più piccolo gruppo non ciclico:
        \[ \mathcal{K}_4 := \{ 1, a, b, c\}\]
        dove si verifica che:
        \begin{itemize}
            \item $a^2 = b^2 = c^2 = 1 \implies o(a) = o(b) = o(c) = 2$
            \item $ab = ba = c$
            \item $bc = cb = a$
            \item $ac = ca = b$
        \end{itemize}
    \end{frameddefn}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Consideriamo il gruppo $\mathcal{D}_2 := \{1, \rho, \sigma_0, \sigma_1 \}$. Notiamo come:
        \begin{itemize}
            \item $\rho^2 = \sigma_0^2 = \sigma_1^2 = 1$
            \item $\rho\sigma_0 = \sigma_0\rho = \sigma_1$
            \item $\rho\sigma_1 = \sigma_1\rho = \sigma_0$
            \item $\sigma_1\sigma_0 = \sigma_0\sigma_1 = \rho$
        \end{itemize}
        Dunque, concludiamo facilmente che $\mathcal{D}_2 \cong \mathcal{K}_4$
    \end{itemize}
    
    \begin{framedprop}{}
        Se $G$ è un gruppo finito dove $\abs{G} = 4$, si verifica che:
        \[ G \cong \Z_4 \text{ oppure } G \cong \mathcal{K}_4\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sia $a \neq 1 \in G$. Per Lagrange, sappiamo che $o(a) \mid \abs{G} = 4 \iff o(a) \in \{1, 2, 4\}$
        \item Come visto nella sezione \ref{g_cong_zn}, sappiamo che è $G$ è cicliclo se:
        \[\exists a \in G \mid o(a) = 4 \implies G \cong \Z_4\]
        \item Ipotizziamo ora che non sia ciclico, dunque che $\nexists a \in G \mid o(a) = 4$, implicando quindi che $G = \{ 1, a, b, c\}$, dove $o(a)=o(b)=o(c)=2$. Verifichiamo che in tal caso $ab = c$:
        \begin{itemize}
            \item Supponiamo per assurdo che $ab = 1$
            \[ ab = 1 \implies b = a^{-1} = a\]
            il che è impossibile
            
            \item Supponiamo per assurdo che $ab = a$
            \[ ab = a \implies a^{-1}ab = a^{-1}a \implies b = 1\]
            il che è impossibile
            
            \item Supponiamo per assurdo che $ab = b$
            \[ ab = b \implies abb^{-1} = bb^{-1} \implies a = 1\]
            il che è impossibile
            
            \item Siccome $ab \neq 1$, $ab \neq a$ e $ab \neq c$, allora l'unica possibilità affinché valga la chiusura del gruppo è $ab = c$
        \end{itemize}
        
        \item Analogamente, dimostriamo che $ac = b$ e $bc = a$, concludendo quindi che:
        \[ G \cong \mathcal{K}_4\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Teorema di Cauchy}
        Sia $G$ un gruppo finito. Dato un numero primo $p \in \mathbb{P}$, si verifica che:
        \[ p \mid \abs{G} \implies \exists g \in G \mid o(g) = p\]
        
        In particolare, se $\abs{G} = q \in \mathbb{P}$, allora $G$ è ciclico poiché
        \[\exists g \in G \mid o(g) = q \implies \abs{G} = o(g) \implies G = \abk{g}\]
    \end{framedthm}
    
    \begin{framedprop}{}
        Se $G$ è un gruppo finito dove $\abs{G} = 6$, si verifica che:
        \[ G \cong \Z_6 \text{ oppure } G \cong \mathcal{S}_3\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Come già visto, se $\exists g \in G \mid o(g) = 6$, allora $G \cong \Z_6$
        \item Ipotizziamo quindi che $G$ non sia ciclico, dunque che $\nexists g \in G \mid o(g) = 6$. Per il teorema di Cauchy sappiamo che
        \begin{itemize}
            \item $\exists \alpha \in G \mid o(\alpha)=3 \implies o(\alpha^k) \mid o(\alpha) = 3, k \neq 0 \implies o(\alpha^3) = 3$
            \item $\exists \beta \in G \mid o(\beta)=2 \implies \beta^{-1} = \beta$
        \end{itemize}
        \item Notiamo che:
        \[ \alpha^i\beta = \alpha^j\beta \iff \alpha^i = \alpha^j \iff 1 = \alpha^j\alpha^{-i}= \alpha^{j-i} \iff 0 = j-i \iff j=i\]
        e che 
        \[ \alpha^i = \alpha^j\beta \iff \alpha^{i-j}=\beta \]
        
        \item Tuttavia, l'ultima affermazione risulta essere assurda poiché:
        $$
        o(\beta) = 2 \qquad o(p^{i-j}) = \left \{ \begin{array}{ll}
            o(1) = 1 & \text{ se } i = j\\
            o(p^k) = 3 & \text{ se } i \neq j
        \end{array}\right .
        $$
        
        di conseguenza si ha che $\beta \neq \alpha^{i-j}$.
        
        \item Mostriamo inoltre che $\beta\alpha = \alpha^2\beta$:
        \begin{itemize}
            \item Supponiamo per assurdo che $\beta\alpha = 1$
            \[ \beta\alpha = 1 \implies \alpha = \beta^{-1} \implies \alpha = \beta \]
            il che è impossibile
            
            \item Supponiamo per assurdo che $\beta\alpha = \alpha$
            \[ \beta\alpha = \alpha \implies \beta = 1 \]
            il che è impossibile
            
            \item Supponiamo per assurdo che $\beta\alpha = \alpha^2$
            \[ \beta\alpha = \alpha^2 \implies \beta = \alpha \]
            il che è impossibile
            
            \item Supponiamo per assurdo che $\beta\alpha = \beta$
            \[ \beta\alpha = \beta \implies \alpha = 1 \]
            il che è impossibile
            
            \item Supponiamo per assurdo che $\beta\alpha = \alpha\beta$, implicando che $o(\beta\alpha) = o(\alpha\beta)$:
            \begin{itemize}
                \item $(\alpha\beta)^1 = \alpha\beta$
                \item $(\alpha\beta)^2 = (\beta\alpha)(\beta\alpha) = \beta\beta\alpha\alpha = \beta^2\alpha^2 = \alpha^2$
                \item $(\alpha\beta)^3 = (\alpha\beta)(\alpha\beta)^2 = (\alpha\beta)\alpha^2 = \alpha^3\beta = \beta$
                \item $(\alpha\beta)^4 = (\alpha\beta)(\alpha\beta)^3 = (\alpha\beta)\beta = \alpha\beta^2 = \alpha$
                \item $(\alpha\beta)^5 = (\alpha\beta)(\alpha\beta)^4 = (\alpha\beta)\alpha = \alpha^2\beta$
                \item $(\alpha\beta)^6 = (\alpha\beta)(\alpha\beta)^5 = (\alpha\beta)\alpha^2\beta = \beta^2\alpha^3 = 1$
            \end{itemize}
            dunque $o(\alpha\beta) = 6 \implies o(\alpha\beta) = \abs{G} \implies G = \abk{\beta\alpha}$, ossia che il gruppo sia ciclico, contro l'ipotesi che invece esso non lo sia.
            
            \item Quindi l'unica possibilità è che $\beta\alpha = \alpha^2\beta$
        \end{itemize}
        
        \item Concludiamo quindi che:
        \[ G = \{1, \alpha, \alpha^2, \beta, \alpha\beta, \alpha^2\beta\}\]
        
        \item A questo punto, possiamo stendere una \textbf{tavola di Cayley}, ossia una tavola moltiplicativa:
        
        \begin{center}
            \begin{tabular}{c | c c c c c c}
                & $1$ & $\alpha$ & $\alpha^2$ & $\beta$ & $\alpha\beta$ & $\alpha^2\beta$ \\
                \hline
                $1$ & $1$ & $\alpha$ & $\alpha^2$ & $\beta$ & $\alpha\beta$ & $\alpha^2\beta$ \\
                
                $\alpha$ & $\alpha$ & $\alpha^2$ & $1$ & $\alpha\beta$ & $\alpha^2\beta$ & $\beta$\\
                
                $\alpha^2$ & $\alpha^2$ & $1$ & $\alpha$ & $\alpha^2\beta$ & $\beta$ & $\alpha\beta$ \\
                
                $\beta$ & $\beta$ & $\alpha^2\beta$ & $\alpha\beta$ & $1$ & $\alpha^2$ & $\alpha$\\
                
                $\alpha\beta$ & $\alpha\beta$ & $\beta$ & $\alpha^2\beta$ & $\alpha$ & $1$ & $\alpha^2$\\
                
                $\alpha^2\beta$ & $\alpha^2\beta$ & $\alpha\beta$ & $\beta$ & $\alpha^2$ & $\alpha$ & $1$\\
             \end{tabular}
        \end{center}
        
        \item Ricordando le proprietà dei prodotti dei gruppi diedrali (sezione \ref{dihedral}), si ottiene una mappatura univoca $\alpha^i \mapsto \rho^i$ e analogamente $\alpha^i\beta \mapsto \sigma_i$. Ciò implica quindi che:
        \[ G \cong \mathcal{D}_3\]
        
        \item Inoltre, abbiamo visto che $\mathcal{D}_3 \cong H_3 \subgrp \mathcal{S}_3$ dove
        \[ H_3:= \{\sigma_{\alpha} \in \mathcal{S}_3 \mid \sigma_{\alpha} = \alpha, \alpha \in \mathcal{D}_3\}\]
        
        e dove $\abs{\mathcal{D}_3} = 2 \cdot 3 = 6$ e $\abs{\mathcal{S}_n} = 3! = 6$.
        
        \item Affinché l'isomorfismo esista si ha necessariamente che $\abs{H} = \abs{\mathcal{D}_3} = 6$, implicando quindi che
        \[H_3\subgrp \mathbb{S}_3, \abs{H_3} = \abs{\mathcal{S}_3} \implies G \cong \mathcal{D}_3 \cong H = \mathcal{S}_3\]
        $\hfill\qed$
    \end{itemize}

    \newpage

    \textbf{Esempio:}

    \begin{enumerate}
        \item \begin{itemize}
            \item Vogliamo stabilire se $\Z_9^*$ e $\Z_{14}^*$ siano isomorfi tra loro.
            \item Prima di tutto, calcoliamo $\Z_9^*$ e $\Z_{14}^*$:
            \[\Z_9^* = \{[1], [2], [4], [5], [7], [8]\}\]
            \[\Z_{14}^* = \{[1],[3],[5],[9],[11],[13]\}\]
            \item Notiamo quindi che $\abs{\Z_9^*} = \abs{\Z_{14}^*} = 6$, implicando che essi siano potenzialmente isomorfi tra loro.
            
            \item A questo punto, calcoliamo gli ordini degli elementi:
            \begin{center}
                \begin{tabular}{cc|cc}
                    \multicolumn{2}{c}{\textbf{$\Z_{9}^*$}} & \multicolumn{2}{c}{\textbf{$\Z_{14}^*$}} \\
                    Elemento & Ordine & Elemento & Ordine\\
                    \hline
                    [1] & 1 & [1] & 1\\\relax
                    [2] & 6 & [3] & 6\\\relax
                    [4] & 3 & [5] & 6\\\relax
                    [5] & 6 & [9] & 3\\\relax
                    [7] & 3 & [11] & 3\\\relax
                    [8] & 2 & [13] & 2
                \end{tabular}
            \end{center}

            \quad

            \item Poiché entrambi i gruppi possiedono un elemento di ordine 6, per la proposizione precedente si ha che
            \[\Z_9^* \cong \Z_6 \land \Z_{14}^* \cong \Z_6 \implies \Z_9^* \cong \Z_6 \cong \Z_{14}^* \]
            dunque i due gruppi sono isomorfi tra loro
        \end{itemize}

        \begin{itemize}
            \item Vogliamo stabilire se $\Z_{24}^*$ e $\Z_{30}^*$ siano isomorfi tra loro.
            \item Prima di tutto, calcoliamo $\Z_{24}^*$ e $\Z_{30}^*$:
            \[\Z_{24}^* = \{[1], [5], [7], [11], [13], [17], [19], [23]\}\]
            \[\Z_{30}^* = \{[1],[7],[11],[13],[17],[19], [23], [29]\}\]
            \item Notiamo quindi che $\abs{\Z_{24}^*} = \abs{\Z_{30}^*} = 8$, implicando che essi siano potenzialmente isomorfi tra loro.
            
            \item A questo punto, calcoliamo gli ordini degli elementi:
            \begin{center}
                \begin{tabular}{cc|cc}
                    \multicolumn{2}{c}{\textbf{$\Z_{24}^*$}} & \multicolumn{2}{c}{\textbf{$\Z_{30}^*$}} \\
                    Elemento & Ordine & Elemento & Ordine\\
                    \hline
                    [1] & 1 & [1] & 1\\\relax
                    [5] & 2 & [7] & 4\\\relax
                    [7] & 2 & [11] & 2\\\relax
                    [11] & 2 & [13] & 4\\\relax
                    [13] & 2 & [17] & 4\\\relax
                    [17] & 2 & [19] & 2\\\relax
                    [19] & 2 & [23] & 4\\\relax
                    [24] & 2 & [29] & 2\\
                \end{tabular}
            \end{center}

            \quad

            \item Poiché gli ordini degli elementi sono diversi tra loro, per il teorema di isomorfismo tra sottogruppi ciclici (sezione \ref{iso_subgrp}) risulta impossibile che $\Z_{24}^*$ e $\Z_{30}^*$ siano isomorfi tra loro
        \end{itemize}
    \end{enumerate}
        
    \chapter{Polinomi}
    
    \begin{frameddefn}{Anello polinomiale}
        Dato un anello commutativo $A$, definiamo l'\textbf{insieme dei polinomi} aventi come coefficienti elementi in $A$ come:
        \[ A[x] := \{ a_0+a_1x+\ldots+a_nx^n \mid a_0, \ldots, a_n \in A, a_n \neq 0\}\]
        
        Inoltre, $A[x]$ risulta essere un \textbf{anello commutativo}
    \end{frameddefn}
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dati due polinomi $p(x), q(x) \in A[x]$, dunque definiti come
        \[ p(x) = \sum_{i=0}^n a_ix^i \qquad q(x) = \sum_{i=0}^m b_ix^i\]
        abbiamo che:
        
        \item Nell'anello la somma è definita come:
        \[ p(x),q(x) \in A[x] \implies p(x) + q (x) = \sum_{i=0}^{\max(n,m)} (a_i+b_i)x^i\]
            
        \item Nell'anello il prodotto è definita come:
        \[ p(x),q(x) \in A[x] \implies p(x) \cdot q (x) = \sum_{i=0}^n \left ( \sum_{j=0}^m a_ib_jx^{i+j} \right )\]
        
        \item Gli assiomi di associatività e commutatività possono essere facilmente verificati tramite la definizione stessa della somma
        
        \item L'elemento neutro nella somma è il polinomio neutro $e(x)=0$, mentre nel prodotto è il polinomio costante $d(x)=1$
        
        \item L'elemento inverso nella somma è:
        \[ \forall p(x) \in A[x], \exists -p(x) \in A[x] \mid p(x)+(-p(x)) = 0\]
        
        \item Per via della definizione data di polinomio, non esiste un inverso moltiplicativo.
        
        Si pensi ad esempio a $p(x)=x+3$. Tale polinomio non ammette inverso moltiplicativo poiché $\frac{1}{x+3} \notin A[x]$.
        
        Di conseguenza, $A[x]$ non può essere un campo.
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Se $K$ è un campo, allora $K[x]$ è un anello commutativo, poiché non ammette comunque l'esistenza dell'inverso nel prodotto
    \end{framedobs}
    
    \begin{frameddefn}{Grado di un polinomio}
        Dato $p(x) \in A[x]$ indichiamo il \textbf{grado del polinomio} come $\deg(p(x))$, dove:
        \begin{itemize}
            \item $p(x)=0 \iff \deg(p(x)) = -\infty$ (polinomio nullo)
            \item $p(x)=a_0+a_1x+\ldots+a_nx^n \neq 0, a_n \neq 0 \implies \deg(p(x)) = n$
        \end{itemize}
    \end{frameddefn}
    
    \begin{frameddefn}{Coefficienti direttori}
        Dato $p(x)=a_0+a_1x+\ldots+a_nx^n \neq 0 \in A[x]$, definiamo $a_n$ come \textbf{coefficiente direttore} del polinomio
    \end{frameddefn}
    
    \begin{framedprop}{}
        Ogni elemento $a \in A$ può essere visto come un \textbf{polinomio costante}, ossia di grado 0:
        \[\forall a \in A, \exists a(x) \in A[x] \mid a(x) = a \iff \deg(a(x)) = 0\]
        
        Dunque, si ha che $A \subgrp A[x]$ sottoanello
    \end{framedprop}
    
    \begin{framedobs}{}
        Siano $p(x),q(x) \in A[x]$. Si verifica che:
        \[ \deg(p(x)q(x))=\deg(p(x))+\deg(q(x))\]
    \end{framedobs}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché il prodotto è definito come
        \[p(x) \cdot q (x) = \sum_{i=0}^n \left ( \sum_{j=0}^m a_ib_jx^{i+j} \right ) = a_0b_0+a_0b_1x^1+\ldots+a_nb_mx^{n+m}\]
        allora $\deg(p(x)q(x))=\deg(p(x))+\deg(q(x)) = n+m$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Dato l'anello commutativo $K[x]$, si ha che:
        \[ K[x]^* = K^* = K- \{0\}\]
        dunque gli unici elementi invertibili di $K[x]$ sono i \textbf{polinomi costanti}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo per assurdo che $\exists a(x) \neq 0\in K[x] \mid \deg(a(x)) \geq 1$ e che $\exists a(x)^{-1} \neq 0\in K[x] \mid \deg(a(x)^{-1}) \geq 0$, da cui otteniamo che:
        \[ \deg(1) = \deg(a(x)a(x)^{-1}) = \deg(a(x))+\deg(a(x)^{-1})\geq 1\]
        giungendo quindi ad una contraddizione, poiché $1 \in K \implies \deg(1) = 0$. Dunque, l'unica possibilità è:
        \[\deg(a(x)) \geq 1 \implies a(x) \neq 0 \notin K[x]^*\]
        da cui ricaviamo che
        \[a(x) \neq 0\in K[x]^* \implies \deg(a(x)) = 0\]
        
        \item Supponiamo invece che $\deg(a(x))=0$, implicando che 
        \[\exists a_0 \neq 0 \in K \mid a(x) = a_0 \implies \exists a_0^{-1} \in K \mid a_0a_0^{-1} = 1 \implies \]
        
        da cui concludiamo che:
        \[\deg(a(x))=0 \implies a(x) \neq 0\in K[x]^*\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Divisione con resto di polinomi}
    
    \quad
    
    \begin{framedthm}{Divisione con resto di polinomi}
        Dati $a(x),b(x) \in K[x]$ con $b(x) \neq 0$ allora
        \[ \exists! q(x), r(x) \in K[x] \mid a(x) = b(x)q(x)+r(x)\]
        dove $\deg(r(x)) < \deg(b(x))$
    \end{framedthm}
    
    \textit{Dimostrazione unicità (esistenza omessa):}
    \begin{itemize}
        \item Supponiamo che 
        \[ b(x)q_1(x)+r_1(x) = a(x) = b(x)q_2(x)+r_2(x)\]
        dove $\deg(r_1(x)), \deg(r_2(x)) < \deg(b(x))$, da cui otteniamo che:
        \[ \deg(r_1(x)), \deg(r_2(x)) < \deg(b(x)) \implies \deg(r_1(x) - r_2(x)) < \deg(b(x)\]
        
        \item Dunque si ha che:
        \[ b(x)q_1(x)+r_1(x) = b(x)q_2(x)+r_2(x) \implies b(x)[q_1(x)-q_2(x)] = r_2(x)-r_1(x) \implies\]
        \[ \implies \deg(r_2(x)-r_1(x)) = \deg(b(x))+\deg(q_1(x)-q_2(x)) \]
        \item Nel caso in cui $\deg(q_1(x)-q_2(x)) \geq 0$, avremmo $\deg(r_2(x)-r_1(x)) \geq \deg(b(x))$, contraddicendo l'ipotesi. Di conseguenza, l'unica possibilità è
        \[\deg(q_1(x)-q_2(x)) = - \infty \iff q_1(x)-q_2(x)=0 \iff q_1(x)=q_2(x) \]
        \item A questo punto, quindi, si ha che:
        \[ b(x)[q_1(x)-q_2(x)] = r_2(x)-r_1(x) \implies b(x) \cdot 0 = r_1(x)-r_1(x) \iff \]
        \[ \iff 0 = r_2(x)-r_1(x) \iff r_1(x)=r_2(x)\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Calcolo della divisione con resto di $a(x) = 2x^4+3x^3-2x^2+x-4$ e $b(x) = x^2-x+1$
        \begin{center}
            \begin{tabular}{c|c}
                \begin{tabular}{c c c c c}
                    $+2x^4$ & $+3x^3$ & $-2x^2$ & $+x$ & $-4$\\ 
                    $-2x^4$ & $+2x^3$ & $-2x^2$ &      &     \\
                    \hline
                            & $+5x^3$ & $-4x^2$ & $+x$  & $-4$\\
                            & $-5x^3$ & $+5x^2$ & $-5x$ &    \\
                    \hline
                            &         & $x^2$   & $-4x$ & $-4$ \\
                            &         & $-x^2$  & $+x$  & $-1$ \\
                    \hline
                            &         &         & $+3x$ & $-5$
                        
                \end{tabular}
                &
                \begin{tabular}{c c c }
                    $+x^2$ & $-x$ & $+1$ \\
                    \hline
                    $2x^2$ & $+5x$ & $+1$
                    \\\\\\\\\\\\
                \end{tabular}
                \\
            \end{tabular}
        \end{center}
        
        Quindi concludiamo che:
        \[ 2x^4+3x^3-2x^2+x-4 = (x^2-x+1)(2x^2+5x+1)+3x-5\]
    \end{itemize}
    
    \begin{framedmeth}{Regola di Ruffini}
        Dati $a(x), b(x) \in K[x]$ dove $b(x)=x-c, \exists c \in K$, è facile calcolare il quoziente $q(x) \in K[x]$ e il resto $r(x) = r_0 \in K$ della divisione di $a(x)$ per $b(x)$:
        
        \begin{enumerate}
            \item Sia $a(x) = a_0+\ldots+a_nx^n$ con $a_n \neq 0$
            \item Poiché $\deg(a(x)) = \deg(b(x))+\deg(q(x)) = 1+\deg(q(x)) \implies \deg(q(x)) = \deg(a(x))-1$, allora
            \[ q(x) = q_0+\ldots+q_{n-1}x^{n-1}\]
            dove $q_0, \ldots, q_{n-1}$ sono dati da:
            
            \begin{itemize}
                \item $q_{n-1} = a_n$
                \item $q_{n-1-k} = cq_{n-k}+a_{n-k}$
                \item $r_0 = cq_0 + a_0$
            \end{itemize}
        
            \item In formato grafico, riassumiamo tale concetto con:
            \begin{center}
                \begin{tabular}{c | c c c c | c}
                    & $a_n$ & $a_{n-1}$ & $\ldots$ & $a_1$ & $a_0$\\
                $c$ &       & $cq_{n-1}$ & $\ldots$ & $cq_1$ & $cq_0$\\
                \hline
                    & $q_{n-1}$ & $q_{n-2}$ & $\ldots$ & $q_0$ & $r_0$\\
                \end{tabular}
            \end{center}
        \end{enumerate}
    \end{framedmeth}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Calcolare la divisione tra $a(x)=x^4-3x^3+2x-5$ e $b(x)=x+2$
        
        \begin{center}
            \begin{tabular}{c | c c c c | c}
                 & $1$ & $-3$ & $0$ & $2$ & $-5$\\
            $-2$ &       & $-2$ & $10$ & $-20$ & $36$\\
            \hline
                & $1$ & $-5$ & $10$ & $-18$ & $31$\\
            \end{tabular}
        \end{center}
        
        Dunque si ha che:
        \[ x^4-3x^3+2x-5 = (x+2)(x^3-5x^2+10x-18)+31\]
    \end{itemize}
    
    \begin{framedthm}{Teorema del fattore}
        Dato $p(x) \in K[x]$ e dato $c \in K$
        \[ p(c) = 0 \iff x-c \mid p(x)\]
        in tal caso, $c$ viene detta \textbf{radice (o zero) del polinomio}
    \end{framedthm}

    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item $x-c \mid p(x) \implies p(c)=0$
        \[ x-c \mid p(x) \implies p(x)=(x-c)q(c) \implies p(c)=(c-c)q(c) = 0\]
        
        \item $p(c) = 0 \implies x-c \mid p(x)$
        \begin{itemize}
            \item Siano $q(x)$ e $r(x)$ il quoziente e il resto della divisione di $p(x)$ per $(x-c)$
            \[ p(x)=(x-c)q(x)+r(x)\]

            \item Poiché per definizione stessa di divisione con resto tra polinomi si ha che $\deg(r(x)) < \deg(x-c)$, ne segue che:
            \[\deg(r(x)) < \deg(x-c) \implies \deg(r(x)) < 1 \implies r:=r(x) \in K\]
            \item Infine, per ipotesi si ha che
            \[ 0 = p(c) = (c-c)q(c)+r \implies 0 = (c-c)q(c)+r \implies\]
            \[\implies 0 = 0+r \implies r=0\]
            dunque, la divisione non ha resto, implicando che:
            \[ (x-c) \mid p(x)\]
            $\hfill\qed$
        \end{itemize}
    \end{itemize}
    
    \begin{framedprop}{}
        Dato $p(x) \in K[x] \mid \deg(a(x)) = n$, allora $a(x)$ ha al massimo $n$ radici
        
        Inoltre, se $p(x) \in \C[x]$, allora, per il teorema fondamentale dell'algebra, esistono esattamente $n$ radici
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia $\deg(p(x))=n$ e siano per assurdo $c_1, \ldots, c_m$ dove $m > n$ e $c_i \neq c_j \iff i \neq j$ tali che \[p(c_i) = 0, \forall 1 \leq i \leq m\]
        \item Poiché un polinomio può essere scritto come il prodotto di tutte le sue radici, si verifica che:
        $$
        \left \{
        \begin{array}{c}
            x-c_1 \mid p(x)\\
            \vdots\\
            x-c_m \mid p(x)
        \end{array}
        \right .
        \implies \underbrace{(x-c_1) \cdot \ldots \cdot (x-c_m)}_{q(x)} \mid p(x)
        $$
        
        \item Poiché $\deg(q(x)) = m$, tale divisione risulta essere impossibile, poiché un polinomio non può dividere un polinomio di grado minore
        
        $\hfill\qed$
        
    \end{itemize}
    
    \newpage
    
    \section{Proprietà dell'anello polinomiale}
    
    \quad
    
    \begin{framedprop}{}
        L'anello commutativo $K[x]$ è un \textbf{dominio di integrità} poiché vale la \textbf{legge di annullamento del prodotto}
    \end{framedprop}
    
    \begin{framedcor}{}
        Dato il dominio di integrità $K[x]$ e dati $p(x), q(x) \in K[x]$, si ha che:
        \[ I(p(x)) = I(q(x)) \iff p(x) = c \cdot q(x), \exists c \in K[x]^*\]
        (\textit{dimostrazione nella sezione} \ref{inv})
    \end{framedcor}
    
    \begin{framedthm}{}
        L'anello commutativo $K[x]$ è un \textbf{dominio ad ideali principali}, dunque
        \[ \exists! p(x) \in I \mid I = I(p(x))\]
    \end{framedthm}
    
    \textit{Dimostrazione esistenza:}
    \begin{itemize}
        \item $I \subseteq I(p(x))$
        \begin{itemize}
            \item Sia $p(x) \neq 0 \in I$ del più piccolo grado possibile.
            
            \item Dato $a(x) \in I \mid a(x)=p(x)q(x)+r(x)$ si ha che $\deg(r(x)) < \deg(p(x))$, da cui ricaviamo che:
            \[a(x)=p(x)q(x)+r(x) \in I \implies r(x)=a(x)-p(x)q(x) \in I\]
            \item Tuttavia, poiché $p(x)$ è il polinomio non nullo all'interno dell'ideale del più piccolo grado possibile e poiché $\deg(r(x)) < \deg(p(x))$, ne segue necessariamente che $r(x) = 0$. 
            \item Dunque, si ha che:
            \[a(x)=p(x)q(x)+r(x) \in I \implies a(x)=p(x)q(x) \implies a(x) \in I(p(x))\]
        \end{itemize}
        
        \item $I(p(x)) \subseteq I$
        \begin{itemize}
            \item Dato $p(x)) \neq 0 \in I$ del più piccolo grado possibile, si ha che:
            \[ a(x) \in I(p(x)) \implies a(x) = p(x)b(x), \exists b(x) \in K[x] \implies a(x) \in I\]
            poiché $p(x) \in I$
        \end{itemize}
    \end{itemize}
    
    \newpage
    
    \textit{Dimostrazione unicità:}
    \begin{itemize}
        \item Se $I = \{ 0\}$, allora $I = I(0)$
        \item Sia invece $p(x) \neq 0\in I$ del più piccolo grado possibile tale che $I=I(p(x))$, implicando che
        \[\forall q(x) \in I \mid \deg(q(x)) < \deg(p(x)) \implies q(x)=0\]
        dunque non può esistere un polinomio in $I$ con grado minore di $p(x)$
    \end{itemize}
    
    $\hfill\qed$

    \begin{frameddefn}{MCD di polinomi}
        Dati $a_1(x), \ldots, a_n(x) \in K[x]$, definiamo $d(x) \in K[x]$ come \textbf{massimo comun divisore di $a_1(x), \ldots, a_n(x)$}, indicato come $d(x) := \MCD(a_1(x), \ldots, a_n(x))$, se dato $k(x) \in K[x]$ si verifica che
        \[k(x) \mid a_1(x) \land \ldots \land k(x) \mid a_n(x) \iff k(x) \mid d(x)\]
    \end{frameddefn}
    
    \begin{framedprop}{Identità di Bézout tra polinomi}
    Dati $a_1(x), \ldots, a_n(x) \in K[x]$ e dato $d(x) := \MCD(a_1(x), \ldots, a_n(x))$, si ha che:
    \[I(a_1(x), \ldots, a_n(x)) = I(d(x))\]
    
    In altre parole, si ha che:
    \[ \exists x_1(x), \ldots, x_n(x) \in \Z \mid  a_1(x)x_1(x) + \ldots + a_n(x)x_n(x) = d(x)\]
    che definiamo come \textbf{identità di Bézout} tra polinomi.
    \end{framedprop}

    \begin{frameddefn}{mcm di polinomi}
        Dati $a_1(x), \ldots, a_n(x) \in K[x]$, definiamo $m(x) \in K[x]$ come \textbf{minimo comune multiplo di $a_1(x), \ldots, a_n(x)$}, indicato come $m(x) := \mcm(a_1(x), \ldots, a_n(x))$, se dato $k(x) \in K[x]$ si verifica che
        \[a_1(x) \mid k(x) \land \ldots \land a_n(x) \mid k(x) \iff m(x) \mid k(x)\]
    \end{frameddefn}
    
    \begin{framedobs}{}
        Poiché $K[x]$ è un dominio dominio di integrità, sappiamo che
        \[ I(p(x)) = I(q(x)) \iff p(x) = c \cdot q(x), \exists c \in K[x]^*\]
        
        Dunque, dati $a_1(x), \ldots, a_n(x) \in K[x]$, si ha che $d(x) := \MCD(a_1(x), \ldots, a_n(x))$ e $m(x) := \mcm(a_1(x), \ldots, a_n(x))$ possano essere ben definiti solo a meno di una costante moltiplicativa non nulla.
        
        Affinché valga l'unicità, quindi, basta imporre che i polinomi $d(x)$ e $m(x)$ abbiano \textbf{coefficiente direttore} $a_n = 1$
    \end{framedobs}
    
    \begin{frameddefn}{Polinomio monico}
        Dato $a(x) = a_0+\ldots+a_nx^n \in K[x]$, definiamo $a(x)$ come \textbf{polinomio monico} se e solo se $a_n = 1$
    \end{frameddefn}
    
    \begin{framedprop}{Prodotto tra mcm e MCD di polinomi}
        Dati $a(x), b(x) \in K[x]$, si ha che:
        \[ \mcm(a(x), b(x)) \cdot \MCD(a(x), b(x)) = a(x)b(x)\]

        \textit{(dimostrazione omessa)}
    \end{framedprop}
    
    \begin{framedthm}{Radici comuni tra due polinomi}
        Dati $a_1(x), \ldots, a_n(x) \in K[x]$ data $c \in K$, si ha che:
        \[ a_1(c) =\ldots = a_n(c)= 0 \iff d(c)=0\]
        dove $d(x) := \MCD(a_1(x), \ldots, a_n(x)) \in K[x]$.
        
        In altre parole, le uniche radici in comune tra due polinomi sono le radici del loro MCD
    \end{framedthm}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sia $c \in K$ tale che:
        \[a_i(c) = 0, \forall i \in [1,n] \iff (x-c) \mid a_i(c), \forall i \in [1,n]\]
        \item Poiché $d(x) : \MCD(a_1(x), \ldots, a_n(x))$, per definizione stessa si ha che:
        \[(x-c) \mid a_i(c), \forall i \in [1,n] \implies (x-c) \mid d(x) \iff d(c)=0\]
        \item Viceversa, sempre per definizione stessa di $d(x)$ si ha che:
        \[d(c)=0 \implies (x-c) \mid d(x), d(x) \mid a_i(x), \forall i \in [1,n] \implies (x-c) \mid a_i(x), \forall i \in [1,n]\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Dato $p(x) \in K[x]$, si ha che:
        \[p(x) \text{ elemento irriducibile} \iff p(x) \text{ elemento primo}\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sappiamo già che in un dominio di integrità si ha: 
        \[p(x) \text{ primo} \implies p(x) \text{ irriducibile}\]
        \textit{(dimostrazione nella sezione \ref{primi})}
        
        \item Dati $a(x),b(x) \in K[x]$, supponiamo che $p(x) \mid a(x)b(x)$:
        \[p(x) \mid a(x)b(x) \iff a(x)b(x) = p(x)k(x), \exists k(x) \in K[x]\]
    
        \item  Se $p(x) \nmid a(x)$, si ha che $d(x) := \MCD(p(x), a(x))=1$. Dunque, tramite l'identità di Bézout otteniamo che:
        \[ \exists f(x),g(x) \in K[x] \mid d(x)=p(x)f(x)+a(x)g(x) \implies\]
        \[\implies 1 = p(x)f(x)+a(x)g(x) \implies  b(x)=p(x)b(x)f(x)+a(x)b(x)g(x) \implies\]
        \[ \implies b(x)=p(x)b(x)f(x)+[a(x)b(x)]g(x) \implies  b(x)=p(x)b(x)f(x)+p(x)k(x)g(x) \implies \]
        \[ \implies b(x) = p(x)[q(x)f(x)+g(x)b(x)] \implies p(x) \mid b(x)\]
        
        \item Analogamente, se $p(x) \nmid b(x)$ si ha che $d(x) := \MCD(p(x), b(x))=1$. Dunque, seguendo gli stessi passaggi otteniamo che $p(x) \mid a(x)$
        \item Concludiamo quindi che se $p(x)$ è irriducibile, allora:
        \[p(x) \mid a(x)b(x) \implies p(x) \mid a(x) \lor p(x) \mid b(x)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Dato $p(x) \in K[x]$, si ha che:
        \[\deg(p(x))=1 \implies p(x) \text{ irriducibile} \]
    \end{framedlem}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Se $\deg(p(x))=1$ allora
        $$
        p(x)=a(x)b(x) \implies \left \{ \begin{array}{l}
            \deg(a(x)) = 0, \deg(b(x)) = 1 \implies a(x) \in \C[x]^*\\
            \deg(a(x)) = 1, \deg(b(x)) = 0 \implies b(x) \in \C[x]^*\\
        \end{array}\right .
        $$
        dunque $p(x)$ è irriducibile
    \end{itemize}
    
    $\hfill\qed$
    
    \begin{framedprop}{}
        Dato $p(x) \in \C[x]$, si ha che:
        \[p(x) \in \C[x] \text{ irriducibile} \iff \deg(p(x))=1\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sappiamo già che $\deg(p(x)) = 1 \implies p(x)$ irriducibile
        \item Consideriamo quindi il caso in cui $\deg(p(x)) = 0$, allora 
        \[\deg(p(x)) = 0 \iff p(x) \in \C^*\]
        dunque $p(x)$ non può essere irriducibile per definizione stessa
        \item Sia invece $\deg(p(x)) > 1$. Per il teorema fondamentale dell'algebra si ha che:
        \[ \exists z \in \C \mid p(z)=0 \iff x-z \mid p(x) \iff \]
        \[\iff p(x) = (x-z)q(x), \exists q(x) \in \C[x] \implies\]
        \[\implies \deg(q(x)) = \deg(p(x))-1 > 0 \implies q(x) \notin \C^* = \C[x]^*\]
        dunque $p(x)$ non può essere irriducibile poiché $\deg((x-z)) = 1 \implies (x-z) \notin \C[x]^*$ e $q(x) \notin \C[x]^*$.
        
        \item Dunque si ha che
        \[\deg(p(x)) \neq 1 \implies p(x) \text{ non irriducibile}\]
        che per contronominale implica che
        \[p(x) \text{ irriducibile} \implies \deg(p(x)) = 1\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Dato $p(x) \in \R[x]$, si ha che:
        \[p(x) \in \R[x] \text{ irriducibile} \iff \deg(p(x))=1 \text{ oppure }  \deg(p(x)) = 2, \Delta < 0 \]
        dove $\deg(p(x)) = 2 \implies p(x) = ax^2+bx+c, \Delta := b^2-4ac$
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché $\R[x] \subset \C[x]$, la dimostrazione in entrambi i lati dei casi con $\deg(p(x)) = 1$ è analoga alla precedente
        \item Supponiamo quindi per assurdo che $\deg(p(x)) = 2, \Delta < 0$ e che $p(x)$ non sia irriducibile, implicando che:
        \[\exists a(x)b(x) \in \R[x]- \R[x]^* \mid p(x) = a(x)b(x) \land \deg(p(x)) = 2\] \[\implies \deg(a(x)) = \deg(b(x)) = 1 \implies \]
        \[ \implies \left \{ \begin{array}{c}
            \exists c,d \in \R \mid a(x) = cx+d \implies a(-c^{-1}d) = 0 \iff (x+c^{-1}d) \mid a(x)\\
            \exists f,g \in \R \mid b(x) = fx+g \implies b(-f^{-1}g) = 0 \iff (x+f^{-1}g) \mid b(x)\\
        \end{array}\right . \implies \]
        \[\implies \left \{ \begin{array}{l}
            (x+c^{-1}d) \mid a(x), a(x) \mid p(x) \implies (x+c^{-1}d) \mid p(x) \iff  p(-c^{-1}d) = 0\\
            (x+f^{-1}g) \mid b(x), b(x) \mid p(x) \implies (x+f^{-1}g) \mid p(x) \iff  p(-f^{-1}g) = 0
        \end{array} \right .\]
        dunque $x_1 := -c^{-1}d \in \R$ e $x_2 := -f^{-1}g \in \R$ sarebbero le radici di $p(x)$, contraddicendo l'ipotesi per cui $\Delta < 0 \implies x_1, x_2 \in \C-\R$.
        
        Dunque, l'unica possibilità è:
        \[\deg(p(x)) = 2, \Delta < 0 \implies p(x) \in \R[x] \text{ irriducibile}\]
        
        \item Sia quindi $p(x) := a_0+\ldots+a_nx^n \in \R[x] \subset \C[x]$ irriducibile con $\deg(p(x)) > 1$. Per il teorema fondamentale dell'algebra, $\exists z := c+id\in \C \mid p(z)=0 \iff (x-z) \mid p(x)$
        
        \item  Poiché $a_1, \ldots, a_n \in \R$, si ha che:
        \[\forall j \in [1,n], \exists d_j \in \R\mid a_j := d_j+i \cdot 0 = d_j - i \cdot 0 =: \overline{a_j} \implies a_j = \overline{a_j}, \forall j \in [1,n]\]
        
        dunque, per le proprietà dei complessi coniugati (sezione \ref{complex_conj}), ne segue che:
        \[ p(\overline{z}) = a_0+\ldots a_n\overline{z}^n = a_0+\ldots a_n\overline{z^n} = \overline{a_0}+\ldots \overline{a_n}\overline{z^n} =  \overline{a_0}+\ldots \overline{a_nz^n} = \]
        \[ \overline{a_0+\ldots+a_nz^n} = \overline{p(z)} = \overline{0} = 0 \implies p(\overline{z}) = 0 \iff (x-\overline{z}) \mid p(x) \]
        dove per definizione si ha $\overline{z} = c-id$

        \item Nel caso in cui $d = 0$, ne seguirebbe che $z = \overline{z}$, implicando che:
        \[(x-z) \mid p \iff p(x) = q(x)(x-z), \exists q(x) \in \R[x] \implies \]
        \[ \implies \deg(q(x)) + \deg(x-z) = \deg(p(x)) > 1 \implies \]
        \[\implies \deg(q(x)) + 1 > 1 \implies \deg(q(x)) > 0 \implies q(x) \notin K^*\]
        
        rendendo quindi tale caso è impossibile, poiché altrimenti si avrebbe che $p(x)$ non sia irriducibile in quanto $q(x),(x-z) \notin K^*$
        \item Sia quindi $d \neq 0$, implicando che $z \neq \overline{z}$:
        \[(x-z) \mid p(x), (x-\overline{z}) \mid p(x) \implies (x-z)(x-\overline{z}) \mid p(x) \implies\]
        \[\implies x^2-\overline{z}x-zx + z \cdot \overline{z} \mid p(x) \implies x^2-(\overline{z}+z)x+ z \cdot \overline{z} \mid p(x) \implies \]
        \[\implies x^2-(c-id+c+id)x+(c+id)(c-id) \mid p(x) \implies x^2-2cx+c^2+d^2 \mid p(x)\]

        \item Poiché $p(x)$ è irriducibile, l'unica possibilità è:
        \[x^2-2cx+c^2+d^2 \mid p(x) \implies p(x) = k(x^2-2cx+c^2+d^2), \exists k \in \R[x]^* = \R^* \implies\]
        \[\implies p(x) = kx^2-2kcx+kc^2+kd^2 \implies \Delta = (-2kc)^2-4k^2(c^2+d^2) \implies \]
        \[ \implies \Delta = 4k^2c^2-4k^2c^2-4k^2d^2 \implies \Delta = -4k^2d^2 < 0\]
        $\hfill\qed$
    \end{itemize}
    
    
    \begin{framedthm}{Fattorizzazione in polinomi irriducibili e monici}
        Dato $p(x) \neq 0 \in K[x]$, si ha che:
        \[ \exists! q_1(x), \ldots, q_k(x) \neq 0\in K[x], \exists c \in K^* \mid p(x)=c \cdot q_1(x) \cdot \ldots \cdot q_k(x)\]
        dove $q_1(x), \ldots, q_k(x)$ sono \textbf{polinomi monici} e \textbf{irriducibili}
    \end{framedthm}
    
    \textit{Dimostrazione esistenza:}
    
    \begin{itemize}
        \item Supponiamo che $\deg(p(x)) = 1$, implicando che $p(x) = ax+b, \exists a,b \neq 0 \in K$

        \item Poiché $a,b \neq 0 \implies a,b \in K^* \implies \exists a^{-1},b^{-1} \in K^*$, ne segue che
        \[p(x) = ax+b \implies p(x) = a \left ( x+ \frac{b}{a} \right ) \implies p(x) = a(x+ba^{-1})\]
        dove $a$ e $\deg(x-ba^{-1})=1 \implies (x-ba^{-1})$ irriducibile, dunque esiste una fattorizzazione di $p(x)$ in polinomi monici ed irriducibili

        \item Sia quindi $p(x) \in K[x] \mid \deg(p(x)) > 1$ dove 
        \[\exists a(x), b(x) \in K[x] \mid p(x) = d(x)f(x)\]
        e supponiamo per ipotesi induttiva che $a(x)$ e $b(x)$ siano fattorizzabili in polinomi monici ed irriducibili:
        \[\exists! q_1(x), \ldots, q_k(x) \in K[x], \exists c \in K^* \mid d(x)=c \cdot q_1(x) \cdot \ldots \cdot q_k(x) \]
        \[\exists! q_1'(x), \ldots, q_k'(x) \in K[x], \exists c' \in K^* \mid f(x)=c' \cdot q_1'(x) \cdot \ldots \cdot q_k'(x) \]

        da cui ne segue che:
        \[p(x) = d(x)f(x) = c \cdot q_1(x) \cdot \ldots \cdot q_k(x) \cdot c' \cdot q_1'(x) \cdot \ldots \cdot q_k'(x) =\]
        \[= (c \cdot c') \cdot q_1(x) \cdot \ldots \cdot q_k(x) \cdot q_1'(x) \cdot \ldots \cdot q_k'(x)\]
        dunque $p(x)$ è fattorizzabile in polinomi monici ed irriducibili
    \end{itemize}
    
    \textit{Dimostrazione unicità:}
    
    \begin{itemize}
        \item Se $\deg(p(x))=0$ allora $\exists! c \in K^* \mid p(x)=c$
        \item Sia quindi $\deg(p(x))>0$. Notiamo inoltre che dato $p(x):=a_0+a_1x+\ldots+a_nx^n$, affinché la fattorizzazione possa essere in polinomi monici ed irriducibili ne segue necessariamente che $c = c' = a_n$.
        
        \item Supponiamo quindi che esistano due fattorizzazioni possibili in polinomi monici ed irriducibili per $p(x)$:
        \[ c \cdot q_1(x)\cdot \ldots \cdots q_k(x) = p(x) = c' \cdot q_1'(x) \cdot \ldots \cdot q_j'(x) \implies \]
        \[c \cdot q_1(x)\cdot \ldots \cdots q_k(x) = c' \cdot q_1'(x) \cdot \ldots \cdot q_j'(x) \implies q_1(x) \mid q_1'(x) \cdot \ldots \cdot q_j'(x)\]
        \item Tuttavia, poiché $q_1(x)$ irriducibile se e solo se è primo, ne segue che:
        \[q_1'(x) \cdot \ldots \cdot q_j'(x) \mid q_1'(x) \lor \ldots \lor q_1(x) \mid q_j'(x)\]
        \item Per comodità, assumiamo che $q_1'(x)$ il polinomio per cui $q_1(x) \mid q_1'(x)$:
        \[q_1(x) \mid q_1'(x) \iff q_1'(x) = d \cdot q_1(x), \exists d \in K^* \implies \]
        \[c \cdot q_1(x)\cdot \ldots \cdots q_k(x) = c' \cdot d \cdot q_1(x) \cdot \ldots \cdot q_j'(x) \implies \]
        \[ \implies c \cdot q_2(x)\cdot \ldots \cdots q_k(x) = \frac{p(x)}{q_1(x)} = c' \cdot d \cdot q_2'(x) \cdot \ldots \cdot q_j'(x)\]
        
        \item Poiché $\deg\left ( \frac{p(x)}{q(x)} \right ) < \deg(p(x))$ possiamo concludere che $k=k$ e, a meno di riordinare i fattori, possiamo assumere che $q_2(x)=q_2'(x), \ldots, q_k(x)=q_j'(x)$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{}
        Sia $p(x) : =a_0+\ldots a_nx^n \in \Z[x]$ dove $a_0, a_n \neq 0$. Se $\frac{a}{b} \in \Q$ è radice di $p(x)$ e $\MCD(a,b)=1$, allora
        \[ p\left ( \frac{a}{b}\right ) = 0 \implies a \mid a_0, b \mid a_n\]
        e di conseguenza che:
        \[a \nmid a_0 \lor b \nmid a_n \implies p\left ( \frac{a}{b}\right ) \neq 0 \]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Supponendo che $\frac{a}{b} \in \Q \mid p\left ( \frac{a}{b}\right ) = 0, \MCD(a,b)=1$, ne segue che:
        \[ 0 = p\left(\frac{a}{b}\right) = a_0+ a_1 \cdot \left ( \frac{a}{b} \right ) + \ldots + a_n \cdot \left ( \frac{a}{b} \right )^n \implies \]
        \[ \implies b^n \cdot 0 = b^n \left ( a_0+ a_1 \cdot \left ( \frac{a}{b} \right ) + \ldots + a_n \cdot \left ( \frac{a}{b} \right )^n \right ) \implies \]
        \[ \implies 0 = a_0b^n+ \ldots + a_{n-1} \cdot a^{n-1} \cdot b +a_na^n \implies \]
        \[ \implies a_na^n = -a_0b^n - \ldots - a_{n-1} \cdot a^{n-1} \cdot b \implies \]
        \[ \implies a_na^n \cdot \frac{1}{b} = -a_0b^{n-1} - \ldots - a_{n-1} \cdot a^{n-1} \implies b \mid a_na^n\]
        \item Poiché $\MCD(a, b) = 1 \implies \MCD(a^n, b) = 1$, allora
        \[ b \mid a_na^n \implies b \mid a_n\]
        \item  Analogamente, seguendo gli stessi passaggi arriviamo a dimostrare che $\MCD(a,b) = 1 \implies \MCD(a,b^n) = 1$ implica che
        \[ a \mid a_0b^n \implies a \mid a_0 \]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Dato $p(x)=x^3-19x-30$, se $\frac{a}{b} \in \Q$ fosse soluzione, allora
        \[ p\left( \frac{a}{b} \right) \implies a \mid 30, b \mid 1\]
        quindi le uniche soluzioni possibili di $p(x)$ possono essere:
        \[ x = \pm 1, \pm 2, \pm 3, \pm 5, \pm 6, \pm 10, \pm 15, \pm 30\]
        
        \item Dato $p(x)=6x^3-11x^2+6x-1$, se $\frac{a}{b} \in \Q$ fosse soluzione, allora
        \[ p\left( \frac{a}{b} \right) \implies a \mid  -1, b \mid 6\]
        quindi le uniche soluzioni possibili di $p(x)$ possono essere:
        \[ x = \pm 1, \pm\frac{1}{2}, \pm\frac{1}{3}, \pm\frac{1}{6}\]
    \end{itemize}
    
    \quad

    \section{Polinomi in $\Z_p$}
    
    \quad
    
    \begin{framedlem}{}
        Dato $p \in \mathbb{P}$, si ha che:
        \[ \prod_{0<a<p} (x-a) \equiv x^{p-1}-1 (\texttt{mod }p)\]
    \end{framedlem}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia $q(x) := x^{p-1}-[1] \in \Z_p$. Per il piccolo teorema di Fermat, dato $0<a<p$ si ha che:
        \[ a^{p-1}\equiv 1 (\texttt{mod }p) \implies a^{p-1}-1\equiv 0 (\texttt{mod }p) \implies \]
        \[\implies q([a]) \equiv 0 (\texttt{mod }p) \iff x-[a] \mid q(x)\]
        
        \item Dunque, si ha che:
        \[x-[a] \mid q(x), \forall 0 < a < p \implies \prod_{0<a<p} (x-[a]) \mid q(x) \implies\]
        \[ \implies q(x) = k \cdot \prod_{0<a<p} (x-[a]), \exists k \in \Z \implies \]
        ottenendo quindi una fattorizzazione in polinomi monici ed irriducibili
        \item Poiché il coefficiente direttore di $q(x)$ è $1$, affinché la fattorizzazione sia valida ne segue necessariamente che $k = 1$, concludendo che:
         \[ \prod_{0<a<p} (x-a) \equiv x^{p-1}-1 (\texttt{mod }p)\]
         $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Dato $d \in \N$ tale che $d \mid p-1$, l'equazione $x^d \equiv 1 (\texttt{mod }p)$ ammette $d$ soluzioni distinte in $\Z_p$
    \end{framedlem}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia $d \in \N$ tale che $d \mid p-1$. Ne segue che:
        \[d \mid p-1 \implies p-1 = dk, \exists k \in \Z\]
        \item Per dimostrazione precedente, si ha che:
        \[\prod_{0<a<p} (x-a) \equiv x^{p-1}-1 (\texttt{mod }p) \implies \prod_{0<a<p} (x-a) \equiv x^{dk}-1 (\texttt{mod }p) \implies \]
        \[\implies \prod_{0<a<p} (x-a) \equiv (x^{d})^k-1^k (\texttt{mod }p) \implies \prod_{0<a<p} (x-a) \equiv (x^d-1)^k (\texttt{mod }p) \implies\]
        \[\implies \prod_{0<a<p} (x-a) \equiv (x^d-1)(x^d-1)^{k-1} (\texttt{mod }p)\]
        \item Posto $q(x) := (x^{d}-1)^{n-1}$, si ha che:
        \[\prod_{0<a<p} (x-a) \equiv (x^{d}-1)q(x) (\texttt{mod }p) \]
        \[\implies x-[a] \mid x^d-[1] \lor x-[a] \mid q(x), \forall 0<a<p\]
        
        \item Dunque, sia $0 < b <p$ tale che $x-[b] \mid x^d-[1]$. Ne segue che:
        \[\prod_{0<a<p, a \neq b} (x-a) \equiv q(x) (\texttt{mod }p) \]
        
        \item Per motivi di grado, ripetendo tale procedimento su $q(x)$ e i suoi fattori, otterremo esattamente $d$ radici di $x^d-[1]$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{}
        Dato $d \in \N$ tale che $d \mid p-1$, allora
        \[\exists [a] \in \Z_p \mid o([a]) = d\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se $d=1$, allora $\exists 1 \in \Z_p^* \mid o([1]) = 1$
        \item Se invece $d=q^k$ dove $q \in \mathbb{P}$, allora per il lemma precedente si ha che:
        \[d=q^k \implies q^k \mid p-1 \implies \left \{ \begin{array}{l}
            x^{q^k}\equiv 1 (\texttt{mod }p) \text{ ha } q^k \text{ soluzioni}\\
            x^{q^k-1}\equiv 1 (\texttt{mod }p) \text{ ha } q^k-1 \text{ soluzioni}
        \end{array}\right .\]
        dunque $\exists[a] \in \Z_p$ che è soluzione di $x^{q^k}= [1]$ ma non di $x^{q^k-1}=[1]$, implicando che:
        \[ o([a]) \mid q^k, o([a]) \nmid q^{k-1} \implies o([a]) = q^k\]
        
        \item Supponiamo per ipotesi induttiva di aver verificato che per tutti gli $n$ divisori di $p-1$ più piccoli di $d$ che $\exists [b] \in \Z_p \mid o([b]) = n$
        
        \item Sia quindi $d = nq^k$ dove $q \in \mathbb{P} \mid \MCD(n, q^k)=1$. Per induzione, si ha che:
        \[ \exists [b],[c] \in \Z_p \mid o([b])=n, o([c])=q^k\]
        
        \item Infine, come visto nella sezione \ref{mcd_order}, poiché $\MCD(o([b]), o([c]))=1$ si ha che:
        \[\exists a \in \Z_p \mid [a]=[bc] \implies o([a])=nq^k = d\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Il gruppo $(\Z^*_p, \cdot)$, dove $p \in \mathbb{P}$, è \textbf{sempre ciclico}
    \end{framedprop}
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per il lemma precedente, si ha che:
        \[ p-1 \mid p-1 \implies \exists [a] \in \Z_p^* \mid o([a])=p-1= \abs{\Z_p^*} \implies \Z_p^* = \abk{[a]}\]
        $\hfill\qed$
    \end{itemize}
    
    \chapter{Spazi vettoriali}
    
    \begin{frameddefn}{Spazio vettoriale}
        Dato un campo $K$, definiamo come \textbf{spazio vettoriale su $K$} una struttura algebrica $(V, +, \cdot)$, dove 
        \[+ : V \times V \to V : (u,v) \mapsto w\]
        \[\cdot : K \times V \to V : (\lambda, v) \mapsto w\]
        soddisfacente i seguenti assiomi:
        \begin{itemize}
            \item $(V, +)$ è un gruppo abeliano
            \item $\forall s, t \in K, v \in V \implies s(t\cdot v) = stv = (s \cdot t)v$ (\textbf{Associatività scalare})
            \item $1 \in K, v \in V \implies 1 \cdot v = v$ (\textbf{Elemento neutro})
            \item $\forall s, t \in K, v \in V \implies (s+t)v = sv+tv$ (\textbf{Distributività vettoriale})
            \item $\forall s \in K, v,w \in V \implies s(v+w) = sv+sw$ (\textbf{Distributività scalare})
        \end{itemize}
        
        Inoltre, definiamo $\lambda \in K$ come \textbf{scalare} e $v \in V$ come \textbf{vettore}.
    \end{frameddefn}
    
    \begin{frameddefn}{Insieme di coordinate}
        Dato un campo $K$, definiamo come \textbf{insieme di coordinate} un insieme i cui elementi sono tuple di $n$ elementi appartenenti a $K$:
        \[K^n = K \times \ldots \times K = \{(t_1, \ldots, t_n) \mid t_1, \ldots, t_n \in K\}\]
    \end{frameddefn}
    
    \begin{framedprop}{Spazio di coordinate}
        La struttura $(K^n, +, \cdot)$ è uno \textbf{spazio vettoriale}, dove l'\textbf{addizione tra vettori} e il \textbf{prodotto per scalare} sono definite come:
        $$
        \begin{array}{l}
            v := (t_1, \ldots, t_n) \in K^n\\
            w := (s_1, \ldots, s_n) \in K^n
        \end{array}
        \implies
        v+w := (t_1+s_1, \ldots, t_n+s_n) \in K^n
        $$
        \[ \lambda \in K, v :=(t_1, \ldots, t_n) \in K^n \implies  \lambda v := (\lambda t_1, \ldots, \lambda t_n) \in K^n\]
        
        \textit{(dimostrazione omessa)}
    \end{framedprop}
    
    \textbf{Interpretazione geometrica:}
    
    \begin{itemize}
        \item Dato lo spazio di coordinate $\R^2$ e un vettore $v := (x,y) \in \R^2$, possiamo rappresentare tale vettore come:
        \begin{center}
            \includegraphics[scale=0.55]{images/vector_1.png}
        \end{center}
        
        \item Preso $w := (s,t) \in \R^2$, la somma vettoriale $v+w$ corrisponde al classico \textit{metodo del parallelogramma} utilizzato in fisica elementare:
        \begin{center}
            \includegraphics[scale=0.525]{images/vector_2.png}
        \end{center}
    
        \item Preso $\lambda \in \R$, il prodotto per scalare $\lambda \cdot v$ ha la stessa direzione del vettore $v$, ma con lunghezza aumentata o diminuita e verso uguale o invertito
        \begin{center}
            \includegraphics[scale=0.475]{images/vector_3.png}
        \end{center}
    \end{itemize}
    
    \begin{framedobs}{}
        Dato uno spazio vettoriale $V$ su un campo $K$, si ha che:
        
        \begin{itemize}
            \item $\forall \lambda \in K \implies \lambda \cdot 0_V = 0_V$
            \item $\forall v \in V \implies 0 \cdot v = 0_V$
        \end{itemize}
        
        Dato $0_V = (0, \ldots, 0)\in V$ è detto \textbf{vettore nullo}, ossia l'elemento neutro di $V$, e dove $0$ è l'elemento neutro di $K$
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dato $\lambda \in K$, si ha che:
        \[\lambda \cdot 0_V = (\lambda \cdot 0, \ldots, \lambda \cdot 0) = (0, \ldots, 0) = 0_V\]
        \item Dato $v = (t_1, \ldots, t_n) \in V$, si ha che:
        \[0 \cdot v = (0 \cdot t_1, \ldots, 0\cdot t_n) = (0, \ldots, 0) = 0_V\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Sottospazio vettoriale}
        Dato un spazio vettoriale $V$ su $K$, definiamo $W \subseteq V$ come \textbf{sottospazio vettoriale} di $V$ su $K$ se:
        \begin{itemize}
            \item $(W,+) \subgrp (V, +)$
            \item $w \in W,  \lambda \in K \implies \lambda w \in W$
        \end{itemize}
    \end{frameddefn}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item $\Z^n \subseteq \R^n$ non è sottospazio vettoriale di $\R^n$ poiché non vale la seconda condizione
        \item $\R^n_{\geq 0} \subseteq \R^n$ non è sottospazio vettoriale di $\R^n$ poiché non vale nessuna delle due condizioni
    \end{itemize}
    
    \quad
    
    \section{Span, Generatori e Indipendenza lineare}
    
    \quad
    
    \begin{frameddefn}{Span}
        Dato uno spazio vettoriale $V$ su $K$ e dei vettori $v_1, \ldots, v_n \in V$, definiamo \textbf{span} (o \textbf{sottospazio generato} da  $v_1, \ldots, v_n$) l'insieme di tutte le \textbf{combinazioni lineari} di tali vettori:
        \[ \Span(v_1, \ldots, v_n) = \{ \lambda_1v_1+\ldots+\lambda_nv_n \mid \lambda_1, \ldots, \lambda_n \in K\}\]
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $0_V = 0 \cdot v_1 + \ldots + 0 \cdot v_n \in \Span(v_1, \ldots, v_n)$
        \item $v,w \in \Span(v_1, \ldots, v_n) \implies v+w = \lambda_1v_1+\ldots+\lambda_nv_n+\mu_1v_1+\ldots+\mu_nv_n = (\lambda_1+\mu_1)v_1 + \ldots + (\lambda_n+\mu_n)v_n \implies v+w \in \Span(v_1, \ldots, v_n)$
        \item $v \in \Span(v_1, \ldots, v_n) \implies -v = (-\lambda_1)v_1+\ldots+(-\lambda_n)v_n \in \Span(v_1, \ldots, v_n)$
        \item $v \in \Span(v_1, \ldots, v_n), c \in K \implies cv = (c\lambda_1)v_1+\ldots+(c\lambda_n)v_n \in \Span(v_1, \ldots, v_n)$

        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Insieme di generatori}
        Dato uno spazio vettoriale $V$ su $K$, definiamo i vettori $v_1, \ldots, v_n \neq 0_V \in V$ come un \textbf{insieme di generatori di $V$} se e solo se ogni vettore di $v$ può essere espresso come una combinazione lineare di $v_1, \ldots, v_n$:
        \[\forall v \in V, \exists \lambda_1, \ldots, \lambda_n \mid v = v_1+\ldots+\lambda_nv_n\]
        o analogamente se e solo se:
        \[V = \Span(v_1, \ldots, v_n)\]
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $v_1, \ldots, v_n \in V$, per definizione stessa si ha sempre che $\Span(v_1, \ldots, v_n) \subseteq V$
        \item Dunque, affinché le due definizioni siano equivalenti è sufficiente che:
        \[\forall v \in V, \exists \lambda_1, \ldots, \lambda_n \mid v = v_1+\ldots+\lambda_nv_n \iff V \subseteq \Span(v_1, \ldots, v_n) \]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Indipendenza lineare}
        Dato uno spazio vettoriale $V$ su $K$, definiamo i vettori $v_1, \ldots, v_n \neq 0_V \in V$ come \textbf{linearmente indipendenti} se e solo se:
        \[ \lambda_1v_1+\ldots+\lambda_nv_n = 0_V \iff \lambda_1 = \ldots = \lambda_n = 0\]
        In caso contrario, vengono detti \textbf{linearmente dipendenti}
    \end{frameddefn}
    
    
    \begin{framedobs}{}
        Sia $V$ uno spazio vettoriale. Dati i vettori $v_1, \ldots, v_n \neq 0_V \in V$, si ha che:
        \[v_1, \ldots, v_n \text{ lin. ind.} \iff v_1, \ldots, v_{n-1} \text{ lin. ind.} \land v_{n} \notin \Span(v_1, \ldots, v_{n-1})\]
    \end{framedobs}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Nel caso in cui $v_1, \ldots, v_n$ siano linearmente indipendenti, si ha che:
        \[\lambda_1v_1+\ldots+\lambda_{n-1} v_{n-1} +\lambda_n v_n = 0_V \iff \lambda_1 = \ldots = \lambda_{n-1} = \lambda_n = 0\]
        da cui ne segue automaticamente che:
        \[\lambda_1v_1+\ldots+\lambda_{n-1} v_{n-1} = 0_V \iff \lambda_1 = \ldots = \lambda_{n-1} = 0\]
        dunque anche $v_1, \ldots, v_{n-1}$ sono linearmente indipendenti
        
        \item Supponiamo quindi che $v_n \in \Span(v_1, \ldots, v_{n-1})$, implicando che:
        \[\mu_1 v_1 + \ldots + \mu_{n-1}v_{n-1} = v_n \iff \mu_1 v_1 + \ldots + \mu_{n-1}v_{n-1}-v_n = 0_V \iff \]
        \[\iff \mu_1 v_1 + \ldots + \mu_{n-1}v_{n-1} +(-1)v_n = 0_V\]
        dunque $v_1, \ldots, v_n$ sono linearmente dipendenti. Per contronominale, quindi, si ha che:
        \[v_1, \ldots, v_n \text{ lin. indipendenti} \implies v_{n} \notin \Span(v_1, \ldots, v_{n-1})\]
        
        \item Viceversa, supponiamo per assurdo che $v_{n} \notin \Span(v_1, \ldots, v_n)$ e che $\exists \lambda_n \neq 0 \mid \lambda_1v_1+\ldots+\lambda_nv_n = 0_V$. Ne segue che
        \[\lambda_1v_1+\ldots+\lambda_nv_n = 0_V \implies \lambda_nv_n = -\lambda_1v_1-\ldots-\lambda_{n-1} v_{n-1} \implies\]
        \[ \implies v_n = (-\lambda_n^{-1}\lambda_1)v_1+\ldots+(-\lambda_n^{-1}\lambda_{n-1})v_{n-1} \implies v_n \in \Span(v_1, \ldots, v_{n-1})\]
        contraddicendo quindi l'ipotesi $v_{n} \notin \Span(v_1, \ldots, v_n)$, implicando quindi che l'unica possibilità sia $\lambda_n = 0$.
        
        \item Di conseguenza, nel caso in cui $ v_1, \ldots, v_{n-1}$ siano linearmente indipendenti e $v_{n} \notin \Span(v_1, \ldots, v_n)$, otteniamo che:
        \[ \lambda_1v_1+\ldots+\lambda_{n-1} v_{n-1} + \lambda_nv_n = 0_V \iff \lambda_1v_1+\ldots+\lambda_{n-1} v_{n-1} + 0 \cdot v_n = 0_V \iff \]
        \[ \iff \lambda_1v_1+\ldots+\lambda_{n-1} v_{n-1} = 0_V \iff \lambda_1 = \ldots = \lambda_{n-1} = 0 = \lambda_n\]
        dunque $v_1, \ldots, v_n$ sono linearmente indipendenti
        
        $\hfill\qed$
    \end{itemize}
    
    
    \label{lin_ind_k_less_n}
    \begin{framedprop}{Estensione a generatore}
        Dati i vettori \textbf{linearmente indipendenti} $v_1, \ldots, v_k \in \Span(w_1, \ldots, w_n)$, allora:
        \begin{itemize}
            \item $k \leq n$
            \item $\exists v_{k+1}, \ldots, v_n \in \Span(w_1, \ldots, w_n) \mid \Span(v_1, \ldots, v_n) = \Span(w_1, \ldots, w_n)$
            dove $v_1, \ldots, v_n$ sono linearmente indipendenti
        \end{itemize}
    \end{framedprop}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dato $v_1 \neq 0 \in \Span(w_1, \ldots, w_n)$, il quale è ovviamente linearmente indipendente con se stesso, si ha che:
        \[ \exists \lambda_i \neq 0 \in K, i \in [1,n] \mid v = \lambda_1 w_1 + \ldots + \lambda_n w_n \neq 0_V\]
        \item A meno di riordinare i termini, assumiamo che $\lambda_1 \neq 0$
        \[ v = \lambda_1 w_1 + \ldots + \lambda_n w_n \implies \lambda_1 w_1 = v- \lambda_2 w_2 - \ldots -\lambda_n w_n \implies \]
        \[ \implies w_1 = (\lambda_1^{-1})v + (-\lambda_1^{-1}\lambda_2) w_2 - \ldots + (-\lambda_1^{-1}\lambda_n) w_n \implies w_1 \in \Span(v_1, w_2, \ldots, w_n)\]
        \item Poiché $w_1 = \mu_1 v_1 + \mu_2 w_2 + \ldots + \mu_n w_n \in \Span(v_1, w_2, \ldots, w_n)$, ne segue che:
        \[ u \in \Span(w_1, \ldots, w_n) \iff u = \lambda_1w_1 +\lambda_2 w_2 + \ldots + \lambda_n w_n = \]
        \[ = \lambda_1(\mu_1 v_1 + \mu_2 w_2 + \ldots + \mu_n w_n) + \lambda_2 w_2 + \ldots + \lambda_n w_n =\]
        \[ = (\lambda_1\mu_1)v_1 + (\lambda_1\lambda_2\mu_2)w_2 + \ldots + (\lambda_1\lambda_n\mu_n)w_n \iff u \in \Span(v_1, w_2, \ldots, w_n)\]
        dunque si ha che $\Span(v_1, w_2, \ldots, w_n) = \Span(w_1, \ldots, w_n)$
        
        \item Supponiamo quindi induttivamente che dati $v_1, \ldots, v_i \in \Span(w_1, \ldots, w_n)$ linearmente indipendenti, dove $i \leq n$, si ha che:
        \[ \Span(v_1, \ldots, v_i, w_{i+1}, \ldots, w_n) = \Span(w_1, \ldots, w_n)\]
        
        \item Preso $v_{i+1} = \mu_1 v_1+\ldots+\mu_iv_i + \lambda_{i+1} w_{i+1} + \ldots + \lambda_n w_n \in \Span(v_1, \ldots, v_i, w_{i+1}, \ldots, w_n)$, supponiamo per assurdo che $\lambda_{i+1}, \ldots, \lambda_n = 0$, implicando che:
        \[v_{i+1} = \mu_1 v_1+\ldots+\mu_iv_i + \lambda_{i+1} w_{i+1} + \ldots + \lambda_n w_n \implies\]
        \[\implies v_{i+1} = \mu_1 v_1+\ldots+\mu_iv_i + 0 \cdot  w_{i+1} + \ldots + 0 \cdot w_n \implies\]
        \[\implies v_{i+1} = \mu_1 v_1+\ldots+\mu_iv_i \implies 0_V = (-1) v_{i+1} + \mu_1 v_1+\ldots+\mu_iv_i\]
        contraddicendo l'ipotesi per cui $v_1, \ldots, v_k$ siano linearmente indipendenti, dunque l'unica possibilità è
        \[ \exists  \lambda_j \neq 0, j \in [i+1, n] \mid v_{i+1} = \mu_1 v_1+\ldots+\mu_iv_i + \lambda_{i+1} w_{i+1} + \ldots + \lambda_n w_n \neq 0_V \iff \]
        
        \item A meno di riordinare i termini, assumiamo che $\lambda_{i+1} \neq 0$
        \[ v_{i+1} = \mu_1 v_1+\ldots+\mu_iv_i + \lambda_{i+1} w_{i+1} + \ldots + \lambda_n w_n \implies\]
        \[\implies \lambda_{i+1} w_{i+1} = v_{i+1} - \mu_1 v_1-\ldots-\mu_iv_i - \lambda_{i+2} w_{i+2} - \ldots - \lambda_n w_n \implies\]
        \[ w_{i+1} = (\lambda_{i+1}^{-1})v_{i+1} + (-\lambda_{i+1}^{-1}\mu_1) v_1+\ldots+(-\lambda_{i+1}^{-1}\mu_i) v_i + (-\lambda_{i+1}^{-1}\lambda_{i+2}) w_{i+2} + \ldots + (-\lambda_{i+1}^{-1}\lambda_n) w_n\]
        \[ \implies w_{i+1} \in \Span(v_1, \ldots, v_i, v_{i+1}, w_{i+2}, \ldots, w_n) \]
        
        \item Poiché $w_{i+1} = \mu_1 v_1 + \ldots \mu_{i+1}v_{i+1} + \mu_{i+2}w_{i+2} + \ldots + \mu_n w_n \in \Span(v_1,\ldots, v_{i+1}, w_{i+2}, \ldots, w_n)$, procedendo analogamente al caso base si ha che::
        \[ u \in \Span(w_1, \ldots, w_n) \iff u \in \Span(v_1, \ldots,v_{i+1},w_{i+2}, \ldots, w_n)\]
        dunque si ha che $\Span(w_1, \ldots, w_n) = \Span(v_1, \ldots,v_{i+1},w_{i+2}, \ldots, w_n)$, implicando quindi per induzione che
        \[\Span(w_1, \ldots, w_n) = \Span(v_1, \ldots, v_n)\]
        
        \item Supponiamo per assurdo che vi possano essere $k > n$ vettori linearmente indipendenti, dunque che $\exists v_{n+1} \in \Span(w_1, \ldots, w_n) \mid v_1, \ldots, v_n, v_{n+1}$ linearmente indipendenti. Poiché $\Span(w_1, \ldots, w_n) = \Span(v_1, \ldots, v_n)$, ne segue che:
        \[ v_{n+1} \in \Span(w_1, \ldots, w_n) = \Span(v_1, \ldots, v_n)\]
        contraddicendo l'ipotesi per cui $v_1, \ldots, v_n, v_k$ siano indipendenti, dunque l'unica possibilità è che i vettori linearmente indipendenti siano $k \leq n$
        
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Base e Dimensione}

    \quad
    
    \begin{frameddefn}{Base}
        Dato uno spazio vettoriale $V$ su $K$, definiamo i vettori $v_1, \ldots, v_n \neq 0_V \in V$ come una \textbf{base} se e solo se sono un \textbf{insieme di generatori} e \textbf{linearmente indipendenti}
    \end{frameddefn}

    \begin{framedobs}{}
        Dato uno spazio vettoriale $V$, si ha che:
        \[ v_1, \ldots, v_n \text{ base di } V \iff \forall v \in V, \exists! \lambda_1, \ldots, \lambda_n \mid v = \lambda_1 v_1 + \ldots + \lambda_n v_n\]
        
        Inoltre, chiamiamo tali unici scalari come \textbf{coordinate di $v$ in base $ v_1, \ldots, v_n$} 
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Nel caso in cui $\forall v \in V, \exists! \lambda_1, \ldots, \lambda_n \mid v = \lambda_1 v_1 + \ldots + \lambda_n v_n$, per definizione stessa di vettori generatori si ha che $v_1, \ldots, v_n$ sono generatori di $V$
        \item Inoltre, poiché tali coordinate sono uniche, ne segue naturalmente che:
        \[\exists! \lambda_1 = \ldots = \lambda_n = 0 \in K \mid \lambda_1 v_1 + \ldots + \lambda_n v_n = 0_V\]
        dunque $v_1, \ldots, v_n$ sono linearmente indipendenti
        \item Viceversa, se $v_1, \ldots, v_n \in V$ sono una base di $V$, si ha che:
        \[V = \Span(v_1, \ldots, v_n) = \{\lambda_1 v_1 + \ldots + \lambda_n v_n \mid \lambda_1, \ldots, \lambda_n \in K\}\]
        \item Dato $v \in V$, supponiamo per assurdo che esistano due combinazioni lineari di $v_1, \ldots, v_n$ tali che
        \[ \mu_1 v_1+\ldots+\mu_n v_n = v = \lambda_1 v_1 + \ldots + \lambda_n v_n \implies \mu_1 v_1+\ldots+\mu_n v_n = \lambda_1 v_1 + \ldots + \lambda_n v_n \implies\]
        \[ \implies \mu_1 v_1+\ldots+\mu_n v_n - \lambda_1 v_1 - \ldots - \lambda_n v_n = 0_V \implies (\lambda_1-\mu_1)v_1 + \ldots + (\lambda_n-\mu_n)v_n = 0_V\]
        \item Poiché $v_1, \ldots, v_n$ sono linearmente indipendenti, si ha che:
        \[ (\lambda_1-\mu_1)v_1 + \ldots + (\lambda_n-\mu_n)v_n = 0_V \iff (\lambda_1-\mu_1)=\ldots = (\lambda_n-\mu_n) = 0 \iff\]
        \[ \iff \lambda_1 = \mu_1, \ldots, \lambda_n = \mu_n\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{Base canonica}
        Dato uno spazio di coordinate $K^n$, definiamo i vettori $e_1, \ldots, e_n \in K^n$ come \textbf{base canonica di $K^n$}, dove:
        \[e_i = (a_1, \ldots, a_n) \mid \left \{ \begin{array}{ll}
            a_j = 0 & \text{ se } j \neq i\\
            a_j = 1 & \text{ se } j = i\\
        \end{array} \right .\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dati $e_1, \ldots, e_n \in K^n$ definiti come:
        \begin{itemize}
            \item $e_1 = (1, 0, 0, \ldots, 0, 0)$
            \item $e_2 = (0, 1, 0, \ldots, 0, 0)$
            \item $\vdots$
            \item $e_{n} = (0, 0, 0, \ldots, 0, 1)$
        \end{itemize}
        \item Si ha che:
        \[v = (t_1, \ldots, t_n) \in K^n \iff v = (t_1,\ldots, 0) + \ldots + (0,\ldots, t_n)\]
        \[\iff v = t_1e_1 + \ldots + t_ne_n \in \Span(e_1, \ldots, e_n)\]
        dunque tali vettori sono generatori poiché $K^n = \Span(e_1, \ldots, e_n)$
        \item Analogamente, si ha che:
        \[ \lambda_1e_1+\ldots+\lambda_n e_n = (0, \ldots, 0) \iff (\lambda_1, 0, \ldots, 0) + \ldots + (0,0,\ldots, \lambda_n) = (0, \ldots, 0)\]
        \[ (\lambda_1, \lambda_2, \ldots, \lambda_n) = (0, \ldots, 0) \iff \lambda_1 = \ldots = \lambda_n = 0\]
        dunque tali vettori sono anche linearmente indipendenti, costituendo quindi una base di $K^n$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Teorema della cardinalità delle basi}
        Sia $V$ uno spazio vettoriale. Se $v_1, \ldots, v_n$ e $w_1, \ldots, w_m$ sono due basi di $V$, si ha necessariamente che $n=m$.
        
        Dunque tutte le basi di uno spazio vettoriale hanno la \textbf{stessa cardinalità}
    \end{framedthm}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché i due insiemi di vettori sono entrambi base di $V$, si ha che
        \[ \Span(v_1, \ldots, v_n) = V = \Span(w_1, \ldots, w_m)\]
        \item Di conseguenza, poiché i vettori $v_1, \ldots, v_n \in V = \Span(w_1, \ldots, w_m)$ sono linearmente indipendenti, ne segue che $n \leq m$
        \item Analogamente, poiché i vettori $w_1, \ldots, w_m \in V = \Span(v_1, \ldots, v_m)$ sono linearmente indipendenti, ne segue che $m \leq n$
        \item Dunque, l'unica possibilità è che $n=m$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Dimensione di uno spazio vettoriale}
        Dato uno spazio vettoriale $V$, definiamo come \textbf{dimensione di $V$}, indicata come $\dim(V)$ la \textbf{cardinalità di una sua qualsiasi base} (poiché ogni base di $V$ ha la stessa cardinalità).
        
        Nel caso in cui non esista un insieme finito di generatori di $V$, definiamo la sua dimensione come \textbf{infinita}.
    \end{frameddefn}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Lo spazio di coordinate $K^n$ e la sua base canonica $e_1, \ldots, e_n$, si ha che:
        \[\dim(K^n) = n\]
        \item Lo spazio vettoriale $K[x]$ non può avere base finita: dati $p_1(x), \ldots, p_n(x) \in V$ e $\lambda_1, \ldots, \lambda_n \in K$ si ha che:
        \[ \deg(\lambda_1p_1(x) + \ldots + \lambda_n p_n(x) \leq \max(\deg(p_1(x)), \ldots, \deg(p_n(x)))\]
        Infatti, in tale esempio la base è data dai monomi $1, x, x^2, \ldots$, dunque non esiste un insieme finito di generatori di $K[X]$
        \item Lo spazio vettoriale $K^S := \{ f : S \to K\}$ ha dimensione finita se e solo se $S$ ha cardinalità finita
    \end{itemize}
    
    \begin{framedlem}{Estensione a base}
        Sia $V$ uno spazio vettoriale dove $\dim(V) = n$. Dati i vettori \textbf{linearmente indipendenti} $v_1, \ldots, v_k \in V$, dove $k < n$ allora:
        \[\exists v_{k+1}, \ldots, v_n \in V \mid v_1, \ldots, v_n \text{ sono base di }V\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché $\dim(V)=n$, sia $w_1, \ldots, w_n$ una base qualsiasi di $V$. Per dimostrazione precedente, dato $k <n$ si ha che:
        \[\exists v_{k+1}, \ldots, v_n \in V \mid \Span(v_1, \ldots, v_n) = \Span(w_1, \ldots, w_n) = V\]
        dove $v_1, \ldots, v_n$ sono anche linearmente indipendenti, dunque costituiscono una base di $V$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedlem}{Riduzione a base}
         Sia $V$ uno spazio vettoriale dove $\dim(V) = n$. Dato l'\textbf{insieme di generatori} $v_1, \ldots, v_k$ di $V$, dove $k \geq n$ allora:
         \[\exists v_{i_1}, \ldots, v_{i_n} \in \{v_1, \ldots, v_k\} \mid v_{i_1}, \ldots, v_{i_n} \text{ sono base di }V\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dati $v_1, \ldots, v_m$ generatori di $V$, assumiamo che $\exists v_{k_1} \neq 0 \in \{v_1, \ldots, v_m\}$, il quale è ovviamente linearmente indipendente con se stesso
        
        \item Poiché $\Span(v_{i_1}) \subsetneqq \Span(v_1, \ldots, v_m) = W$, allora
        \[\exists v_{i_2} \in \{v_1, \ldots, v_m\} \mid v_{i_2} \notin \Span(v_{i_1}) \iff v_{i_1}, v_{i_2} \text{ lin. ind.}\]
        
        \item Ripetendo tale procedimento $n$, estendiamo l'insieme $v_{i_1}, \ldots, v_{i_n}$ di vettori linearmente indipendenti fino a che essi non siano generatori di $V$:
        \[\Span(v_{i_1}, \ldots, v_{i_n}) = \Span(v_1, \ldots, v_m) = V\]
        dunque $v_{i_1}, \ldots, v_{i_n}$ sono una base di $V$
        
        $\hfill\qed$
    \end{itemize}
    
    
    \begin{framedprop}{}
        Sia $V$ uno spazio vettoriale dove $\dim(V) = n$. Dati i vettori $v_1, \ldots, v_n \in V$, si ha che:
        \[v_1, \ldots, v_n \text{ lin. ind.} \iff v_1, \ldots, v_n \text{ generatori}\]
    \end{framedprop}
    
    \newpage
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo per assurdo che $v_1, \ldots, v_n \in V$ siano linearmente indipendenti ma che non siano generatori di $V$, dunque $\Span(v_1, \ldots, v_n) \subsetneqq V$. Ne segue che:
        \[\exists v_{n+1} \in W \mid v_{n+1} \notin \Span(v_1, \ldots, v_n) \implies v_1, \ldots, v_{n+1} \text{ lin. ind.}\]
        
        contraddicendo la condizione per cui $\dim(V) = n$ implica che non possano esistere più di $n$ vettori linearmente indipendenti (sezione \ref{lin_ind_k_less_n}), dunque l'unica possibilità è che $v_1, \ldots, v_n$ siano anche generatori di $V$
        
        \item Viceversa, supponiamo per assurdo che $v_1, \ldots, v_n$ siano generatori di $V$ ma non linearmente indipendenti. Ne segue che:
        \[\exists \lambda_i \neq 0 \in K, i \in [1,n] \mid \lambda_1 v_1 + \ldots \lambda_i v_i + \ldots + \lambda_n v_n = 0_V \implies \]
        \[v_i = (\lambda_i^{-1}\lambda_1)v_1 + \ldots + (\lambda_i^{-1}\lambda_n) v_n = 0_V \implies w_i \in \Span(v_1, \ldots, \hat{v_i}, \ldots, v_n)\]

        dove $\hat{v_i}$ indica che tale elemento è escluso

        \item A questo punto, si ha che:
        \[u \in \Span(v_1, \ldots, v_n) \iff  u \in \Span(v_1, \ldots, \hat{v_i}, \ldots, v_n)\]

        da cui otteniamo che:
        \[ W = \Span(v_1, \ldots, v_n) = \Span(v_1, \ldots, \hat{v_i}, \ldots, v_n)\]

        contraddicendo la condizione per cui $\dim(V) = n$ implica che non possano esistere meno di $n$ generatori di $V$, dunque l'unica possibilità è che $v_1, \ldots, v_n$ siano anche linearmente indipendenti
        
        $\hfill\qed$
    \end{itemize}
    
    \subsection{Formula di Grassman}
    
    \quad
    
    \begin{framedthm}{Formula di Grassmann}
        Dato uno spazio vettoriale $W$ su $K$ e dati due sottospazi $U, V \subseteq W$, i seguenti insiemi:
        \[ U+V := \{u+v\mid u \in U, v \in V\}\]
        \[ U \cap V := \{ w \mid w \in U, w \in V\}\]
        
        sono due sottospazi di $W$, dove:
        \[ \dim(U+V) = \dim(U)+\dim(V)-\dim(U \cap V)\]
    \end{framedthm}
    
    \newpage

    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dati: $k := \dim(U \cap V), m:= \dim(U), n := \dim(V)$, siano:
        \begin{itemize}
            \item $w_1, \ldots, w_k$ una base di $U \cap V$
            \item $\mathcal{B}_1 := w_1, \ldots, w_k, u_{k+1}, \ldots, u_m$ una base di $U$
            \item $\mathcal{B}_2 := w_1, \ldots, w_k, v_{k+1}, \ldots, v_n$ una base di $V$ 
        \end{itemize}
        
        \item Consideriamo quindi il seguente insieme di vettori:
        \[\mathcal{B}_1 \cup \mathcal{B}_2 = w_1, \ldots, w_k, u_{k+1}, \ldots, u_m, v_{k+1}, \ldots, v_n\]
        
        \item Dati $u \in \Span(B_1) = U$ e $v \in \Span(B_2) = V$, dove
        \[ u = \sum_{i=1}^k \lambda_i w_i + \sum_{j=k+1}^m \lambda_j u_j \qquad\qquad v = \sum_{i=1}^k \lambda_i w_i + \sum_{j=k+1}^n \lambda_j v_j\]
        
        si ha che :
        \[ u+v \in U+V \iff u+v = \sum_{i=1}^k \lambda_i w_i + \sum_{j=k+1}^m \lambda_j u_j + \sum_{i=1}^k \mu_i w_i + \sum_{j=k+1}^n \mu_j v_j \iff\]
        \[ \iff u+v = \sum_{i=1}^k (\lambda_i + \mu_i)w_i + \sum_{j=k+1}^m \lambda_j u_j +  \sum_{j=k+1}^n \mu_j v_j \iff u+v \in \Span(\mathcal{B}_1 \cup \mathcal{B}_2)\]
        dunque $\mathcal{B}_1 \cup \mathcal{B}_2$ sono generatori di $U+V$
        
        \item Consideriamo quindi $0_W \in U+V$ scritto come combinazione lineare di tale base
        \[ \sum_{i=1}^k \beta_i w_i + \sum_{j=k+1}^m \gamma_j u_j + \sum_{h=k+1}^n \eta_h v_h = 0_W\]
        
        \item Posti:
        \[ a := \sum_{i=1}^k \beta_i w_i \qquad\qquad b := \sum_{j=k+1}^m \gamma_j u_j \qquad\qquad c := \sum_{h=k+1}^n \eta_h v_h\]
        
        si ha che:
        \[ a+b+c = 0_W \iff b = -a-c \implies b \in \Span(\mathcal{B}_2) = V\]
        
        inoltre, poiché $b \in \Span(u_{k+1}, \ldots, u_m) \subsetneqq U \implies b \in U$, ne segue che $b \in U \cap V$
        
        \item Di conseguenza, si ha che:
        \[\left \{ \begin{array}{l}
            b := \sum_{j=k+1}^m \gamma_j u_j\\
            b \in U \cap V \iff b = \sum_{t=0}^k \alpha_t w_t
        \end{array} \right .  \implies \sum_{j=k+1}^m \gamma_j u_j = \sum_{t=0}^k \alpha_t w_t \implies\]
        \[ \implies \sum_{t=0}^k \alpha_t w_t - \sum_{j=k+1}^m \gamma_j u_j = 0_W\]
        
        \item Poiché $\mathcal{B}_1 = w_1, \ldots, w_k, u_{k+1}, \ldots, u_m$ è una base di $U$, dunque sono vettori linearmente indipendenti, ne segue che:
        \[ \sum_{t=0}^k \alpha_t w_t - \sum_{j=k+1}^m \gamma_j u_j= 0_W \iff \alpha_1 = \ldots = \alpha_k = \gamma_{k+1} = \ldots = \gamma_m = 0\]
        
        \item In particolare, quindi, otteniamo che $\gamma_{k+1} = \ldots = \gamma_m = 0 \implies b = 0_W$, da cui traiamo che:
        \[a+b+c =0_W \implies a+0_W+c=0_W \implies a+c = 0_W\]
        
        \item A questo punto, poiché:
        \[a+c = 0_W \implies \sum_{i=1}^k \beta_i w_i + \sum_{h=k+1}^n \eta_h v_h = 0_W\]
        e poiché $\mathcal{B}_2 := w_1, \ldots, w_k, v_{k+1}, \ldots, v_n$ è una base di $V$, dunque sono vettori linearmente indipendenti, ne segue che:
        \[\sum_{i=1}^k \beta_i w_i + \sum_{h=k+1}^n \eta_h v_h = 0_W \iff \beta_{1} = \ldots = \beta_{k} = \eta_{k+1} = \ldots = \eta_{n} = 0\]
        
        concludendo quindi che i vettori $\mathcal{B}_1 \cup \mathcal{B}_2 = w_1, \ldots, w_k, u_{k+1}, \ldots, u_m, v_{k+1}, \ldots, v_n$ siano anche linearmente indipendenti, costituendo quindi una base di $U+V$
        
        \item Infine, si ha che:
        \[ \dim(U+V) = \dim(\Span(w_1, \ldots, w_k, u_{k+1}, \ldots, u_m, v_{k+1}, \ldots, v_n)) =\]
        \[=k+(m-k)+(n-k) = m+n-k = \dim(U)+\dim(V)-\dim(U \cap V)\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \section{Trasformazioni lineari}
    
    \quad
    
    \begin{frameddefn}{Trasformazione lineare}
        Dati due spazi vettoriali $V$ e $W$ definiti sullo stesso campo $K$, la funzione $f : V \to W$ viene detta \textbf{trasformazione lineare} (o omomorfismo tra spazi vettoriali) se:
        \begin{itemize}
            \item $\forall v, v' \in V, f(v+v') = f(v)+f(v')$
            \item $\forall \lambda \in K, v \in V, f(\lambda v) = \lambda f(v)$
        \end{itemize}
    \end{frameddefn}
    
    \label{trasform_lin}
    \begin{framedlem}{}
        Dato uno spazio vettoriale $V$ si  ha che:
        \[\dim(V) = n \implies V \cong K^n\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dato $\dim(V) = n$, sia $v_1, \ldots, v_n$ una base di $V$. Definiamo la funzione
        \[ \varphi : K^n \to V : (t_1, \ldots, t_n) \mapsto (t_1 v_1 + \ldots + t_n v_n)\]
        dunque $\varphi$ mappa ogni vettore di $K^n$ ad una combinazione lineare di $V$
        
        \item Poiché:
        \[v_1, \ldots, v_n \text{ base di } V \iff \forall v \in V, \exists! \lambda_1, \ldots, \lambda_n \in K \mid v = \lambda_1 v_1 + \ldots + \lambda_n v_n\]
        
        e poiché $\dim(K^n)=n$, la funzione $\varphi$ risulta essere automaticamente biettiva:
        \[\forall v := \varphi(u) \in V, \exists! u :=(t_1, \ldots, t_n) \in K^n \mid \varphi(u) = t_1v_1+\ldots+t_nv_n\]
        
        \item Dati quindi $x = (x_1, \ldots, x_n), y = (y_1, \ldots, y_n) \in K^n$, si ha che:
        \[ \varphi(x+y) = (x_1+y_1)v_1 + \ldots + (x_n+y_n)v_n = x_1v_1+ \ldots + x_nv_n + y_1 v_1 + \ldots + y_n v_n =\varphi(x)+\varphi(y)\]
        
        \item Dato invece $\lambda \in K$, si ha che:
        \[ \varphi(\lambda v) = \lambda x_1 v_1 + \ldots \lambda x_n v_n = \lambda(x_1 v_1 + \ldots + x_n v_n) = \lambda \varphi(x) \]
        
        \item Dunque, concludiamo che $\varphi$ sia un isomorfismo e di conseguenza che:
        \[ V \cong K^n\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Teorema delle dimensioni}
        Dati due spazi vettoriali $V$ e $W$ definiti sullo stesso $K$, si ha che:
        \[ V \cong W \iff \dim(V) = \dim(W)\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Nel caso in cui $\dim(V) = \dim(W) = n$ dove $n \in \N$, per il lemma precedente si ha che:
        \[\dim(V) = n, \dim(W) = n \implies V \cong K^n, W \cong K^n \implies\]
        \[\implies V \cong K^n, K^n \cong W \implies V \cong W\]
        
        \item Viceversa, supponiamo che $V \cong W$, dove $f$ è l'isomorfismo che rende $V$ isomorfo a $W$. Sia inoltre $v_1, \ldots, v_n$ la base di $V$.
        
        \item Poiché $f$ è un omomorfismo suriettivo, si ha che:
        \[\forall w \in W, \exists v \in V \mid w = f(v) = f(\lambda_1v_1+\ldots+\lambda_nv_n) =\]
        \[f(\lambda_1v_1)+\ldots+f(\lambda_nv_n) = \lambda_1f(v_1)+\ldots+\lambda_n f(v_n)\]
        
        dunque, poiché anche $f(v_1),\ldots,f(v_n)$ sono vettori di $W$, ne segue che $f(v_1),\ldots,f(v_n)$ siano generatori di $W$
        
        \item Inoltre, poiché $f$ è iniettiva se e solo se $\ker(f) = \{0_V\}$ e poiché ne segue che:
        \[\mu_1 f(v_1) + \ldots + \mu_n f(v_n) = 0_W \iff f(\mu_1v_1) + \ldots +  f(\mu_nv_n) = 0_W \iff\]
        \[\iff f(\mu_1 v_1+\ldots+\mu_nv_n) = 0_W \iff \mu_1 v_1+\ldots+\mu_nv_n = 0_V \iff \mu_1 = \ldots = 0\]
        dunque $f(v_1),\ldots,f(v_n)$ sono anche linearmente indipendenti, costituendo quindi una base di $W$, implicando quindi che $\dim(W) = n = \dim(V)$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Spazio quoziente}
        Dato uno spazio vettoriale $V$ e un sottospazio $W \subseteq V$, la struttura $(V/W, +, \cdot)$ è uno spazio vettoriale, detto \textbf{spazio quoziente}, dove la somma è definita come $[v]+[v']=[v+v']$ e il prodotto per scalare è definito come $\lambda[x] = [\lambda x]$.
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item La dimostrazione della buona definizione della somma risulta analoga a quella del normale gruppo quoziente
        \item Dimostriamo quindi che il prodotto per scalare sia ben definito
        \[ [v] = [v'] \implies v'-v \in W \implies \lambda(v'-v) \in W \implies [\lambda v] = [\lambda v']\]
        \item La dimostrazione di $(V/W, +, \cdot)$ spazio vettoriale viene omessa poiché banale
        
        $\hfill\qed$
    \end{itemize}
    
    
    \begin{framedthm}{Dimensione di uno spazio quoziente}
        Dato uno spazio vettoriale $V$ e un sottospazio $W \subseteq V$, si verifica che
        \[ \dim(V/W) = \dim(V)-\dim(W)\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Siano $n := \dim(V)$, $k := \dim(W)$ e  $w_1, \ldots, w_k$ una base di $W$.
        \item Poiché $k \leq n$, è possibile estendere con $n-k$ vettori di $V$ l'insieme $w_1, \ldots, w_k$ fino a formare una base di $V$:
        \[\exists v_{k+1}, \ldots, v_{n} \in V \mid w_1, \ldots, w_k, v_{k+1}, \ldots, v_n \text{ base di V}\]
        
        \item Dato $v \in V = \Span(w_1, \ldots, w_k, v_{k+1}, \ldots, v_n)$, si ha che:
        \[ v = \lambda_1 w_1 + \ldots + \lambda_k w_k + \lambda_{k+1} v_{k+1} + \ldots+ \lambda_n v_n \implies\]
        \[ \implies [v] = \lambda_1 [w_1] + \ldots + \lambda_k [w_k] + \lambda_{k+1} [v_{k+1}] + \ldots+ \lambda_n [v_n]\]
        
        \item Poiché:
        \[ w_1, \ldots, w_k \in W \iff w_1 \sim 0_V, \ldots, w_k \sim 0_V \iff [0_V] = [w_1] = \ldots = [w_k]\]
        
        si ha che:
        \[ [v] = \lambda_1 [w_1] + \ldots + \lambda_k [w_k] + \lambda_{k+1} [v_{k+1}] + \ldots+ \lambda_n [v_n] \iff\]
        \[ \iff [v] = \lambda_1 [0_V] + \ldots + \lambda_k [0_V] + \lambda_{k+1} [v_{k+1}] + \ldots+ \lambda_n [v_n] \iff\]
        \[\iff [v] = \lambda_{k+1} [v_{k+1}] + \ldots+ \lambda_n [v_n]\]
        dunque, $[v_{k+1}], \ldots, [v_n]$ sono generatori di $V/K$
        
        \item Preso $[0_V] \in V/W$, si ha che:
        \[ [0_V] = \lambda_{k+1} [v_{k+1}] + \ldots+ \lambda_n [v_n] \iff [0_V] = [\lambda_{k+1} v_{k+1} + \ldots+ \lambda_n v_n] \iff\]
        \[\iff u := \lambda_{k+1} v_{k+1} + \ldots+ \lambda_n v_n \in W\]
        
        \item Poiché anche $w_1, \ldots, w_k$ è base di $W$ e poiché $u \in W$, si ha che:
        \[u = \lambda_{k+1} v_{k+1} + \ldots+ \lambda_n v_n = \mu_1 w_1+\ldots+\mu_kw_k \iff \]
        \[ \iff \lambda_{k+1} v_{k+1} + \ldots+ \lambda_n v_n = \mu_1 w_1+\ldots+\mu_kw_k \iff\]
        \[\iff \mu_1 w_1+\ldots+\mu_kw_k - \lambda_{k+1} v_{k+1} + \ldots+ \lambda_n v_n = 0_V \iff\]
        \[\iff \mu_1 = \ldots = \mu_k = \lambda_{k+1} = \ldots = \lambda_n = 0\]
        
        dato che $w_1, \ldots, w_k, v_{k+1}, \ldots, v_n$ sono base di $V$, dunque linearmente indipendenti.
        
        \item In particolare, quindi, dati $\lambda_{k+1} = \ldots = \lambda_n = 0$ ne segue che:
        \[ [0_V] = \lambda_{k+1} [v_{k+1}] + \ldots+ \lambda_n [v_n] \iff \lambda_{k+1} = \ldots = \lambda_n = 0\]
        implicando che anche $[v_{k+1}], \ldots, [v_{n}]$ siano linearmente indipendenti, costituendo quindi base di $V/W$, la cui dimensione risulta essere:
        \[\dim(V/W) = \dim(\Span([v_{k+1}], \ldots, [v_{n}])) = n-k = \dim(V) - \dim(W)\]
        $\hfill\qed$
    \end{itemize}
    
    \newpage
    
    \subsection{Teorema del Rango}
    
    \quad
    
    \begin{framedprop}{Nucleo ed Immagine di una trasf. lineare}
        Data una trasformazione lineare $f : V \to W$, il nucleo  $\ker(f) \subseteq V$ e l'immagine  $\im(f) \subseteq W$ di $f$ corrispondono a:
        \[ \ker(f) := \{ v \in V \mid  f(v) = 0_W\}\]
        \[ \im(f) := \{ w \in W \mid  f(v) = w, \exists v\in V\}\]
        dove entrambi sono sottospazi vettoriali rispettivamente di $V$ e $W$
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sappiamo già $\ker(f) \normsubgrp V$ e che $\im(f) \subgrp W$
        \item Verifichiamo quindi che siano chiusi nel prodotto per scalare
        \[ v \in \ker(f), \lambda \in K \implies f(\lambda v) = \lambda f(v) = \lambda 0_W = 0_W \implies \lambda v \in \ker(f)\]
        \[ w = f(v) \in \im(f), \lambda \in K \implies \lambda w = \lambda f(v) = f(\lambda v) \implies \lambda v \in \im(f)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Primo teorema d'isomorfismo}
        Data una trasformazione lineare $f : V \to W$, si ha che
        \[ V/\ker(f) \cong \im(f)\]
        
        \textit{(dimostrazione analoga agli altri casi del teorema)}
    \end{framedthm}
    
    \begin{framedthm}{Teorema del Rango}
        Siano $V$ e $W$ due spazi vettoriali. Data una trasformazione lineare $f : V \to M$, definiamo come \textbf{rango di $f$} la dimensione della sua immagine, la quale equivale a:
        \[ \rk(f) := \dim(\im(f)) = \dim(V) - \dim(\ker(f))\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $f : V \to W$ è una trasformazione lineare, per il primo teorema d'isomorfismo si ha che $V/\ker(f) \cong \im(f)$, da cui ne segue automaticamente che:
        \[V/\ker(f) \cong \im(f) \iff \dim(V/\ker(f)) = \dim(\im(f)) \iff\]
        \[\iff \dim(V)-\dim(\ker(f)) = \dim(\im(f))\]
        $\hfill\qed$
    \end{itemize}
    
    \newpage
    
    \section{Spazi affini, Sottospazi affini e Giacitura}
    
    \quad
    
    \begin{frameddefn}{Spazio affine}
        Dato uno spazio vettoriale $(V, +, \cdot)$, definiamo come \textbf{spazio affine a $V$} la struttura $(A, V, \phi)$, dove:
        \begin{itemize}
            \item Gli elementi $P \in A$ vengono detti \textbf{punti}
            \item La funzione $\phi : A \times V \to A$ gode delle seguenti proprietà
            \begin{itemize}
                \item \textbf{Associatività mista:}
                \[\forall P \in A, \forall v,w \in V, \phi(\phi(P, v), w) = \phi(P, (v+w))\]
                \item \textbf{Elemento nullo destro:}
                \[\forall a \in A, \exists 0_V \in V \mid \phi(a, 0_V) = a\]
                \item \textbf{Azione libera e transitiva:}
                \[\forall P \in A, \text{ la funzione } \phi_P : V \to A : v \to \phi(P,v) \text{ è biettiva}\]
            \end{itemize}
        \end{itemize}
        
        Detto in parole molto povere (e poco esatte) uno spazio affine $A$ è una \textbf{traslazione totale di uno spazio vettoriale} $V$ dove ogni punto del primo corrisponde biunivocamente ad un punto dell'altro
    \end{frameddefn}
    
    \begin{frameddefn}{Sottospazio affine e Giacitura}
        Dato uno spazio vettoriale $V$, un suo sottospazio vettoriale $W \subseteq V$ e un vettore $v \in V$, definiamo come \textbf{sottospazio affine} l'insieme di \textbf{punti} generato da:
        \[ U = v + W := \{ v + w \mid w \in W\}\]
        
        dove $W$ viene definito \textbf{giacitura} di $U$, indicato con $\mathrm{Giac}(U)$ e dove:
        \[\dim(U) = \dim(\mathrm{Giac}(U)) = \dim(W) \]
        
        In altre parole, il sottospazio affine $U$ corrisponde ad una \textbf{traslazione} del sottospazio $W$ tramite il vettore $v$. Ogni sottospazio affine può essere visto come una \textbf{classe laterale di un sottospazio} (poiché l'operazione primaria è la somma).
    \end{frameddefn}
    
    \newpage
    \section{Prodotto scalare e Spazio ortogonale}

    \quad
    
    \begin{frameddefn}{Prodotto scalare}
        Dato lo spazio di coordinate $K^n$, definiamo come \textbf{prodotto scalare} l'operazione:
        \[\cdot : K^n \times K^n \to K : ((\lambda_1, \ldots, \lambda_n),(\mu_1, \ldots, \mu_n)) \mapsto v \cdot v' = \sum_{i=1}^n \lambda_i \mu_i\]
        la quale gode delle seguenti proprietà:
        \begin{itemize}
            \item $u, v \in K^n \implies u \cdot v = v \cdot u$ (\textbf{Simmetria})
            \item $u, v, w \in K^n \implies (v+w)u = u(v+m) = vu+vu$ (\textbf{Distributività})
            \item $u, v \in K^n, \lambda \in K \implies (\lambda u)v = u(\lambda v) = \lambda u v$ (\textbf{Linearità per scalare})
        \end{itemize}
        
        \textit{\textbf{Attenzione:}} il prodotto scalare differisce dal prodotto \textbf{per} scalare
    \end{frameddefn}
    
    \begin{frameddefn}{Norma di un vettore}
        Dato $v \in \R^n$, definiamo la \textbf{norma (o lunghezza)} di tale vettore come
        \[ \norm{v} := \sqrt{ v \cdot v} = \sqrt{x_1^2 + \ldots x_n^2}\]
    \end{frameddefn}

    \label{ortho}
    \begin{framedthm}{}
        Dati $u, v \in \R^n$ e l'angolo $0 < \theta < \pi$ interno tra $u$ e $v$, si ha che
        \[ \cos \theta = \frac{uv}{\norm{u}\norm{v}}\]
        Inoltre, si verifica che:
        \begin{itemize}
            \item $\theta < \frac{\pi}{2} \iff uv > 0$
            \item $\theta = \frac{\pi}{2} \iff uv = 0$
            \item $\theta > \frac{\pi}{2} \iff uv < 0$
        \end{itemize}
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Tramite il \href{https://it.wikipedia.org/wiki/Teorema_del_coseno}{Teorema del Coseno} (normalmente utilizzato per calcolare la lunghezza di un lato di un triangolo qualsiasi sapendo la lunghezza degli altri due lati), si ha che:
        \[ \norm{v-u}^2 = \norm{u}^2 + \norm{v}^2 -2 \norm{u}\norm{v} \cos \theta \iff \]
        \[ \iff \norm{v-u}^2 - \norm{u}^2 - \norm{v}^2 = -2 \norm{u}\norm{v} \cos \theta \iff \]
        
        \[ \iff \sum_{i=1}^n (y_i-x_i)^2 - \sum_{j=1}^n x_n^2 - \sum_{k=1}^n y_k^2 = -2 \norm{u}\norm{v} \cos \theta \iff \]
        
        \[ \iff \sum_{i=1}^n (y_i^2-2y_ix_i + x_i^2) - \sum_{j=1}^n x_j^2 - \sum_{k=1}^n y_k^2 = -2 \norm{u}\norm{v} \cos \theta \iff \]
        
        \[ \iff \sum_{i=1}^n y_i^2 -2 \sum_{h=1}^n y_hx_h + \sum_{t=1}^n + x_t^2 - \sum_{j=1}^n x_j^2 - \sum_{k=1}^n y_k^2 = -2 \norm{u}\norm{v} \cos \theta \iff \]
        
        \[ \iff \sum_{h=1}^n y_hx_h =  \norm{u}\norm{v} \cos \theta \iff \sum_{h=1}^n y_hx_h = \norm{u}\norm{v} \cos \theta \iff \]
        
        \[ \iff uv=  \norm{u}\norm{v} \cos \theta \iff \frac{uv}{\norm{u}\norm{v}} = \cos \theta\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{Relazione di ortogonalità}
        Siano $u,v \in \R^n$. Il vettore $u$ viene detto "\textbf{ortogonale a $v$}", indicato come $v \perp w$, se e solo se:
        \[u \perp v \iff uv = 0\]
        
        \textit{Nota}: l'ortogonalità è una generalizzazione del concetto di perpendicolarità nell'ambito dell'algebra lineare
    \end{framedprop}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Per definizione stessa di perpendicolarità, l'essere perpendicolari implica che vi sia un angolo retto tra due linee. Generalizzando tale concetto all'ortogonalità tra vettori, dato l'angolo $0<\theta<\pi$ interno a $u$ e $v$, per il teorema precedente si ha che:
        \[uv =0 \iff \theta = \frac{\pi}{2} \iff \cos(\theta) = 1 \iff  \text{angolo retto tra } u \text{ e }v\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Sottospazio ortogonale}
        Dato il sottospazio vettoriale $V \subseteq K^n$, definiamo $V^{\perp}$ il \textbf{sottospazio ortogonale a $V$} come:
        \[V^{\perp} = \{v \in K^n \mid vw = 0, \forall w \in V\}\]
        dove in particolare si ha che:
        \[\dim(K^n) = \dim(V)+\dim(V^{\perp})\]
    \end{frameddefn}
    
    \newpage

    \textit{Dimostrazione:}
    \begin{itemize}
        \item $V^{\perp} \subgrp K^n$
        \begin{itemize}
            \item $\forall v \in V, 0_{K^n} \cdot v = 0 \implies 0_{K^n} \in V^{\perp}$
            \item $\forall v \in V, w_1, w_2 \in V^{\perp} \implies (w_1+w_2)v = w_1v+w_2v = 0 + 0 = 0 \implies w_1+w_2 \in V^{\perp} $
            \item $\forall v \in V, w \in V^{\perp} \implies (-w)v = -(wv) = 0 \implies -w \in V^{\perp}$
        \end{itemize}
        \item $\forall v \in V, w \in V^{\perp}, \lambda \in K \implies (\lambda w)v = \lambda(wv) = 0 \implies \lambda w \in V^{\perp}$
        
        \item Poiché $V \cap V^{\perp} = \{0_{K^n}\} \implies \dim(V \cap V^{\perp}) = 0$ e poiché $K^n = V+V^{\perp}$, per la formula di Grassman ne segue che:
        \[\dim(K^n) = \dim(V+V^{\perp}) =  \dim(V)+\dim(V^{\perp})-\dim(V \cap V^{\perp}) = \dim(V)+\dim(V^{\perp})\]
        $\hfill\qed$
    \end{itemize}
    
    \chapter{Matrici}
    
    \begin{frameddefn}{Matrici}
        Dati $m, n \neq 0 \in \N$, una \textbf{matrice $m \times n$ a coefficienti in un campo $K$} è una griglia con $m$ righe e $n$ colonne, le cui entrate sono elementi in $K$
        
        $$
        A \in \matspace{m}{n}{K} \implies A = \left ( \begin{array}{ccc}
            a_{1,1} & \cdots & a_{1,n}\\
            \vdots & \ddots & \vdots \\
            a_{m,1} & \cdots & a_{m,n}
        \end{array}\right )
        $$
        
        dove $\matspace{m}{n}{K} = \underbrace{K^n \times \ldots \times K^n}_{\text{m volte}} = \underbrace{K^m \times \ldots \times K^m}_{\text{n volte}}$
    \end{frameddefn}
    \textbf{Esempio:}
    $$
    A = \left ( \begin{array}{ccc}
        1 & 2 & 3\\
        4 & 5 & 6
    \end{array}\right ) \in \matspace{2}{3}{\R} 
    $$
    
    \begin{frameddefn}{Vettori colonna e Vettori riga}
        Definiamo una matrice $1 \times n$ come \textbf{vettore riga}
        \[ \left( \begin{array}{c c c}
            a_1 & \cdots & a_n
        \end{array} \right ) \in \matspace{1}{n}{K} = K^n\]
        
        e analogamente definiamo una matrice $m \times 1$ come \textbf{vettore colonna}
        \[ \left( \begin{array}{c}
            a_1\\
            \vdots\\
            a_m
        \end{array} \right ) \in \matspace{m}{1}{K} = K^m\]
    \end{frameddefn}
    
    \begin{framedobs}{}
        Una matrice $m \times n$ può essere definita come un \textbf{vettore di $m$ vettori riga aventi $n$ elementi} (che vengono indicati con un pedice)
        \[ A_1, \ldots, A_n \in K^m \implies A = \left( \begin{array}{c c c}
            A_1 & \cdots & A_n
        \end{array} \right )\]
        
        o come un \textbf{vettore di $n$ vettori colonna aventi $m$ elementi} (che vengono indicati con un apice)
        \[ A^1, \ldots, A^m \in K^n \implies A = \left( \begin{array}{c}
            A^1\\
            \vdots\\
            A^m
        \end{array} \right )\]
    \end{framedobs}
    
    \begin{framedobs}{}
        La struttura $(\matspace{m}{n}{K}, +, \cdot)$ è uno \textbf{spazio vettoriale} di dimensione:
        \[\dim(\matspace{m}{n}{K}) = m \cdot n\]
        
        e la cui \textbf{base canonica} è composta dalle matrici:
        \[e_{i,j} = \left ( \begin{array}{ccc}
            a_{1,1}& \cdots & a_{1,n}\\
            \vdots & \ddots & \vdots \\
            a_{m,1} & \cdots & a_{m,n}
        \end{array}\right ) \left | \left \{ \begin{array}{ll}
            a_{k,h} = 0 & \text{ se } k \neq i \lor h \neq j\\
            a_{k,h} = 1 & \text{ se } k=i, h = j\\
        \end{array} \right .\right .\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Date $A, B \in \matspace{m}{n}{K}$ e $\lambda \in K$, le operazioni di somma e prodotto sono definite come:
        $$
        A + B = \left ( \begin{array}{ccc}
            a_{1,1}+b_{1,1} & \cdots & a_{1,n}+b_{1,n}\\
            \vdots & \ddots & \vdots \\
            a_{m,1}+b_{m,1} & \cdots & a_{m,n}+b_{m,n}
        \end{array}\right )
        $$
        
        $$
        \lambda A = \left ( \begin{array}{ccc}
            \lambda a_{1,1}& \cdots & \lambda a_{1,n}\\
            \vdots & \ddots & \vdots \\
            \lambda a_{m,1} & \cdots & \lambda a_{m,n}
        \end{array}\right )
        $$
        
        \item Le dimostrazioni di $(\matspace{m}{n}{K}, +, \cdot)$ spazio vettoriale e della base canonica vengono omesse poiché analoghe alle dimostrazioni per $K^n$
        
        $\hfill\qed$
    \end{itemize}
    
    \newpage

    \textbf{Esempio:}
    \begin{itemize}
        \item La base canonica di $\matspace{2}{2}{A}$ corrisponde a:
        \[ \left( \begin{array}{c c}
            1 & 0\\
            0 & 0
        \end{array} \right ),
        \left( \begin{array}{c c}
            0 & 1\\
            0 & 0
        \end{array} \right ),
        \left( \begin{array}{c c}
            0 & 0\\
            1 & 0
        \end{array} \right ),
        \left( \begin{array}{c c}
            0 & 0\\
            0 & 1
        \end{array} \right ) \]
    \end{itemize}
    
    \begin{framedobs}{}
        Dato lo spazio $\matspace{m}{n}{K}$, si ha che:
        \[\matspace{m}{n}{K} \cong K^{m \cdot n}\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché $\dim(\matspace{m}{n}{K}) = m \cdot n$ e $\dim(K^{m \cdot n}) = m \cdot n$, per dimostrazione precedente si ha che:
        \[ \dim(\matspace{m}{n}{K}) =\dim(K^{m \cdot n}) \iff \matspace{m}{n}{K} \cong K^{m \cdot n}\]
        $\hfill\qed$
    \end{itemize}
    
    \quad
    
    \begin{frameddefn}{Prodotto tra matrici}
        Sia $A = (A_1, \ldots, A_h) \in \matspace{h}{m}{K}$ e sia $B = (B^1, \ldots, B^n) \in \matspace{m}{n}{K}$.
        
        Definiamo come \textbf{prodotto tra matrici} la trasformazione lineare:
        \[ \cdot : \matspace{h}{m}{K} \times \matspace{m}{n}{K} \to \matspace{h}{n}{K} : (A,B) \mapsto AB\]
        dove :
        $$
        AB = \left ( \begin{array}{ccc}
            A_1B^1 & \cdots & A_1B^n\\
            \vdots & \ddots & \vdots \\
            A_hB^1 & \cdots & A_hB^n
        \end{array}\right )
        $$
        
        \textit{Attenzione}: affinché il prodotto sia ben definito è \textbf{necessario} che la quantità di colonne di $A$ sia uguale alla quantità di righe di $B$
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item $
        \left ( \begin{array}{cc}
            1 & 2\\
            3 & 4
        \end{array}\right )
        \left ( \begin{array}{cc}
            5 & 6\\
            7 & 8
        \end{array}\right )
        =
        \left ( \begin{array}{cc}
            1\cdot5+2\cdot7 & 1\cdot6+2\cdot8\\
            3\cdot5+4\cdot7 & 3\cdot6+4\cdot8
        \end{array}\right )
        =
        \left ( \begin{array}{cc}
            19 & 22\\
            43 & 50
        \end{array}\right )
        $
        
        \item $
        \left ( \begin{array}{cc}
            5 & 6\\
            7 & 8
        \end{array}\right )
        \left ( \begin{array}{cc}
            1 & 2\\
            3 & 4
        \end{array}\right )
        =
        \left ( \begin{array}{cc}
            5\cdot1+6\cdot3 & 5\cdot2+6\cdot4\\
            7\cdot1+8\cdot3 & 7\cdot2+8\cdot4\\
        \end{array}\right )
        =
        \left ( \begin{array}{cc}
            23 & 34\\
            32 & 46
        \end{array}\right )
        $
        
        \item $
        \left ( \begin{array}{cc}
            1 & 2\\
            4 & 5
        \end{array}\right )
        \left ( \begin{array}{ccc}
            3 & 6 & 7\\
            -2 & 0 & 8
        \end{array}\right )
        =
        \left ( \begin{array}{ccc}
            1\cdot3+2\cdot(-2) & 1\cdot6+2\cdot0 & 1\cdot7+2\cdot8\\
            4\cdot3+5\cdot(-2) & 4\cdot6+5\cdot0 & 4\cdot7+5\cdot8\\
        \end{array}\right )
        =\\
        =\left ( \begin{array}{ccc}
            -1 & 6 & 23\\
            2 & 24 & 68
        \end{array}\right )
        $
        
        \item  $
        \left ( \begin{array}{ccc}
            3 & 6 & 7\\
            -2 & 0 & 8
        \end{array}\right )
        \left ( \begin{array}{cc}
            1 & 2\\
            4 & 5
        \end{array}\right )
        = \nexists
        \text{ poiché la quantità di colonne nella prima non}$ corrisponde a quella delle righe della seconda
        
    \end{itemize}
    
    \begin{framedobs}{}
        Date tre matrici $A, B, C$ ed uno scalare $\lambda$, se i prodotti sono \textbf{ben definiti} si ha che:
        \begin{itemize}
            \item $(AB)C = ABC = A(BC)$
            \item $A(B+C) = AB+AC$
            \item $(A+B)C = AC+BC$
            \item $\lambda(AB) = (\lambda A)B = A(\lambda B)$
        \end{itemize}
    \end{framedobs}
    
    \begin{framedcor}{}
        Uno spazio vettoriale $\matspace{n}{n}{K}$ (anche detto spazio delle matrici quadrate di ordine $n$) è un \textbf{anello non commutativo}, dove l'elemento neutro è:
        $$
        I_n = \left ( \begin{array}{ccccc}
            1 & 0 & \cdots & \cdots & 0\\
            0 & 1 & 0 & \ddots & 0\\
            \vdots & \ddots & \ddots & \ddots & \vdots\\
            0 & \ddots & 0 & 1 & 0\\
            0 & \cdots & \cdots & 0 & 1
        \end{array}\right )
        $$
        
    \end{framedcor}
    
    \quad
    
    \section{Rango di una matrice}
    
    \quad
    
    \begin{frameddefn}{Trasformazione lineare di una matrice}
        Data una matrice $A \in \matspace{m}{n}{K}$, definiamo la \textbf{trasformazione lineare associata ad $A$} come
        \[ L_A: K^n \to K^m : x := \left ( \begin{array}{c}
            x_1 \\ \vdots \\ x_n
        \end{array}\right ) \to Ax\]
        
        dove:
        \[ L_A(x) = Ax = 
            \left ( \begin{array}{ccc}
                a_{1,1} & \cdots & a_{1,n}\\
                \vdots & \ddots  & \vdots\\
                a_{m,1} & \cdots & a_{m,n}
            \end{array}\right ) \cdot
            \left ( \begin{array}{c}
                x_1 \\ \vdots \\ x_n
            \end{array}\right ) = \left ( \begin{array}{c}
                a_{1,1}x_1 + \ldots + a_{1,n}x_n \\
                \vdots \\
                a_{m,1}x_1 + \ldots + a_{m,n}x_n
            \end{array}\right )\]
    \end{frameddefn}
    
    \begin{frameddefn}{Sistema lineare e Matrice completa}
        Data una \textbf{matrice di coefficienti} $A$ ed un \textbf{vettore di incognite} $b$, definiti come:
        
        \[ A = \left ( \begin{array}{ccc}
            a_{1,1} & \cdots & a_{1,n}\\
            \vdots & \ddots & \vdots\\
            a_{m,1} & \cdots & a_{m,n}
        \end{array}\right )
        \qquad\qquad
        b = \left ( \begin{array}{ccc}
            b_1 \\ \vdots \\ b_m
        \end{array}\right )\]
        
        l'equazione $Ax=b$ corrisponde ad un \textbf{sistema lineare di equazioni cartesiane} nella forma:
        \[ \left \{ \begin{array}{l}
            a_{1,1}x_1 + \ldots + a_{1,n}x_n = b_1\\
            \qquad \vdots\\
            a_{m,1}x_1 + \ldots + a_{m,n}x_n = b_m\\
        \end{array}\right .\]
        
        il quale è rappresentato fedelmente dalla seguente \textbf{matrice completa}:
        \[ A_b = \left ( \begin{array}{ccc|c}
            a_{1,1} & \cdots & a_{1,n} & b_1\\
            \vdots & \ddots & \vdots & \vdots\\
            a_{m,1} & \cdots & a_{m,n} & b_m
        \end{array}\right )\]
    \end{frameddefn}
    
    \newpage
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Si nota facilmente che:
        \[Ax = b \iff \left ( \begin{array}{ccc}
                a_{1,1} & \cdots & a_{1,n}\\
                \vdots & \ddots  & \vdots\\
                a_{m,1} & \cdots & a_{m,n}
            \end{array}\right ) \cdot
            \left ( \begin{array}{c}
                x_1 \\ \vdots \\ x_n
            \end{array}\right ) = \left ( \begin{array}{c}
                b_1 \\ \vdots \\ b_n
            \end{array}\right ) \iff\]
            \[ \iff \left ( \begin{array}{c}
                a_{1,1}x_1 + \ldots + a_{1,n}x_n \\
                \vdots \\
                a_{m,1}x_1 + \ldots + a_{m,n}x_n
            \end{array}\right ) = \left ( \begin{array}{c}
                x_1 \\ \vdots \\ x_n
            \end{array}\right ) \iff \left \{ \begin{array}{c}
            a_{1,1}x_1 + \ldots + a_{1,n}x_n = b_1 \\
            \vdots \\
            a_{m,1}x_1 + \ldots + a_{m,n}x_n = b_n
        \end{array}\right .\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Data una matrice $A \in \matspace{m}{n}{K}$, si ha che:
        \begin{itemize}
            \item $\im(L_A) = \Span(A^1, \ldots, A^n)$
            \item $\ker(L_A) = \Span(A_1, \ldots, A_m)^{\perp}$
        \end{itemize}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
    
        \item Data $A \in \matspace{m}{n}{K}$, si ha che:
        \[ L_A(x) \in \im(L_A) \iff L_A(x) = Ax =
            \left ( \begin{array}{ccc}
                a_{1,1} & \cdots & a_{1,n}\\
                \vdots & \ddots  & \vdots\\
                a_{m,1} & \cdots & a_{m,n}
            \end{array}\right ) \cdot
            \left ( \begin{array}{c}
                x_1 \\ \vdots \\ x_n
            \end{array}\right ) = \]
        \[x_1 \left ( \begin{array}{c}
            a_{1,1} \\ \vdots \\ a_{m,1}
        \end{array}\right )+\ldots+
        x_n \left ( \begin{array}{c}
            a_{1,n} \\ \vdots \\ a_{m,n}
        \end{array}\right ) = x_1A^1 + \ldots x_nA^n \iff L_A(x) \in \Span(A^1, \ldots , A^n)\]
        
        dunque si ha che $\im(L_A) = \Span(A^1, \ldots , A^n)$
        
        \item Inoltre, notiamo che:
        \[ \ker(L_A) = \{ x \in K^n \mid L_A(x) = Ax = 0_{K^m} \} = \]
        \[ = \left \{ x \in K^n \mid \left ( \begin{array}{c}
            A_1 \\ \vdots \\ A_m
        \end{array} \right )x = 0_{K^m} \right \} = \Span(A_1, \ldots, A_m)^{\bot}\]
        
        dunque $\ker(L_A)$ contiene tutte i vettori $x = (x_1, \ldots, x_n) \in K^n$ che sono soluzione del seguente sistema:
        \[ \left \{ \begin{array}{c}
            a_{1,1}x_1 + \ldots + a_{1,n}x_n = 0 \\
            \vdots \\
            a_{m,1}x_1 + \ldots + a_{m,n}x_n = 0
        \end{array}\right .\]
        $\hfill\qed$
    \end{itemize}
        
    \begin{frameddefn}{Rango di una matrice}
        Data una matrice $A \in \matspace{m}{n}{K}$, definiamo come \textbf{rango di $A$} il rango della sua trasformazione lineare associata:
        \[ \rk(A) := \rk(L_A)\]
    \end{frameddefn}
    
    \begin{framedprop}{}
        Data una matrice $A \in \matspace{m}{n}{K}$, si ha che:
        \[ \rk(A) = n - \dim(\ker(L_A)) = \dim(\Span(A^1, \ldots, A^n)) = \dim(\Span(A_1, \ldots, A_m))\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Poiché $\rk(A) := \rk(L_A)$ e poiché $L_A : K^n \to K^m$, per il teorema del rango si ha che:
        \[\rk(A) := \rk(L_A) = \dim(L_A) = \dim(K^n) - \dim(\ker(L_A)) = n - \dim(\ker(L_A))\]
        
        \item Inoltre, per dimostrazioni precedenti si ha che:
        \[ \dim(\Span(A^1, \ldots, A^n)) = \dim(\im(L_A)) = n - \dim(\ker(L_A)) = \]
        \[ = n - \dim(\Span(A_1, \ldots, A_n)^{\perp})=  \dim(\Span(A_1, \ldots, A_m)) \]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
         Data una matrice $A \in \matspace{m}{n}{K}$, si ha che:
        \[ 0 \leq \rk(A) \leq \min(m,n)\]
    \end{framedcor}
    
    \quad
    
    \subsection{Riduzione a scala di una matrice}
    
    \quad
    
    \begin{frameddefn}{Operazioni elementari su matrici}
    Data una matrice $A \in \matspace{m}{n}{K}$, definiamo come \textbf{operazioni elementari} su righe e colonne le seguenti tre operazioni:
    \begin{enumerate}
        \item \textbf{Scambiare} due righe (o colonne) tra di loro
        \item \textbf{Moltiplicare} una riga (o colonna) per $\lambda \in K^*$
        \item \textbf{Sommare} ad una riga (o colonna) un multiplo di un'altra riga (o colonna)
    \end{enumerate}
    \end{frameddefn}
    
    \begin{frameddefn}{Matrici equivalenti}
        Date due matrici $A, B \in \matspace{m}{n}{K}$, definiamo tali matrici come \textbf{equivalenti} se sono uguali se è possibile ottenere l'una partendo dall'altra utilizzando \textbf{solo operazioni elementari}.

        In particolare, se vengono utilizzate solo operazioni su righe tali matrici vengono dette \textbf{equivalenti per righe}, mentre vengono dette \textbf{equivalenti per colonne} se vengono utilizzate solo operazioni su colonne.
    \end{frameddefn}
    
    \begin{framedobs}{}
        Date due matrici $A, B \in \matspace{m}{n}{K}$, si ha che:
        \begin{itemize}
            \item Se $A$ e $B$ sono \textbf{equivalenti per righe}, allora
            \[\ker(L_A) = \ker(L_B) \land \im(L_A) \neq \im(L_B)\]
            \item Se $A$ e $B$ sono \textbf{equivalenti per colonne}, allora 
            \[\ker(L_A) \neq \ker(L_B) \land \im(L_A) = \im(L_B)\]
        \end{itemize}
        
        In entrambi i casi si verifica che:
        \[\rk(A) = \rk(B)\]
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Basti pensare che effettuando solo operazioni su righe non si vadano ad alterare i vettori $x \in \ker(L_A)$ in grado di risolvere il sistema $Ax=0$, dunque $\ker(L_A) = \ker(L_B)$, mentre i vettori $b \in \im(L_A)$ generati dal sistema $Ax=b$ vanno ad essere modificati, dunque $\im(L_A) \neq \im(L_B)$
        \item Analogamente, effettuando solo operazioni su colonne non si vanno ad alterare i vettori $b \in \im(L_A)$ generati dal sistema $Ax=b$, dunque $\im(L_A) = \im(L_B)$, mentre i vettori $x \in \ker(L_A)$ risolventi il sistema $Ax=b$ vanno ad essere modificati, dunque $\ker(L_A) \neq \ker(L_B)$
        \item Nel caso in cui il nucleo non venga alterato, si ha che:
        \[\rk(A) = \dim(\im(L_A)) = n - \dim(\ker(L_A)) =\]
        \[=n - \dim(\ker(L_B)) = \dim(\im(L_B) = \rk(B)\]
        mentre nel caso in cui l'immagine non venga alterata si ha che:
        \[\rk(A) = \dim(\im(L_A)) = \dim(\im(L_B)) = \rk(L_B)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Pivot e Matrice a scala}
        Data una matrice $A \in \matspace{m}{n}{K}$, definiamo come \textbf{pivot} di una riga il primo elemento non nullo a partire da sinistra di tale riga.
        
        Inoltre, la matrice $A$ viene detta \textbf{matrice a scala} se $\forall i \in [i, m]$ si verifica che il pivot della riga $A_i$ è più a sinistra pivot della riga $A_{i+1}$
    \end{frameddefn}
    
    \textbf{Esempi:}
    
    \begin{itemize}
        \item Le seguenti matrici sono a scala, i cui pivot sono cerchiati:
    
        \[\left ( \begin{NiceArray}{ccccc}[left-margin=2pt]
      1 & 0 & 2 & 0 & -1 \\
      0 & 1 & 0 & 3 & 4 \\
      0 & 0 & 0 & 1 & 2 \\
      0 & 0 & 0 & 0 & 0
    \CodeAfter
      \begin{tikzpicture} [shorten < = 2pt , shorten > = 2pt, radius = 2mm]
          \draw (1-1) circle ;
          \draw (2-2) circle ; 
          \draw (3-4) circle ; 
          \draw [very thin] (2-|1) -| (3-|2) -| (4-|4) -- (4-|5) -- (4-|6) ; 
      \end{tikzpicture}
    \end{NiceArray} \right )
    \qquad\qquad
    \left ( \begin{NiceArray}{ccccc}[left-margin=2pt]
      0 & 1 & 2 & 3 & 4 \\
      0 & 0 & 0 & 5 & 6 \\
      0 & 0 & 0 & 0 & 7 \\
    \CodeAfter
      \begin{tikzpicture} [shorten < = 2pt , shorten > = 2pt, radius = 2mm]
          \draw (1-2) circle ;
          \draw (2-4) circle ; 
          \draw (3-5) circle ; 
          \draw [very thin] (1-|1) -| (2-|2) -| (3-|4) -| (4-|5) -- (4-|6) ; 
      \end{tikzpicture}
    \end{NiceArray} \right )\]
    
    \item Le seguenti matrici non sono a scala:
    \[ \left ( \begin{array}{ccc}
        1 & 3 & 0\\
        0 & 3 & 3\\
        0 & 9 & 2
    \end{array}\right )
    \qquad\qquad
    \left ( \begin{array}{ccc}
        1 & 2 & 6\\
        0 & 0 & 3\\
        8 & 0 & 0
    \end{array}\right )
    \]
    \end{itemize}

    \begin{framedalgo}{Algoritmo di Gauss-Jordan}
        Ogni matrice $A \in \matspace{n}{m}{K}$ può essere ridotta ad una \textbf{matrice a scala} $S \in \matspace{n}{m}{K}$ tramite il seguente algoritmo:
        
        \begin{enumerate}
            \item Sia $A^j$, dove $j \in [1,n]$ la prima colonna a partire da sinistra non nulla, ossia $\exists i [1,n] \mid c := a_{i,j} \neq 0$
            \item Se $c \notin A_1$, allora la $i$-esima riga contenente $c$ viene scambiata con $A_1$
            \item Per $k = 2, \ldots, m$, sottraiamo $\lambda A_1$ alla riga $A_k$, dove $\lambda$ è uno scalare scelto apposta in modo da annullare l'$i$-esimo elemento della
            \item Se la matrice risultante non è ancora ridotta a scala, allora viene ripetuto ricorsivamente l'algoritmo su $A_2$, ignorando le prime $i$ colonne, e così via
        \end{enumerate}
    \end{framedalgo}
    
    \begin{framedthm}{Riduzione a scala}
        Sia $A = (A^1, \ldots, A^n) \in \matspace{m}{n}{K}$ e sia $S =(S^1, \ldots, S^n)$ la sua versione ridotta a scala.
        
        Se $S^{j_1}, \ldots, S^{j_h}$ sono le colonne di $S$ contenenti i pivot della scala, allora $A^{j_1}, \ldots, A^{j_h}$ sono una base di $\im(A) = \Span(A^1, \ldots, A^n)$, implicando che
        \[h = \rk(S) = \rk(A)\]
    \end{framedthm}
    
    \textbf{Esempio:}
    
    \begin{itemize}
    
        \item Consideriamo il seguente sistema e la matrice completa ad essa associata:
        
        \[ \left \{ \begin{array}{lll}
            x+2y+3z=0  \\
            4x+5y+6z=0 \\
            7x+8y+9z=0
        \end{array} \right .\implies A_b = \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 9
        \end{array}\right )\]

        dove il vettore incognito $b$ viene omesso dalla matrice completa in quanto $b = 0_{K^m}$
        
        \item Date le righe $R_1, \ldots, R_3$ della matrice, procediamo quindi con l'algoritmo di Gauss-Jordan fino ad ottenere la versione a scala di tale matrice:
        \[ \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 9
        \end{array}\right ) \xrightarrow{R_2-4R_1, R_3-7R_1} \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & -3 & -6\\
            0 & -6 & -12
        \end{array}\right ) \xrightarrow{R_3-2R_2} \left ( \begin{NiceArray}{ccc}[left-margin=2pt]
            1 & 2 & 3\\
            0 & -3 & -6\\
            0 & 0 & 0
        \CodeAfter
          \begin{tikzpicture} [shorten < = 2pt , shorten > = 2pt, radius = 2mm]
              \draw [very thin] (2-|1) -| (3-|2) -- (3-|4); 
          \end{tikzpicture}
        \end{NiceArray}\right )\]

        \item Dunque, otteniamo che $A^1$ e $A^2$ sono una base di $\im(L_A) = \Span(A^1, A^2, A^3)$ e il rango della matrice corrisponde a $\rk(A) = 2$
        
        \item Riducendo ancora tale matrice (mantenendo la forma a scala), si ha che:
        \[\left ( \begin{NiceArray}{ccc}[left-margin=2pt]
            1 & 2 & 3\\
            0 & -3 & -6\\
            0 & 0 & 0
        \CodeAfter
          \begin{tikzpicture} [shorten < = 2pt , shorten > = 2pt, radius = 2mm]
              \draw [very thin] (2-|1) -| (3-|2) -- (3-|4); 
          \end{tikzpicture}
        \end{NiceArray}\right ) \xrightarrow{-\frac{1}{3}R_2} \left ( \begin{NiceArray}{ccc}[left-margin=2pt]
            1 & 2 & 3\\
            0 & 1 & 2\\
            0 & 0 & 0
        \CodeAfter
          \begin{tikzpicture} [shorten < = 2pt , shorten > = 2pt, radius = 2mm]
              \draw [very thin] (2-|1) -| (3-|2) -- (3-|4); 
          \end{tikzpicture}
        \end{NiceArray} \right )
        \xrightarrow{R_1 - 2R_2}
        \left ( \begin{NiceArray}{ccc}[left-margin=2pt]
            1 & 0 & -1\\
            0 & 1 & 2\\
            0 & 0 & 0
        \CodeAfter
          \begin{tikzpicture} [shorten < = 2pt , shorten > = 2pt, radius = 2mm]
              \draw [very thin] (2-|1) -| (3-|2) -- (3-|4); 
          \end{tikzpicture}
        \end{NiceArray} \right )\]

        \item Dunque, il sistema di partenza risulta essere equivalente al seguente:
        
        \[ \left \{ \begin{array}{lll}
            x+2y+3z=0  \\
            4x+5y+6z=0 \\
            7x+8y+9z=0
        \end{array} \right .\iff \left \{ \begin{array}{l}
            x-z=0\\
            y+2z=0
        \end{array}\right .\]

        \item Poiché abbiamo svolto solo operazioni per riga, la matrice originale e la sua versione a scala risultano essere equivalenti per righe, implicando che il nucleo non sia stato alterato. 
        
        \item Di conseguenza, risolvendo tale sistema in funzione di una variabile ausiliaria $t$, siamo in grado di ottenere una base di $\ker(L_A)$:
        \[ \left \{ \begin{array}{l}
            x-z=0\\
            y+2z=0
        \end{array}\right .
        \implies
        \left \{ \begin{array}{l}
            x=z\\
            y=-2z
        \end{array}\right .
        \implies
         \left \{ \begin{array}{l}
            x=t\\
            y=-2t\\
            z=t
        \end{array}\right .
        \implies
        \left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right ) = t\left (\begin{array}{c}
            1\\
            -2\\
            1
        \end{array}\right )\]        
    
    \item Infine, quindi, concludiamo che:
    \[\left ( \begin{array}{ccc}
            1 \\
            4 \\
            7 
        \end{array}\right ),
        \left ( \begin{array}{ccc}
            2\\
            5\\
            8 
        \end{array}\right ) \text{ base di } \im(L_A)
        \qquad\qquad \left (\begin{array}{c}
            1\\
            -2\\
            1
        \end{array}\right ) \text{ base di } \ker(L_A)\]
    \end{itemize}
    \quad
    
    \section{Teorema di Rouché-Capelli}
    
    \quad
    
    
    \begin{framedthm}{Teorema di Rouché-Capelli}
        Data una matrice di coefficienti $A \in \matspace{m}{n}{K}$ e un vettore di coefficienti $b \in K^m$, il sistema $Ax=b$ \textbf{ammette soluzioni} se e solo se $\rk(A) = \rk(A_b)$, dove $A_b$ è la matrice completa associata al sistema.
        \[\exists x \in K^n \mid Ax=b \iff \rk(A) = \rk(A_b)\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Dato il sistema $Ax=b$, si verifica che:
        \[\exists x \in K^n \mid Ax = b \iff \exists x_1, \ldots, x_n \in K \mid x_1A^1+\ldots+x_nA^n=b \iff\]
        \[ \iff b \in \Span(A^1, \ldots, A^n) \iff \Span(A^1, \ldots, A^n) = \Span(A^1, \ldots, A^n, b) \iff\]
        \[ \iff \dim(\Span(A^1, \ldots, A^n)) = \dim(\Span(A^1, \ldots, A^n, b)) \iff \rk(A) = \rk(A_b)\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{}
        Dato il sistema $Ax=b$, l'\textbf{insieme delle soluzioni} $V$ (se esistenti) è un \textbf{sottospazio affine} di $\ker(L_A)$ di dimensione $\dim(V) = n-\rk(A)$, dove in particolare si ha che:
        \begin{itemize}
            \item Se $\rk(A) = \rk(A_b) = n$, la soluzione al sistema è \textbf{unica} e il sistema viene detto \textbf{determinato}
            \[\exists! x \in K^n \mid Ax=b \iff \rk(A) = \rk(A_b) = n\]
            \item Se $\rk(A) = \rk(A_b) \neq n$, allora il sistema ammette \textbf{infinite soluzioni} e il sistema viene detto \textbf{indeterminato}
            \[\exists x \in K^n \mid Ax=b \iff \rk(A) = \rk(A_b) \neq n\]
        \end{itemize}
    \end{framedprop}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dato $V = \{x \in K^n \mid Ax=b\}$ l'insieme delle soluzioni del sistema (ipotizzando che ne esista almeno una), si ha che:
        \[x_0,x \in V \iff Ax_0 = b = Ax \iff Ax_0 = Ax \iff A(x-x_0) = 0 \]
        \[\iff x'-x_0 \in \ker(L_A) \iff x' \in x_0 + \ker(L_A)\]
        dunque $V$ è un sottospazio affine a $\ker(L_A)$ traslato da una soluzione particolare $x_0$ del sistema
        \item In quanto sottospazio affine del nucleo, ne segue che:
        \[\dim(V) = \dim(\ker(L_A)) = \dim(K^n) - \dim(\im(L_A)) = n - \dim(\im(L_A)) \]
        \[=n-\rk(A)= \left \{ \begin{array}{ll}
            0 & \text{ se } \rk(A) = n\\
            >1 & \text{ se }\rk(A) \neq n
        \end{array} \right .\]
        \item Dunque, nel caso in cui $\rk(A) =n$ l'unica soluzione all'interno di $V$ sarà $x_0$, altrimenti esisteranno infinite soluzioni generate dalla traslazione della base di $\ker(L_A)$
        
        $\hfill\qed$
    \end{itemize}
    
    
    \textbf{Esempi:}
    \begin{enumerate}
        \item \begin{itemize}
            \item Consideriamo il seguente sistema lineare:
            \[ \left \{ \begin{array}{l}
                w+2x+z=1\\
                2w+4x+y=3\\
                w+2x+y-z=2
            \end{array}
            \right . \implies
            A_b = \left ( \begin{array}{cccc|c}
                1 & 2 & 0 & 1 & 1\\
                2 & 4 & 1 & 0 & 3\\
                1 & 2 & 1 & -1 & 2
            \end{array}\right ) \xrightarrow{R_3-R_1, R_2-2R_1}\]
            \[ \xrightarrow{R_3-R_1, R_2-2R_1}
            \left ( \begin{array}{cccc|c}
                1 & 2 & 0 & 1 & 1\\
                0 & 0 & 1 & -2 & 1\\
                0 & 0 & 1 & -2 & 1
            \end{array}\right ) \xrightarrow{R_3-R_2}
            \left ( \begin{NiceArray}{cccc|c}[left-margin=1pt]
                1 & 2 & 0 & 1 & 1\\
                0 & 0 & 1 & -2 & 1\\
                0 & 0 & 0 & 0 & 0
                \CodeAfter
                  \begin{tikzpicture} [shorten < = 1pt , shorten > = 1pt, radius = 2mm]
                      \draw [very thin] (1-|1) -| (2-|1) -| (2-|3) -| (3-|3) -- (3-|6); 
                  \end{tikzpicture}
                \end{NiceArray} \right )\]
                
            \item Dunque, poiché $\rk(A) = \rk(A_b) = 2 \neq 4$, il sistema ammette infinite soluzioni e le colonne $A^1, A^3$ sono base di $\im(L_A)$. Inoltre, il sistema è equivalente a:
                
            \[ \left \{ \begin{array}{l}
                w+2x+z=1\\
                y-2z=1
            \end{array}\right .
            \iff
            \left \{ \begin{array}{l}
                w=1-2x-z\\
                y=1+2z
            \end{array}\right .
            \iff\]
            \[\iff
            \left \{ \begin{array}{l}
                w=1-2t_1-t_2\\
                x=t_1\\
                y=1+2t_2\\
                z=t_1\\
            \end{array}\right .
            \iff \left ( \begin{array}{c}
                w \\ x \\ y \\ z
            \end{array}\right ) =  \left ( \begin{array}{c}
                1 \\ 0 \\ 1 \\ 0
            \end{array}\right )+t_1 \left ( \begin{array}{c}
                -2 \\ 1 \\ 0 \\ 0
            \end{array}\right )+t_2  \left ( \begin{array}{c}
                -1 \\ 0 \\ 2 \\ 1
            \end{array}\right )\]
        \end{itemize}
        
        \quad
        
        \item \begin{itemize}
            \item Consideriamo il seguente sistema
            
            \[  \left \{ \begin{array}{l}
                    x+y+z=1\\
                    -x+y+5z=0\\
                    2y+6z=0
                \end{array}\right . \implies
            A_b = \left ( \begin{array}{ccc|c}
                    1 & 1 & 1 & 1\\
                    -1 & 1 & 5 & 0\\
                    0 & 2 & 6 & 0
            \end{array} \right ) \xrightarrow{R_2+R_1}\]
            
            \[ \xrightarrow{R_2+R_1} \left ( \begin{array}{ccc|c}
                1 & 1 & 1 & 1\\
                0 & 2 & 6 & 1\\
                0 & 2 & 6 & 0
            \end{array} \right ) \xrightarrow{R_3-R_2}
            \left ( \begin{NiceArray}{ccc|c}[left-margin=1pt]
                1 & 1 & 1 & 1\\
                0 & 2 & 6 & 1\\
                0 & 0 & 0 & -1
            \CodeAfter
              \begin{tikzpicture} [shorten < = 1pt , shorten > = 1pt, radius = 2mm]
                  \draw [very thin] (1-|1) -| (2-|1) -| (2-|2) -| (3-|2) -| (4-|4) -- (4-|5); 
              \end{tikzpicture}
            \end{NiceArray} \right )\]
            
            \item Poiché $\rk(A) \neq \rk(A_b)$, il sistema non ammette soluzioni
        \end{itemize}
            
        \quad
            
        \item \begin{itemize}
            
            \item Consideriamo il seguente sistema:
            
            \[\left \{ \begin{array}{l}
                x+ky=4-k\\
                kx+4y=4
            \end{array}\right . \implies A_b = \left ( \begin{array}{cc|c}
                1 & k & 4-k\\
                k & 4 & 4
            \end{array} \right ) \xrightarrow{R_2-kR_1}\]
            \[\xrightarrow{R_2-kR_1} \left ( \begin{array}{cc|c}
                1 & k & 4-k\\
                0 & 4-k^2 & 4-2k+k^2
            \end{array} \right ) = \left ( \begin{array}{cc|c}
                1 & k & 4-k\\
                0 & 4-k^2 & (2-k)^2
            \end{array} \right )
            \]
            
            \item A questo punto, a seconda del valore di $k$ si verificano tre casi:
            
            \begin{itemize}
                \item Se  $k \neq \pm 2$, ne segue che $4-k^2\neq 0$ e di conseguenza che $\rk(A)=\rk(A_b)=2$, implicando che il sistema ammetta un'unica soluzione
                
                \item Se $k = -2$, si ha che $\rk(A)\neq \rk(A_b)$, dunque il sistema non ammette soluzioni
                
                \item Se $k = 2$, si ha che $\rk(A)=\rk(A_b)=1 \neq 2$, dunque il sistema ammette infinite soluzioni
                \[A_b = \left ( \begin{array}{cc|c}
                1 & 2 & 2\\
                0 & 0 & 0
                \end{array} \right )
                \iff
                \left \{ x+2y=2 \right . \implies
                \left ( \begin{array}{c}
                    x\\y
                \end{array}\right ) = \left ( \begin{array}{c}
                    2\\0
                \end{array}\right )+t\left ( \begin{array}{c}
                    -2\\2
                \end{array}\right )  \]
            \end{itemize}
        \end{itemize}
    \end{enumerate}
        
    \quad
    
    
    \subsection{Equazioni parametriche}
    
    \quad
    
    \begin{framedprop}{Equazioni parametriche}
        Sia $V$ un sottospazio e sia $W$ un sottospazio affine al un sottospazio $\mathrm{Giac}(W) \subseteq V$, dunque $\exists x_0 \in V \mid W = x_0 + \mathrm{Giac}(W)$.
        
        Data la base $g_1, \ldots, g_n$ di $\mathrm{Giac}(W)$, si verifica che:
        \[ \forall x \in W, \exists! t_1, \ldots, t_n \in K \mid x = x_0+t_1g_1+\ldots+t_ng_n\]
        Definiamo l'insieme di tali equazioni come \textbf{equazioni parametriche} di $W$
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Dato $x_0 \in V \mid W = x_0 + \mathrm{Giac}(W)$ e data la base $g_1, \ldots, g_n$ di $\mathrm{Giac}(W)$, si ha che:
        \[ x \in W \iff x-x_0 \in \mathrm{Giac}(W) \iff\]
        \[ \iff \exists! t_1, \ldots, t_n \in K \mid x-x_0 = t_1g_1+\ldots+t_ng_n\]
        \[ \iff x = x_0+t_1g_1+\ldots+t_ng_n\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{}
        Dato uno spazio vettoriale $V$ e un sottospazio affine $W$ espresso come l'insieme delle sue equazioni parametriche:
        \[ W = \{ x_0+t_1g_1+\ldots+t_ng_n \mid t_1, \ldots, t_n \in K\}\]
        
        dove $g_1, \ldots, g_n$ sono base di $\mathrm{Giac}(W)$, si verifica che:
        \[x \in W \iff \rk(A_b) = \dim(V)\]
        dove $A_b := (g_1, \ldots, g_n, x-x_0) \in \matspace{n}{n}{K}$
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Data la matrice $A_b := (g_1, \ldots, g_n, x-x_0)$, per il teorema di Rouché-Capelli, si ha che:
        \[x \in W \iff x-x_0 \in \mathrm{Giac}(W) = \Span(g_1, \ldots, g_n) \iff \]
        \[ \iff \Span(g_1, \ldots, g_n) = \Span(g_1, \ldots, g_n, x-x_0) \iff\]
        \[ \iff \dim(\Span(g_1, \ldots, g_n)) = \dim(\Span(g_1, \ldots, g_n, x-x_0)) \iff\]
        \[ \iff \dim(\mathrm{Giac}(W)) = \rk(A_b) \iff \dim(W) = \rk(A_b)\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Consideriamo il seguente insieme di equazioni parametriche corrispondenti ad un sottospazio affine in $\R^3$
        \[ V = \left \{ \left ( \begin{array}{c}
            1 \\ 0 \\ -1
        \end{array}\right ) + t_1 \left ( \begin{array}{c}
            1 \\ 2 \\ 3
        \end{array}\right )+t_2 \left ( \begin{array}{c}
            4 \\ 5 \\ 6
        \end{array}\right ) \mid t_1, t_2 \in \R \right \} \]
        
        \item Poiché $\dim(V)=2$, si verifica che
        
        \[\left ( \begin{array}{c}
            x \\ y \\ z
        \end{array}\right ) \in V \iff \rk\left ( \begin{array}{ccc}
            1 & 4 & x-1\\
            2 & 5 & y\\
            3 & 6 & z+1
        \end{array}\right )=2\]
        
        \item Effettuando la riduzione a scala di tale matrice, si ha che:
        \[ \left ( \begin{array}{ccc}
            1 & 4 & x-1\\
            2 & 5 & y\\
            3 & 6 & z+1
        \end{array}\right ) \xrightarrow{R_2 -2R_1} \left ( \begin{array}{ccc}
            1 & 4 & x-1\\
            0 & -3 & y-2x+2\\
            3 & 6 & z+1
        \end{array}\right )\]
        \[\xrightarrow{R_3-3R_1} \left ( \begin{array}{ccc}
            1 & 4 & x-1\\
            0 & -3 & y-2x+2\\
            0 & -6 & z-3x+4
        \end{array}\right )\xrightarrow{R_3-2R_2} \left ( \begin{array}{ccc}
            1 & 4 & x-1\\
            0 & -3 & y-2x+2\\
            0 & 0 & z+x-2y
        \end{array}\right )\]
        
        \item Affinché la riduzione in scala abbia solo 2 pivot, è necessario che l'ultima riga della matrice contenga tutti zeri, implicando quindi che:
        \[ \rk(A)=2 \iff z+x-2y=0\]
        
        \item L'insieme di equazioni parametriche dato, quindi, equivale al seguente sistema di equazioni cartesiane: 
        \[\left \{ \begin{array}{l}
            x-2y+z=0
        \end{array}\right .\]
        corrispondente ad una retta in $\R^3$
    \end{itemize}
    
    \quad
    
    \section{Determinante di una matrice}
    
    \quad
    
    \begin{frameddefn}{Trasformazione multilineare}
        Una trasformazione lineare del tipo
        \[f : V_1 \times \ldots \times V_k \to W : (v_1, \ldots, v_k) \to f(v_1, \ldots, v_k)\]
        viene detta \textbf{multilineare} se $\forall i \in [1,k]$ dati $\lambda, \mu \in K$ si verifica che:
        \[ f(v_1, \ldots, \lambda v_i'+\mu v_i'', \ldots, v_n) = \lambda f(v_1, \ldots, v', \ldots v_n) + \mu f(v_1, \ldots, v'', \ldots v_n)\]
    \end{frameddefn}
    
    
    \begin{frameddefn}{Determinante di una matrice}
        Definiamo come \textbf{determinante} di una matrice l'unica trasformazione lineare
        \[\det : \matspace{n}{n}{K} \to K\]
        che verifica le seguenti tre proprietà:
        \begin{enumerate}
            \item $\det$ è \textbf{multilineare} su righe e colonne della matrice
            \item $A_1, \ldots, A_n$ e $A^1, \ldots, A^n$ sono \textbf{basi} di $K^n$ se e solo se $\det(A) \neq 0$
            \item $\det(I_n) = 1$, dove $I_n$ è la matrice identità di ordine $n$
        \end{enumerate}
    \end{frameddefn}
    
    \begin{framedobs}{}
        Data una matrice $A \in M_{n \times n}(K)$, si ha che:
        
        \begin{itemize}
            \item $\exists A_i, A_j \in A, i \neq j \mid A_i = A_j \implies \det(A) = 0$
            \item $\exists A^i, A^j \in A, i \neq j \mid A^i = A^j \implies \det(A) = 0$
            \item $\exists A_i \in A \mid A_i = 0_{K^n} \implies \det(A) = 0$
            \item $\exists A^i \in A \mid A^i = 0_{K^n} \implies \det(A) = 0$
        \end{itemize}
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se esistono due righe $A_i,A_j \in A$ uguali, allora $\Span(A_1, \ldots, A_n)$ non è linearmente indipendente, dunque non può costituire base di $K^n$
        \item Se esiste una riga $A_h \in A$ uguale al vettore nullo, allora $\Span(A_1, \ldots, A_n)$ non è né linearmente indipendente né generatore di $K^n$, dunque non può costituire base di $K^n$
        \item Per le colonne vale il ragionamento analogo alle righe
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Determinante alternante su righe e colonne}
        Data una matrice $A = (A_1, \ldots, A_i, \ldots A_j, \ldots, A_n) \in \matspace{n}{n}{K}$, si verifica che:
        \[ \det(A_1, \ldots, A_i, \ldots A_j, \ldots, A_n) = -\det(A_1, \ldots, A_j, \ldots A_i, \ldots, A_n)\]
        dunque, scambiando due righe (o colonne) della matrice il \textbf{determinante cambia segno}
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Data una matrice $A = (A_1, \ldots, A_i+A_j, \ldots A_i+A_j, \ldots, A_n)$, per la proprietà $2)$ del determinante si ha che:
        \[ \det(A_1, \ldots, A_i+A_j, \ldots A_i+A_j, \ldots, A_n) = 0\]
        
        \item Per multilinearità del determinante, si ha che:
        \[ 0 = \det(A_1, \ldots, A_i+A_j, \ldots A_i+A_j, \ldots, A_n) \implies \]
        \[ \implies 0 = \det(A_1, \ldots, A_i, \ldots A_i+A_j, \ldots, A_n) + \det(A_1, \ldots, A_j, \ldots A_i+A_j, \ldots, A_n) \implies \]
        \[ \implies 0 = \det(A_1, \ldots, A_i, \ldots A_i, \ldots, A_n) + \det(A_1, \ldots, A_i, \ldots A_j, \ldots, A_n) + \]
        \[+\det(A_1, \ldots, A_j, \ldots A_i, \ldots, A_n) + \det(A_1, \ldots, A_j, \ldots A_j, \ldots, A_n) \implies \]
        \[ \implies 0 = 0 + \det(A_1, \ldots, A_i, \ldots A_j, \ldots, A_n) + \det(A_1, \ldots, A_j, \ldots A_i, \ldots, A_n) + 0 \implies \]
        \[ \implies \det(A_1, \ldots, A_i, \ldots A_j, \ldots, A_n) = - \det(A_1, \ldots, A_j, \ldots A_i, \ldots, A_n)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Teorema di Binet}
        Date due matrici $A,B \in \matspace{n}{n}{K}$, si ha che:
        \[ \det(AB) = \det(A) \cdot \det(B)\]
        
        \textit{(dimostrazione omessa)}
    \end{framedthm}
    
    \quad
    
    \subsection{Formula di Leibniz e Regola di Sarrus}
    
    \quad
    
    \begin{frameddefn}{Formula di Leibniz}
        Tramite le sue proprietà $1)$, $3)$ e la sua alternanza su righe e colonne, il determinante di una matrice $A \in \matspace{n}{n}{K}$ può essere definito come:
        \[ \det(A) := \sum_{\sigma \in S_n} \sgn(\sigma) \cdot a_{1, \sigma(1)} \cdot \ldots \cdot a_{n, \sigma(n)}\]
        
        \textit{(dimostrazione omessa)}
    \end{frameddefn}
    
    \begin{framedcor}{}
        Se $A \in \matspace{2}{2}{K}$, allora
        \[ A = \left ( \begin{array}{cc}
            a & b\\
            c & d
        \end{array}\right) \implies \det(A) = ad-bc\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia 
        \[ A = \left ( \begin{array}{cc}
            a & b\\
            c & d
        \end{array}\right) :=\left ( \begin{array}{cc}
            a_{1,1} & a_{1,2}\\
            a_{2,1} & a_{2,2}
        \end{array}\right) \]
        
        \item Dato $S_2 = \{ (1)(2), (12)\}$, si verifica che:
        \[ \det(A) = \sum_{\sigma \in S_2} \sgn(\sigma) \cdot a_{1, \sigma(1)} \cdot a_{2, \sigma(2)} = a_{1,1}a_{2,2}-a_{1,2}a_{2,1} = ad-bc\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
         Se $A \in \matspace{3}{3}{K}$, allora
        \[ A = \left ( \begin{array}{ccc}
            a & b & c\\
            d & e & f\\
            g & h & i
        \end{array}\right) \implies \det(A) = aei+bfg+cdh-afh-bdi-ceg\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia 
        \[ A = \left ( \begin{array}{ccc}
            a & b & c\\
            d & e & f\\
            g & h & i
        \end{array}\right) := \left ( \begin{array}{ccc}
            a_{1,1} & a_{1,2} & a_{1,3}\\
            a_{2,1} & a_{2,2} & a_{2,3}\\
            a_{3,1} & a_{3,2} & a_{3,3}\\
        \end{array}\right) \]
        
        \item Dato $S_3 = \{ (1)(2)(3), (12)(3), (1)(23), (13)(2), (123), (321)\}$, si verifica che:
        \[ \det(A) = \sum_{\sigma \in S_3} \sgn(\sigma) \cdot a_{1, \sigma(1)} \cdot a_{2, \sigma(2)} \cdot a_{3, \sigma(3)} =\]
        \[=a_{1,1}a_{2,2}a_{3,3}-a_{1,2}a_{2,1}a_{3,3}-a_{1,1}a_{2,3}a_{3,2}-a_{1,3}a_{2,2}-a_{3,1}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2} =\]
        \[= aei-bdi-ceg-afh+bfg+cdh\]
        $\hfill\qed$
        
    \end{itemize}
    
    \begin{framedmeth}{Regola di Sarrus}
        
        La \textbf{Regola di Sarrus} permette di ricordare facilmente il calcolo del determinante di una matrice quadrata di ordine $3$, ricopiando a destra della matrice le sue prime due colonne, per poi \textbf{sommare le tre diagonali} e \textbf{sottrarre le tre anti-diagonali}:
        
        \begin{center}
            \includegraphics[scale=0.5]{images/sarrus.png}
        \end{center}
        
        \[ A \in \matspace{3}{3}{K} \implies \det(A) = aei+bfg+cdh-afh-bdi-ceg\]
        
    \end{framedmeth}
    
    \textbf{Esempi:}
    \begin{itemize}
        \item Data la matrice
        \[ A = \left ( \begin{array}{cc}
            1 & 2\\
            3 & 4
        \end{array}\right )\]
        si ha che:
        \[ \det(A) = 1 \cdot 4 - 2 \cdot 3 = -2\]
        
        \item Data la matrice
        \[ A = \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 0
        \end{array}\right )\]
        si ha che:
        \[ \det(A) = 1 \cdot 5 \cdot 0 + 2 \cdot 6 \cdot 7 + 3 \cdot 4 \cdot 8-2 \cdot 4 \cdot 0 - 3 \cdot 5 \cdot 7 - 1 \cdot 6 \cdot 8 = 27\]
    \end{itemize}
    
    \quad
    
    \subsection{Determinante tramite riduzione a scala}
    
    \quad
    
    \begin{frameddefn}{Matrice triangolare}
        Sia $A \in \matspace{n}{n}{K}$. Definiamo $A$ come \textbf{triangolare superiore} se $\forall i > j$ si ha che $a_{i,j} = 0$, ossia se sotto la diagonale principale vi sono tutti zeri
        
        \[ A = \left ( \begin{array}{ccccc}
            a_{1,1} & a_{1,2} & \cdots & \cdots & a_{1,n}\\
            0 & a_{2,2} & a_{2,3} & \ddots & \vdots\\
            \vdots & \ddots & \ddots & \ddots & \vdots\\
            \vdots & \ddots & 0 & a_{n-1,n-1} & a_{n-1,n}\\
            0 & \cdots & \cdots & 0& a_{n,n}\\
        \end{array}\right )\]
        
        o \textbf{triangolare inferiore} se $\forall i < j$ si ha che $a_{i,j} = 0$, ossia se sopra la diagonale principale vi sono tutti zeri
        \[ A = \left ( \begin{array}{ccccc}
            a_{1,1} & 0 & \cdots & \cdots & 0\\
            a_{1,2} & a_{2,2} & 0 & \ddots & \vdots\\
            \vdots & \ddots & \ddots & \ddots & \vdots\\
            \vdots & \ddots & a_{n-1,n-2} & a_{n-1,n-1} & 0\\
            a_{n,1} & \cdots & \cdots & a_{n,n-1}& a_{n,n}\\
        \end{array}\right )\]
    \end{frameddefn}
    
    \begin{framedobs}{}
        Una \textbf{matrice a scala} è sempre \textbf{triangolare}.
    \end{framedobs}
    
    \begin{framedobs}{}
        Se $A \in \matspace{n}{n}{K}$ è una matrice triangolare, allora il suo determinante corrisponde al \textbf{prodotto della sua diagonale}
        \[\det(A) = a_{1,1}\cdot a_{2,2}\cdot\ldots\cdot a_{n,n}\]
        
        \textit{(dimostrazione omessa)}
    \end{framedobs}

    \begin{framedmeth}{Calcolo del determinante tramite riduzione a scala}
        Data una matrice $A \in \matspace{n}{n}{K}$, è possibile ricavare il suo determinante tramite la sua riduzione a scala, poiché:
        \begin{enumerate}
            \item Scambiare due righe (o colonne) \textbf{inverte il segno del determinante}
            \item Moltiplicare una riga (o colonna) per uno scalare $\lambda \in K^*$ \textbf{moltiplica anche il determinante} per tale scalare
            \item Sommare ad una riga (o colonna) un multiplo di un'altra riga (o colonna) \textbf{non altera il determinante}
        \end{enumerate}
        
        Dunque, data la riduzione a scala $S$ della matrice $A$, è possibile calcolare $\det(A)$ tramite il calcolo di $\det(S)$, per poi \textbf{invertire gli effetti subiti dal determinante} in base alle operazioni svolte durante la riduzione.
    \end{framedmeth}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Sia $A = (A_1, \ldots, A_i, \ldots, A_j, \ldots, A_n) \in \matspace{n}{n}{K}$
        \item Abbiamo già dimostrato come scambiare due righe o colonne implichi che il determinante cambi segno, dunque
        \[ \det(A_1, \ldots, A_i, \ldots, A_j, \ldots, A_n) = -\det(A_1, \ldots, A_j, \ldots, A_i, \ldots, A_n)\]
        
        \item Data $A' = (A_1, \ldots, \lambda A_i, \ldots, A_j, \ldots, A_n)\in \matspace{n}{n}{K}$, per multilinearità del determinante si ha che:
        \[ \det(A') = \det(A_1, \ldots, \lambda A_i, \ldots, A_j, \ldots, A_n) = \]
        \[= \lambda \cdot \det(A_1, \ldots, A_i, \ldots, A_j, \ldots, A_n) = \lambda \det(A) \]
        
        
        \item Data $A'' = (A_1, \ldots, A_i+\mu A_j, \ldots, A_j, \ldots, A_n)\in \matspace{n}{n}{K}$, per multilinearità del determinante si ha che:
        \[ \det(A'') = \det(A_1, \ldots, A_i+\mu A_j, \ldots, A_j, \ldots, A_n) =\]
        \[=\det(A_1, \ldots, A_i, \ldots, A_j, \ldots, A_n)+ \mu \cdot \det(A_1, \ldots, A_j, \ldots, A_j, \ldots, A_n) =\]
        \[=\det(A_1, \ldots, A_i, \ldots, A_j, \ldots, A_n)+ \mu \cdot 0 = \det(A_1, \ldots, A_i, \ldots, A_j, \ldots, A_n) = \det(A)\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Riprendiamo la matrice dell'esempio precedente, il cui determinante sappiamo già essere 27:
        \[ A = \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 0
        \end{array}\right )\]
        
        \item Effettuiamo la riduzione a scala di tale matrice:
        \[ \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 0
        \end{array}\right ) \xrightarrow{R_2 -4R_1} \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & -3 & -6\\
            7 & 8 & 0
        \end{array}\right ) \xrightarrow{R_3 -7R_1} \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & -3 & -6\\
            0 & -6 & -21
        \end{array}\right ) \]
        \[ \xrightarrow{R_2 \cdot \rbk{-\frac{1}{3}}} \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & 1 & 2\\
            0 & -6 & -21
        \end{array}\right ) \xrightarrow{R_3 +6R_2} \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & 1 & 2\\
            0 & 0 & -9
        \end{array}\right )\]
        
        \item Poiché una matrice a scala è sempre triangolare, si ha che:
        \[S := \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & 1 & 2\\
            0 & 0 & -9
        \end{array}\right ) \implies \det(S) = 1 \cdot 1 \cdot (-9) = -9\]
        
        \item Tra i vari passaggi effettuati durante la riduzione a scala, l'unico ad influenzare il determinante è il terzo. Dunque, si ha che:
        \[ \det(S) = -\frac{1}{3} \cdot \det(A) \implies -9 = -\frac{1}{3} \cdot \det(A) \implies \det(A) = 27\]
    \end{itemize}
    
    \quad
    
    \quad
    
    \subsection{Sviluppo di Laplace}
    
    \quad
    
    \begin{frameddefn}{Sottomatrice}
        Data una matrice $A \in \matspace{m}{n}{K}$, definiamo come \textbf{sottomatrice di $A$} una matrice ottenuta cancellando un determinato numero di righe e/o colonne dalla matrice originale
    \end{frameddefn}
    
    \begin{frameddefn}{Minore di una matrice}
        Data una matrice $A \in \matspace{m}{n}{K}$, definiamo come \textbf{minori di $A$} tutte le sottomatrici quadrate ottenute da $A$.
        
        Denotiamo come $M_{i,j}$ il minore ottenuto cancellando la riga $i$ e la colonna $j$ dalla matrice $A$.
    \end{frameddefn}
    
    \begin{framedthm}{Sviluppo di Laplace}
         Data una matrice $A \in \matspace{n}{n}{K}$, lo \textbf{sviluppo di Laplace sulla $i$-esima riga di $A$} è definito come:
         \[ \det(A) = \sum_{k=1}^n (-1)^{i+k} \cdot a_{i,k} \cdot  \det(M_{i,k}) = \sum_{k=1}^n a_{i,k} \cdot cof_{i,k}(A)\]
         
         dove $cof_{i,k}(A) = (-1)^{i+k} \cdot \det(M_{i,k})$ viene detto il \textbf{cofattore (o complemento algebrico)} dell'entrata $a_{i,k}$.
         
         Analogamente, lo sviluppo di Laplace sulla $j$-esima colonna di $A$ è definito come:
         \[ \det(A) = \sum_{h=1}^n (-1)^{h+j} \cdot a_{h,j} \cdot  \det(M_{h,j}) = \sum_{h=1}^n a_{h,j} \cdot cof_{h,j}(A)\]
        \textit{(dimostrazione omessa)}
    \end{framedthm}
    
    \textbf{Esempi:}
    \begin{enumerate}
        \item \begin{itemize}
            \item Riprendiamo la matrice già vista in vari esempi precedenti, il cui determinante sappiamo già essere 27:
            \[ A = \left ( \begin{array}{ccc}
                1 & 2 & 3\\
                4 & 5 & 6\\
                7 & 8 & 0
            \end{array}\right )\]
            
            \item Effettuiamo lo sviluppo di Laplace su $A_3$:
        \end{itemize}
        
        \[ \det(A) = (-1)^{3+1} \cdot 7 \cdot \det \left ( \begin{array}{cc}
            2 & 3 \\
            5 & 6
        \end{array}\right ) + (-1)^{3+2} \cdot 8 \cdot \det \left ( \begin{array}{cc}
            1 & 3 \\
            4 & 6
        \end{array}\right ) + (-1)^{3+3} \cdot 0 \cdot \det \left ( \begin{array}{cc}
            1 & 2 \\
            4 & 5
        \end{array}\right ) = \]
        \[ = 7(2 \cdot 6 - 3 \cdot 5) - 8 (1 \cdot 6 - 3 \cdot 4) + 0 = -21 + 48 = 27\]
        
        \item \begin{itemize}
            \item Calcoliamo il determinante della seguente matrice quadrata di ordine 4:
            \[ A = \left ( \begin{array}{cccc}
                1 & 4 & 0 & -1\\
                2 & 3 & 5 & 4\\
                7 & 8 & 0 & -2\\
                0 & 1 & 0 & 6
            \end{array}\right )\]
            
            \item Effettuiamo lo sviluppo di Laplace su $A^3$:
            
            \[ \det(A) = 0 + (-1)^{2+3} \cdot 5 \cdot  \det \left ( \begin{array}{ccc}
                1 & 4 & -1\\
                7 & 8 & -2\\
                0 & 1 & 6
            \end{array}\right ) + 0 - 0 = -5 \cdot  \det \left ( \begin{array}{ccc}
                1 & 4 & -1\\
                7 & 8 & -2\\
                0 & 1 & 6
            \end{array}\right )\]
            
            \item Il calcolo di $\det(A)$ viene quindi ridotto al calcolo di $\det(M_{2,3})$, il quale può essere facilmente calcolato tramite la regola di Sarrus o tramite un nuovo sviluppo di Laplace
            
            \item Utilizzando la regola di Sarrus, abbiamo che:
            
            \[ \det(A) = -5 \cdot  \det \left ( \begin{array}{ccc}
                1 & 4 & -1\\
                7 & 8 & -2\\
                0 & 1 & 6
            \end{array}\right ) = -5 (48+0+(-7) - 168 - 0 - (-2)) = 625\]
            
            \item Utilizzando lo sviluppo di Laplace su $(M_{2,3})^1$, invece, abbiamo che:
            \[ \det(A) = -5 \cdot  \det \left ( \begin{array}{ccc}
                1 & 4 & -1\\
                7 & 8 & -2\\
                0 & 1 & 6
            \end{array}\right ) = -5 \left [(-1)^{1+1}\cdot1 \cdot \det \left ( \begin{array}{cc}
                8 & -2\\
                1 & 6
            \end{array}\right ) + \right .\]
            \[ \left . +(-1)^{2+1} \cdot 7 \cdot \det\left ( \begin{array}{ccc}
                4 & -1\\
                1 & 6
            \end{array}\right ) \right ] = -5[50-175] = 625\]
        \end{itemize}
    
    \end{enumerate}
    
    \quad
    
    \subsection{Regola di Cramer}
    
    \quad
    
    \begin{framedthm}{Regola di Cramer}
        Dato un sistema lineare $Ax = b$:
        \[ \left \{ \begin{array}{l}
            a_{1,1}x_1+\ldots+a_{1,n}x_n = b_1\\ 
            \vdots\\
            a_{m,1}x_1+\ldots+a_{m,n}x_n = b_m\\ 
        \end{array}\right .\]
        dove $A$ è una matrice di coefficienti tale che $\det(A) \neq 0$, allora il sistema ammette la seguente unica soluzione:
        \[ \left \{ \begin{array}{l}
           x_1 = \frac{1}{\det(A)} \cdot \det(b, A^2, \ldots, A^{n-1}, A^n)\\
           x_2 = \frac{1}{\det(A)} \cdot \det(A^1, b, \ldots, A^{n-1},A^n)\\
           \vdots\\
           x_{n-1} = \frac{1}{\det(A)} \cdot \det(A^1, A^2, \ldots, b, A^n)\\
           x_n = \frac{1}{\det(A)} \cdot \det(A^1, A^2, \ldots, A^{n-1},b)\\
        \end{array}\right .\]
        
        \textit{(dimostrazione omessa)}
    \end{framedthm}
    
    \newpage
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Consideriamo il sistema
        \[ \left \{ \begin{array}{l}
            x+2y+3z=1\\
            4x+5y+6z=0\\
            7x+8y=0
        \end{array}\right .\]
        
        la cui matrice dei coefficienti corrisponde è
        \[ A = \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 0
        \end{array}\right )\]
        
        dove $\det(A) = 27$
        
        \item Per la regola di Cramer, si ha che:
        
        \[ \left \{ \begin{array}{l}
           x = \frac{1}{27} \cdot \det \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            0 & 5 & 6\\
            0 & 8 & 0
        \end{array}\right )\\
        y = \frac{1}{27} \cdot \det \left ( \begin{array}{ccc}
            1 & 1 & 3\\
            4 & 0 & 6\\
            7 & 0 & 0
        \end{array}\right )\\
        z = \frac{1}{27} \cdot \det \left ( \begin{array}{ccc}
            1 & 2 & 1\\
            4 & 5 & 0\\
            7 & 8 & 0
        \end{array}\right )
        \end{array}\right . \iff \left \{ \begin{array}{l}
           x = \frac{1}{27} \cdot (-48) \\
        y = \frac{1}{27} \cdot (42) \\
        z = \frac{1}{27} \cdot (-3) 
        \end{array}\right . \iff \left \{ \begin{array}{l}
        x = -\frac{16}{9}\\
        y =  \frac{14}{9}\\
        z =  -\frac{1}{9}
        \end{array}\right . \]
    \end{itemize}
    
    \quad
    
    \section{Matrici inverse}
    
    \quad
    
    \begin{frameddefn}{Gruppo generale lineare}
        Definiamo come \textbf{gruppo generale lineare} il gruppo $(GL(n,K),\cdot)$ contenente tutte le matrici invertibili dell'anello $\matspace{n}{n}{K}$:
        \[ GL(n,K) = \{ A \in \matspace{n}{n}{K} \mid \exists A^{-1} \in \matspace{n}{n}{K}\}\]
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item $A,B,C \in GL(n,K), A(BC) = ABC = (AB)C$
        \item $\forall A\in GL(n,K), \exists! I_n \in GL(n,K) \mid AI_n = I_nA = A$
        \item $\forall A \in GL(n,K), \exists! A^{-1} \in GL(n,K) \mid AA^{-1} = A^{-1}A=I_n$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedthm}{Invertibilità di una matrice}
        Data una matrice $A \in \matspace{n}{n}{K}$, le seguenti condizioni sono equivalenti tra loro:
        
        \begin{enumerate}
            \item $A \in GL(n,K)$
            \item $\rk(A) = n$
            \item $\det(A) \neq 0$
            \item $A_1, \ldots, A_n$ sono una base di $K^n$
            \item $A^1, \ldots, A^n$ sono una base di $K^n$
            \item $A$ è equivalente per righe e colonne a $I_n$ 
        \end{enumerate}
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item $3) \iff 4),5)$
        
        \begin{itemize}
            \item Per definizione stessa di determinante, si ha che:
            \[\det(A) \neq 0 \iff \left \{ \begin{array}{l}
                A^1, \ldots, A^n \text{ base di }K^n\\
                A_1, \ldots, A_n \text{ base di }K^n\\
            \end{array} \right . \]
        \end{itemize}
        
        \item $2) \iff 3), 4)$
        \begin{itemize}
            \item Poiché $\dim(K^n)=n$ implica che $n$ vettori possono essere generatori se e solo se sono anche linearmente indipendenti, si ha che:
            \[ \rk(A) = n \iff \dim(\Span(A^1, \ldots, A^n)) = \dim(\Span(A_1, \ldots, A_n)) = n \iff\]
            \[ \iff \left \{ \begin{array}{l}
                \Span(A^1, \ldots, A^n) = K^n \iff A^1, \ldots, A^n \text{ sono base di } K^n\\
                \Span(A_1, \ldots, A_n) = K^n \iff A_1, \ldots, A_n \text{ sono base di } K^n\\
            \end{array} \right .\]
        \end{itemize}
        
        \item $1) \iff 2)$
        
        \begin{itemize}
            \item Supponiamo che $\exists! B = (B^1, \ldots, B^n) \in \matspace{n}{n}{K} \mid A \cdot B = B \cdot A = I_n$. Per definizione di prodotto tra matrici ne segue che:
            \[ AB = I_n \iff AB^1 = e_1, AB^2 = e_2, \ldots, AB^n = e_n \iff\]
            \[ \iff L_A(B^1) = e_1, L_A(B^2) = e_2, \ldots, L_A(B^n) = e_n \iff \]
            \[\iff e_1, e_2, \ldots, e_n \in \im(L_A) \iff  \im(L_A)=\Span(e_1, \ldots, e_n) = K^n \iff \]
            \[\iff \rk(A) = n\]
        \end{itemize}
        
        \newpage

        \item $2) \iff 6)$
        
        \begin{itemize}
            \item Se $\rk(A) = n$, è possibile ridurre a scala $A$ fino ad ottenere $I_n$
            \item Viceversa, poiché $\rk(I_n)=n$, se $I_n$ è equivalente per righe e colonne ad $A$ ne segue automaticamente che $\rk(A) = n$
        \end{itemize}
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedcor}{}
        È possibile definire il \textbf{gruppo generale lineare}  anche come:
        \[ GL(n,K) = \{ A \in \matspace{n}{n}{K} \mid \det(A) \neq 0\}\]
    \end{framedcor}
    
    \begin{framedobs}{}
         Data una matrice $A \in \matspace{n}{n}{K}$, si ha che:
         \[ \det(A^{-1}) = \det(A)^{-1}\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Per il teorema di Binet, si ha che:
        \[ 1 = \det(I_n) = \det(A \cdot A^{-1}) = \det(A)\det(A^{-1})  \implies \det(A)^{-1} = \det(A^{-1})\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Gruppo speciale lineare}
        Dato il gruppo $(GL(n,K), \cdot)$, definiamo $SL(n,K) \normsubgrp GL(n,K)$ come  \textbf{gruppo speciale lineare}, dove
        \[ SL(n,K) = \{ A \in GL(n,K) \mid \det(A)=1\}\]
    \end{frameddefn}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item $SL(n,K) \subgrp GL(n,K)$
        \begin{itemize}
            \item $\det(I_n) = 1 \implies I_n \in SL(n,K)$
            \item $A,B \in SL(n,K) \implies \det(AB) = \det(A)\det(B) = 1 \cdot 1 = 1 \implies AB \in SL(n,K)$
            \item $A \in SL(n,K) \implies \det(A^{-1}) = \det(A)^{-1}) = 1 \implies A^{-1} \in SL(n,K)$
        \end{itemize}

        \item Date le matrici $A,B \in SL(n,K)$, si ha che:
        \[A \sim_L B \iff A^{-1}B \in SL(n,K) \iff \det(A^{-1}B)=1 \iff \det(A^{-1})\det(B)=1 \iff\]
        \[\iff \det(A)^{-1}\det(B) = 1 \iff \det(A)^{-1} = \det(B)^{-1} \iff \det(A) = \det(B)\]
        \[A \sim_R B \iff AB^{-1} \in SL(n,K) \iff \det(AB^{-1})=1 \iff \det(A)\det(B^{-1})=1 \iff\]
        \[\iff \det(A)\det(B)^{-1} = 1 \iff \det(B)^{-1} = \det(A)^{-1} \iff \det(B) = \det(A)\]

        dunque le classi laterali sinistre e destre coincidono, implicando che $SL(n,K) \normsubgrp GL(n,K)$

        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Data una matrice $A \in GL(n,K)$, poiché è $A$ invertibile se e solo se equivalente per righe e colonne a $I_n$, si ha che
        \[(A \mid I_n), (I_n \mid B) \text{ equivalenti per righe} \iff B = A^{-1}\]
        
    \end{framedobs}
    
    \textbf{Esempi:}
    
    \begin{enumerate}
        \item \begin{itemize}
            \item Sia 
            \[ A = \left ( \begin{array}{cc}
                1 & 2\\
                3 & 4
            \end{array}\right )\]
            
            dove $\det(A) = -2$, dunque $A$ è invertibile
            
            \item Procedendo con l'algoritmo di Gauss-Jordan, si ha che:
            \[ (A \mid I_n) = \left ( \begin{array}{cc|cc}
                1 & 2 & 1 & 0\\
                3 & 4 & 0 & 1
            \end{array}\right ) \xrightarrow{R_2-3R_1} \left ( \begin{array}{cc|cc}
                1 & 2 & 1 & 0\\
                0 & -2 & -3 & 1
            \end{array}\right )\]
            \[  \xrightarrow{R_1 += R_2} \left ( \begin{array}{cc|cc}
                1 & 0 & -2 & 1\\
                0 & -2 & -3 & 1
            \end{array}\right ) \xrightarrow{-\frac{1}{3}R_2} \left ( \begin{array}{cc|cc}
                1 & 0 & -2 & 1\\
                0 & 1 & \frac{3}{2} & -\frac{1}{2}
            \end{array}\right ) = (I_n \mid B) \]
            \[ \implies A^{-1} = \left ( \begin{array}{cc}
               -2 & 1\\
                \frac{3}{2} & -\frac{1}{2}
            \end{array}\right )\]
            
            dove $\det(A^{-1}) = -\frac{1}{2}$
        \end{itemize}
        
        \item \begin{itemize}
            \item Sia 
            \[ A = \left ( \begin{array}{ccc}
                1 & 2 & 3\\
                4 & 5 & 6\\
                7 & 8 & 0
            \end{array}\right )\]
            
            dove $\det(A) = 27$, dunque $A$ è invertibile
            
            \item Procedendo con l'algoritmo di Gauss-Jordan, si ha che:
            \[ (A \mid I_N) = \left ( \begin{array}{ccc|ccc}
                1 & 2 & 3 & 1 & 0 & 0\\
                4 & 5 & 6 & 0 & 1 & 0\\
                7 & 8 & 0 & 0 & 0 & 1
            \end{array}\right ) \xrightarrow{R_2-4R_1} \left ( \begin{array}{ccc|ccc}
                1 & 2 & 3 & 1 & 0 & 0\\
                0 & -3 & -6 & -4 & 1 & 0\\
                7 & 8 & 0 & 0 & 0 & 1
            \end{array}\right )\]
            \[ \xrightarrow{R_3 += -7R_1} \left ( \begin{array}{ccc|ccc}
                1 & 2 & 3 & 1 & 0 & 0\\
                0 & -3 & -6 & -4 & 1 & 0\\
                0 & -6 & -21 & -7 & 0 & 1
            \end{array}\right ) \xrightarrow{R_3-2R_2} \left ( \begin{array}{ccc|ccc}
                1 & 2 & 3 & 1 & 0 & 0\\
                0 & -3 & -6 & -4 & 1 & 0\\
                0 & 0 & -9 & 1 & -2 & 1
            \end{array}\right )\]
            \[\xrightarrow{R_2 *= -\frac{1}{3}} \left ( \begin{array}{ccc|ccc}
                1 & 2 & 3 & 1 & 0 & 0\\
                0 & 1 & 2 & \frac{4}{3} & -\frac{1}{3} & 0\\
                0 & 0 & -9 & 1 & -2 & 1
            \end{array}\right ) \xrightarrow{-\frac{1}{9}R_3} \left ( \begin{array}{ccc|ccc}
                1 & 2 & 3 & 1 & 0 & 0\\
                0 & 1 & 2 & \frac{4}{3} & -\frac{1}{3} & 0\\
                0 & 0 & 1 & -\frac{1}{9} & \frac{2}{9} & -\frac{1}{9}
            \end{array}\right )\]
            \[ \xrightarrow{R_1-2R_2} \left ( \begin{array}{ccc|ccc}
                1 & 0 & -1 & -\frac{5}{3} & \frac{2}{3} & 0\\
                0 & 1 & 2 & \frac{4}{3} & -\frac{1}{3} & 0\\
                0 & 0 & 1 & -\frac{1}{9} & \frac{2}{9} & -\frac{1}{9}
            \end{array}\right ) \xrightarrow{R_1+R_3} \left ( \begin{array}{ccc|ccc}
                1 & 0 & 0 & -\frac{16}{9} & \frac{8}{9} & -\frac{1}{9}\\
                0 & 1 & 2 & \frac{4}{3} & -\frac{1}{3} & 0\\
                0 & 0 & 1 & -\frac{1}{9} & \frac{2}{9} & -\frac{1}{9}
            \end{array}\right )\]
            \[ \xrightarrow{R_2-2R_3} \left ( \begin{array}{ccc|ccc}
                1 & 0 & 0 & -\frac{16}{9} & \frac{8}{9} & -\frac{1}{9}\\
                0 & 1 & 0 & \frac{14}{9} & -\frac{7}{9} & \frac{2}{9}\\
                0 & 0 & 1 & -\frac{1}{9} & \frac{2}{9} & -\frac{1}{9}
            \end{array}\right ) \implies A^{-1} = \left ( \begin{array}{ccc}
                -\frac{16}{9} & \frac{8}{9} & -\frac{1}{9}\\
                \frac{14}{9} & -\frac{7}{9} & \frac{2}{9}\\
                -\frac{1}{9} & \frac{2}{9} & -\frac{1}{9}
            \end{array}\right ) \implies\]
            \[ \implies A^{-1} = \frac{1}{9} \left ( \begin{array}{ccc}
                -16 & 8 & -1\\
                14 & -7 & 2\\
                -1 & 2 & -1
            \end{array}\right )\]
            
            dove $\det(A^{-1}) = \frac{1}{27}$
        \end{itemize}
    \end{enumerate}
        
    \quad
    
    \begin{frameddefn}{Matrice trasposta}
        Data una matrice $A \in \matspace{m}{n}{K}$, definiamo come \textbf{matrice trasposta} la matrice $A^T \in \matspace{n}{m}{K}$ avente come $i$-esima riga la $i$-esima colonna della matrice $A$ e come $j$-esima colonna la $j$-esima colonna di $A$
        
        \[ A = \left ( \begin{array}{cccc}
            a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
            a_{2,1} & \ddots & \ddots & \vdots\\
            \vdots & \ddots & \ddots & \vdots\\
            a_{m,1} & \cdots & \cdots & a_{m,n}\\
        \end{array}\right ) \implies A^T = \left ( \begin{array}{cccc}
            a_{1,1} & a_{2,1} & \cdots & a_{m,1}\\
            a_{1,2} & \ddots & \ddots & \vdots\\
            \vdots & \ddots & \ddots & \vdots\\
            a_{1,n} & \cdots & \cdots & a_{m,n}\\
        \end{array}\right ) \]
    \end{frameddefn}
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Data la seguente matrice $A$, la sua trasposta corrisponde a:
        \[ A = \left ( \begin{array}{cccc}
            1 & 2 & 3 & 4\\
            5 & 6 & 7 & 8\\
            9 & 10 & 11 & 12
        \end{array}\right ) \implies A^T = \left ( \begin{array}{cccc}
            1 & 5 & 9\\
            2 & 6 & 10\\
            3 & 7 & 11\\
            4 & 8 & 12
        \end{array}\right )\]
    \end{itemize}
    
    \begin{framedobs}{}
        Date due matrici $A, B \in \matspace{n}{n}{K}$, si verifica che:
        \begin{itemize}
            \item $\det(A) = \det(A^T)$
            \item $(AB)^T = B^TA^T$
        \end{itemize}
        
        \textit{(dimostrazioni omessa)}
    \end{framedobs}
    
    \begin{frameddefn}{Matrice dei cofattori}
        Data una matrice $A \in \matspace{n}{n}{K}$, definiamo come \textbf{matrice dei cofattori} la matrice $cof(A) \in \matspace{n}{n}{K}$ avente come entrate i cofattori di ogni entrata della matrice $A$
        
        \[ A = \left ( \begin{array}{ccc}
            a_{1,1} &  \cdots & a_{1,n}\\
            \vdots &  \ddots & \vdots\\
            a_{n,1} &  \cdots & a_{n,n}\\
        \end{array}\right ) \implies cof(A) = \left ( \begin{array}{ccc}
            cof(A)_{1,1} & \cdots & cof(A)_{1,n}\\
            \vdots & \ddots & \vdots\\
            cof(A)_{n,1} & \cdots & cof(A)_{n,n}\\
        \end{array}\right ) \]
    
        dove ricordiamo che
        \[ cof(A)_{i,j} = (-1)^{i+j} \cdot \det(M_{i,j})\]
    \end{frameddefn}
    
    \begin{frameddefn}{Matrice aggiunta}
         Data una matrice $A \in \matspace{n}{n}{K}$, definiamo come \textbf{matrice aggiunta} la trasposta della matrice dei cofattori di $A$:
         \[ adj(A) = (cof(A))^T\]
    \end{frameddefn}
    
    \begin{framedthm}{Inversa di una matrice tramite aggiunta}
        Data una matrice $A \in \matspace{n}{n}{K}$ dove $\det(A) \neq 0$, si verifica che:
        \[ A^{-1} = \frac{1}{\det(A)} \cdot adj(A)\]
    \end{framedthm}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Come conseguenza dello sviluppo di Laplace, si verifica che:
        \[ A \cdot adj(A) = adj(A) \cdot A = \det(A) \cdot I_n \implies adj(A) \cdot A = \det(A) \cdot I_n \implies\]
        \[\implies adj(A) = \det(A) \cdot I_n \cdot A^{-1} \implies \det(A)^{-1} adj(A) = A^{-1}\]
        $\hfill\qed$
    \end{itemize}
    
    \newpage

    \textbf{Esempio:}
    
    \begin{itemize}
        \item Prendiamo ancora una volta la nostra solita matrice esempio, il cui determinante sappiamo essere $\det(A) = 27$:
        \[ A = \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            4 & 5 & 6\\
            7 & 8 & 0
        \end{array}\right )\]
        
        \item La sua matrice dei cofattori corrisponde a:
            \[ cof(A) = \left ( \begin{array}{ccc}
                cof_{1,1}(A) & cof_{1,2}(A) & cof_{1,3}(A)\\
                cof_{2,1}(A) & cof_{2,2}(A) & cof_{2,3}(A)\\
                cof_{3,1}(A) & cof_{3,2}(A) & cof_{3,3}(A)\\
            \end{array}\right ) = \left ( \begin{array}{ccc}
                -48 & 42 & -3\\
                24 & -21 & 6\\
                -3 & 6 & -3
            \end{array}\right )\]
            
        mentre la conseguente matrice aggiunta corrisponde a:
        \[ adj(A) = \left ( \begin{array}{ccc}
                -48 & 24 & -3\\
                42 & -21 & 6\\
                -3 & 6 & -3
            \end{array}\right )\]
            
        \item Dunque, la matrice inversa di $A$ corrisponde a:
        \[ A^{-1} = \frac{1}{27} \left ( \begin{array}{ccc}
                -48 & 24 & -3\\
                42 & -21 & 6\\
                -3 & 6 & -3
            \end{array}\right ) = \frac{1}{9} \left ( \begin{array}{ccc}
                -16 & 8 & -1\\
                14 & -7 & 2\\
                -1 & 2 & -1
            \end{array}\right )\]
    \end{itemize}
    
    \begin{framedcor}{}
        Data una matrice $A \in \matspace{2}{2}{K}$, si verifica che:
        \[ A = \left ( \begin{array}{cc}
        a & b\\
        c  & d
        \end{array}\right ) \implies A^{-1} = \frac{1}{ad-bc} \left ( \begin{array}{cc}
        d & -b\\
        -c & a
        \end{array}\right )\]
    \end{framedcor}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia 
        \[ A = \left ( \begin{array}{cc}
        a & b\\
        c & d
        \end{array}\right )\]
        
        \item La sua matrice aggiunta corrisponde a:
        \[ cof(A) =\left ( \begin{array}{cc}
        d & -c\\
        -b  & a
        \end{array}\right ) \implies adj(A) = (cof(A))^T = \left ( \begin{array}{cc}
        d & -b\\
        -c  & d
        \end{array}\right )\]
        
        \item Di conseguenza, la sua inversa sarà:
        \[ A^{-1} = \frac{1}{\det(A)} \cdot adj(A) = \frac{1}{ad-bc}\left ( \begin{array}{cc}
        d & -b\\
        -c  & d
        \end{array}\right ) \]
    \end{itemize}
    
    \quad
    
    \section{Teorema degli orlati}
    
    \quad
    
    \begin{frameddefn}{Orlato di un minore}
        Data una matrice $A \in \matspace{m}{n}{K}$ e dati un suo minore $M$ di ordine $k$ e un minore $M'$ di ordine $k+1$, definiamo $M'$ come \textbf{orlato di $M$} se quest'ultimo è anche un minore di $M'$
    \end{frameddefn}
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Consideriamo la matrice
        \[ A = \left ( \begin{array}{cccc}
            1 & 2 & 3 & 4 \\
            5 & 6 & 7 & 8 \\
            9 & 10 & 11 & 12
        \end{array}\right )\]
        
        e il suo minore $M$ di ordine 2 ottenuto eliminando le colonne 2 e 4 e la riga 3
        \[ M = \left ( \begin{array}{cc}
            1 & 3\\
            5 & 7
        \end{array}\right )\]
        
        \item Gli orlati di $M$ corrispondono a:
        \[ M_1'= \left ( \begin{array}{ccc}
            1 & 3 & 4\\
            5 & 7 & 8\\
            9 & 11 & 12
        \end{array}\right ) \qquad\quad
        M_2' = \left ( \begin{array}{ccc}
            1 & 2 & 3\\
            5 & 6 & 7\\
            9 & 10 & 11
        \end{array}\right )\]
    \end{itemize}
    
    \begin{framedobs}{}
        Data una matrice $A \in \matspace{m}{n}{K}$ e un suo minore $M$ di ordine $k$, esistono $(m-k)(n-k)$ orlati di $M$ in $A$
    \end{framedobs}
    
    \begin{framedthm}{Teorema degli orlati (o di Kronecker)}
        Data una matrice $A \in \matspace{m}{n}{K}$, si ha che:
        \[\rk(A)=k \iff \exists M \text{ minore di } A \mid \left \{ \begin{array}{l}
            \text{ordine di }M = k\\
            \det(M)\neq 0\\
            \det(M') = 0, \forall M' \text{ orlato di } M\\
        \end{array}\right .\]
        \textit{(dimostrazione omessa)}
    \end{framedthm}
    
    \newpage

    \textbf{Esempi:}
    
    \begin{enumerate}
        \item \begin{itemize}
            \item Vogliamo discutere il comportamento di tale sistema al variare dei parametri $a,b \in \R$:
            \[ \left \{ \begin{array}{l}
                ax+y+z=1\\
                x+ay+z=0\\
                x+y+az=b
            \end{array}\right . \implies A_b = \left ( \begin{array}{ccc|c}
                a & 1 & 1 & 1\\
                1 & a & 1 & 0\\
                1 & 1 & a & b
            \end{array}\right ) \implies\]
            \[ \implies \left \{ \begin{array}{lr}
                \rk(A_b) = \rk(A) & \text{ se } \dim(A^1, \ldots, A^n, b) = \dim(A^1, \ldots, A^n) \\
                \rk(A_b) = \rk(A)+1 & \text{ se } \dim(A^1, \ldots, A^n, b) \neq \dim(A^1, \ldots, A^n) \\
            \end{array}\right .\]
            
            \item Tramite la regola di Sarrus, otteniamo che il determinante corrisponde a:
            \[ \det(A) = a^3+1+1-a-a-a = a^3-3a+2 = (a+2)(a-1)^2\]
            
            \item Se $a \neq 1$ o $a \neq -2$ allora $\det(A) \neq 0$, implicando che $\rk(A) = 3$.
            
            Inoltre, siccome $\rk(A_b) \leq \min(3,4) = 3$ e siccome $\rk(A_b) = \rk(A)$ oppure $\rk(A_b) = \rk(A)+1$, allora ne segue necessariamente che $\rk(A) = \rk(A_b) = 3$.
            
            Per il teorema di Rouche-Capelli, lo spazio affine generato dalle soluzioni ha dimensione pari a 0, dunque il sistema è \textbf{determinato} ed esiste un'unica soluzione dipendente dai parametri assunti da $a$ e $b$.
            
            \item Se $a = 1$, allora $\det(A) = 0$, poiché radice del polinomio precedentemente trovato.
            
            Tuttavia, dalla riduzione a scala otteniamo che::
            \[ A_b = \left ( \begin{array}{ccc|c}
                1 & 1 & 1 & 1\\
                1 & 1 & 1 & 0\\
                1 & 1 & 1 & b
            \end{array}\right ) \xrightarrow{R_2 += -R_1} \left ( \begin{array}{ccc|c}
                1 & 1 & 1 & 1\\
                0 & 0 & 0 & -1\\
                1 & 1 & 1 & b
            \end{array}\right )\]
            \[ \xrightarrow{R_3 += -R_1} \left ( \begin{array}{ccc|c}
                1 & 1 & 1 & 1\\
                0 & 0 & 0 & -1\\
                0 & 0 & 0 & b-1
            \end{array}\right ) \xrightarrow{R_3 += -R_2} \left ( \begin{array}{ccc|c}
                1 & 1 & 1 & 1\\
                0 & 0 & 0 & -1\\
                0 & 0 & 0 & b
            \end{array}\right )\]
            \[ \xrightarrow{R_3 += b \cdot R_2} \left ( \begin{array}{ccc|c}
                1 & 1 & 1 & 1\\
                0 & 0 & 0 & -1\\
                0 & 0 & 0 & 0
            \end{array}\right )\]
            
            implicando che $\rk(A) = 1$ e $\rk(A_b) = 2$, dunque il sistema \textbf{non ammette soluzioni}.
            
            \item Se $a = -2$, allora $\det(A) = 0$, poiché radice del polinomio precedentemente trovato.
            
            Per il teorema degli orlati, si ha che l'unico orlato del minore $M_{3,3}$, dove $\det(M_{3,3}) = 3$ è la matrice $A$ stessa, che sappiamo avere determinante nullo, dunque $\rk(A) = 2$
            \[ A = \left ( \begin{array}{ccc}
                \color{red}-2 & \color{red}1 & \color{blue}1\\
                \color{red}1 & \color{red}-2 & \color{blue}1\\
                \color{blue}1 & \color{blue}1 & \color{blue}-2
            \end{array}\right ) \implies \rk(A)=2\]
            
            Nel caso della matrice $A_b$, invece, consideriamo il seguente minore:
            \[A_b = \left ( \begin{array}{ccc|c}
                \color{red}-2 & \color{red}1 & 1 & 1\\
                \color{red}1 & \color{red}-2 & 1 & 0\\
                1 & 1 & -2 & b
            \end{array}\right )\]
            
            Gli unici due orlati di tale minore sono:
            \[ \left \{
            \begin{array}{ll}
                M_1' = \left( \begin{array}{ccc}
                -2 & 1 & 1\\
                1 & -2 & 1\\
                1 & 1 & -2
            \end{array}\right ) = A & \implies \det(M_1') = \det(A) = 0\\
                M_2' = \left( \begin{array}{ccc}
                -2 & 1 & 1\\
                1 & -2 & 0\\
                1 & 1 & b
            \end{array}\right ) & \implies \det(M_2') = 4b+0+1+2-b = 3b+3
            \end{array}\right .\]
            
            Dunque, si ha che:
            \[ \rk(A) = \left \{ \begin{array}{ll}
                2 & \text{ se } a = -2, b = -1 \implies \rk(A)=\rk(A_b) \implies \exists \text{ inf. soluzioni}\\
                3 & \text{ se } a = -2, b \neq -1 \implies \rk(A)\neq \rk(A_b) \implies \nexists \text{ soluzioni}\\
            \end{array}\right .\]
            
            \item In particolare, se $a = -2$ e $b=-1$, possiamo trovare le infinite soluzioni del sistema in funzione di $x$:
            
            \[ \left \{ \begin{array}{l}
                -2x+y+z=1\\
                x-2y+z=0\\
                x+y-2z=-1
            \end{array}\right . \implies \left \{ \begin{array}{l}
                -2y+z = -x\\
                y -2z = 1-x\\
            \end{array}\right . \]
            
            applichiamo la regola di Cramer per trovare il valore di $y$ in funzione di $x$, per poi sostituire il valore ottenuto nella seconda equazione, trovando il valore di $z$ in funzione di $x$:
            \[\left \{ \begin{array}{l}
                y = \frac{1}{3} \cdot \det\left ( \begin{array}{cc}
                    -x & 1\\
                    -1-x & -2 
                \end{array}\right ) = \frac{1}{3}(3x+1) = x + \frac{1}{3}\\
                y -2z = 1-x\\
            \end{array}\right . \implies \left \{ \begin{array}{l}
                y = x + \frac{1}{3}\\
                z = \frac{2}{3}+x\\
            \end{array}\right . \]
            
            Le soluzioni del sistema indeterminato, quindi, appaiono nella forma:
            \[\left ( \begin{array}{c}
                    y\\
                    z
                \end{array}\right ) = \left ( \begin{array}{cc}
                    \frac{1}{3}\\
                    \frac{2}{3}
                \end{array}\right )+x\left ( \begin{array}{cc}
                    1\\
                    1
                \end{array}\right ) \]
                
            dunque generanti una retta (difatti, la dimensione dello spazio affine generato è $n-\rk(A) = 3-2 = 1$)
        \end{itemize}
        
        \item \begin{itemize}
            \item  Consideriamo il seguente insieme di equazioni parametriche corrispondenti ad un sottospazio affine in $\R^3$, già analizzato precedentemente:
        \[ V = \left \{ \left ( \begin{array}{c}
            1 \\ 0 \\ -1
        \end{array}\right ) + t_1 \left ( \begin{array}{c}
            1 \\ 2 \\ 3
        \end{array}\right )+t_2 \left ( \begin{array}{c}
            4 \\ 5 \\ 6
        \end{array}\right ) \mid t_1, t_2 \in \R \right \} \]
        
        \item Siccome $\dim(V)=2$, si verifica che
        
        \[\left ( \begin{array}{c}
            x \\ y \\ z
        \end{array}\right ) \in V \iff \rk(A) = \rk\left ( \begin{array}{ccc}
            1 & 4 & x-1\\
            2 & 5 & y\\
            3 & 6 & z+1
        \end{array}\right )=2\]
        
        \item Considerando il minore $M_{3,3}$, si ha che:
        
        \[ M_{3,3} = \left ( \begin{array}{cc}
            1 & 4\\
            2 & 5\\
        \end{array}\right ) \implies \det(M_{3,3}) = 5-8 = -3 \neq 0\]
        
        \item Siccome la matrice iniziale stessa è l'unico orlato di $M_{3,3}$, per il teorema degli orlati si verifica che:
        \[ \rk(A) = \left \{ \begin{array}{ll}
            2 & \text{ se } \det(A)=0\\
            3 & \text{ se } \det(A)\neq 0
        \end{array}\right . \]
        
        \item Dunque, si ha che:
        \[ \left ( \begin{array}{c}
            x \\ y \\ z
        \end{array}\right ) \in V \iff \rk(A) = 2 \iff \det(A) = 0\]
        
        \item Utilizzando lo sviluppo di Laplace sulla terza colonna, si ha che:
        \[ \det(A) = 0 \iff (x-1) \cdot \det\left ( \begin{array}{cc}
            2 & 5\\
            3 & 6
        \end{array}\right ) - y \cdot \det\left ( \begin{array}{cc}
            1 & 4\\
            3 & 6
        \end{array}\right ) + (z-1) \cdot \det\left ( \begin{array}{cc}
            1 & 4\\
            2 & 5
        \end{array}\right ) = 0 \]
        \[ \iff -3(x-1)+6y-3(z+1)  \iff -3x+6y-3z=0 \iff x-2y+z=0\]
        
        \item L'insieme di equazioni parametriche dato, quindi, equivale al seguente sistema di equazioni cartesiane: 
        \[\left \{ \begin{array}{l}
            x-2y+z=0
        \end{array}\right .\]
        corrispondente ad una retta in $\R^3$
        
        \end{itemize}
        
        \item \begin{itemize}
            \item Dati la seguente retta $r$ e il seguente piano $\pi$
            
            \[r = \left \{\begin{array}{l}
                x=-2+3a\\
                y = 1-2a\\
                z=5a
            \end{array} \right . \qquad\quad \pi = \left \{\begin{array}{l}
                x=4-2b+5c\\
                y=3b-c\\
                z=1+b-2c
            \end{array} \right .\]
            
            possiamo trovare l'intersezione $r \cap \pi$ in $\R^3$ nei seguenti tre modi:
            
            \begin{enumerate}
                \item Troviamo i valori di $a,b$ e $c$ unendo i due sistemi:
                
                \[ \left \{\begin{array}{l}
                    -2+3a=4-2b+5c\\
                    1-2a=3b-c\\
                    5a=1+b-2c
                \end{array} \right . = \left \{\begin{array}{l}
                    3a+2b-5c=6\\
                    2a+3b-c=1\\
                    5a-b+2c=1
                \end{array} \right . \implies A = \left ( \begin{array}{ccc|c}
                    3 & 2 & -5 & 6\\
                    2 & 3 & -1 & 1\\
                    5 & -1 & 2 & 1
                \end{array}\right )\]
            
                Siccome $\det(A)=82$ \textit{(calcolo omesso)}, possiamo applicare la regola di Cramer per trovare il valore di $a$, per poi sostituirlo nel sistema e trovare il valore di $b$ e $c$:
                
                \[ \left \{\begin{array}{l}
                    a = \frac{1}{82} \cdot \det\left ( \begin{array}{ccc}
                    6 & 2 & -5\\
                    1 & 3 & -1\\
                    1 & -1 & 2
                \end{array}\right ) = \frac{1}{82}\cdot 44 = \frac{22}{41}\\
                    c = 2a+3b-1\\
                    b = 5a+2c-1
                \end{array} \right . \implies \]
                \[ \left \{\begin{array}{l}
                    a = \frac{22}{41}\\
                    c = \frac{44}{41}+3b-1 = 3b+\frac{3}{41}\\
                    b = \frac{110}{41}+2c-1 = 2c+\frac{69}{41}
                \end{array} \right . \implies \left \{\begin{array}{l}
                    a = \frac{22}{41}\\
                    c = 3\left( 2c+\frac{69}{41}\right)+\frac{3}{41} = 6c+\frac{210}{41}\\
                    b = 2c+\frac{69}{41}
                \end{array} \right . \implies \]
                \[ \implies \left \{\begin{array}{l}
                    a = \frac{22}{41}\\
                    c = -\frac{42}{41}\\
                    b = 2 \left ( -\frac{42}{41}\right )+\frac{69}{41} = -\frac{15}{41}
                    \end{array} \right . \implies \left \{\begin{array}{l}
                    x=-2+3a \\
                    y = 1-2a\\
                    z=5a
                    \end{array} \right . = \left \{\begin{array}{l}
                        x=-\frac{16}{41}\\
                        y =-\frac{3}{41}\\
                        z=\frac{110}{41}
                    \end{array} \right .\]
                    
            \item Troviamo il sistema di equazioni cartesiane descriventi $\pi$:
            
            \[ \pi = \left \{\begin{array}{l}
                x=4-2b+5c\\
                y=3b-c\\
                z=1+b-2c
            \end{array} \right . \iff \left ( \begin{array}{c}
                x-4\\y\\z-1
            \end{array}\right ) = b\left ( \begin{array}{c}
                -2\\3\\1
            \end{array}\right )+c\left ( \begin{array}{c}
                5\\-1\\-2
            \end{array}\right ) \iff \]
            \[ \iff \rk\left ( \begin{array}{ccc}
                -2 & 5 & x-4\\
                3 & -1 & y\\
                1 & -2 & z-1
            \end{array}\right ) = 2\]
            
            \newpage

            Siccome $\det(M_{3,3}) = -13 \neq 0$, per il teorema degli orlati, si ha che:
            
            \[ \iff \rk\left ( \begin{array}{ccc}
                -2 & 5 & x-4\\
                3 & -1 & y\\
                1 & -2 & z-1
            \end{array}\right ) = 2 \iff \det\left ( \begin{array}{ccc}
                -2 & 5 & x-4\\
                3 & -1 & y\\
                1 & -2 & z-1
            \end{array}\right ) = 0 \iff \]
            \[ (x-4)(-5)-y(-1)+(z-1)(-13) = 0 \iff 5x-y+13z=33\]
            
            Siccome $x \in r \cap \pi \iff x \in r \land x \in \pi$, sostituendo i valori assunti da $x,y$ e $z$ nell'equazione cartesiana di $\pi$ otteniamo che:
            \[-5x+y-13z=33 \iff 5(-2+3a)-(1-2a)+13(5a)=33 \iff a = \frac{22}{41}\]
            
            Una volta trovato il valore di $a$, procediamo analogamente al metodo precedente, sostituendo $a$ nell'equazione parametrica di $\pi$ e ricavando $b$ e $c$ per sostituzione.
            
            \item Troviamo il sistema di equazioni cartesiane descriventi $\pi$ e il sistema di equazioni cartesiane descriventi $r$.
            
            Sappiamo già che:
            \[ \left ( \begin{array}{c}
                x \\ y \\ z
            \end{array} \right ) \in \pi \iff 5x-y+13z=33\]
            
            dunque ricaviamo le equazioni cartesiane di $r$:
            
            \[r = \left \{\begin{array}{l}
                x=-2+3a\\
                y = 1-2a\\
                z=5a
            \end{array} \right . \iff \left ( \begin{array}{c}
                x+2\\y-1\\z
            \end{array}\right ) = a\left ( \begin{array}{c}
                3\\-2\\5
            \end{array}\right ) \iff \]
            \[ \iff \rk\left ( \begin{array}{ccc}
                3 & x+2\\
                -2 & y-1\\
                5 & z
            \end{array}\right ) = 1\]
            
            \end{enumerate}
            
            Considerando il minore di ordine 1 corrispondente all'entrata $a_{1,1} = 3$, dove quindi $\det(a_{1,1})=3$, per il teorema degli orlati si ha che:
            
            \[\rk\left ( \begin{array}{ccc}
                3 & x+2\\
                -2 & y-1\\
                5 & z
            \end{array}\right ) = 1 \iff \left \{ \begin{array}{cc}
                \det\left ( \begin{array}{ccc}
                3 & x+2\\
                -2 & y-1\\
            \end{array}\right ) = 0 \\ \det\left ( \begin{array}{ccc}
                3 & x+2\\
                5 & z
            \end{array}\right ) = 0
            \end{array} \right . \iff\]
            \[\iff \left \{ \begin{array}{l}
                2x+3y+1 = 0 \\ -5x+3z-10 = 0
            \end{array} \right .\]
        \end{itemize}
        
        Possiamo quindi costruire un nuovo sistema di equazioni cartesiane corrispondente a $r \cap \pi$ utilizzando le due equazioni cartesiane descriventi $r$ e l'equazione cartesiana descrivente $\pi$:
        
        \[ \left ( \begin{array}{c}
                x \\ y \\ z
            \end{array} \right ) \in r \cap \pi \iff \left \{ \begin{array}{l}
                5x-y+13z=33\\
                2x+3y+1 = 0 \\
                -5x+3z-10 = 0\\
            \end{array} \right . \]
            
        Poiché il determinante della matrice dei coefficienti associata a tale sistema è diverso da 0, è possibile ricavare i valori di $x,y$ e $z$ tramite la regola di Cramer, ottenendo una soluzione analoga agli altri due metodi
    \end{enumerate}
    
    \quad
    
    \section{Matrici simili}
    
    \quad
    
    \begin{frameddefn}{Matrici simili}
        Date due matrici $A, B \in \matspace{n}{n}{K}$, tali matrici vengono dette \textbf{simili} se si verifica che:
        \[ \exists C \in GL(n,K) \mid A = C^{-1}BC\]
        
        \textit{\textbf{Attenzione:}} tale condizione non è equivalente ad affermare che $A$ è coniugato a $B$, poiché nella relazione di coniugio gli elementi $C, C^{-1}$ dovrebbero essere presi in $\matspace{n}{n}{K}$ e non in $ GL(n,K)$. Inoltre, ricordiamo che $GL(n,K) \not\subgrp \matspace{n}{n}{K}$.
    \end{frameddefn}
    
    \begin{framedprop}{Determinante invariante}
        Date due matrici $A,B \in \matspace{n}{n}{K}$, se $A$ è simile a $B$, allora
        \[ \det(A) = \det(B)\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    
    \begin{itemize}
        \item Sia $C \in GL(n,K) \mid B = C^{-1}AC$. Siccome $\det(C^{-1}) = \det(C)^{-1}$, si verifica che:
        \[ \det(B) = \det(C^{-1}AC) = \det(C^{-1})\det(A)\det(C) =\det(C)^{-1}\det(A)\det(C) = \det(A)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Traccia di una matrice}
        Data una matrice $A \in \matspace{n}{n}{K}$, definiamo come \textbf{traccia} di $A$ la somma delle entrate sulla diagonale principale:
        \[ \tr(A) = \sum_{k=1}^{n} a_{k,k}\]
    \end{frameddefn}
    
    \begin{framedlem}{}
        Date $A \in \matspace{m}{n}{K}$ e $B \in \matspace{n}{m}{K}$, si verifica che:
        \[ \tr(AB)=\tr(BA)\]
    \end{framedlem}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Il risultato segue dalla definizione stessa di prodotto tra matrici:
        \[ \tr(AB) = \sum_{k=1}^n (ab)_{k,k} = \sum_{k=1}^n \sum_{j=1}^m a_{k,j} b_{j,k} = \sum_{j=1}^m \sum_{k=1}^n b_{j,k} a_{k,j}= \sum_{k=1}^n (ba)_{k,k} = \tr(BA)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Traccia invariante}
        Date due matrici $A,B \in \matspace{n}{n}{K}$, se $A$ è simile a $B$, allora
        \[ \tr(A) = \tr(B)\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se $\exists C \in GL(n,K) \mid B = C^{-1}AC$, allora
        \[ \tr(B) = \tr(C^{-1}AC) = \tr(C^{-1}CA) = \tr(A)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Polinomio caratteristico}
        Data una matrice $A \in \matspace{n}{n}{K}$, definiamo come \textbf{polinomio caratteristico} di $A$ come:
        \[ p_A(x) := \det(x I_n-A)\]
    \end{frameddefn}
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Data la matrice
        \[A = \left ( \begin{array}{cc}
            1 & 2\\
            3 & 4
        \end{array}\right )\]
        
        il suo polinomio caratteristico corrisponde a:
        \[ p_A(x) = \det \left ( \left ( \begin{array}{cc}
            x & 0\\
            0 & x
        \end{array}\right ) -  \left ( \begin{array}{cc}
            1 & 2\\
            3 & 4
        \end{array}\right ) \right ) =\det \left (\begin{array}{cc}
            x-1 & -2\\
            -3 & x-4
        \end{array}\right ) = x^2-5x-2\]
        
        \item Data la matrice
        \[A = \left ( \begin{array}{ccc}
            1 & 0 & -1\\
            2 & 3 & 0\\
            1 & 0 & 1
        \end{array}\right )\]
        
        il suo polinomio caratteristico corrisponde a:
        \[ p_A(x) =\det \left ( \begin{array}{ccc}
            x-1 & 0 & +1\\
            -2 & x+3 & 0\\
            -1 & 0 & x-1
        \end{array}\right ) = (x-3)(x^2-2x+2)\]
    \end{itemize}
    
    \begin{framedprop}{Polinomio invariante}
        Date due matrici $A,B \in \matspace{n}{n}{K}$, se $A$ è simile a $B$, allora
        \[ p_A(x) = p_B(x)\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Se $\exists C \in GL(n,K) \mid B = C^{-1}AC$, allora
        \[ p_B(x) = \det(x I_n - B) = \det(x I_n - C^{-1}AC) = \det(x C^{-1}C - C^{-1}AC) =\] \[ = \det(C^{-1}x I_nC - C^{-1}AC) = \det(C^{-1}(x I_n-A)C) = \det(C^{-1})\det(x I_n-A)\det(C) =\]
        \[= \det(C)^{-1}\det(x I_n-A)\det(C) = \det(x I_n-A) = p_A(x)\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Data una matrice $A \in \matspace{n}{n}{K}$, si verifica che:
        \[ p_A(x) = x^n-\tr(A)x^{n-1}+\ldots+(-1)^n \det(A)\]
        \textit{(dimostrazione omessa)}
    \end{framedobs}
    
    \begin{frameddefn}{Autovalori ed Autovettori}
        Data $A \in \matspace{n}{n}{K}$ e uno scalare $\lambda \in K$, le seguenti condizioni sono equivalenti:
        \begin{enumerate}
            \item $p_A(\lambda) = 0$
            \item $\exists v \neq 0_{K^n} \in K^n \mid Av = \lambda v$
        \end{enumerate}
        Dove $\lambda$ viene detto \textbf{autovalore di $A$} e $v$ viene detto \textbf{autovettore relativo a $\lambda$}
    \end{frameddefn}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Supponiamo quindi che esista $\exists v \neq 0_{K^n} \in K^n \mid Av = \lambda v$:
        \[ \exists v \neq 0_{K^n} \in K^n \mid (\lambda I_n - A)v = 0 \iff \ker(L_{(\lambda I_n - A)}) \neq \{ 0_{K^n}\} \iff \]
        \[ \iff \dim(\ker(L_{(\lambda I_n - A)})) > 0 \iff \rk(\lambda I_n - A)= n - \dim(\ker(L_{(\lambda I_n - A)})) < n\]
        \[\iff \det(\lambda I_n - A) = 0 \iff p_A(\lambda)=0\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedobs}{}
        Data $A \in \matspace{n}{n}{K}$ e uno scalare $\lambda \in K$, il vettore $v \neq 0_{K^n}\in K^n$ è un autovettore relativo a $\lambda$ se e solo se:
        \[(A - \lambda I_n)v = 0\]
        o in alternativa
        \[ 0 = (\lambda I_n - A)v\]
    \end{framedobs}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Notiamo facilmente che:
        \[ \exists v \neq 0 \in K^n \mid Av = \lambda v \iff \left \{ \begin{array}{ll}
            0 = \lambda v - Av \iff 0 = (\lambda I_n - A)v\\
            Av - \lambda v = 0 \iff (A - \lambda I_n)v = 0
        \end{array}\right .\]
        $\hfill\qed$
    \end{itemize}
    
    \begin{frameddefn}{Spettro e Autospazio relativo}
        Data una matrice $A \in \matspace{n}{n}{K}$, definiamo come \textbf{spettro} di $A$ l'insieme dei suoi autovalori
        \[ \Sp(A) = \{\lambda \in K \mid p_A(\lambda)=0 \}\]
        e come \textbf{autospazio relativo a $\lambda \in \Sp(A)$} il sottospazio di $K^n$ generato dagli autovettori relativi a $\lambda$:
        \[ E_{\lambda}(A) = \{ v \in K^n \mid Av = \lambda v\}\]
    \end{frameddefn}
    
    \begin{framedprop}{Spettro invariante}
        Date due matrici $A,B \in \matspace{n}{n}{K}$, se $A$ è simile a $B$, allora
        \[ \Sp(A) = \Sp(B)\]
        
        Inoltre, dato $\lambda \in \Sp(A)=\Sp(B)$ si ha che:
        \[ E_{\lambda}(A) = E_{\lambda}(B) \iff A = B\]
    \end{framedprop}
    
    \newpage

    \textit{Dimostrazioni:}
    \begin{itemize}
        \item Se $\exists C \in GL(n,K) \mid B = C^{-1}AC$, allora $p_A(x) = p_B(x)$, dunque si ha che:
        \[ \mu \in \Sp(A) \iff p_A(\mu) = p_B(\mu) = 0 \iff \mu \in \Sp(B)\]
        
        \item Dato $\lambda \in \Sp(A) = \Sp(B)$, si ha che:
        \[ E_{\lambda}(A) = E_{\lambda}(B) \iff Av = \lambda v = Bv, \forall v \in E_{\lambda}(A) \iff A = B\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempio:}
    \begin{itemize}
        \item Consideriamo la seguente matrice
        \[A = \left ( \begin{array}{ccc}
            1 & 1 & 3\\
            1 & 0 & -5\\
            -2 & -1 & 2
        \end{array}\right )\]
        
        \item Il suo polinomio caratteristico corrisponde a:
        \[ p_A(x) =\det \left ( \begin{array}{ccc}
            x-1 & -1 & -3\\
            -1 & x & 5\\
            2 & 1 & x-2
        \end{array}\right ) = x(x-1)(x-2)\]
        
        dunque otteniamo che $\Sp(A) = \{ 0,1,2\}$
        
        \item Gli autovettori dell'autospazio $E_0(A)$ corrispondono a:
        \[ v=\left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right) \in E_0(A) \iff (0 \cdot I_n-A)v=0 \iff \left \{ \begin{array}{l}
            -x-y-3z=0\\
            -x+5z=0\\
            2x+y-2z=0
        \end{array}\right . \iff \]
        \[ \iff \left \{ \begin{array}{l}
            x=5t\\
            y=-8t\\
            z=t
        \end{array}\right . \iff \left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right) = t \left ( \begin{array}{c}
            5\\-8\\1
        \end{array}\right)\]
        
        \item Gli autovettori dell'autospazio $E_1(A)$ corrispondono a:
        \[ v= \left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right) \in E_1(A) \iff (1 \cdot I_n-A)v=0 \iff \left \{ \begin{array}{l}
            -y-3z=0\\
            -x+y+5z=0\\
            2x+y-z=0
        \end{array}\right . \iff \]
        \[ \iff \left \{ \begin{array}{l}
            x=2t\\
            y=-3t\\
            z=t
        \end{array}\right . \iff \left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right) = t \left ( \begin{array}{c}
            2\\-3\\1
        \end{array}\right)\]
        
        \item Gli autovettori dell'autospazio $E_2(A)$ corrispondono a:
        \[ v=\left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right) \in E_0(A) \iff (2 \cdot I_n-A)v=0 \iff \left \{ \begin{array}{l}
            x-y-3z=0\\
            -x+2y+5z=0\\
            2x+y=0
        \end{array}\right . \iff \]
        \[ \iff \left \{ \begin{array}{l}
            x=t\\
            y=-2t\\
            z=t
        \end{array}\right . \iff \left ( \begin{array}{c}
            x\\y\\z
        \end{array}\right) = t \left ( \begin{array}{c}
            1\\-2\\1
        \end{array}\right)\]
    \end{itemize}
    
    \begin{frameddefn}{Molteplicità algebrica e geometrica}
        Data una matrice $A \in \matspace{n}{n}{K}$, per ogni autovalore $\lambda \in \Sp(A)$ definiamo la sua:
        \begin{itemize}
            \item \textbf{Molteplicità algebrica $\mu(\lambda)$}, ossia la sua molteplicità come radice del polinomio caratteristico $p_A(x)$, corrispondente al più grande intero tale che:
            \[\mu(\lambda) \in \N, (x-\lambda)^{\mu(\lambda)} \mid p_A(x)\]
            \item \textbf{Molteplicità geometrica $\nu(\lambda)$}, ossia la dimensione del suo autospazio relativo, corrispondente a:
            \[\nu(\lambda) = \dim(E_{\lambda}(A)) = n - \rk(\lambda I_n - A)\]
        \end{itemize}
        
        Per ogni $\lambda \in \Sp(A)$, si verifica che:
        \[ 1 \leq \nu(\lambda) \leq \mu(\lambda)\]
    \end{frameddefn}
    
    \begin{framedprop}{Molteplicità invarianti}
        Se $A, B \in \matspace{n}{n}{K}$ sono matrici simili, allora $\forall \lambda \in \Sp(A) = \Sp(B)$ si ha che:
        \[ \mu_A(\lambda) = \mu_B(\lambda) \qquad\qquad \nu_A(\lambda) = \nu_B(\lambda)\]
        \textit{(dimostrazione omessa)}
    \end{framedprop}
    
    \newpage
    
    \subsection{Diagonalizzazione di una matrice}
    
    \quad
    
    \begin{frameddefn}{Matrice diagonale}
        Sia $A \in \matspace{n}{n}{K}$. Tale matrice viene detta \textbf{diagonale} se $\forall i \neq j$ si ha che $a_{i,j} = 0$, ossia se sopra e sotto la diagonale principale vi sono tutti zeri
        
        \[ A = \left ( \begin{array}{ccccc}
            a_{1,1} & 0 & \cdots & \cdots & 0\\
            0 & a_{2,2} & \ddots & \ddots & \vdots\\
            \vdots & \ddots & \ddots & \ddots & \vdots\\
            \vdots & \ddots & \ddots & a_{n-1,n-1} & 0\\
            0 & \cdots & \cdots & 0 & a_{n,n}\\
        \end{array}\right )\]
    \end{frameddefn}
    
    \begin{frameddefn}{Matrici triangolarizzabili e diagonalizzabili}
        Una matrice $A \in \matspace{n}{n}{K}$ viene detta \textbf{triangolarizzabile} se simile ad una matrice triangolare $T \in \matspace{n}{n}{K}$, mentre viene detta \textbf{diagonalizzabile} se simile ad una matrice diagonale $D \in \matspace{n}{n}{K}$
    \end{frameddefn}
    
    \begin{framedprop}{}
        Data una matrice $A \in \matspace{n}{n}{K}$, le seguenti condizioni sono equivalenti:
        \begin{enumerate}
            \item $A$ è triangolarizzabile
            
            \item La somma di tutte le sue molteplicità algebriche è $n$:
            \[ \sum_{\lambda \in \Sp(A)} \mu(\lambda) = n = \dim(K^n)\]
            
            \item Il suo polinomio caratteristico è completamente fattorizzabile:
            \[ p_A(x) = \prod_{\lambda \in \Sp(A)} (x-\lambda)^{\mu(\lambda)}\]
        \end{enumerate}
        \textit{(dimostrazione omessa)}
    \end{framedprop}
    
    \begin{framedcor}{}
        Per il teorema fondamentale dell'algebra, \textbf{ogni matrice} $A \in \matspace{n}{n}{K}(\C)$ è \textbf{triangolarizzabile}.
        
        Analogamente, una matrice $B \in \matspace{n}{n}{\R}$ è \textbf{triangolarizzabile} se e solo se 
        \[\forall \lambda \in \Sp(B), \lambda \in \R \iff \Sp(B) \subseteq \R\]
    \end{framedcor}
    
    \textbf{Esempio:}
    
    \begin{itemize}
        \item Data la matrice
        \[ A = \left ( \begin{array}{cc}
            0 & -1\\
            1 & 0
        \end{array}\right ) \in \matspace{2}{2}{\R}\]
        
        il suo polinomio caratteristico corisponde a:
        \[p_A(x) = \det \left ( \begin{array}{cc}
            x & 1\\
            -1 & x
        \end{array}\right ) = x^2+1\]
        
        \item Siccome $p_A(x) \in \R[x]$, $\deg(p_A(x)) = 2$ e  $\Delta_{p_A(x)} < 0$, tale polinomio non è fattorizzabile, dunque $A$ non è triangolarizzabile
        
        \item Considerando invece la matrice $A' \in \matspace{2}{2}{\C}$ avente entrate coincidenti a quelle di $A$, otteniamo che:
        \[p_{A'}(x) = (x-i)(x+i) \in \C[x] \implies \Sp(A') = \{ \pm i\} \subseteq \C\] 
        
        dunque $A'$ è triangolarizzabile
    \end{itemize}
    
    \begin{framedprop}{}
        Data una matrice $A \in \matspace{n}{n}{K}$, le seguenti condizioni sono equivalenti:
        \begin{enumerate}
            \item $A$ è diagonalizzabile
            
            \item La somma di tutte le sue molteplicità geometriche è $n$:
            \[ \sum_{\lambda \in \Sp(A)} \nu(\lambda) = n = \dim(K^n)\]
            
            \item Esistono $B^1, \ldots, B^n$ autovettori di $A$ tali che:
            \[ B^1, \ldots, B^n \text{ base di } K^n\]
            
            \item Il suo spettro contiene $n$ autovalori diversi tra loro:
            \[ \abs{\Sp(A)} =n \]
        \end{enumerate}
    \end{framedprop}
    
    \begin{framedobs}{}
        Data una matrice $A \in \matspace{n}{n}{K}$, dati $\lambda, \mu \in \Sp(A) \mid \lambda \neq \mu$ si ha che:
        \[ E_{\lambda}(A) \cap E_{\mu}(A) = \{0_{K^n}\}\]
    \end{framedobs}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Ovviamente, essendo sottospazi vettoriali, si ha che:
        \[ 0_{K^n }\in E_{\lambda}(A), 0_{K^n } \in E_{\mu}(A) \implies 0_{K^n } \in E_{\lambda}(A) \cap E_{\mu}(A)\]
        
        \item Supponiamo quindi che $\exists v \neq 0_{K^n} \in K^n \mid \lambda v = Av =\mu v$, dunque un autovettore relativo sia $\lambda$ sia a $\mu$, dove $\lambda \neq \mu$. Ne segue che:
        \[\lambda v = Av =\mu v \implies \lambda v = \mu v \implies (\lambda -\mu)v=0_{K^n}\]
        
        \item Poiché $\lambda \neq \mu \implies \lambda - \mu \neq 0$, si ha che:
        \[(\lambda -\mu)v=0_{K^n} \iff v = 0_{K^n}\]
        contraddicendo quindi l'ipotesi iniziale, dunque l'unica possibilità è che:
        \[\nexists v \neq 0_{K^n} \mid v \in E_{\lambda}(A), v \in E_{\mu}(A) \iff E_{\lambda}(A) \cap E_{\mu}(A) = \{0_{K^n}\}\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedcor}{}
        Data una matrice $A \in \matspace{n}{n}{K}$ e dati i vettori $v_1 \neq 0_{K^n} \in E_{\lambda_1}(A), \ldots, v_k \neq 0_{K^n}\in E_{\lambda_k}(A)$, dove $\lambda_i \neq \lambda_j, \forall i \neq j$, si ha che:
        \[v_1, \ldots, v_k \text{ linearmente indipendenti}\]
    \end{framedcor}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché:
        \[\lambda, \mu \in \Sp(A) \mid \lambda \neq \mu \implies E_{\lambda}(A) \cap E_{\mu}(A) = \{0_{K^n}\}\]
        ne segue automaticamente che:
        \[v_i \neq 0_{K^n} \in E_{\lambda_i}(A) \implies v_i \neq 0_{K^n} \notin E_{\lambda_j}(A), \forall j \neq i\]
        dunque $v_1, \ldots, v_k$ sono linearmente indipendenti

        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Matrice diagonalizzante}
        Data una matrice $A \in \matspace{n}{n}{K}$, se esiste una base $B^1, \ldots,B^i, \ldots, B^j, \ldots, B^n$ di $K^n$ tale che:
        \begin{itemize}
            \item $B^1, \ldots, B^i$ è base di $E_{\lambda_1}(A)$
            \item $\vdots$
            \item $B^j \ldots, B^n$ è base di $E_{\lambda_n}(A)$
        \end{itemize}
        
        dove $\lambda_1, \ldots, \lambda_n \in \Sp(A)$ e dove $i \neq j$ allora:
        \[ \exists B = (B^1, \ldots,B^i, \ldots, B^j, \ldots, B^n) \in GL(n,K) \mid D = B^{-1}AB\]
        dove $D \in \matspace{n}{n}{K}$ è una matrice diagonale e dove $B$ viene detta \textbf{matrice diagonalizzante}.
    \end{framedprop}

    \textbf{Esempio:}
    
    \begin{enumerate}
        \item \begin{itemize}
            \item Consideriamo la seguente matrice
        
            \[ A = \left ( \begin{array}{ccc}
                 5 & -8 & 3\\
                 4 & -8 & 4\\
                 5 & -12 & 7
            \end{array}\right ) \in \matspace{3}{3}{\R}\]
            
            \item Il suo polinomio caratteristico corrisponde a:
            \[ p_A(x) = \det \left ( \begin{array}{ccc}
                 x-5 & 8 & -3\\
                 -4 & x+8 & -4\\
                 -5 & 12 & x-7
            \end{array}\right ) = \]
            \[=(x-5)\det \left ( \begin{array}{cc}
                 x+8 & -4\\
                 12 & x-7
            \end{array}\right )+4det \left ( \begin{array}{cc}
                 8 & -3\\
                 12 & x-7
            \end{array}\right )-5det \left ( \begin{array}{ccc}
                 8 & -3\\
                 x+8 & -4\\
            \end{array}\right )=\]
            \[ = (x-5)(x^2+x-8)+4(8x-20)-5(3x-8) = x^3-4x^2+4x = x(x-2)^2\]
            
            dunque sappiamo che $A$ è triangolarizzabile e che il suo spettro è $\Sp(A) = \{ 0,2\}$.
            
            \item Siccome affinché $A$ sia anche diagonalizzabile è necessario che $\nu(0)+\nu(2)=3 = \dim(\R^3)$, notiamo come:
            \[ 1 \leq \nu(0) \leq \mu(0) \iff 1 \leq \nu(0) \leq 1 \iff \nu(0) = 1\]
            
            di conseguenza, si ha che:
            \[ \nu(0)+\nu(2)=3 \iff 1+\nu(2)=3 \iff \nu(2)=2\]
            
            \item Consideriamo quindi il sistema $(2 \cdot I_n-A)v=0$:
            \[\left \{ \begin{array}{l}
                -3x+8y-3z=0\\
                -4x+10y-4z=0\\
                -5x+12y-5z=0
            \end{array} \right . \implies (2 \cdot I_n-A) = \left ( \begin{array}{ccc}
                -3 & 8 & -3\\
                -4 & 10 & -4\\
                -5 & 12 & -5 
            \end{array}\right)\]
            
            Si ha che:
            \[ \nu(2) = 2 = \dim(E_2(A)) = 3-\rk(2 \cdot I_n-A) = \iff \rk(2 \cdot I_n-A) = 1\]
            
            \item Tuttavia, considerando il minore $M_{3,3}$, per il teorema degli orlati si verifica che:
            \[ \det\left ( \begin{array}{ccc}
                -3 & 8\\
                -4 & 10\\
            \end{array}\right) = -30+32 = 2 \neq 0 \implies \rk(2 \cdot I_n-A) \geq 2\]
            
            Dunque si ha che $\rk(2 \cdot I_n-A) \neq 1$, implicando quindi che $A$ non sia diagonalizzabile.
            
            \item Difatti, si ha che:
            \[ 1 \leq \nu(2) = 3-\rk(2 \cdot I_n-A) \leq 3-2 = 1 \iff \nu(2) = 1\]
            e dunque che:
            \[ \nu(0)+\nu(2) = 1+1 = 2 \neq 3 = \dim(\R^3)\]
        \end{itemize}
        
        \item \begin{itemize}
            \item Consideriamo la seguente matrice
        
            \[ A = \left ( \begin{array}{cc}
                 1  & k\\
                 2  & k-1
            \end{array}\right ) \in \matspace{2}{2}{\R}\]
            
            \item Il suo polinomio caratteristico corrisponde a:
            
            \[ p_A(x) = \det\left ( \begin{array}{cc}
                 x-1  & -k\\
                 -2  & x-k-1
            \end{array}\right ) = (x-1)(x-k-1)-2k\]
            \[  = x^2-kx-k-1 = (x+1)(x-k-1)\]
            
            dunque si ha che $\Sp(A) = \{ -1, k+1\}$
            
            \item Siccome $p_A(x)$ è completamente fattorizzabile indipendentemente dal valore assunto da $k$, allora $A$ è sempre triangolarizzabile.
            
            \item Se $k = -2$, si ha che:
            \[  A = \left ( \begin{array}{cc}
                 1  & -2\\
                 2  & -3
            \end{array}\right ) \implies p_A(x) = (x+1)^2 \implies \Sp(A) = \{ -1\}\]
            
            Dunque $A$ è diagonalizzabile se e solo se $\nu(-1)=2$.
            
            Tuttavia, considerando il sistema $(-1 \cdot I_n -A)v=0$, notiamo che :
            \[ \left \{ \begin{array}{l}
                -2x+2y=0\\
                -2x+2y=0
            \end{array}\right . \iff \left \{ \begin{array}{l}
                x=t\\
                y=t
            \end{array}\right . \iff\]
            \[ \iff \left ( \begin{array}{c}
                x\\y
            \end{array}\right ) = t \left ( \begin{array}{c}
                1\\1
            \end{array}\right ) \implies \nu(-1) = \dim(E_{-1}(A)) = 1\]
            
            Dunque $A$ non è diagonalizzabile se $k = -2$
            
            \item Se invece $k \neq -2$, si ha che:
            \[ p_A(x) = (x+1)(x-k-1) \implies \Sp(A) = \{ -1,k+1\}\]
            
            Dunque $A$ è diagonalizzabile, poiché vi sono $2$ autovalori distinti.
            
            \item Difatti, notiamo come considerando il sistema $(-1 \cdot I_n -A)v=0$ si ha che :
            \[ \left \{ \begin{array}{l}
                -2x+ky=0\\
                -2x+ky=0
            \end{array}\right . \iff \left \{ \begin{array}{l}
                x=t\\
                y=-\frac{k}{2}
            \end{array}\right . \iff\]
            \[ \iff \left ( \begin{array}{c}
                x\\y
            \end{array}\right ) = t \left ( \begin{array}{c}
                1\\-\frac{k}{2}t
            \end{array}\right ) \implies \nu(-1) = \dim(E_{-1}(A)) = 1\]
            
            mentre per il sistema  $((k+1)\cdot I_n -A)v=0$ si ha che :
            \[ \left \{ \begin{array}{l}
                kx-ky=0\\
                -2x+2y=0
            \end{array}\right . \iff \left \{ \begin{array}{l}
                x=t\\
                y=t
            \end{array}\right . \iff\]
            \[ \iff \left ( \begin{array}{c}
                x\\y
            \end{array}\right ) = t \left ( \begin{array}{c}
                1\\1
            \end{array}\right ) \implies \nu(-1) = \dim(E_{-1}(A)) = 1\]
            
            dunque si ha che $\nu(-1)+\nu(k+1)= 2 = \dim(\R^2)$
            
            \item Inoltre, otteniamo che i due vettori trovati sono base della matrice diagonalizzante $B$:
            
            \[ B = \left ( \begin{array}{cc}
                1&1\\
                -\frac{k}{2} & 1
            \end{array}\right ) \implies D = B^{-1}AB \implies D = \left ( \begin{array}{cc}
                k+1 & 0\\
                0 & -1
            \end{array}\right )\]
        \end{itemize}
    \end{enumerate}
    
    \quad
    
    \section{Matrice di una trasformazione lineare}
    
    \quad
    
    \begin{frameddefn}{Matrice di una trasformazione lineare}
        Siano $V$ e $W$ due spazi vettoriali e sia $f : V \to W$ una trasformazione lineare.
        
        Data una base $\mathcal{B} = v_1, \ldots, v_n$ di $V$, una base $\mathcal{C} = w_1, \ldots, w_m$ di $W$ e i seguenti due isomorfismi  (sezione \ref{trasform_lin}):
        \[ \varphi_{\mathcal{B}} : K^n \to V : (t_1, \ldots, t_n) \mapsto (t_1v_1+\ldots+t_nv_n)\]
        \[ \varphi_{\mathcal{C}} : K^m \to W : (s_1, \ldots, s_n) \mapsto (s_1w_1+\ldots+s_nw_m)\]
        
        Definiamo come \textbf{matrice di $f$ nelle basi $\mathcal{B}$ e $\mathcal{C}$} l'unica matrice $M_f \in \matspace{m}{n}{K}$ tale che:
        \[ \exists! M_f \in \matspace{m}{n}{K} \mid f = \varphi_{\mathcal{C}} \circ L_{M_f} \circ \varphi_{\mathcal{B}}^{-1}\]
        \begin{center}
            \includegraphics[scale=0.57]{images/matrix_f.png}
        \end{center}
    \end{frameddefn}
    
    \begin{itemize}
        \item Sia $L_A : K^n \to K^m$ una trasformazione lineare tale che:
        \[f = \varphi_{\mathcal{C}} \circ L_A \circ \varphi_{\mathcal{B}^{-1}}\]
        dove per definizione stessa di $L_A$ è associata ad un'unica matrice $A \in \matspace{n}{m}{K}$

        \item Data la base canonica $e_1, \ldots, e_n$ di $K^n$, per ogni $i \in [1,n]$ si ha che:
        \[\varphi_{\mathcal{B}}(e_i) = 0\cdot v_1 + \ldots+ 1 \cdot v_i + \ldots + 0 \cdot v_n = v_i \implies \varphi_{\mathcal{B}}^{-1}(v_i) = e_i\]
        
        \item Inoltre, per ogni $i \in [1,n]$ si ha che:
        \[ L_A(e_i) = Ae_i =  \left ( \begin{array}{ccc}
            a_{1,1} & \cdots & a_{1,n}\\
            \vdots & \ddots & \vdots\\
            a_{m,1} & \cdots & a_{m,n}\\
        \end{array} \right )\left ( \begin{array}{c}
            0\\
            \vdots\\
            1\\
            \vdots\\
            0
        \end{array} \right ) = \left ( \begin{array}{c}
            a_{1,i}\\
            \vdots\\
            a_{m,i}
        \end{array} \right ) = A^i\]
        
        \item Infine, dato il seguente isomorfismo:
        \[ \varphi_{\mathcal{C}} : K^m \to W : (s_1, \ldots, s_n) \mapsto (s_1v_1+\ldots+s_nv_n)\]
        per ogni $i \in [1,n]$ si ha che:
        \[\varphi_{\mathcal{C}}(A^i) = a_{1,i}w_1+\ldots+a_{m,i}w_m\]

        \item A questo punto, per ogni $i \in [1,n]$ si ha che:
        \[f(v_i) = \varphi_{\mathcal{C}}(L_A(\varphi_{\mathcal{B}^{-1}}(v_i))) \iff\]
        \[\iff f(v_i) = \varphi_{\mathcal{C}}(L_A(e_i)) \iff f(v_i) = \varphi_{\mathcal{C}}(A^i) \iff\]
        \[f(v_i) = a_{1,i}w_1+\ldots+a_{m,i}w_m\]

        \item Dunque, è possibile ricostruire la matrice $A$ tramite le seguenti combinazioni lineari:
        \[f(v_1) = a_{1,1}w_1+\ldots+a_{m,1}w_m \implies A^1 = \left ( \begin{array}{c}
            a_{1,1}\\
            \vdots\\
            a_{m,1}
        \end{array}\right )\]
        \[\vdots\]
        \[f(v_n) = a_{1,n}w_1+\ldots+a_{m,n}w_m \implies A^n = \left ( \begin{array}{c}
            a_{1,n}\\
            \vdots\\
            a_{m,n}
        \end{array}\right )\]
        $\hfill\qed$
    \end{itemize}
    
    \textbf{Esempi:}
    
    \begin{enumerate}
        \item \begin{itemize}
            \item Consideriamo i seguenti due sottospazi vettoriali di $\R[x]$:
            \[V :=\R[x]_{\leq 4} = \{p(x) \in \R[x] \mid \deg(p(x)) \leq 4\}\]
            \[W := \R[x]_{\leq 3} = \{p(x) \in \R[x] \mid \deg(p(x)) \leq 3\}\]

            dove $\mathcal{B}: x^4, x^3, x^2, x, 1$ è base di $V$ e $\mathcal{C}: x^4, x^3, x^2, x, 1$ è base di $W$, da cui ne segue che:
            \[\dim(V) = 5 = \dim(\R^5) \iff V \cong \R^5\]
            \[\dim(V) = 4 = \dim(\R^5) \iff V \cong \R^4\]
            
            \item Sia $f': V \to W : p(x) \mapsto p'(x)$ la trasformazione lineare corrispondente alla derivata di un polinomio. La matrice di $f'$ nelle basi $\mathcal{B}$ e $\mathcal{C}$ corrisponde a:
            
            \[\left \{\begin{array}{l}
                f'(x^4) = 4x^3 = 4x^3+0x^2+0x+0\\
                f'(x^3) = 3x^2 = 0x^3+3x^2+0x+0\\
                f'(x^2) = 2x = 0x^3+0x^2+2x+0\\
                f'(x) = x = 0x^3+0x^2+0x+1\\
                f'(1) = 0 = 0x^3+0x^2+0x+0\\
            \end{array} \right . \implies M_f = \left ( \begin{array}{ccccc}
                4 & 0 & 0 & 0 & 0\\ 
                0 & 3 & 0 & 0 & 0\\ 
                0 & 0 & 2 & 0 & 0\\ 
                0 & 0 & 0 & 1 & 0\\ 
            \end{array}\right )\]
            
            
        \item Consideriamo quindi i seguenti isomorfismi:
        \[\varphi_{\mathcal{B}}^{-1} : V \to \R^5 : ax^4+bx^3+cx^2+dx+e \mapsto \left ( \begin{array}{c}
            a\\
            b\\
            c\\
            d\\
            e
        \end{array}\right )\]
        \[\varphi_{\mathcal{C}} : \R^4 \to W :\left ( \begin{array}{c}
            a\\
            b\\
            c\\
            d\\
        \end{array}\right ) \mapsto ax^3+bx^2+cx+d\]
        
        \item Dato il polinomio $p(x) := 4x^4+2x^3+x+5 \in V$, si ha che:
        \end{itemize}
        \[\varphi_{\mathbb{B}}^{-1}(p(x)) = \left ( \begin{array}{c}
            4\\
            2\\
            0\\
            1\\
            5
        \end{array}\right ) \implies L_{M_f}(\varphi_{\mathcal{B}}^{-1}(p(x))) = \left ( \begin{array}{ccccc}
                4 & 0 & 0 & 0 & 0\\ 
                0 & 3 & 0 & 0 & 0\\ 
                0 & 0 & 2 & 0 & 0\\ 
                0 & 0 & 0 & 1 & 0\\ 
            \end{array}\right )\left ( \begin{array}{c}
            4\\
            2\\
            0\\
            1\\
            5
        \end{array}\right ) = \left ( \begin{array}{c}
            16\\
            6\\
            0\\
            1\\
        \end{array}\right )\]
        \[\implies \varphi_{\mathcal{C}}L_{M_f}(\varphi_{\mathcal{B}}^{-1}(p(x)))) = 16x^3+6x^2+1\]
        
        
        \quad
        
        \item \begin{itemize}
            \item Consideriamo ancora gli spazi $V = \R[x]_{\leq 4}$ e $W = \R[x]_{\leq 3}$.
            
            \item Sia $\Delta : V \to W : p(x) \mapsto p(x+1)-p(x)$ la trasformazione lineare corrispondente all'operatore differenza (anche detta "derivata discreta")
            
            \item La matrice di $\Delta$ nelle basi $\mathcal{B}$ e $\mathcal{C}$ corrisponde a:
            \[\left \{\begin{array}{l}
                \Delta(x^4) = (x+1)^4-x^4 = 4x^3+6x^2+4x+1\\
                \Delta(x^3) = (x+1)^3-x^3 = 3x^2+3x+1 \\
                \Delta(x^2) = (x+1)^2-x^2 =2x+1\\
                \Delta(x) = (x+1)-x = 1\\
                \Delta(1) = (x+1)^0-x^0 = 0\\
            \end{array} \right . \implies M_{\Delta} =\left ( \begin{array}{ccccc}
                4 & 0 & 0 & 0 & 0\\ 
                6 & 3 & 0 & 0 & 0\\ 
                4 & 3 & 2 & 0 & 0\\ 
                1 & 1 & 1 & 1 & 0\\ 
            \end{array}\right )\]
        \end{itemize}
        
        \item \begin{itemize}
            \item Siano $V = W = \R^2$ e sia:
            \[ f : \R^2 \to \R^2 : \left ( \begin{array}{c}
                x\\
                y
            \end{array} \right ) \mapsto \left ( \begin{array}{cc}
                1 & 3\\
                2 & 4
            \end{array} \right )\left ( \begin{array}{c}
                x\\
                y
            \end{array} \right ) = \left ( \begin{array}{c}
                x+3y\\
                2x+4y
            \end{array} \right )\]
            
            \item Siano inoltre
            \[\mathcal{B} = \left ( \begin{array}{c}
                1\\
                1
            \end{array} \right ), \left ( \begin{array}{c}
                1\\
                -1
            \end{array} \right ) \qquad\qquad
            \mathcal{C} = \left ( \begin{array}{c}
                1\\
                3
            \end{array} \right ), \left ( \begin{array}{c}
                -2\\
                1
            \end{array} \right )\]
            
            rispettivamente la base di $V = \R^2$ e di $W = \R^2$
            
            \item Consideriamo quindi la matrice $M_f$ di $f$ nelle basi $\mathbb{B}$ e $\C$ 
            \[ M_f= \left ( \begin{array}{cc}
                a & b\\
                c & d
            \end{array} \right )\]
            
            \item Le coordinate della colonna $M_f^1$ corrisponderanno a:
            \[ a \left ( \begin{array}{c}
                1\\1
            \end{array} \right )+ c \left ( \begin{array}{c}
                -2\\1
            \end{array} \right ) = f\left ( \begin{array}{c}
                1 \\ 1
            \end{array} \right ) \implies a \left ( \begin{array}{c}
                1\\1
            \end{array} \right )+ c \left ( \begin{array}{c}
                -2\\1
            \end{array} \right ) = \left ( \begin{array}{c}
                1 \cdot 1 + 3 \cdot 1\\
                2 \cdot 1 + 4 \cdot 1
            \end{array} \right ) \implies\]
            \[\implies a \left ( \begin{array}{c}
                1\\1
            \end{array} \right )+ c \left ( \begin{array}{c}
                -2\\1
            \end{array} \right ) = \left ( \begin{array}{c}
                4\\
                6
            \end{array} \right ) \implies
            \left \{ \begin{array}{l}
                a-2c = 4\\
                3a+4c = 6
            \end{array} \right . \implies\]
            \[\implies \left ( \begin{array}{cc|c}
                1 & -2  & 4\\
                3 & 4 & 6
            \end{array} \right ) \xrightarrow{R_2-3R_1 }
            \left ( \begin{array}{cc|c}
                1 & -2  & 4\\
                0 & 10 & -6
            \end{array} \right ) \xrightarrow{\frac{1}{10}R_2 }
            \left ( \begin{array}{cc|c}
                1 & -2  & 4\\
                0 & 1 & -\frac{3}{5}
            \end{array} \right )\xrightarrow{R_1-2R_2 }\]
            \[\xrightarrow{R_1-2R_2 }
            \left ( \begin{array}{cc|c}
                1 & 0  & \frac{14}{5}\\
                0 & 1 & -\frac{3}{5}
            \end{array} \right ) \implies
            \left \{ \begin{array}{l}
                a = \frac{14}{5}\\
                c = -\frac{3}{5}
            \end{array}\right .\]
            
            \item Analogamente, le coordinate di $M_f^2$ saranno:
            \[ b \left ( \begin{array}{c}
                1\\1
            \end{array} \right )+ d \left ( \begin{array}{c}
                -2\\1
            \end{array} \right ) = f\left ( \begin{array}{c}
                1 \\ -1
            \end{array} \right ) \implies b \left ( \begin{array}{c}
                1\\1
            \end{array} \right )+ d \left ( \begin{array}{c}
                -2\\1
            \end{array} \right ) = \left ( \begin{array}{c}
                1+3(-1)\\
                2+4(-1)
            \end{array} \right ) \implies \]
            \[ \implies b \left ( \begin{array}{c}
                1\\1
            \end{array} \right )+ d \left ( \begin{array}{c}
                -2\\1
            \end{array} \right ) = \left ( \begin{array}{c}
                -2\\
                -2
            \end{array} \right ) \implies \left \{ \begin{array}{l}
                b-2d=-2\\
                3b+4=-2
            \end{array}\right . \implies \]
            \[ \implies \left ( \begin{array}{cc|c}
                1 & -2 & -2\\
                3 & 4 & -2
            \end{array}\right ) \xrightarrow{R_2 += -3R_1 } \left ( \begin{array}{cc|c}
                1 & -2 & -2\\
                0 & 10 & 4
            \end{array}\right ) \xrightarrow{R_2 *= \frac{1}{10} } \left ( \begin{array}{cc|c}
                1 & -2 & -2\\
                0 & 1 & \frac{2}{5}
            \end{array}\right )\]
            \[\xrightarrow{R_1 += -2R_1 } \left ( \begin{array}{cc|c}
                1 & 0 & -\frac{6}{5}\\
                0 & 1 & \frac{2}{5}
            \end{array}\right ) \implies \left \{ \begin{array}{l}
                b= -\frac{6}{5}\\
                d = \frac{2}{5}
            \end{array}\right .\]
            
            \item Dunque, concludiamo la matrice di $f$ nelle basi $\mathcal{B}$ e $\mathcal{C}$ sia:
            \[ M_f = \left ( \begin{array}{cc}
                \frac{14}{5} & -\frac{6}{5}\\
                -\frac{3}{5} & \frac{2}{5}
            \end{array}\right )\]
        \end{itemize}
        
    \end{enumerate}
    
    \quad
    
    \begin{framedprop}{Matrice di cambiamento di base}
        Sia $V$ uno spazio vettoriale e siano $\mathcal{B} = v_1, \ldots, v_n$ e $\mathcal{C} = w_1, \ldots, w_n$ due basi di $V$.
        
        Dato il vettore $v \in V$ tale che:
        \[v = a_1v_1+\ldots+a_nv_n = b_1w_1+\ldots+b_nw_n\]
        
        e dati i suoi \textbf{vettori delle coordinate} in base $\mathcal{B}$ e $\mathcal{C}$:
        \[v_{\mathcal{B}}=\left ( \begin{array}{c}
            a_1\\
            \vdots\\
            a_n
        \end{array}\right ) \qquad\qquad v_{\mathcal{C}}=\left ( \begin{array}{c}
            b_1\\
            \vdots\\
            b_n
        \end{array}\right )\]
        
        Definiamo come \textbf{matrice di cambiamento di base} l'unica matrice $M_{\mathcal{C}}^{\mathcal{B}} \in \matspace{n}{m}{K}$ tale che:
        \[ \exists! M_{\mathcal{C}}^{\mathcal{B}} \in \matspace{n}{m}{K} \mid M_{\mathcal{C}}^{\mathcal{B}} \cdot v_{\mathcal{B}} = v_{\mathcal{C}}\]
    \end{framedprop}
    
    \textit{Dimostrazione:}
    \begin{itemize}
        \item Consideriamo l'automorfismo $\id : V \to V : v \mapsto v$ e consideriamo l'endomorfismo $L_A : K^n \to K^n$ tale che
        \[\id = \varphi_{\mathcal{C}} \circ L_A \circ \varphi_{\mathcal{B}}^{-1}\]
        dove
        \[ \varphi_{\mathcal{B}} : K^n \to V : (t_1, \ldots, t_n) \mapsto (t_1v_1+\ldots+t_nv_n)\]
        \[ \varphi_{\mathcal{C}} : K^n \to V : (s_1, \ldots, s_n) \mapsto (s_1w_1+\ldots+s_nw_n)\]
        
        \item Le colonne della matrice $A$ corrisponderanno a:
        \[\id(v_1) = a_{1,1}w_1+\ldots+a_{m,1}w_n \implies v_1 = a_{1,1}w_1+\ldots+a_{m,1}w_n \implies A^1 = \left ( \begin{array}{c}
            a_{1,1}\\
            \vdots\\
            a_{m,1}
        \end{array}\right )\]
        \[\vdots\]
        \[\id(v_n) = a_{1,n}w_1+\ldots+a_{m,n}w_n \implies v_n = a_{1,n}w_1+\ldots+a_{m,n}w_n \implies A^n = \left ( \begin{array}{c}
            a_{1,n}\\
            \vdots\\
            a_{m,n}
        \end{array}\right )\]
        
        \item Dato un vettore $v \in V$, poiché l'automorfismo $\id : V \to V : v \mapsto v$ ha alcun effetto primario, l'unico effetto secondario ottenuto applicando la matrice di $\id$ al vettore contenente le coordinate di $v$ in base $\mathbb{B}$ sarà quello di restituire le coordinate di $v$ in base $\mathcal{C}$
        
        $\hfill\qed$
    \end{itemize}
    
    \newpage

    \textbf{Esempio:}
    
    \begin{itemize}
        \item Consideriamo le seguenti due basi dello spazio $\R^2$
        \[\mathcal{B}=\left ( \begin{array}{c}
            1\\
            2
        \end{array}\right ),\left ( \begin{array}{c}
            3\\
            4
        \end{array}\right ) \qquad\qquad
        \mathcal{C}=\left ( \begin{array}{c}
            5\\
            6
        \end{array}\right ),\left ( \begin{array}{c}
            7\\
            8
        \end{array}\right )\]
        
        \item Consideriamo la matrice del cambiamento dalla base $\mathcal{B}$ alla base $\mathcal{C}$:
        \[M_{\mathcal{C}}^{\mathcal{B}} = \left ( \begin{array}{cc}
            a & b\\
            c & d
        \end{array}\right )\]
        \item Le coordinate della matrice del cambiamento di base corrisponderanno a:
        \[a \left ( \begin{array}{c}
            5\\
            6
        \end{array}\right )+c\left ( \begin{array}{c}
            7\\
            8
        \end{array}\right ) = \left ( \begin{array}{c}
            1\\
            2
        \end{array}\right ) \implies
        \left \{ \begin{array}{l}
            5a+7c=1\\
            6a+8c=2
        \end{array}\right . \implies
        \left \{ \begin{array}{l}
            a=3\\
            c=-2
        \end{array}\right .\]
        \[b \left ( \begin{array}{c}
            5\\
            6
        \end{array}\right )+d\left ( \begin{array}{c}
            7\\
            8
        \end{array}\right ) = \left ( \begin{array}{c}
            3\\
            4
        \end{array}\right ) \implies
        \left \{ \begin{array}{l}
            5b+7d=3\\
            6b+8d=4
        \end{array}\right . \implies
        \left \{ \begin{array}{l}
            b=2\\
            d=-1
        \end{array}\right .\]
        \[\implies M_{\mathcal{C}}^{\mathcal{B}} = \left ( \begin{array}{cc}
            3 & 2\\
            -2 & -1
        \end{array}\right )\] 
        
        \item Consideriamo quindi il seguente vettore e le sue coordinate in base $\mathcal{B}$:
        \[\left ( \begin{array}{c}
            -5\\
            -4
        \end{array}\right ) = x\left ( \begin{array}{c}
            1\\
            2
        \end{array}\right )+y\left ( \begin{array}{c}
            3\\
            4
        \end{array}\right ) \implies\]
        \[\implies \left \{ \begin{array}{l}
            x+3y=-5\\
            2x+4y=-4
        \end{array}\right . \implies
        \left \{ \begin{array}{l}
            x=4\\
            y=-3
        \end{array}\right .\]
        
        \item Le sue coordinate in base $\mathcal{C}$ corrisponderanno a:
        \[\left ( \begin{array}{cc}
            3 & 2\\
            -2 & -1
        \end{array}\right )\left ( \begin{array}{c}
            4\\
            -3
        \end{array}\right )=\left ( \begin{array}{c}
            12-6\\
            -8+3
        \end{array}\right ) =\left ( \begin{array}{c}
            6\\
            -5
        \end{array}\right )\]
        
        \item Difatti, notiamo che:
        \[6\left ( \begin{array}{c}
            5\\
            6
        \end{array}\right )-5\left ( \begin{array}{c}
            7\\
            8
        \end{array}\right ) = \left ( \begin{array}{c}
            -5\\
            -4
        \end{array}\right )\]
    \end{itemize}
    
    \newpage
    
    \section{Matrici ortogonali}
    
    \quad
    
    \begin{frameddefn}{Base ortogonale}
        Sia $v_1, \ldots, v_n$ una base di $\R^n$. Tale base viene detta \textbf{ortogonale} se i vettori sono tutti \textbf{ortogonali} tra loro, dunque se
        \[ v_iv_j = 0, \forall i \neq j \]
    \end{frameddefn}
    
    \begin{frameddefn}{Base ortonormale}
        Sia $v_1, \ldots, v_n$ una base di $\R^n$. Tale base è detta \textbf{ortonormale} se i vettori sono tutti \textbf{ortogonali} tra loro e sono \textbf{versori}, ossia aventi norma pari ad 1, dunque se:
        \[ v_iv_j =  \delta_{ij} = \left \{ \begin{array}{ll}
            \norm{v_i}=1 & \text{ se } i = j\\
            0 & \text{ se } i \neq j
        \end{array}\right .\]
        
        dove $\delta_{ij}$ viene detto \textbf{delta di Kroneker}.
    \end{frameddefn}
    
    \begin{framedobs}{}
        Le basi ortonormali possono essere ottenute da dalla base canonica $e_1, \ldots, e_n$ attraverso \textbf{rotazioni} e \textbf{riflessioni}
    \end{framedobs}
    
    \begin{framedobs}{}
        Data una base ortonormale $\mathcal{B} = v_1, \ldots, v_n$ di $\R^n$ e dato un vettore $w \in \R^n$, le coordinate di $v$ nella base $\mathcal{B}$ corrispondono a:
        \[ w = (w \cdot v_1) \cdot v_1 + \ldots + (w \cdot v_1) \cdot v_n\]
    \end{framedobs}
    
    \begin{framedprop}{Matrice ortogonale}
        Data una matrice $A \in \matspace{n}{m}{\R}$, tale matrice viene detta \textbf{matrice ortogonale} se si verifica una delle seguenti seguenti condizioni equivalenti:
        \begin{itemize}
            \item $A \cdot A^T = A^T \cdot A = I_n \iff A \in GL(n, \R) \mid A^{-1} = A^T$
            \item Le colonne $A^1, \ldots, A^n$ sono base ortonormale di $\R^n$
            \item Le righe $A_1, \ldots, A_n$ sono base ortonormale di $\R^n$
            \item $L_A : \R^n \to \R^n$ è isometria, ossia non cambia la distanza tra i punti del piano
        \end{itemize}

        \textit{(dimostrazione omessa)}
    \end{framedprop}
    
    \begin{frameddefn}{Gruppo ortogonale}
        Dato il gruppo $(GL(n,\R), \cdot)$, definiamo $O(n) \subgrp GL(n,K)$ come  \textbf{gruppo ortogonale}, dove
        \[ O(n) = \{ A \in GL(n,\R) \mid A^{-1} = A^T\}\]
    \end{frameddefn}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item $I_n^{-1} = I_n = I_n^T\implies I_n \in O(n)$
        \item $A,B \in O(n) \implies A^{-1} = A^T, B^{-1} = B^T \implies (AB)^{-1} =B^{-1}A^{-1}=B^TA^T=(AB)^T \implies AB \in O(n)$
        \item $A \in O(n) \implies A^{-1} = A^T \implies (A^{-1})^{-1} = A = (A^T)^T \implies (A^{-1})^{-1} = (A^{-1})^T \implies A^{-1} \in O(n)$

        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{Normalizzazione di un vettore}
        Dato uno spazio vettoriale $V$ e un vettore $v \in V$, la \textbf{normalizzazione di $v$} corrisponde a:
        \[w = \frac{v}{\norm{v}}\]
    \end{framedprop}

    \textit{Dimostrazione:}
    \begin{itemize}
        \item Poiché la norma di $v$, ossia $\norm{v}$, corrisponde alla lunghezza geometrica di $v$, si vede facilmente che il vettore $w$ ottenuto corrisponde ad un vettore avente la stessa direzione di $v$ ma norma pari ad $1$
        
        $\hfill\qed$
    \end{itemize}
    
    \begin{framedprop}{Proiezione di un vettore}
            Dato uno spazio vettoriale $V$ e due vettori $v,w \in V$, la \textbf{proiezione di $v$ su $w$} corrisponde a:
            \[\mathrm{proj}_w(v) = \frac{v\cdot w}{w \cdot w}w\]
    \end{framedprop}

    \textit{Dimostrazione:}

    \begin{itemize}
            \item Consideriamo la normalizzazione $u$ del vettore $w$:
            \[u = \frac{w}{\norm{w}}\]
            
            \item Sia $x:= \mathrm{proj}_w(v)$ il vettore corrispondente alla proiezione di $v$ su $w$. Per definizione stessa di proiezione su vettore, tale vettore avrà la stessa direzione del vettore $w$ e di conseguenza anche la stessa direzione del vettore $u$. Dunque, si ha che:
            \[x = \norm{x} \cdot u\]

            \item Consideriamo quindi l'angolo $\theta$ interno ai vettori $v$ e $x$. Per definizione stessa di coseno, si ha che:
            \[\cos(\theta) = \frac{\norm{x}}{\norm{v}}\]

            \item Poiché $w$ ha la stessa direzione di $x$, l'angolo interno ai vettori $v$ e $w$ coincide con l'angolo $\theta$. Come visto nella sezione \ref{ortho}, si ha che:
            \[\cos(\theta) = \frac{v \cdot w}{\norm{v}\norm{w}}\]

            \item Dunque, si ha che:
            \[\frac{\norm{x}}{\norm{v}} = \cos(\theta) = \frac{v \cdot w}{\norm{v}\norm{w}} \implies \frac{\norm{x}}{\norm{v}} = \frac{v \cdot w}{\norm{w}}\]

            \item Infine, concludiamo che:
            \[x = \norm{x} \cdot u = \frac{v \cdot w}{\norm{w}} \cdot \frac{w}{\norm{w}} = \frac{v \cdot w}{\norm{w}^2}w = \frac{v \cdot w}{w \cdot w} w\]

            \item Di seguito, vi è un'interpretazione grafica dei passaggi effettuati:
            \begin{center}                \includegraphics[scale=0.5]{images/projection.png}
            \end{center}

            $\hfill\qed$
    \end{itemize}

    \begin{frameddefn}{Matrice simmetrica}
        Data una matrice $A \in \matspace{n}{n}{K}$, tale matrice viene detta \textbf{simmetrica} se
        \[ A = A^T\]
    \end{frameddefn}

    \newpage

    \begin{framedthm}{Teorema spettrale}
        Data una matrice simmetrica $S \in \matspace{n}{n}{\R}$, le seguenti condizioni sono equivalenti:
        \begin{enumerate}
            \item $\Sp(S) \subset \R$
            \item $S$ è diagonalizzabile
            \item Esiste una base ortonormale $\mathcal{B} = B^1, \ldots, B^n$ di $\R^n$ tale che $B^i, \forall i \in [1,n]$ è autovettore di $S$
            \item $\exists B \in O(n) \mid D = B^{-1}AB = B^TAB$ dove $D \in \matspace{n}{n}{\R}$ è una matrice diagonale
        \end{enumerate}

        \textit{(dimostrazione omessa)}
    \end{framedthm}
        
    \begin{framedalgo}{Ortonormalizzazione di Gram-Schimdt}
        Sia $V$ uno spazio vettoriale e sia $v_1, \ldots, v_n$ una sua base. Il seguente algoritmo restituisce una base ortogonale $u_1, \ldots, u_n$ di $V$ e una base ortonormale $w_1, \ldots, w_n$ di $V$:

        \begin{enumerate}
                \item Poniamo $u_1 := v_1$
                \item Il vettore $u_2$ corrisponderà a: $u_2 = v_2 - \mathrm{proj}_{u_1}(v_2)$
                \item Difatti, notiamo che:
                \[u_1 \cdot u_2 = u_1 (v_2-\mathrm{proj}_{u_1}(v_2)) = u_1 \rbk{v_2- \frac{u_1 \cdot v_2}{u_1 \cdot u_1}}u_1 =\]
                \[= u_1v_2 - \frac{u_1\cdot v_2}{\norm{u_1}^2}\norm{u_1} = u_1v_2-u_1v_2 = 0\]
                dunque $u_1$ risulta essere ortogonale a $u_2$
                \item In generale, il vettore $u_k$, dove $k \in [1,n]$, corrisponderà a:
                \[u_k = v_k-\sum_{i=1}^{k-1}\mathrm{proj}_{u_i}(v_k)\]
                \item I vettori $u_1, \ldots, u_k$ costituiscono una base ortogonale di $V$.
                \item Per ottenere la base ortonormale $w_1, \ldots, w_n$, basterà normalizzare ogni vettore della base ortogonale:
                \[w_k = \frac{u_k}{\norm{u_k}}, \forall k \in [1,n]\]
        \end{enumerate}
    \end{framedalgo}

    \chapter{Algoritmi di crittografia}

    \section{Algoritmo RSA}

    \quad

    \begin{framedalgo}{Algoritmo di crittografia RSA}
        Siano:
        \begin{itemize}
            \item $p,q \in \mathbb{P} \mid p \neq q$ e sufficientemente grandi
            \item $n:=pq$
            \item $\lambda(n) := \mcm(p-1,q-1)$
            \item $e \in \N \mid 1<e<\lambda(n) \land \MCD(e, \lambda(n))=1$
            \item $d:=e{-1} (\texttt{mod }\lambda(n))$
        \end{itemize}
        
        Dato un \textbf{messaggio da cifrare $m$} tale che $\MCD(m,n) = 1$, il \textbf{messaggio cifrato $c$} ottenuto tramite la \textbf{chiave pubblica $(e,n)$} corrisponde a:
        \[m^e \equiv c (\texttt{mod }n)\]
        Una volta ottenuto il messaggio cifrato, applicando la \textbf{chiave privata $(d,n)$} è possibile riottenere il messaggio $m$:
        \[c^d \equiv m (\texttt{mod }n)\]
        Poiché le due chiavi sono \textbf{l'una l'inversa dell'altra}, distribuendo la propria chiave pubblica $(e,n)$ è possibile permettere ad un interlocutore di poterci inviare messaggi cifrati, per poi decifrarli tramite la propria chiave privata $(d,n)$, la quale dovrà essere mantenuta \textbf{nascosta} al fine di non concedere ad altre persone di poter leggere il contenuto del messaggio $m$.
    \end{framedalgo}

    \newpage
    
    \textit{Dimostrazione:}

    \begin{itemize}
        \item Poiché $\lambda(n):=\mcm(p,q)$, si ha che:
        \[\left \{ \begin{array}{l} (p-1)\mid\lambda(n) \implies \exists k \in \Z \mid \lambda(n) = (p-1)k\\(q-1)\mid \lambda(n) \implies \exists h \in \Z \mid \lambda(n) = (q-1)h \end{array}\right .\]
        \item Per il piccolo teorema di Fermat si ha che:
        \[  m^{p}\equiv m (\texttt{mod }p) \iff m^{p-1} \equiv 1 (\texttt{mod }p) \implies\]
        \[\implies m^{(p-1)k} \equiv 1 (\texttt{mod }p) \iff m^{\lambda(n)} \equiv m (\texttt{mod }p)\]
        e analogamente:
        \[m^q \equiv m (\texttt{mod }q) \iff m^{q-1} \equiv 1 (\texttt{mod }q) \implies\]
        \[\implies m^{(q-1)h} \equiv 1 (\texttt{mod }q) \iff m^{\lambda(n)} \equiv 1 (\texttt{mod }q)\]
        
        \item Poiché $\MCD(p,q)=1$, per il teorema cinese dei resti si ha che:
        \[m^{\lambda(n)}\equiv 1 (\texttt{mod }p) \\ m^{\lambda(n)} \equiv 1 (\texttt{mod }q) \iff m^{\lambda{n}} \equiv 1 (\texttt{mod }n)\]
        \item Inoltre, si ha che:
        \[\MCD(e, \lambda(n))=1 \iff [e] \in \Z_{\lambda(n)}^* \iff \exists [d] := [e]^{-1} \in \Z_{\lambda(n)}^* \iff\]
        \[\iff ed \equiv 1 (\texttt{mod }\lambda(n)) \iff ed = 1+b\lambda(n), \exists b \in \Z\]
        \item Dunque, concludiamo che:
        \[(m^e)^d \equiv m^{ed} \equiv m^{1+\lambda(n)j} \equiv m(m^{\lambda(n)})^j \equiv m(1)^j \equiv m (\texttt{mod }n)\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedobs}{}
        La condizione $\MCD(m,n) = 1$ è \textbf{necessaria} affinché non vi sia una \textbf{perdita del messaggio} durante il processo di cifratura e de-cifratura. Difatti, tramite tale condizione si ha che:
        \[\MCD(m,n) = 1 \implies \left \{ \begin{array}{l}
        n \nmid m \iff \nexists k \in \Z \mid m = nk \iff m \not\equiv 0 (\texttt{mod }n)\\
        p \nmid m \iff \nexists h \in \Z \mid m = ph \iff m \not\equiv 0 (\texttt{mod }p) \\
        q \nmid m \iff \nexists b \in \Z \mid m = qb \iff m \not\equiv 0 (\texttt{mod }q)
        \end{array} \right .\]

        Senza tale condizione, quindi, potrebbe verificarsi che $n \mid m \lor p \mid m \lor q \mid m$, portando ad una perdita del messaggio.
    \end{framedobs}

    \begin{framedobs}{}
        I due primi $p,q \in \mathbb{P}$ devono essere \textbf{sufficientemente grandi} poiché altrimenti sarebbe possibile ricavare gli interi componenti dell'algoritmo tramite essi, ottenendo quindi anche la \textbf{chiave privata} $(d,n)$.

        Difatti, poiché l'intero $n:=pq$ è contenuto nella chiave pubblica $(e,n)$, se $p$ o $q$ fossero due numeri piccoli si potrebbe ricavare l'uno dall'altro procedendo per \textbf{bruteforce}:
        \begin{enumerate}
            \item Preso $k \in \mathbb{P}$, se $k \mid n$ allora $k = p$ e $q = \frac{n}{k}$
            \item Se invece $k \nmid n$, allora verrà ripetuto il passo precedente con il numero primo successivo
            \item Una volta trovati $p$ e $q$, basterà calcolare $[d] := [e]^{-1} (\texttt{mod }\lambda(n))$ per poter ottenere la chiave privata $(d,n)$
        \end{enumerate}
    \end{framedobs}

    \section{Interpolazione di Lagrange e Algoritmo SSS}

    \quad

    \begin{frameddefn}{Matrice di Vandermonde}

        Dati $x_0, \ldots,x_n \in K$, definiamo la seguente matrice $V \in \matspace{n}{n}{K}$ come \textbf{matrice di Vandermonde a coefficienti $x_0, \ldots, x_n$}:
        \[V(x_0,x_1, \ldots, x_n) = \left ( \begin{array}{ccccc}
        x_0^0 & x_0^1 & x_0^2 & \cdots & x_0^n \\
        x_1^0 & x_1^1 & x_1^2 & \cdots & x_1^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        x_n^0 & x_n^1 & x_n^2 & \cdots & x_n^n
        \end{array}\right )\]
    \end{frameddefn}

    \begin{framedprop}{}
        Dati $x_0, \ldots, x_n \in K$, si ha che:
        \[\det(V(x^0, \ldots, x^n)) = \prod_{0 \leq i < j \leq n}(x_j-x_i)\]
    \end{framedprop}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Consideriamo il determinante di $V(x_0, \ldots, x_n)$:
        \[\det(V(x_0,x_1, \ldots, x_n)) = \det\left ( \begin{array}{ccccc}
        x_0^0 & x_0^1 & x_0^2 & \cdots & x_0^n \\
        x_1^0 & x_1^1 & x_1^2 & \cdots & x_1^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        x_n^0 & x_n^1 & x_n^2 & \cdots & x_n^n
        \end{array}\right ) =\det\left ( \begin{array}{ccccc}
        1 & x_0 & x_0^2 & \cdots & x_0^n \\
        1 & x_1 & x_1^2 & \cdots & x_1^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        1 & x_n & x_n^2 & \cdots & x_n^n
        \end{array}\right ) \]
        
        \newpage
        
        \item Poiché sottrarre un multiplo di una riga non ha effetti sul determinante, sottraendo la prima riga a tutte le altre si ha che:
        \[\det\left ( \begin{array}{ccccc}
        1 & x_0 & x_0^2 & \cdots & x_0^n \\
        1 & x_1 & x_1^2 & \cdots & x_1^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        1 & x_n & x_n^2 & \cdots & x_n^n
        \end{array}\right ) \xrightarrow{R_i-R_1, \forall i > 1}	
        \det \left( \begin{array}{ccccc}
        1 & x_0 & x_0^2 & \cdots & x_0^n \\
        0 & x_1-x_0 & x_1^2-x_0^2 & \cdots & x_1^n-x_0^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & x_n-x_0 & x_n^2-x_0^2 & \cdots & x_n^n-x_0^n
        \end{array}\right )\]

        \item Eseguendo lo sviluppo di Laplace sulla prima colonna, si ha che:
        \[\det\left ( \begin{array}{ccccc}
        1 & x_0 & x_0^2 & \cdots & x_0^n \\
        0 & x_1-x_0 & x_1^2-x_0^2 & \cdots & x_1^n-x_0^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & x_n-x_0 & x_n^2-x_0^2 & \cdots & x_n^n-x_0^n
        \end{array}\right ) = 1 \cdot \det\left ( \begin{array}{cccc}
        x_1-x_0 & x_1^2-x_0^2 & \cdots & x_1^n-x_0^n \\
        \vdots & \vdots & \ddots & \vdots\\
        x_n-x_0 & x_n^2-x_0^2 & \cdots & x_n^n-x_0^n
        \end{array}\right )\]

        \item Notiamo che $\forall i,k \in [1,n]$ si ha che:
        \[x_i^k-x_0^k = (x_i-x_0)(x_i^{k-1}+ x_i^{k-2}x_0+\ldots+x_ix_0^{k-2}+x_0^{k-1})\]
        da cui ne segue che:
        \end{itemize}
        
        \[\det\left ( \begin{array}{cccc}
        x_1-x_0 & x_1^2-x_0^2 & \cdots & x_1^n-x_0^n \\
        \vdots & \vdots & \ddots & \vdots\\
        x_n-x_0 & x_n^2-x_0^2 & \cdots & x_n^n-x_0^n
        \end{array}\right ) = \]
        \[=\det\left ( \begin{array}{cccc}
        x_0 & x_0^2 & \cdots & x_0^n \\
        x_1-x_0 & (x_1-x_0)(x_1+x_0) & \cdots & (x_1-x_0)(x_1^{n-1}+x_1^{n-2}x_0+\ldots+x_1x_0^{n-2}+x_0^{n-1}) \\
        \vdots & \vdots & \ddots & \vdots\\
        x_n-x_0 & (x_n-x_0)(x_n+x_0) & \cdots & (x_n-x_0)(x_n^{n-1}+x_n^{n-2}x_0+\ldots+x_nx_0^{n-2}+x_0^{n-1})
        \end{array}\right )\]
        
        \quad
        
        \begin{itemize}
        \item Per multilinearità del determinante, si ha che:
        \end{itemize}
        
        \[\det\left ( \begin{array}{cccc}
        x_1-x_0 & (x_1-x_0)(x_1+x_0) & \cdots & (x_1-x_0)(x_1^{n-1}+x_1^{n-2}x_0+\ldots+x_1x_0^{n-2}+x_0^{n-1}) \\
        \vdots & \vdots & \ddots & \vdots\\
        x_n-x_0 & (x_n-x_0)(x_n+x_0) & \cdots & (x_n-x_0)(x_n^{n-1}+x_n^{n-2}x_0+\ldots+x_nx_0^{n-2}+x_0^{n-1})
        \end{array}\right ) =\]
        \[(x_1-x_0)\cdot\ldots\cdot(x_n-x_0)\cdot \det\left ( \begin{array}{cccc}
        1 & x_1+x_0 & \cdots & x_1^{n-1}+x_1^{n-2}x_0+\ldots+x_1x_0^{n-2}+x_0^{n-1} \\
        \vdots & \vdots & \ddots & \vdots\\
        1 & x_n+x_0 & \cdots & x_n^{n-1}+x_n^{n-2}x_0+\ldots+x_nx_0^{n-2}+x_0^{n-1}
        \end{array}\right ) = \]
        \[=\prod_{j=1}^n (x_j-x_0) \cdot \det\left ( \begin{array}{cccc}
        1 & x_1+x_0 & \cdots & x_1^{n-1}+x_1^{n-2}x_0+\ldots+x_1x_0^{n-2}+x_0^{n-1} \\
        \vdots & \vdots & \ddots & \vdots\\
        1 & x_n+x_0 & \cdots & x_n^{n-1}+x_n^{n-2}x_0+\ldots+x_nx_0^{n-2}+x_0^{n-1}
        \end{array}\right )\]

        \newpage
        
        \begin{itemize}
        \item Sottraendo ad ogni colonna tutte le colonne precedenti moltiplicate per $b_0$, si ha che:
        \end{itemize}
        
        \[\prod_{j=1}^n (x_j-x_0) \cdot \det\left ( \begin{array}{cccc}
        1 & x_1+x_0 & \cdots & x_1^{n-1}+x_1^{n-2}x_0+\ldots+x_1x_0^{n-2}+x_0^{n-1} \\
        \vdots & \vdots & \ddots & \vdots\\
        1 & x_n+x_0 & \cdots & x_n^{n-1}+x_n^{n-2}x_0+\ldots+x_nx_0^{n-2}+x_0^{n-1}
        \end{array}\right ) \xrightarrow{ C^i-\sum_{j=1}^{n-1} x_0 C^j, \forall i}\]
        \[\prod_{j=1}^n (x_j-x_0) \cdot \det\left ( \begin{array}{cccc}
        1 & x_1 & \cdots & x_1^{n-1} \\
        \vdots & \vdots & \ddots & \vdots\\
        1 & x_n & \cdots & x_n^{n-1}
        \end{array}\right ) = \prod_{j=1}^n (x_j-x_0) \cdot \det(V(x_1, \ldots, x_n))\]

        \begin{itemize}
        \item Effettuando gli stessi passaggi ricorsivamente, concludiamo che:
        \[\det(V(x_0,\ldots,x_n)) = \prod_{j=1}^n (x_j-x_0) \cdot \det(V(x_1, \ldots, x_n)) = \]
        \[=\prod_{j=1}^n (x_j-x_0) \cdot \prod_{h=2}^n (x_h-x_1)\cdot \det(V(x_2, \ldots, x_n)) = \ldots = \prod_{0 \leq i < j \leq n} (x_j-x_i)\]
        $\hfill\qed$
    \end{itemize}

    \begin{framedlem}{}
        Dati $x_0,\ldots, x_n \in K$, si ha che:
        \[\det(V(x_0,\ldots,x_n)) \neq 0 \iff x_i \neq x_j, \forall i \neq j\]
    \end{framedlem}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Poiché
        \[\det(V(x_0,\ldots,x_n)) = \prod_{0 \leq h < j \leq n} (x_j-x_h)\]

        si vede facilmente che:
        \[\exists k,h \in [0,n] \mid x_k=x_h \implies x_k-x_h=0 \implies \det(V(x_0,\ldots,x_n))=0\]

        da cui per contronominale otteniamo che:
        \[\det(V(x_0,\ldots,x_n))\neq 0 \implies \nexists k,h \in [0,n] \mid x_k=x_h \implies x_k \neq x_h, \forall k,h \in [0,n]\]
        
        \item Viceversa, supponiamo per assurdo che $x_i \neq x_j, \forall i \neq j$ e che $\det(V(x_0, \ldots, x_n)) = 0$. Ne segue che:
        \[\det(V(x_0, \ldots, x_n))=0 \iff \prod_{0 \leq i < j \leq n} (x_j-x_i) =0\]
        
        \item Per la legge di annullamento del prodotto, ne segue che:
        \[\prod_{0\leq i < j \leq n} (x_j-x_j) = 0 \implies (x_1-x_0) = 0 \; \lor \ldots \lor\; (x_n-x_{n-1}) = 0 \implies\]
        \[x_1 = x_0 \;\lor \ldots \lor\; x_n = x_{n-1}\]
        
        contraddicendo quindi l'ipotesi per cui $x_k \neq x_j, \forall i \neq j$. Dunque, l'unica possibilità è che $\det(V(x_0, \ldots, x_n)) \neq 0$

        $\hfill\qed$
    \end{itemize}

    \begin{framedprop}{}
        Siano $x_0, \ldots, x_n \in K \mid x_i \neq x_j, \forall i \neq j$. Dati $y_0, \ldots, y_n \in K$, si ha che:
        \[\exists! p(x) \in K[x]_{\leq n} \mid \left \{ \begin{array}{l} p(x_0)=y_0 \\ \vdots \\ p(x_n)=y_n \end{array} \right .\]
    \end{framedprop}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Posto $p(x) := a_0+a_1x+\ldots+a_nx^n \in K[x]_{\leq n}$, si ha che:
        \[\left \{ \begin{array}{l} p(x_0)=y_0 \\ \vdots \\ p(x_n)=y_n \end{array} \right . \iff \left \{ \begin{array}{l} a_0+a_1x_0+\ldots+a_nx_0^n=y_0 \\ \vdots \\ a_0+a_1x_n+\ldots a_nx_n^n = y_n \end{array}\right .\]

        \item Considerando $a_0, \ldots, a_n$ come le incognite del sistema, la matrice dei coefficienti associata a risulta essere una matrice di Vandermonde nella forma $V(x_0, \ldots, x_n)$. Di conseguenza, si ha che:
        \[x_i \neq x_j, \forall i \neq j \iff \det(V(x_0, \ldots, x_n))\neq 0 \iff \exists! \text{ soluzione}\]
        
        \item Dunque, esiste può esistere un'unico polinomio $p(x) \in K_{\leq n} \mid p(x_0) = y_0, \ldots, p(x_n)=y_n$

        $\hfill\qed$	
    \end{itemize}

    \begin{framedcor}{}
        Dati $x_0, \ldots ,x_n \in K \mid x_i \neq x_j, \forall i \neq j$, si ha che:
        \[\exists! p_i(x) \in K[x]_{\leq n} \mid p_i(x_j) = \delta_{i,j} = \left \{ \begin{array}{ll} 1 & \text{ se } i = j\\ 0 & \text{ se } i \neq j \end{array} \right .\]
        
        dove $\delta_{i,j}$ è il \textbf{delta di Kroneker} e dove $p_1, \ldots, p_n$ formano una base di $K[x]_{\leq n}$ detta \textbf{base di Lagrange}
    \end{framedcor}

    \newpage

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Dato $i \in [0,n]$, si ha che:
        \[p_i(x_j) = \left \{ \begin{array}{ll} 1 & \text{ se } i = j\\ 0 & \text{ se } i \neq j \end{array} \right . \iff (x-x_j) \mid p_i(x), \forall j \neq i \in [0,n] \iff \left \{ \begin{array}{l} p_i(x_1) = 0 \\ \vdots \\ p_i(x_i) = 1 \\ \vdots \\ p_i(x_n) = 0 \end{array}\right . \]

        \item Per la proposizione precedente, l'unica possibilità è che $p_0(x), \ldots, p_n(x) \in K[x]_{\leq n}$ siano unici, poiché:
        \[\exists! q(x) \in K[x]_{\leq n} \mid q(x_1)=0, \ldots,q(x_i)=1, \ldots,  q(x_n) = 0\]
        $\hfill\qed$

        \item Di conseguenza, ne segue automaticamente che tali polinomi siano linearmente indipendenti tra loro, poiché nessuno di loro può essere espresso come combinazione lineare degli altri.
        \item Inoltre, poiché $\dim(K[x]_{\leq n})=n+1$, sappiamo che $n+1$ vettori possono essere linearmente indipendenti se e solo se sono anche generatori, di conseguenza $p_0, \ldots, p_n$ sono una base di $K[x]_{\leq n}$

        $\hfill\qed$
    \end{itemize}

    \begin{framedthm}{Interpolazione di Lagrange}
        Dati i seguenti \textbf{nodi dell'interpolazione} $(x_0, y_0), \ldots, (x_n, y_n)$, dove $x_i\neq x_j, \forall i \neq j$, e dati i seguenti polinomi $p_0, \ldots, p_1 \in K[x]_{\leq n}$ tali che:
        \[p_i(x) = \prod_{0 \leq j \leq n \mid j \neq i} \frac{x-x_j}{x_i-x_j}\]

        L'\textbf{unico polinomio} $p(x) \in K[x]_{\leq n}$ \textbf{passante per ogni nodo} corrisponde a:
        \[p(x) = y_0p_0(x)+\ldots+y_np_n(x)\]
    \end{framedthm}

    \textit{Dimostrazione:}

    \begin{itemize}
        \item Consideriamo la base di Lagrange $p_0, \ldots, p_n \in K[x]_{\leq n}$ dello spazio $K[x]_{\leq n}$ vista nel corollario precedente:
        \[\exists! p_i(x) \in K[x]_{\leq n} \mid p_i(x_j) = \delta_{i,j} = \left \{ \begin{array}{ll} 1 & \text{ se } i = j\\ 0 & \text{ se } i \neq j \end{array} \right . \]
        
        \item Per ogni polinomio della base si ha che:
        \[p_i(x_j)=0, \forall j \neq i \in [0,n] \iff (x-x_j) \mid p_i(x), \forall j \neq i \in [0,n] \implies\]
        \[\implies (x-x_1)\cdot\ldots\cdot(x-x_{i-1})(x-x_{i+1})\cdot\ldots\cdot(x-x_n) \mid p_i(x) \iff\]
        \[\iff p_i(x) = c_i(x-x_1)\cdot\ldots\cdot(x-x_{i-1})(x-x_{i+1})\cdot\ldots\cdot(x-x_n), \exists c_i \in K\]

        \item Poiché $p_i(x_i) = 1, \forall i \in [0,n]$, l'unica possibilità è che $c_i = 1, \forall i \in [0,n]$. Inoltre, poiché $p_i(x_i) = 1$, ne segue che:
        \[p_i(x) = (x-x_1)\cdot\ldots\cdot(x-x_{i-1})(x-{i+1})\cdot\ldots\cdot(x-x_n) \implies \]
        \[p_i(x) = \frac{(x-x_1)\cdot\ldots\cdot(x-x_{i-1})(x-{i+1})\cdot\ldots\cdot(x-x_n)}{1} \implies \]
        \[p_i(x) =\frac{(x-x_1)\cdot\ldots\cdot(x-x_{i-1})(x-{i+1})\cdot\ldots\cdot(x-x_n)}{p_i(x_i)} \implies\]
        \[p_i(x) = \frac{(x-x_1)\cdot\ldots\cdot(x-x_{i-1})(x-x_{i+1})\cdot\ldots\cdot(x-x_n)}{(x_i-x_1)\cdot\ldots\cdot(x_i-x_{i-1})(x_i-x_{i+1})\cdot\ldots\cdot(x_i-x_n)} =\]
        \[p_i(x) = \prod_{0 \leq j \leq n \mid j \neq i}\frac{x-x_j}{x_i-x_j}\]

        \item Sia quindi $p(x) \in K[x]_{\leq n}$ definito come:
        \[p(x):=y_0p_0(x)+\ldots+y_np_n(n)\]
        
        \item Dunque, $\forall k \in [0,n]$ si ha che:
        \[p(x_k) = y_0p_0(x_k)+\ldots+y_kp_k(x_k)+\ldots+y_np_n(x_k) \implies \]
        \[p(x_k) = y_0 \cdot 0+ \ldots + y_k \cdot 1 + \ldots + y_n \cdot 0 = y_k \implies \left \{ \begin{array}{l} p(x_0) = y_0 \\ \vdots \\ p(x_n)=y_n \end{array}\right .\]
        
        \item Per la proposizione precedente, concludiamo che $p(x)$ sia l'unico polinomio in $K[x]_{\leq n}$ tale che $p(x_0)=y_0, \ldots, p(x_n) = y_n$
        
        $\hfill\qed$
    \end{itemize}

    \begin{framedobs}{}
        Dati $n+1$ punti del piano cartesiano $(x_0, y_0), \ldots, (x_n, y_n)$, è possibile utilizzare l'interpolazione di Lagrange per trovare l'unico polinomio di grado $n$, dunque $p(x) \in K[x]_{\leq n}$, passante per ognuno di tali punti
    \end{framedobs}

    \newpage

    \begin{framedalgo}{Algoritmo SSS}
        Il seguente algoritmo permette di suddividere in $n+1$ \textbf{partizioni} un \textbf{segreto} $s \in K$, per poi ricostruire il quest'ultimo tramite le partizioni stesse:

        \begin{enumerate}
            \item Scelti casualmente i coefficienti $a_1, \ldots, a_n \in K$, definiamo $p(x) \in K[x]_{\leq n}$ come:
            \[p(x) = s+a_1x+\ldots+a_nx_n\]
            \item Scelti $n+1$ valori $x_0, \ldots, x_n \mid x_i \neq x_j, \forall i \neq j$, costruiamo i nodi dell' \textbf{interpolazione di Lagrange} come $(x_0, p(x_0)), \ldots, (x_n, p(x_n))$
            \item Distribuiamo ogni nodo ad eventuali interlocutori
            \item Una volta riottenuti gli $n+1$ nodi, tramite l'interpolazione di Lagrange è possibile ricostruire $p(x)$
            \item Infine, poiché $s$ è il termine noto di $p(x)$, è possibile riottenere il segreto tramite $p(0)=s$
        \end{enumerate}
    \end{framedalgo}


\end{document}
