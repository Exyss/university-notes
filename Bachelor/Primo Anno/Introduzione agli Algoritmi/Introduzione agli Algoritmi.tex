\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%

\def\useItalian{1}  % 1 = Italian, 0 = English

\def\courseName{Introduzione agli Algoritmi}

\def\coursePrerequisites{Conoscenze discrete di programmazione}

\def\book{\curlyquotes{Introduzione agli algoritmi e strutture dati}, T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein}

\def\authorName{Simone Bianco}
\def\email{bianco.simone@outlook.it}
\def\github{https://github.com/Exyss/university-notes}
\def\linkedin{https://www.linkedin.com/in/simone-bianco}


%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../../packages/Nyx/nyx-packages}
\usepackage{../../../packages/Nyx/nyx-styles}
\usepackage{../../../packages/Nyx/nyx-frames}
\usepackage{../../../packages/Nyx/nyx-macros}
\usepackage{../../../packages/Nyx/nyx-title}
\usepackage{../../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../../packages/Nyx/logo.png}

\if\useItalian1
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} Università di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \ifdefined\book
        \subtitle{Appunti integrati con il libro \book}
    \fi
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \ifdefined\book
        \subtitle{Lecture notes integrated with the book \book}
    \fi
    \author{\textit{Author}\\\authorName}
\fi


\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%

\chapter{Algoritmi, Efficienza e RAM}

\section{Algoritmi e Strutture Dati}

La definizione di informatica proposta dall'\textbf{ACM (Association for Computing Machinery)}, nonché una delle principali organizzazioni scientifiche di informatici di tutto il mondo, è la seguente: \textit{”\textbf{L'informatica è la scienza degli algoritmi} che descrivono e trasformano l'informazione: la loro teoria, analisi, progetto, efficienza, realizzazione e applicazione.”}

Gli \textbf{algoritmi}, dunque, sono un concetto fondamentale per l'informatica, fino ad esserne il fulcro. Ma cosa intendiamo per algoritmo?

\begin{frameddefn}{Algoritmo}

Un \textbf{algoritmo} è “\textbf{una sequenza di comandi elementari} ed univoci che terminano in un \textbf{tempo finito} ed operano su \textbf{strutture dati}”.

\end{frameddefn}

Un comando viene definito \textbf{elementare} quando \textbf{non può essere scomposto} in comandi più semplici. I comandi elementari sono quindi \textbf{univoci} e possono essere interpretati in un solo modo.

Se un algoritmo è \textbf{ben specificato}, chi (o ciò che) lo esegue non ha bisogno di pensare, deve solo eseguire con precisione i passi elencati nell'algoritmo nella sequenza in cui appaiono. Se un calcolatore esegue un algoritmo e l'output è errato, \textbf{la colpa non è del calcolatore, ma del progettista}.

Prima di poter risolvere un problema abbiamo, ovviamente, bisogno di pensare ad un modo per poter \textbf{gestire i dati} che vengono utilizzati dall'algoritmo stesso. A tal fine, sarà necessario definire le opportune \textbf{strutture dati} su cui opererà l'algoritmo, ossia gli strumenti necessari per \textbf{organizzare} e \textbf{memorizzare} i dati veri e propri, semplificandone l'accesso e la modifica.

È importante sottolineare che \underline{\textbf{non esiste una struttura dati che sia adeguata per}} \underline{\textbf{ogni problema}}, dunque è necessario conoscere proprietà, vantaggi e svantaggi delle principali strutture dati in modo da poter scegliere di volta in volta quale sia quella \textbf{più adatta al problema}.

La scelta della struttura dati da adottare nella soluzione di un problema è un \textbf{aspetto fondamentale} per la risoluzione del problema stesso, al pari del progetto dell'algoritmo stesso. Per questa ragione, gli algoritmi e le strutture dati fondamentali vengono sempre studiati e illustrati assieme.

\section{Efficienza di un algoritmo}

Un aspetto fondamentale che va affrontato nello studio degli algoritmi è la loro \textbf{efficienza}, cioè la quantificazione delle loro \textbf{esigenze in termini di tempo e di spazio}, ossia tempo di esecuzione e quantità di memoria richiesta.

La scelta di un algoritmo rispetto ad altro, nel caso i due algoritmi portino allo stesso risultato, è molto importante:

\begin{itemize}
    \item I calcolatori sono molto veloci, ma non infinitamente veloci
    \item La memoria è economica e abbondante, ma non è né gratuita né illimitata.
\end{itemize}

Un parametro fondamentale per la scelta dell'algoritmo è proprio la quantità di risorse spazio-tempo utilizzate. Nelle sezioni successive vedremo il concetto di \textbf{costo computazionale} degli algoritmi in termini di numero di operazioni elementari e quantità di spazio di memoria necessario in funzione della dimensione dell'input.

\subsubsection{Esempio di valutazione dell'efficienza}

Immaginiamo di voler risolvere il seguente problema: vogliamo \textbf{ordinare una lista di $n = \textbf{10}^\textbf{6}$ numeri interi.} Vista l'enorme quantità di numeri, decidiamo di far svolgere questo compito ad un elaboratore. A nostra disposizione abbiamo \textbf{due calcolatori}:

\begin{itemize}
    \item Un \textbf{calcolatore veloce}, che chiameremo \textbf{V}, in grado di svolgere $10^9 \text{ operazioni/sec}$
    \item Un \textbf{calcolatore lento}, che chiameremo \textbf{L}, in grado di svolgere $10^7 \text{ operazioni/sec}$
\end{itemize}

Immaginiamo di essere in grado di saper sviluppare solo \textbf{due algoritmi di ordinamento} (di cui per ora non vedremo il funzionamento, ma solo le specifiche temporali):

\begin{itemize}
    \item L'algoritmo \textbf{Insertion Sort}, richiedente $2n^2$ operazioni \textbf{(più lento)}
    \item L'algoritmo \textbf{Merge Sort}, richiedente $50n \cdot \log(n)$ operazioni \textbf{(più veloce)}
\end{itemize}

Ci chiediamo se la maggior velocità del calcolatore V sia in grado di \textbf{contro-bilanciare} la maggior lentezza dell'algoritmo IS. Proviamo quindi a calcolare il costo temporale di entrambe le scelte (\textbf{ATTENZIONE:} con $\log$ intendiamo il \textbf{logaritmo in base 2}):

\[ V(IS) = \frac{2 \cdot (10^6)^2 \text{ operazioni}}{10^9 \text{ operazioni/sec}} = 2000 \text{ sec} \approx 33 \text{ min}\]

\[ L(MS) = \frac{50 \cdot 10^6 \cdot \log(10^6) \text{ operazioni}}{10^7 \text{ operazioni/sec}} \approx 100 \text{ sec} \approx 1.5 \text{ min}\]

Notiamo quindi che, nonostante la differenza di caratteristiche hardware, \textbf{la scelta dell'algoritmo è cruciale per l'efficienza}. 

Per ricalcare maggiormente il concetto, proviamo ad aumentare l'input a $n = 10^7$

\[ V(IS) = \frac{2 \cdot (10^7)^2 \text{ operazioni}}{10^9 \text{ operazioni/sec}} \approx 55.5 \text{ ore} \approx 2.3 \text{ giorni}\]

\[ L(MS) = \frac{50 \cdot 10^7 \cdot \log(10^7) \text{ operazioni}}{10^7 \text{ operazioni/sec}} \approx 19.5 \text{ min}\]

Aumentando l'input di un solo ordine di grandezza, la \textbf{differenza} in termini di costi temporali dei due algoritmi è \textbf{abissale}.

\quad

\subsection{Random Access Machine (RAM)}

Nell'esempio precedentemente visto, abbiamo considerato \textbf{due macchine diverse}, dove una era più performante dell'altra. Ciò non è un fattore da ignorare, poiché ovviamente le caratteristiche hardware dell'elaboratore \textbf{influiscono} direttamente sulle \textbf{performance dell'algoritmo}: se nell'esempio precedente calcolassimo $V(MS)$ con $n = 10^6$, il tempo impiegato dall'algoritmo sarebbe circa \textbf{1 secondo}, rispetto ai \textbf{100 secondi} impiegati da L(MS).

Per poter valutare la \textbf{vera efficienza} di un algoritmo, dunque, è necessario quantificare le risorse che esso richiede per la sua esecuzione senza che tale analisi sia \textbf{influenzata da una specifica tecnologia} che, inevitabilmente, col tempo \textbf{diviene obsoleta}. Dunque, è necessario valutare l'algoritmo come se venisse eseguito da una \textbf{macchina astratta} rispettante queste caratteristiche, ossia la \textbf{Random Access Machine}.

\begin{frameddefn}{Random Access Machine}
    La \textbf{Random Access Machine (RAM)} è una macchina astratta, la cui validità e potenza concettuale risiede nel fatto che \textbf{non diventa obsoleta}
    
    Le caratteristiche del modello RAM sono:
    \begin{itemize}
        \item Un \textbf{singolo processore} che esegue le operazioni \textbf{sequenzialmente}
        \item Esistono \textbf{solo operazioni elementari} e l'esecuzione di ciascuna delle quali richiede per definizione un \textbf{tempo costante} (es.: operazioni aritmetiche, letture, scritture, salto condizionato, ecc.)
        \item Esiste un \textbf{limite alla dimensione} di ogni valore memorizzato ed al numero complessivo di valori utilizzati, dipendente dalle dimensioni delle word in memoria
    \end{itemize}
\end{frameddefn}

\newpage

\subsection{Misura di Costo Uniforme}

Sia $d$ la \textbf{dimensione di bit di ogni parola} contenuta in memoria. Se è soddisfatta l'ipotesi che ogni dato in input sia un valore \textbf{minore} di $2^d$, ciascuna operazione elementare sui dati del problema verrà eseguita in un \textbf{tempo costante}. In tal caso si parla di \textbf{misura di costo uniforme}.

Tale criterio \textbf{non è sempre realistico}: se un dato del problema non rispetta tale ipotesi, esso dovrà essere comunque memorizzato. In tal caso, sarà necessario usare  \textbf{più parole di memoria} e, di conseguenza, anche le operazioni elementari su di esso dovranno essere reiterate per tutte le parole di memoria che lo contengono, richiedendo quindi un tempo non più costante. Per questo motivo, in ambito scientifico viene utilizzata la \textbf{misura di costo logaritmica}, più realistica rispetto a quella uniforme. Tuttavia, in questo corso essa \textbf{non verrà analizzata}.

\subsubsection{Esempio di costo uniforme}

Analizziamo il seguente codice:

\begin{verbatim}
def PotenzaDi2(n)
    x = 1
    for i in range(n):
        x = x*2
    return x
\end{verbatim}

Il tempo di esecuzione totale è \textbf{proporzionale ad n}, poiché si tratta di un \textbf{ciclo eseguito $n$ volte}, dove ad ogni iterazione vengono compiute tre operazioni, ciascuna di \textbf{costo unitario}:
\begin{itemize}
    \item Viene incrementato il contatore relativo al ciclo for
    \item Viene calcolato $x \cdot 2$
    \item Viene assegnato il risultato del calcolo ad $x$
\end{itemize}

\chapter{Notazione Asintotica}

\section{Notazione O grande, Omega e Teta}

Come abbiamo accennato nel capitolo precedente, per poter \textbf{valutare l'efficienza} di un algoritmo, così da poterlo confrontare con algoritmi diversi che risolvono lo stesso problema, bisogna essere in grado di valutarne il suo \textbf{costo computazionale}, ovvero il suo tempo di esecuzione e/o le sue necessità in termini di memoria.

Questo tipo di valutazione, se effettuata nel dettaglio, risulta molto complessa e spesso contiene dettagli superflui. Per questo motivo, ci limiteremo a dare una visione più \textbf{astratta} e valutare solo quello che informalmente possiamo chiamare \textbf{tasso di crescita}, cioè la velocità con cui il tempo di esecuzione cresce all'aumentare della dimensione dell'input.

Tuttavia, poiché per piccole dimensioni dell'input il tempo impiegato è comunque poco indipendentemente dall'algoritmo, tale valutazione risulta più efficace quando la dimensione dell'input è \textbf{sufficientemente grande}, dunque tendente all'infinito.

\begin{frameddefn}{Notazione Asintotica}
    In matematica la \textbf{notazione asintotica} permette di confrontare il \textbf{tasso di crescita} (comportamento asintotico) di una funzione nei confronti di un'altra.
    
    Il calcolo asintotico è utilizzato per analizzare la \textbf{complessità di un algoritmo}, stimandone in particolar modo, l'aumentare del \textbf{tempo di esecuzione} al crescere della \textbf{dimensione} $n$ dell'input.
\end{frameddefn}

In particolare, esistono \textbf{tre tipologie di notazione asintotica}:
\begin{itemize}
    \item \textbf{Notazione asintotica O grande}: definisce il limite superiore asintotico
    \item \textbf{Notazione asintotica $\Omega$}: definisce il limite inferiore asintotico
    \item \textbf{Notazione asintotica $\Theta$}: definisce il limite asintotico stretto
\end{itemize}

\newpage

\subsection{Notazione O grande}

Per poter comprendere cosa si intende con \textbf{notazione O grande}, partiamo direttamente dalla sua definizione

\begin{frameddefn}{O grande}
    Date due funzioni $f(n), g(n) \geq 0$ si dice che \textbf{$\textbf{\textit{f(n)}}$ è in $\textbf{\textit{O(g(n))}}$} se esistono due costanti $c$ ed $n_0$ tali che $0 \leq f(n) \leq c \cdot g(n)$ per ogni $n \geq n_0$
    
    \begin{center}
        \includegraphics[scale=0.75]{resources/images/chapter_2/bigO_1.png}
    \end{center}
    
    In $O(g(n))$, dunque, troviamo \textbf{tutte} le funzioni  \textbf{«dominate»} dalla funzione $g(n)$
\end{frameddefn}

La notazione \textbf{O grande}, dunque, definisce quello che è il \textbf{limite superiore asintotico} della funzione $f(n)$: una volta superato un certo valore $n_0$ (dove $n \to +\infty$) l'andamento della funzione $f(n)$ viene \textbf{limitato} dalla funzione $c \cdot g(n)$, rimanendo sempre \textbf{al di sotto di essa} (dunque viene «dominata» da essa).

\subsubsection{Esempi sull'O grande}
\begin{itemize}
    \item Sia $f(n) = 3n + 3$. Possiamo dire che \textbf{$\textbf{\textit{f(n)}}$ è in $\textbf{\textit{O(n}}^\textbf{\textit{2}}\textit{)}$}, in quanto esiste almeno una $c$ (ossia $c = 6$) per cui :
    \[ 3n + 3 \leq c \cdot n^2, \;\; \forall n \geq n_0, \;\; c = 6, \;\; n_0 = 1\]
    
    Tuttavia, possiamo anche dire che \textbf{$\textbf{f(n)}$ è in $\textbf{O(n)}$}, in quanto:
    \[ 3n + 3 \leq c \cdot n, \;\; \forall n \geq n_0, \;\; c \geq 6, \;\; n_0 = 1\]
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/bigO_2.png}
    \end{center}
    
    \item Sia $f(n) = n^2 + 4n$. Tale \textbf{$\textbf{\textit{f(n)}}$ è in $\textbf{\textit{O(n}}^\textbf{\textit{2}}\textit{)}$} in quanto:
    \[ n^2 + 4n \leq c \cdot n^2, \;\; \forall n \geq n_0, \;\; c \geq 5, \;\; n_0 = 1\]
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/bigO_3.png}
    \end{center}
    
\end{itemize}

\quad

Notiamo come nel primo esempio abbiamo concluso che un \textbf{polinomio di primo grado} sia in $O(n)$, mentre nel secondo esempio abbiamo concluso che un \textbf{polinomio di secondo grado sia} in $O(n^2)$. Possiamo generalizzare la cosa nel seguente teorema:

\begin{framedthm}{}
    Sia $f(n)$ un \textbf{polinomio} di grado $m$, definito matematicamente come
    \[ f(n) = \sum_{i = 0}^m a_i n^i = a_0 + a_1 n + a_2 n^2 + ... + a_m n^m\]
    
    allora possiamo concludere che $\textbf{\textit{f(n)}}$ \textbf{è in} $\textbf{\textit{O(n}}^\textbf{\textit{m}}\textbf{\textit{)}}$
\end{framedthm}

\subsubsection{Dimostrazione per induzione}

\begin{itemize}
    \item \textbf{Caso base}: Abbiamo $m = 0$, per cui $f(n) = a_0 \cdot n^0$, dunque è una funzione costante e di conseguenza è in $O(1)$, che coincide con $O(n^0)$
    
    \item \textbf{Ipotesi induttiva}: Affermiamo che
    \[ \sum_{i=0}^{k} a_i n^i\]
    è un $O(n^k)$ per ogni $k<m$, cioè esiste una costante $c'$ tale che
    \[\sum_{i=0}^{k} a_i n^i \leq c' \cdot n^k\]
    
    \item \textbf{Passo induttivo}: Dobbiamo dimostrare che \[f(n) = \sum_{i=0}^{m} a_i n^i \leq c' \cdot n^m\]
    Si osservi che, mettendo in evidenza l'ipotesi induttiva, $f(n)$ può essere riscritto come 
        \[f(n) = \sum_{i=0}^{m} a_i n^i = a_m n^m + \sum_{i=0}^{k} a_i n^i\]
        per ogni $k<m$. Inoltre, per ipotesi induttiva, sappiamo che
        \[\sum_{i=0}^{k} a_i n^i \leq c' \cdot n^k\]
        dunque possiamo formulare la seguente catena di disuguaglianze
        \[f(n) = a_m n^m + \sum_{i=0}^{k} a_i n^i \leq a_m n^m + c' \cdot n^k \leq a_m n^m + c' \cdot n^m \]
        riscrivendo $a_m n^m + c' \cdot n^m$ come $(a_m + c') \cdot n^m$
        e ponendo $c'' = a_m + c'$ otteniamo che
        \[f(n) \leq c'' \cdot n^m \]
        che per ipotesi sappiamo essere vera, dunque concludiamo che $f(n)$ è in $O(n^m)$
    
\end{itemize}

\newpage

    \subsection{Notazione Omega}
    
    Nella sezione precedente, abbiamo definito la \textbf{Notazione O grande} come \textbf{limite superiore asintotico}. La \textbf{Notazione Omega}, invece, risulta essere il suo \textbf{opposto}:
    
    \begin{frameddefn}{Omega}
    Date due funzioni $f(n), g(n) \geq 0$ si dice che \textbf{$\textbf{\textit{f(n)}}$ è in $\Omega \textbf{\textit{(g(n))}}$} se esistono due costanti $c$ ed $n_0$ tali che $f(n) \geq c \cdot g(n)$ per ogni $n \geq n_0$
    
    \begin{center}
        \includegraphics[scale=0.75]{resources/images/chapter_2/omega_1.png}
    \end{center}
    
    In $O(g(n))$, dunque, troviamo \textbf{tutte} le funzioni che \textbf{«dominano»} dalla funzione $g(n)$
\end{frameddefn}

La notazione \textbf{Omega}, dunque, definisce quello che è il \textbf{limite inferiore asintotico} della funzione $f(n)$: una volta superato un certo valore $n_0$ (dove $n \to +\infty$) l'andamento della funzione $f(n)$ viene \textbf{limitato} dalla funzione $c \cdot g(n)$, rimanendo sempre \textbf{al di sopra di essa} (dunque «domina» essa).

\subsubsection{Esempi sull'Omega}

\begin{itemize}
    \item Sia $f(n) = 2n^2+3$. Possiamo dire che $f(n) = \Omega(n)$ in quanto
    \[ 2n^2+3 \geq c \cdot n, \;\; \forall n \geq n_0, \;\; c = 1\]
    Tuttavia, possiamo anche dire che $f(n) = \Omega(n^2)$, in quanto:
    \[ 2n^2+3 \geq c \cdot n^2, \;\; \forall n \geq n_0, \;\; c \leq 2\]
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_2/omega_2.png}
    \end{center}
\end{itemize}

Analogamente alla \textbf{notazione O grande}, possiamo formulare il seguente teorema, la cui dimostrazione è analoga a quella già mostrata per O grande:

\begin{framedthm}{}
    Sia $f(n)$ un \textbf{polinomio} di grado $m$, definito matematicamente come
    \[ f(n) = \sum_{i = 0}^m a_i n^i = a_0 + a_1 n + a_2 n^2 + ... + a_m n^m\]
    
    allora possiamo concludere che $\textbf{\textit{f(n)}}$ \textbf{è in} $\Omega \textbf{\textit{(n}}^\textbf{\textit{m}}\textbf{\textit{)}}$
\end{framedthm}

\quad

    \subsection{Ordini di grandezza di O grande ed Omega}
    
    \begin{itemize}
        \item Sia $f(n) = \log (n)$. Allora $f(n)$ è in $O(\sqrt{n})$ e in $\Omega(1)$.
        
        Più in generale, abbiamo che:
        
        \begin{itemize}
            \item $\log^a (n) = O(\sqrt[b]{n})$ per ogni $a,b \geq 1$
            \item $z^a (n) = \Omega(1)$ per ogni $a$
            \item Dunque, possiamo dire che \textbf{un poli-logaritmo è dominato da qualunque radice} e che \textbf{un poli-logaritmo domina qualunque costante}
        \end{itemize}
        
        \item Sia $f(n) = \sqrt[a]{n}$. Allora $f(n)$ è in $O(n)$ e in $\Omega(\log (n))$.
        
        Più in generale, abbiamo che:
        
        \begin{itemize}
            \item $\sqrt[a]{n} = O(n^b)$ per ogni $a,b \geq 1$
            \item$\sqrt[a]{n} = \Omega(\log^b(n))$ per ogni $a,b \geq 1$
            \item Dunque, possiamo dire che \textbf{una radice è dominata da qualunque polinomio} e che \textbf{una radice domina qualunque poli-logaritmo}
        \end{itemize}
        
        \item Sia $f(n) = n^a$. Allora $f(n)$ è in $O(2^n)$ e in $\Omega(\sqrt[b]{n})$.
        
        \begin{itemize}
            \item $n^a = O(b^n)$ per ogni $a \geq 1$ ed ogni $b \geq 2$
            \item $n^a = \Omega(\sqrt[b]{n})$ per ogni $a,b \geq 1$
            \item Dunque, possiamo dire che \textbf{un polinomio è dominato da qualunque esponenziale} e che \textbf{un polinomio domina qualunque radice}
        \end{itemize}
    \end{itemize}
    
    \quad
    
    In termini più generali, notiamo come la nostra \textbf{scala di O grandi ed Omega} segua la \textbf{scala degli ordini di grandezza} delle successioni numeriche (per $n \to +\infty$):
    \[ 1 \prec \log^a (n) \prec \sqrt[b]{n} \prec n^c \prec d^n \prec n! \prec n^n\]
    
    \quad
    
    \subsection{Notazione Teta}
    
    Una volta definite le \textbf{notazioni O grande ed  Omega}, possiamo dare una definizione di \textbf{notazione Teta}:
    
    \begin{frameddefn}{Teta}
    Date due funzioni $f(n), g(n) \geq 0$ si dice che \textbf{$\textbf{\textit{f(n)}}$ è in $\Theta \textbf{\textit{(g(n))}}$} se esistono tre costanti $c_1$, $c_2$ ed $n_0$ tali che $ c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n)$ per ogni $n \geq n_0$
    
    \begin{center}
        \includegraphics[scale=0.75]{resources/images/chapter_2/theta_1.png}
    \end{center}
    
    Dunque, se $f(n)$ è \textbf{sia} in $O(g(n)$ \textbf{sia} in $\Omega(g(n))$, allora è anche in $\Theta(g(n))$.
\end{frameddefn}

La \textbf{notazione Teta}, quindi, rappresenta il \textbf{limite stretto asintotico} della funzione: una volta superata una certa $n$, la funzione $f(n)$ \textbf{si comporta come} $g(n)$.

\newpage

\subsection{Calcolo delle Notazioni Asintotiche con i Limiti}

In termini generali,  possiamo formulare le seguenti regole basandoci sulla definizione di limite per $n \to +\infty$:

\[ \lim_{n \to +\infty} \frac{f(n)}{g(n)} = k > 0 \text{ allora } f(n) = \Theta(g(n))\]

\[ \lim_{n \to +\infty} \frac{f(n)}{g(n)} = +\infty \text{ allora } f(n) = \Omega(g(n)) \text{ \underline{ma} } f(n) \neq \Theta(g(n))\]

\[ \lim_{n \to +\infty} \frac{f(n)}{g(n)} = 0 \text{ allora } f(n) = O(g(n)) \text{ \underline{ma} } f(n) \neq \Theta(g(n))\]

Se il limite del rapporto tra $f(n)$ e $g(n)$ \textbf{non esiste}, allora è necessario procedere diversamente.

\quad

\section{Algebra della Notazione Asintotica}

Oltre all'uso del calcolo con limiti, per semplificare il \textbf{calcolo del costo computazionale} tramite limite asintotico degli algoritmi possono essere utilizzate \textbf{tre regole algebriche}:

\begin{itemize}
    \item \textbf{Regola delle costanti moltiplicative}
    \item \textbf{Regola della commutatività con somma}
    \item \textbf{Regola della commutatività con prodotto}
\end{itemize}

Le \textbf{dimostrazioni} di tali regole possono essere facilmente ricavate dalle definizioni stesse dei vari limiti asintotici, tuttavia verranno omesse poiché non utili ai fini dell'apprendimento.

\begin{frameddefn}{Regola delle Costanti Moltiplicative}
    \begin{itemize}
        \item Per ogni $k > 0$ e per ogni $f(n) \geq 0$, se $f(n)$ è in $O(g(n))$ allora anche $k \cdot f(n)$ è in $O(g(n))$.
        \item Per ogni $k > 0$ e per ogni $f(n) \geq 0$, se $f(n)$ è in $\Omega(g(n))$ allora anche $k \cdot f(n)$ è in $\Omega(g(n))$.
        \item Per ogni $k > 0$ e per ogni $f(n) \geq 0$, se $f(n)$ è in $\Theta(g(n))$ allora anche $k \cdot f(n)$ è in $\Theta(g(n))$.
    \end{itemize}
\end{frameddefn}

In modo informale, quindi, possiamo affermare che \textbf{le costanti moltiplicative possono essere ignorate} durante il calcolo di un qualsiasi limite asintotico.

\textbf{ATTENZIONE}: è necessario sottolineare che è necessario che la costante moltiplicativa \textbf{non sia all'esponente} della funzione (es: nella funzione $f(n) = 2^{k*n}$ non possiamo ignorare la $k$)

\quad

\begin{frameddefn}{Regola della Commutatività con Somma}
Sia $f(n) = p(n) + q(n)$. Per ogni $p(n), q(n) > 0$ abbiamo che:
    \begin{itemize}
        \item Se $p(n)$ è in $O(g(n))$ e $q(n)$ è in $O(h(n))$ $\Longrightarrow$ $f(n)$ è in $O(g(n)+h(n)) = O(max(g(n),h(n)))$.
        \item Se $p(n)$ è in $\Omega(g(n))$ e $q(n)$ è in $\Omega(h(n))$ $\Longrightarrow$ $f(n)$ è in $\Omega(g(n)+h(n)) = \Omega(max(g(n),h(n)))$.
        \item Se $p(n)$ è in $\Theta(g(n))$ e $q(n)$ è in $\Theta(h(n))$ $\Longrightarrow$ $f(n)$ è in $\Theta(g(n)+h(n)) = \Theta(max(g(n),h(n)))$.
    \end{itemize}
\end{frameddefn}

In modo informale, quindi, possiamo affermare che data una funzione $f(n) = p(n) + q(n)$, allora un qualsiasi limite asintotico di $f(n)$ è uguale al \textbf{massimo tra il limite asintotico di $p(n)$ e di $q(n)$}

\quad


\begin{frameddefn}{Regola della Commutatività con Prodotto}
Sia $f(n) = p(n) \cdot q(n)$. Per ogni $p(n), q(n) > 0$ abbiamo che:
    \begin{itemize}
        \item Se $p(n)$ è in $O(g(n))$ e $q(n)$ è in $O(h(n))$ $\Longrightarrow$ $f(n)$ è in $O(g(n) \cdot h(n))$.
        \item Se $p(n)$ è in $\Omega(g(n))$ e $q(n)$ è in $\Omega(h(n))$ $\Longrightarrow$ $f(n)$ è in $\Omega(g(n) \cdot h(n))$.
        \item Se $p(n)$ è in $\Theta(g(n))$ e $q(n)$ è in $\Theta(h(n))$ $\Longrightarrow$ $f(n)$ è in $\Theta(g(n) \cdot h(n))$.
    \end{itemize}
\end{frameddefn}

In modo informale, quindi, possiamo affermare che data una funzione $f(n) = p(n) \cdot q(n)$, allora un qualsiasi limite asintotico di $f(n)$ è uguale al \textbf{prodotto tra il limite asintotico di $p(n)$ e di $q(n)$}

\quad

\subsubsection{Esercizi svolti sull'algebra asintotica}

\begin{enumerate}
    \item Trovare il limite asintotico stretto di $f(n) = 3n^2 + 7$
    \[ f(n) = 3n^2 + 7 = \Theta(n^2) + \Theta(1) = \Theta(n^2)\]
    
    \item Trovare il limite asintotico stretto di $f(n) = 3n2^n + 4n^4$
    \[ f(n) = 3n2^n + 4n^4 = \Theta(n2^n) + \Theta(n^4) = \Theta(n2^n)\]
    
    \item Trovare il limite asintotico stretto di $f(n) = 2^{2n}$
    \[ f(n) = 2^{2n} = 2^n \cdot 2^n = \Theta(2^n) \cdot \Theta(2^n) = \Theta(2^{2n})\]
    
    \item Trovare il limite asintotico stretto di $f(n) = \log^n (n) + 8\cdot 2^{n \cdot \log (n)} + 3$
    \[ f(n) = \log^n (n) + 8\cdot 2^{n \cdot \log (n)} + 3 =
    \log^n(n) + 8 \cdot 2^{\log(n^n)} + 3 = \]
    \[ = \log^n(n) + 8 \cdot n^n + 3 = \Theta(\log^n(n)) + \Theta(n^n) + \Theta(1) = \Theta(n^n)\]
    
    \item Trovare il limite asintotico stretto di $f(n) = n^2 \cdot \log(n)$
    \[ f(n) = n^2 \cdot \log(n) = \Theta(n^2) \cdot \Theta(\log(n)) = \Theta(n^2 \cdot \log(n))\]
    
    \item Trovare il limite asintotico stretto di $f(n) = 3n \cdot \log(n) + 2n^2$
    \[ f(n) = 3n \cdot \log(n) + 2n^2 = \Theta(n) \cdot \Theta(\log(n)) + \Theta(n^2) = \Theta(n \cdot \log(n)) + \Theta(n^2) = \Theta(n^2)\]
    
    \item Trovare il limite asintotico stretto di $f(n) = 2^{\frac{\log(n)}{2}} + 5n$
    \[ f(n) = 2^{\frac{\log(n)}{2}} + 5n = (2^{\log(n)})^{\frac{1}{2}} + 5n = \sqrt{n} + 5n = \Theta(\sqrt{n}) + \Theta(n) = \Theta(n)\]
    
    \item Trovare il limite asintotico stretto di $f(n) = 4^{\log(n)}$
    \[ f(n) = 4^{\log(n)} = (2^2)^{\log(n)} = (2^{\log(n)})^2 = n^2 = \Theta(n^2)\]
            
    \item Trovare il limite asintotico stretto di $f(n) = \sqrt{2}^{\,\log(n)}$
    \[ f(n) = \sqrt{2}^{\,\log(n)} = 2^{\frac{1}{2} \cdot \log(n)} = \sqrt{2^{\log(n)}} = \sqrt{n} = \Theta(\sqrt{n})\]
\end{enumerate}

\newpage

\subsection{Sommatorie e Tecniche di dimostrazione}
\label{sums}

Di seguito vedremo alcune \textbf{tecniche di dimostrazione} che ci permettono di stabilire se una certa \textbf{funzione semplice o complessa} rientri all'interno di una determinata famiglia di funzioni O grandi, Omega o Teta.

\begin{itemize}
    \item Dimostrare o confutare la seguente proposizione
    \[ f(n) = 4^n \text{ è in } O(2^n)\]
    
    Tramite l'algebra asintotica, siamo già in grado di rispondere a tale proposizione:
    \[ 4^n = 2^{2n} = 2^n \cdot 2^n = O(2^n) \cdot O(2^n) = O(2^{2n})\]
    
\end{itemize}
    Dunque, la proposizione è \textbf{falsa}. Tuttavia, scegliamo di dimostrare la cosa in forma più rigorosa:
    
\begin{center}
    
    \textbf{Dimostrazione per assurdo}: supponiamo che $f(n) = O(2^n)$. Allora abbiamo che 
    \[ \exists c, n_0 \;|\; f(n) \leq c \cdot 2^n, \;\; \forall n \geq n_0 \]
    \[4^n \leq c \cdot 2^n \]
    \[2^n \cdot 2^n \leq c \cdot 2^n \]
    \[2^n \leq c \]
    
    \textbf{Falso} \text{una volta superato un certo valore} $n_0$
\end{center}


\begin{itemize}
    \item Dimostrare la seguente proposizione
    \[ f(n) = (n+10)^3 \text{ è in } \Theta(n^3)\]
    
    \begin{itemize}
        \item Per dimostrare che sia $\Theta(n^3)$, dimostriamo che sia in $O(n^3)$ e in $\Omega(n^3)$:
    
        \begin{itemize}
            \item Al crescere di $n$, esisterà un valore $n_0 = 10$ tale che
            \[ (n+10)^3 = (n+n_0)^3 \leq (n+n)^3 = (2n)^3 = 8n^3 = O(n^3)\]
            
            \item Poiché $10 >0$, si vede facilmente che
            \[ (n+10)^3 \geq (n+0)^3 = n^3 = \Omega(n^3)\]
            
        \end{itemize}
        
        Poiché $f(n)$ è \textbf{sia} in $O(n^3)$, \textbf{sia} in $\Omega(n^3)$, allora è \textbf{anche} in $\Theta(n^3)$
    \end{itemize}
    
    \quad
    
    \item Dimostrare la seguente proposizione
    \[ S_n = \sum_{k=1}^{n} k \text{ è in } \Theta(n^2)\]
    
    Come nell'esempio precedente, per dimostrare che sia $\Theta(n^2)$, dimostriamo che sia in $O(n^2)$ e in $\Omega(n^2)$:
    
    \begin{itemize}
        \item Per dimostrare che $S_n$ è in $O(n^2)$, è necessario fare un \textbf{"salto logico"}. Partiamo riscrivendo la sommatoria in forma estesa
        
        \[ S_n = 1+2+3+...+(n-1)+n\]
        
        Notiamo come \textbf{ogni singolo termine della sommatoria sia $\leq n$}. Dunque, possiamo scrivere la seguente disequazione:
        
        \[ 1+2+3+...+ (n-1) + n \leq n+n+n+...+n+n\]
        
        Nella parte destra della disequazione, dunque, abbiamo una \textbf{somma di $n$ volte $n$}, riscrivibile come $n \cdot n$
        \[ S_n \leq n \cdot n \Longrightarrow S_n \leq n^2\]
        
        A questo punto, ci basta notare che \textbf{$n^2$ è in $O(n^2)$} e quindi che, poiché $S_n \leq n^2$, di \textbf{conseguenza} anche \textbf{$S_n$ è in $O(n^2)$}.
        
        \item Dimostriamo ora che $f(n) = \Omega(n^2)$ in modo analogo a quello precedente. Riscriviamo nuovamente la sommatoria in forma estesa
        
        \[ S_n = 1+2+3+...+(n-2)+(n-1)+n\]
        
        Questa volta, notiamo che essa può essere \textbf{divisa a metà}, ottenendo \textbf{due categorie}: 
        
        \begin{center}
            \begin{tabular}{c|c}
                \\
                $\underbrace{1+2+3+4+5+...}_{\text{Numeri } \leq \frac{n}{2}}$ & $\underbrace{...+(n-2)+(n+1)+n}_{\text{Numeri } \geq \frac{n}{2}}$
                \\
            \end{tabular}
        \end{center}
        
        A questo punto, è necessario effettuare un \textbf{ulteriore "salto logico"}: sappiamo che \textbf{tutti i numeri minori di $\frac{n}{2}$ sono anche maggiori di 0}, mentre \textbf{quelli maggiori di $\frac{n}{2}$ sono ovviamente maggiori di $\frac{n}{2}$}.
        
        Dunque, possiamo scrivere la seguente disequazione:
        \[ 1+2+3+...+(n-2)+(n-1)+n \geq \underbrace{0+0+0+...}_{\frac{n}{2} \text{ volte}} + \underbrace{...+\frac{n}{2}+\frac{n}{2}+\frac{n}{2}}_{\frac{n}{2} \text{ volte}}\]
        
        \[ S_n \geq \frac{n}{2} \cdot \frac{n}{2} \Longrightarrow S_n \geq \frac{1}{2} n^2 \]
        
        A questo punto, ci basta notare che \textbf{$\frac{1}{2} n^2$ è in $\Omega(n^2)$} e quindi che, poiché $S_n \geq \frac{1}{2} n^2$, di \textbf{conseguenza} anche \textbf{$S_n$ è in $\Omega(n^2)$}.
        
        \item Poiché $S_n$ è \textbf{sia} in $O(n^2)$, \textbf{sia} in $\Omega(n^2)$, allora è \textbf{anche} in $\Theta(n^2)$
    \end{itemize}
    
    \item Vediamo ora un ulteriore modo per poter dimostrare tale proposizione:
        \[ S_n = \sum_{k=1}^{n} k \text{ è in } \Theta(n^2)\]
        
    \begin{itemize}
        
        \item Anche in questa dimostrazione, riscriviamo nuovamente la sommatoria in forma estesa, ma anche in \textbf{forma invertita}:
        
        \[ S_n = 1+2+3+...+(n-2)+(n-1)+n\]
        \[ S_n = n+(n-1)+(n-2)+...+3+2+1\]
        
        \item Sommando $S_n$ con \textbf{se stessa}, otteniamo il seguente risultato:
        
        \begin{center}
            \begin{tabular}{c | c | c | c | c | c | c | c }
                $S_n$ & 1 & 2 & 3 & ...... & $n-2$ & $n-1$ & $n$ \\
                $S_n$ & $n$ & $n-1$ & $n-2$ & ...... & 3 & 2 & 1 \\
                \hline
                $2 S_n$ & $n+1$ & $n+1$ & $n+1$ & ...... & $n+1$ & $n+1$ & $n+1$
                \\
            \end{tabular}
        \end{center}
        
        Dunque, $2S_n = (n+1) + (n+1) + ... + (n+1) + (n+1) = n (n+1)$
        
        \item A questo punto, ci basta sfruttare alcune \textbf{proprietà algebriche}
        
        \[ 2S_n = n(n+1)\]
        \[ S_n = \frac{n(n+1)}{2} = \frac{n^2+n}{2} = \Theta(n^2)\]
        
    \end{itemize}
    
    \quad
    
    \item Dimostrare la seguente proposizione
    \[ S_n = \sum_{k=0}^{n} 2^k \text{ è in } \Theta(2^n)\]
    
    \begin{itemize}
    \item Riscriviamo la somma in forma estesa per poi \textbf{moltiplicarla per 2}
    
    \[ S_n = 1+2+2^2+2^3+...+2^n\]
    \[ 2 \cdot S_n = 2 \cdot (1+2+2^2+2^3+...+2^{n-1}+2^n)\]
    \[ 2S_n = 2+2^2+2^3+2^4+...+2^n+2^{n+1}\]
    
    \item Gli unici termini \textbf{non condivisi} tra $S_n$ e $2S_n$ sono $1$ e $2^{n+1}$
    \[ 2S_n = \color{red} 2+2^2+2^3+2^4+...+2^n \color{black} +2^{n+1}\]
    \[ S_n = 1+\color{red}2+2^2+2^3+...+2^n\]
    
    \item Dunque otteniamo che
    \[ S_n = 2S_n - S_n = 2^{n+1} - 1 \]
    
    \item A questo punto calcoliamo il \textbf{limite asintotico} del risultato
    
    \[ S_n = 2^{n+1} - 1 = 2^n \cdot 2 - 1 = \Theta(2^n) + \Theta(-1) = \Theta(2^n)\]
    
    \end{itemize}
    
    \quad
    
    \item Dimostrare la seguente proposizione
    \[ \sum_{k=1}^{n} k \cdot 2^k  \text{ è in } \Theta(n \cdot 2^n)\]
    
    \begin{itemize}
        \item Riscriviamo in forma estesa e moltiplichiamo per 2
        
        \[ S_n = 2^1 + 2 \cdot 2^2 + 3 \cdot 2^3 + ... + (n-1) \cdot 2^{n-1} + n \cdot 2^n\]
        \[ 2S_n = 2^2 + 2 \cdot 2^3 + 3 \cdot 2^4 + ... + (n-1) \cdot 2^n + n \cdot 2^{n+1}\]
        
        \item Effettuiamo qualche passaggio algebrico
        
        \[ S_n = 2S_n - S_n = -2^1 -2^2 - 2^3 - 2^4 - ... - 2^{n-1} +  n \cdot 2^{n+1}\]
        
        \[ -S_n = - (-2^1 -2^2 - 2^3 - 2^4 - ... - 2^n + n \cdot 2^{n+1})\]
        \[ -S_n = 2^1 +2^2 + 2^3 + 2^4 + ... + 2^n - n \cdot 2^{n+1}\]
        
        \item A questo punto, è necessario ricordarsi che nella dimostrazione precedente abbiamo ottenuto che
        \[ \sum_{k=0}^{n} 2^k = 1 +2^1 +2^2 + 2^3 + 2^4 + ... + 2^n = 2^{n+1}-1\]
        
        dunque, possiamo riscrivere $-S_n$ come
        \[ -S_n = 2^1 +2^2 + 2^3 + 2^4 + ... + 2^n - n \cdot 2^{n+1} = \left( \sum_{k=0}^{n} 2^k \right) - 1 - n \cdot 2^{n+1}= 2^{n+1}-2 - n \cdot 2^{n+1}\]
        
        per poi calcolare $S_n$
        
        \[ -(-S_n) = -(2^{n+1}-2 - n \cdot 2^{n+1}) = -2^{n+1} + 2 + n \cdot 2^{n+1} \]
        
        \item Infine, calcoliamo il limite asintotico del risultato
        \[ S_n =  -2^{n+1} + 2 + n \cdot 2^{n+1} = \Theta(2^n) + \Theta(2) + \Theta(n \cdot 2^n) = \Theta(n \cdot 2^n)\]
    \end{itemize}
    
    \label{log_n_factorial}
    \item Dimostrare la seguente proposizione
    \[ \sum_{k=1}^{n} \log(k) \text{ è in } \Theta(n \cdot \log(n))\]
    
    \begin{itemize}
        \item Riscriviamo la sommatoria in forma estesa in modo da applicare le proprietà dei logaritmi
        
        \[ S_n = \log(1) + \log(2) + \log(3) + ... + \log(n) = \]
        \[= \log(1 \cdot 2 \cdot 3 \cdot ... \cdot n) = \log(n!) \]
        
        \item Dunque, ignorando la costante $c$, verifichiamo l'ipotesi
        \[ \log(n!) \leq n \cdot \log(n)\]
        \[ \log(n!) \leq \log(n^n)\]
        \[ n! \leq n^n\]
        
        \item Estendendo il fattoriale, possiamo mettere in evidenza due categorie di numeri.
        
        \[ \underbrace{1 \cdot 2 \cdot 3 \cdot ...}_{\text{Numeri } \leq \frac{n}{2}} \cdot \underbrace{... \cdot (n-2) \cdot (n-1) \cdot n}_{\text{Numeri } \geq \frac{n}{2}} \leq n^n \]
        
        Quindi, sappiamo che \textbf{tutti i numeri minori di $ \frac{n}{2}$ sono anche maggior di 1}, mentre tutti i numeri maggiori di $\frac{n}{2}$ sono maggiori di $\frac{n}{2}$.
        
        Dunque possiamo scrivere la seguente disequazione
        
        \[ \underbrace{1 \cdot 1 \cdot 1 \cdot ...}_{\frac{n}{2} \text{ volte}} \cdot \underbrace{... \cdot \frac{n}{2} \cdot \frac{n}{2} \cdot \frac{n}{2}}_{\frac{n}{2} \text{ volte}} \leq 1 \cdot 2 \cdot 3 \cdot ... \cdot (n-2) \cdot (n-1) \cdot n \leq n^n\]
        
        \[ 1^{\frac{n}{2}} \cdot \left (\frac{n}{2} \right)^{\frac{n}{2}} \leq n! \leq n^n\]
        
        A questo punto, ri-applichiamo nuovamente il logaritmo ad ogni componente della disequazione
        
        \[ \log \left(\left (\frac{n}{2} \right)^{\frac{n}{2}} \right) \leq \log(n!) \leq \log(n^n)\]
        \[ \frac{n}{2} \log \left(\frac{n}{2} \right) \leq \log(n!) \leq n \cdot \log(n)\]
        \[ \frac{n}{2} (\log (n) - 1)) \leq \log(n!) \leq n \cdot \log(n)\]
        \[ \Theta(n \cdot \log(n)) \leq \log(n!) \leq \Theta(n \cdot \log(n))\]
        
        \item Poiché $S_n$ si trova \textbf{tra due} funzioni in $\Theta(n \cdot \log(n))$, ne segue che \textbf{anche esso sia in $\Theta(n \cdot \log(n))$}
        
        \end{itemize}
        
\end{itemize}

Dunque, abbiamo visto come in molti casi \textbf{non sia sufficiente conoscere solo le regole dell'algebra asintotica}, soprattutto nel caso delle \textbf{sommatorie}. 

\subsubsection{Esercizi svolti}
\begin{itemize}
    \item Dimostrare che $f(n) = 4^n$ è in $O(2^{n \cdot \log(n)})$
    
    \textbf{Dimostrazione per assurdo}: supponiamo che $f(n) \neq O(2^{n \cdot \log(n)})$. Per definizione, ne segue che $\exists c, n_0 \;|\; f(n) \geq c \cdot 2^{n \cdot \log(n)}, \;\; \forall n \geq n_0$
    
    \[ 4^n \geq c \cdot 2^{n \cdot \log(n)}\]
    \[ 4^n \geq c \cdot n^n\]
    \[ \log(2^{2n}) \geq \log(c \cdot n^n)\]
    \[ 2 \geq \frac{\log(c)}{n} + \log(n)\]
    
    \textbf{Falso} una volta superato un certo valore $n_0$. Dunque, ne segue che \textbf{$f(n)$ è in $O(2^{n \cdot \log(n)})$}.
    
    \item Dimostrare che $f(n) = (n-50)^2$ è in $\Theta(n^2)$
    
    \begin{itemize}
        \item Al crescere di $n$, esisterà un valore $n_0 = -50$ tale che
        \[ (n-50)^2 = n^2-100n+2500 \geq n^2-100n \geq n^2-100n^2 = -99n^2 = \Omega(n^2)\]
        
        \item Poiché $-50 < 0$, si ha che
        
        \[ (n-50)^2 \leq (n+0)^2 = O(n^2)\]
        
        \item Dunque, ne segue che $f(n) = \Theta(n^2)$
    \end{itemize}
    
    \item Dimostrare che per $c \in \mathbb{N}$ vale 
    \[ \sum_{k=0}^{n} k^c \text{ è in } O(n^{c+1})\]
    
    \begin{itemize}
        \item Basta riscrivere la sommatoria ed evidenziare che tutti i numeri sono $\leq n^c$
        
        \[ S_n = 1^c + 2^c + 3^c + ... + n^c \leq n^c + n^c +n^c+ ...+n^c = n\cdot n^c\]
        
        \[ n\cdot n^c = n^{c+1} = O(n\cdot n^c)\]
        
        \[ S_n = O(n^{c+1}) \text{ poiché } n^{c+1} = O(n^{c+1}) \text{ e } S_n \leq n^{c+1}\]
    \end{itemize}
    
    \newpage

    \item Dimostrare che per $c \neq 1$ vale 
    \[ \sum_{k=0}^{n} c^k \text{ è in } O(n \cdot c^n)\]
    
    
    \begin{itemize}
        \item Basta riscrivere la sommatoria ed evidenziare che tutti i numeri sono $\leq c^n$
        
        \[ S_n = c^1 + c^2 + c^3 + ... + c^n \leq c^n + c^n +c^n+ ...+c^n = n\cdot c^n\]
        
        \[ n\cdot c^n = O(n \cdot c^n)\]
        
        \[ S_n = O(n \cdot c^n) \text{ poiché } \leq n^{c+1} = O(n \cdot c^n) \text{ e } S_n \leq n \cdot c^n\]
    \end{itemize}
\end{itemize}

\chapter{Costo Computazionale}

\section{Valutazione del costo computazionale}

Fino ad ora, abbiamo parlato di costo computazionale di funzioni ipotetiche. In questo capitolo vedremo come \textbf{calcolare effettivamente il costo computazionale di un algoritmo}, adottando il criterio della misura del costo uniforme.

Il costo computazionale, inteso come funzione che  rappresenta il tempo di esecuzione di un algoritmo, sia una \textbf{funzione monotona non decrescente} in base alla dimensione dell'input. (poiché ovviamente non è possibile che aumentando l'input diminuisca il tempo di esecuzione)

Dunque, poiché il tempo di esecuzione è strettamente dipendente dalla \textbf{quantità di dati in input}, è prima necessario trovare all'interno del codice il \textbf{parametro} corrispondente ad esso: 
\begin{itemize}
    \item In un \textbf{algoritmo di ordinamento} esso sarà il numero di
    dati da ordinare;
    \item In un \textbf{algoritmo che lavora su una matrice} sarà il numero di righe e di colonne (quindi, 2 parametri);
    \item In un \textbf{algoritmo che opera su alberi} sarà il numero di nodi che compongono l'albero
    \item In altri casi, invece, l'individuazione del parametro è meno scontata.
\end{itemize}

La \textbf{notazione asintotica} è alla base del calcolo del costo computazionale degli algoritmi. Dunque, in base alla sua definizione stessa, tale costo computazionale potrà essere ritenuto valido solo \textbf{asintoticamente}, ossia considerando \textbf{input molto grandi}.

Difatti, esistono degli algoritmi che per dimensioni dell'input relativamente piccole hanno un comportamento diverso rispetto a quello per dimensioni grandi, perciò è opportuno considerare solo input grandi per calcolarne la vera efficienza.

\newpage

\section{Costo delle istruzioni}

Principalmente, siamo in grado di individuare \textbf{tre categorie di istruzioni}:

\begin{itemize}
    \item \textbf{Istruzioni elementari}: tutte le istruzioni con un \textbf{tempo di esecuzione costante}, ossia che non dipendono dalla dimensione dell'input (es: operazioni aritmetiche, lettura e scrittura di una variabile, valutazione di una condizione logica, stampa a video, ...). Poiché il loro tempo di esecuzione è costante, esse hanno un \textbf{costo computazionale pari a $\Theta(1)$}
    
    Ad esempio, le seguenti istruzioni hanno tutte costo $\Theta(1)$:
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \texttt{var = 10 \qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad $\Theta(1)$}
    
    \texttt{var += 10 * 10 \qquad\qquad\qquad\qquad\qquad\qquad $\Theta(1) + \Theta(1) + \Theta(1) = \Theta(1)$}
    
    \texttt{print("Il valore di var è:", var) \quad\;\;\;\, $\Theta(1)$}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \quad
    
    \item \textbf{Blocchi if/else}: hanno un costo pari alla \textbf{somma} tra il \textbf{costo della verifica della condizione} (solitamente $\Theta(1)$ poiché all'interno vi è una istruzione semplice) e il \textbf{massimo} tra i \textbf{costi complessivi} del \textbf{blocco if} e del \textbf{blocco else}.
    
    Ad esempio, il costo del blocco if/else sottostante è $\Theta(1)$, poiché:
    \begin{itemize}
        \item Il costo della \textbf{verifica della condizione} è $\Theta(1)$
        \item Il costo del \textbf{blocco if} è $\Theta(1) + \Theta(1) = \Theta(1)$
        \item Il costo del \textbf{blocco else} è $\Theta(1)$
        \item Il costo finale dell'\textbf{intero blocco} è:
        \[ \text{CostoVerifica} + \text{max(CostoIf, CostoElse)} = \Theta(1) + max(\Theta(1), \Theta(1)) = \Theta(1)\]
    \end{itemize}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \texttt{if (a > b):}
    
    \texttt{\qquad a += b}
    
    \texttt{\qquad print("Il valore di a+b è", a)}
    
    \texttt{else:}
    
    \texttt{\qquad print("Il valore di a è", a)}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \newpage
    
    \item \textbf{Blocchi iterativi}: hanno un costo pari alla \textbf{somma effettiva}, dunque \underline{\textbf{non asintotica}}, dei \textbf{costi di ciascuna iterazione}, compreso il costo di\textbf{ verifica della condizione}.
    
    Ne consegue, quindi, che se \textbf{tutte le iterazioni} hanno lo stesso costo, allora il costo del blocco iterativo è pari al \textbf{prodotto del costo di una singola iterazione per il numero di iterazioni}.
    
    Inoltre, è opportuno sottolineare che la condizione viene valutata \textbf{una volta in più rispetto al numero delle iterazioni}, poiché l'ultima valutazione, che darà esito negativo, è quella che terminerà l'iterazione.
    
    Ad esempio, il costo del seguente blocco for è $\Theta(n)$, poiché:
    \begin{itemize}
        \item La \textbf{verifica della condizione} ha costo pari a $\Theta(1)$
        \item \textbf{Ogni iterazione} ha un costo pari a $\Theta(1)$
        \item Il \textbf{numero di iterazioni} dipende strettamente dalla dimensione dell'array in input
        \item Il costo finale dell'\textbf{intero blocco} sarà quindi:
        \[ \text{NumIterazioni} \cdot \text{CostoIterazione} + \text{UltimaIter} = n \cdot \Theta(1) + \Theta(1)= \Theta(n)\]
    \end{itemize}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \texttt{sum = 0}
    
    \texttt{for i in range(len(A)): \qquad \#n iterazioni + $\Theta(1)$ dell'ultima verifica}
    
    \texttt{\qquad sum += A[i]  \qquad\qquad\qquad $\Theta(1)$}
    
    \texttt{print("La somma degli elementi dell'array è", sum)}
    
    \par\noindent\rule{425pt}{0.3pt}
\end{itemize}

\quad

Il \textbf{costo dell'algoritmo nel suo complesso}, quindi, è pari alla \textbf{somma dei costi delle istruzioni che lo compongono}.

Tuttavia, per via di ciò un dato algoritmo potrebbe avere costi diversi a seconda dell'input, poiché un input particolarmente \textbf{vantaggioso} per i blocchi if/else e iterativi darebbe vita ad un \textbf{caso migliore}, mentre uno particolarmente \textbf{svantaggioso} darebbe vita ad un \textbf{caso peggiore.}

Dunque, per avere un'idea del costo di un algoritmo, preferiamo conoscere quale sia il suo comportamento nel \textbf{caso peggiore}. Ciò ci permette, quindi, di scegliere l'uso di un algoritmo rispetto ad un altro in previsione di \textbf{grandi quantità di input sfavorevoli}.

Per mantenere un'idea ottimale di precisione, tuttavia, utilizziamo comunque la \textbf{notazione Teta}, e non quella O grande. Laddove questo non sia possibile, \textbf{approssimeremo} il costo dell'algoritmo per difetto, dunque notazione Omega, o per eccesso, dunque notazione O grande.

\section{Esempi di valutazione di un algoritmo}

\subsubsection{Esempio 1 - Calcolo del massimo di un array}

Vediamo un primo esempio molto semplice. Proviamo ad analizzare l'algoritmo per il calcolo del massimo in un vettore disordinato contenente n valori:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Trova\_Max(A):}

\texttt{\qquad n = len(A) \qquad\qquad\qquad\quad $\Theta(1)$}

\texttt{\qquad max = A[1] \qquad\qquad\qquad\quad $\Theta(1)$}

\texttt{\qquad for i in range (1,n): \quad\, \#$n-1$ iterazioni + $\Theta(1)$}

\texttt{\qquad\qquad if A[i] > max: \qquad\quad $\Theta(1)$}

\texttt{\qquad\qquad\qquad max = A[i] \qquad\quad $\Theta(1)$}

\texttt{\qquad return max \qquad\qquad\qquad\quad $\Theta(1)$}

\par\noindent\rule{450pt}{0.3pt}
    
Procediamo in modo verticale, "suddividendo" il programma in \textbf{tre blocchi}: uno precedente al ciclo for nel mezzo, uno successivo ed uno corrispondente col ciclo for stesso:

\begin{itemize}
    \item Costo \textbf{blocco precedente al blocco for}:
    \[ B_{PF} = \Theta(1) + \Theta(1) = \Theta(1)\]
    
    \item Costo del \textbf{blocco for}:
    \[ B_F = (n-1) \cdot \Theta(1) + \Theta(1) = \Theta(n-1) + \Theta(1) = \Theta(n) + \Theta(1) = \Theta(n)\]
    
    \textit{Attenzione: ricordiamo $\Theta(n-1) = \Theta(n)$ poiché prendiamo il massimo tra i due costi}
    
    \item Costo \textbf{blocco successivo al for}:
    \[ B_{SF} = \Theta(1)\]
    
    Una volta calcolato il costo di ognuno dei tre blocchi, possiamo \textbf{sommare asintoticamente} (e non somma effettiva) i loro "costi parziali". Il costo complessivo dell'algoritmo sarà quindi:
    
    \item Costo \textbf{complessivo algoritmo}:
    \[ T(n) = B_{PF} + B_{F} + B_{SF} = \Theta(1) + \Theta(n) + \Theta(1) = \Theta(n)\]
\end{itemize}

\newpage

\subsubsection{Esempio 2 - Somma dei primi n interi }

Vediamo ora un esempio di possibile ottimizzazione di un algoritmo: dato un numero $n$ in input, vogliamo ottenere la somma di tutti i numeri a partire da $0$ fino ad $n$. Analizziamo quindi il seguente algoritmo:


\par\noindent\rule{450pt}{0.3pt}

\texttt{def Calcola\_Somma\_1(n):}

\texttt{\qquad somma = 0 \qquad\qquad\qquad\qquad\quad\, $\Theta(1)$}

\texttt{\qquad for i in range (1,n+1): \qquad\, \#n iterazioni + $\Theta(1)$}

\texttt{\qquad\qquad somma += i \qquad\qquad\qquad\quad $\Theta(1)$}

\texttt{\qquad return somma \qquad\qquad\qquad\qquad  $\Theta(1)$}

\par\noindent\rule{450pt}{0.3pt}

Essendo una situazione molto simile all'esempio precedente, siamo in grado di calcolare facilmente il costo di questo algoritmo:

\[ T(n) = \Theta(1) + [n \cdot \Theta(1) + \Theta(1) ] + \Theta(1) = \Theta(n)\]

Tuttavia, come visto nella sezione \ref{sums}, sappiamo che esiste un \textbf{metodo matematico} per calcolare \textbf{direttamente} la somma dei primi $n$ numeri:

\[ \sum_{k = 0}^{n} k = \frac{n(n+1)}{2}\]

Possiamo quindi scrivere una versione \textbf{nettamente ottimizzata} dell'algoritmo, ottenendo un costo pari a $\Theta(1)$, poiché vengono effettuate \textbf{solo istruzioni semplici} di costo costante:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Calcola\_Somma\_2(n):}

\texttt{\qquad somma = n*(n+1)/2}

\texttt{\qquad return somma}

\par\noindent\rule{450pt}{0.3pt}

\[ T(n) = \Theta(1) + \Theta(1) = \Theta(1)\]

Spesso, infatti, è possibile \textbf{ottimizzare} pezzi di algoritmo che si occupano di \textbf{calcolo matematico} con delle \textbf{formule dirette}, che permettono di ridurre notevolmente il costo dell'algoritmo. 

\newpage

\subsubsection{Esempio 3 - Valutazione di un polinomio in un punto}

Vediamo ora un esempio più complesso dei precedenti: vogliamo valutare un polinomio espresso nella seguente forma
\[ \sum_{k=0}^{n} a_ix^i\]
dove ogni $a_i$ corrisponde ad un elemento di un array dato in input assieme al valore assunto dalla variabile $x$.

Ad esempio, il seguente polinomio verrà espresso nell'array nella forma qui riportata
\[ x^2 - 4x + 5 = [a_0, a_1, a_2] = [5, -4, 1]\]

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Calcola\_Polinomio\_1(A, x):}

\texttt{\qquad somma=0 \qquad\qquad\qquad\qquad\qquad\qquad\quad\;\, $\Theta(1)$}

\texttt{\qquad for i in range(len(a)): \qquad\qquad\quad\, \#n iterazioni + $\Theta(1)$}

\texttt{\qquad\qquad potenza = 1 \qquad\qquad\qquad\qquad\quad\, $\Theta(1)$}

\texttt{\qquad\qquad for j in range(i): \qquad\qquad\qquad \#i iterazioni + $\Theta(1)$}

\texttt{\qquad\qquad\qquad potenza = x*potenza \qquad\quad\, $\Theta(1)$}

\texttt{\qquad\qquad somma = somma+A[i]*potenza \qquad $\Theta(1)$}

\texttt{\qquad return somma \qquad\qquad\qquad\qquad\qquad\quad $\Theta(1)$}

\par\noindent\rule{450pt}{0.3pt}

Come possiamo notare, questa volta abbiamo una situazione contorta: vi sono \textbf{due cicli for annidati}, dove il \textbf{contatore del secondo} dipende dal \textbf{contatore del primo}.

Ciò significa che il \textbf{ciclo for interno} verrà eseguito prima 0 volte, poi 1, poi 2 e così via finché il contatore $i$ del primo for non raggiungerà $n$. Poiché il costo di un ciclo for è costituito dalla \textbf{somma effettiva} (e non asintotica) dei costi delle sue iterazioni, questa casistica è perfettamente esprimibile tramite una \textbf{sommatoria}:
\[ \sum_{i = 0}^{n} \Theta(i)\]

Le \textbf{sommatorie}, nell'ambito della notazione asintotica, godono di una particolare proprietà: esse sono \textbf{intercambiabili con la notazione utilizzata}:
\[ \sum_{i = 0}^{n} \Theta(i) = \Theta \left( \sum_{i = 0}^{n} i \right ) = \Theta \left( \frac{n(n+1)}{2} \right ) = \Theta(n^2)\]

Dunque, il costo finale dell'algoritmo sarà:
\[ T(n) = \Theta(1) + \left ( \sum_{i = 0}^{n} (\Theta(1) + \Theta(i) + \Theta(1)) \right ) + \Theta(1) = \Theta(1) + \Theta(n^2) + \Theta(1) = \Theta(n^2)\]

Tuttavia, guardando meglio l'algoritmo notiamo al suo interno una \textbf{grande possibile ottimizzazione}: invece che ricalcolare la potenza corrispondente ad ogni termine del polinomio, possiamo \textbf{conservare} la potenza calcolata per il termine precedente, riducendo la necessità di dover utilizzare il secondo ciclo for:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Calcola\_Polinomio\_2(A, x):}

\texttt{\qquad somma=0 \qquad\qquad\qquad\qquad\qquad\qquad\quad\, $\Theta(1)$}

\texttt{\qquad potenza=1 \qquad\qquad\qquad\qquad\qquad\qquad\; $\Theta(1)$}

\texttt{\qquad for i in range(len(a)): \qquad\qquad\quad\, \#n iterazioni + $\Theta(1)$}

\texttt{\qquad\qquad potenza = x*potenza \qquad\qquad\quad\, $\Theta(1)$}

\texttt{\qquad\qquad somma = somma+A[i]*potenza \qquad $\Theta(1)$}

\texttt{\qquad return somma \qquad\qquad\qquad\qquad\qquad\quad $\Theta(1)$}

\par\noindent\rule{450pt}{0.3pt}

Il nuovo costo dell'algoritmo sarà quindi nettamente migliore della versione precedente:
\[ T(n) = \Theta(1) + n \cdot \Theta(1) + \Theta(1) = \Theta(n)\]

\quad

\subsubsection{Esempio 4 - Analisi del caso migliore e caso peggiore}

Analizziamo il seguente algoritmo dove esistono un caso migliore ed un caso peggiore con un costo computazionale differente.

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es4(n):}

\texttt{\qquad if n<0: n=-n}

\texttt{\qquad while n:}

\texttt{\qquad\qquad if n\%2==1: return 1}

\texttt{\qquad\qquad n-=2}

\texttt{\qquad return 0}

\par\noindent\rule{450pt}{0.3pt}

\newpage

\begin{itemize}
    \item Analisi del ciclo while:
    
    \begin{itemize}
        \item Se $n$ è \textbf{dispari}, allora la condizione $if \; n \% 2==1$ restituirà \textbf{True}, eseguendo l'istruzione \textbf{return} e terminando istantaneamente il ciclo.
        
        Dunque abbiamo un costo pari a $\Theta(1)$.
        
        \item Se $n$ è \textbf{pari}, allora il comportamento del ciclo, sarà 
        
        \begin{center}
            \begin{tabular}{c | c | c | c | c | c | c}
                n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
                \hline
                Valore di $n$ & $n-2$ & $n-4$ & $n-6$ & $n-4$ & ... & $n - 2k$\\
            \end{tabular}
        \end{center}
        
        finché $n-2k = 0$ (condizione necessaria a terminare il while).
        
        Dunque, il costo sarà $\Theta(n)$
        \[ n - 2k = 0\]
        \[ n = 2k \]
        \[ k = \frac{n}{2}\]
        \[ \frac{1}{2} \cdot \Theta(n) = \Theta(n)\]
        
    \end{itemize}
    
    \item Possiamo quindi dire che
    
    $$
    T(n) = \left \{
    \begin{array}{cl}
        \Theta(1) & \text{se } n \text{ è dispari (caso migliore)}\\
        \Theta(n) & \text{se } n \text{ è pari (caso peggiore)}
    \end{array}
    \right .
    $$
\end{itemize}

\quad

\subsubsection{Esempio 5 - Iterazioni con radice}

Vediamo ora un esempio in cui il numero di iterazioni del ciclo descritto corrisponde ad una radice:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es5(n):}

\texttt{\qquad n=abs(n)}

\texttt{\qquad x=r=0}

\texttt{\qquad while x*x<n:}

\texttt{\qquad\qquad x+=1}

\texttt{\qquad\qquad r*=3*x}

\texttt{\qquad return r}

\par\noindent\rule{450pt}{0.3pt}

\newpage

\textbf{Analisi del ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
        \begin{center}
            \begin{tabular}{c | c | c | c | c | c | c}
                n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
                \hline
                Valore di $x$ & 1 & 2 & 3 & 4 & ... & k\\
                Valore di $x \cdot x$ & $1^2$ & $2^2$ & $3^2$  & $4^2$  & ...  & $k^2$
            \end{tabular}
        \end{center}
        
        finché $x \cdot x = n $ (condizione necessaria a terminare il while).
        
        Dunque, il numero di iterazioni sarà 
        \[ x \cdot x = n\]
        \[ x^2 = n \]
        \[ x = \sqrt{n} \]
        
        \item Costo finale:
        
        \[ T(n) = \Theta(1) + \sqrt{n} \cdot \Theta(1) + \Theta(1) = \Theta(\sqrt{n})\]
    
\end{itemize}

\quad

\subsubsection{Esempio 6 - Iterazioni logaritmiche}

Dopo aver visto un esempio con iterazioni con radice, vediamo un caso in cui otteniamo delle iterazioni logaritmiche

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es6(n):}

\texttt{\qquad n=abs(n)}

\texttt{\qquad x=r=0}

\texttt{\qquad while n>1:}

\texttt{\qquad\qquad r+=2}

\texttt{\qquad\qquad n=n//3}

\texttt{\qquad return r}

\par\noindent\rule{450pt}{0.3pt}

\textbf{Analisi del ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Valore di $n$ & $\frac{n}{3}$ & $\frac{n}{3^2}$ & $\frac{n}{3^3}$ & $\frac{n}{3^4}$ & ... & $\frac{n}{3^k}$\\
        \end{tabular}
    \end{center}
    
    finché $\frac{n}{3^k}=1$ (condizione necessaria a terminare il ciclo while).
    
    Dunque, il numero di iterazioni sarà
    
    \[ \frac{n}{3^k}=1\]
    \[ n=3^k\]
    \[ k = \log_3(n)\]
    
    \item Costo finale:
    \[ T(n) = \Theta(1) + \log_3(n) \cdot \Theta(1) + \Theta(1) = \Theta(\log(n))\]
\end{itemize}

\quad

\subsubsection{Esempio 7 - Iterazioni esponenziali}

Infine, per completezza vediamo un esempio con iterazioni esponenziali

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es7(n):}

\texttt{\qquad n=abs(n)}

\texttt{\qquad x=t=1}

\texttt{\qquad for i in range(n):}

\texttt{\qquad\qquad t=3*t}

\texttt{\qquad t-=1}

\texttt{\qquad while t>=x:}

\texttt{\qquad\qquad x+=2}

\texttt{\qquad\qquad t-=2}

\texttt{\qquad return x}

\par\noindent\rule{450pt}{0.3pt}

\textbf{Analisi del ciclo for:}

\begin{itemize}
    \item Il ciclo viene eseguito $n$ volte, dove ad ogni iterazione la variabile $t$ viene moltiplicata per 3. Dunque, alla fine del ciclo avremo $t = 3^n$.
    
\end{itemize}

\textbf{Analisi del ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Valore di $x$ & 3 & 5 & 7 & 9 & ... & 1+2k\\
            Valore di $t$ & $3^n-3$ & $3^n-5$ & $3^n-7$ & $3^n-9$ & ... & $3^n-(1+2k)$\\
        \end{tabular}
    \end{center}
    
    finché $3^n-(1+2k) = 1+2k$ (condizione necessaria a terminare il while)
    
    Dunque, il numero di iterazioni sarà
    
    \[ 3^n-(1+2k) = 1+2k \]
    \[ 3^n - 2 = 4k \]
    \[ k = \frac{3^n-2}{4}\]
    
    \item Costo finale:
    
    \[ T(n) = \Theta(1) + n \cdot \Theta(1) + \frac{3^n-2}{4} \cdot \Theta(1) + \Theta(1) = \Theta(n) + \Theta(3^n) = \Theta(3^n)\]
\end{itemize}

\quad


\subsubsection{Esempio 8}

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es8(n):}

\texttt{\qquad n=abs(n)}

\texttt{\qquad p=2}

\texttt{\qquad while n>=p:}

\texttt{\qquad\qquad p=p*p}

\texttt{\qquad return p}

\par\noindent\rule{450pt}{0.3pt}

\textbf{Analisi del ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Valore di $p$ & $2^2$ & $2^4$ & $2^6$ & $2^8$ & ... & $2^{2^k}$\\
        \end{tabular}
    \end{center}
    
    finché $2^{2^k} = n+1$ (condizione necessaria a terminare il while)
    
    Dunque, il numero di iterazioni sarà
    
    \[ 2^{2^k} = n+1\]
    \[ 2^k = \log_2(n+1)\]
    \[ k = \log_2(\log_2(n+1))\]
    
    \item Costo finale:
    
    \[ T(n) = \Theta(1) + \log_2(\log_2(n+1)) \cdot \Theta(1) + \Theta(1) = \Theta(\log(\log(n)))\]
\end{itemize}

\quad

\subsubsection{Esempio 9}

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es9(n):}

\texttt{\qquad n=abs(n)}

\texttt{\qquad i,j,t,s=1}

\texttt{\qquad while i*i<=n:}

\texttt{\qquad\qquad for j in range(t)}

\texttt{\qquad\qquad\qquad s+=1}

\texttt{\qquad\qquad i=i+1}

\texttt{\qquad\qquad t+=1}

\texttt{\qquad return s}

\par\noindent\rule{450pt}{0.3pt}

\textbf{Analisi del ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Valore di $t$ & 2 & 3 & 4 & 5 & ... & k+1\\
            Valore di $i$ & 2 & 3 & 4 & 5 & ... & k+1\\
            Valore di $i^2$ & $2^2$ & $3^2$ & $4^2$ & $5^2$ & ... & $(k+1)^2$\\
        \end{tabular}
    \end{center}
    
    finché $(k+1)^2 = n+1$ (condizione necessaria a terminare il while)
    
    Dunque, il numero di iterazioni sarà
    
    \[ (k+1)^2 = n+1\]
    \[ k+1= \sqrt{n+1} \]
    \[ k = \sqrt{n+1} - 1\]
    
\end{itemize}

\newpage

\textbf{Analisi del ciclo for annidato:}

\begin{itemize}
    \item Ad ogni iterazione del ciclo while, il ciclo for viene eseguito $t$ volte. Tuttavia, il valore di $t$ aumenta di 1 ad ogni iterazione del while, dunque il numero di iterazioni sarà
    
    \[ \sum_{t=1}^{\sqrt{n+1}-1} t = \frac{(\sqrt{n+1}-1)\cdot \sqrt{n+1}}{2} = \frac{n+1 - \sqrt{n+1}}{2}\]
    
    \item Costo finale:
    
    \[ T(n) = \Theta(1) + \frac{n+1 - \sqrt{n+1}}{2} \cdot \Theta(1) + \Theta(1) = \Theta(n)\]
    
\end{itemize}

\quad

\subsubsection{Esempio 10}

\par\noindent\rule{450pt}{0.3pt}

\texttt{def es10(n):}

\texttt{\qquad n=abs(n)}

\texttt{\qquad s=n}

\texttt{\qquad p=2}

\texttt{\qquad i,r=1}

\texttt{\qquad while s>=1:}

\texttt{\qquad\qquad s=s//5}

\texttt{\qquad\qquad p+=2}

\texttt{\qquad p=p*p}

\texttt{\qquad while i*i*i<n:}

\texttt{\qquad\qquad for j in range(p):}

\texttt{\qquad\qquad\qquad r+=1}

\texttt{\qquad\qquad i+=1}

\texttt{\qquad return r}

\par\noindent\rule{450pt}{0.3pt}

\newpage

\textbf{Analisi del primo ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Valore di $p$ & 4 & 6 & 8 & 10 & ... & 2+2k\\
            Valore di $s$ & $\frac{n}{5}$ & $\frac{n}{5^2}$ & $\frac{n}{5^3}$ & $\frac{n}{5^4}$ & ... & $\frac{n}{5^k}$\\
        \end{tabular}
    \end{center}
    
    finché $\frac{n}{5^k} = 1$ (condizione necessaria a terminare il while)
    
    Dunque, il numero di iterazioni sarà
    
    \[ \frac{n}{5^k} = 1 \]
    \[ n = 5^k \]
    \[ k = \log_5(n) \]
    
    \item Comportamento di $p$:
    
    Poiché il primo ciclo while viene eseguito $\log_5(n)$ volte, anche l'istruzione \texttt{p += 2} viene eseguita $\log_5(n)$ volte.
    
    Dunque, il valore di $p$, una volta concluso il primo ciclo while, sarà $p = 2+2\log_5(n)$. Inoltre, viene eseguita l'istruzione \texttt{p = p * p}, dunque $p = (2+2\log_5(n))^2$.
    
\end{itemize}

\textbf{Analisi del secondo ciclo while:}

\begin{itemize}
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Valore di $i$ & 1 & 2 & 3 & 4 & ... & k\\
            Valore di $i^3$ & $2^3$ & $3^3$ & $4^3$ & $5^3$ & ... & $(k+1)^3$\\
        \end{tabular}
    \end{center}
    
    finché $(k+1)^3 = n$ (condizione necessaria a terminare il while)
    
    Dunque, il numero di iterazioni sarà
    \[ (k+1)^3 = n \]
    \[ k+1 = \sqrt[3]{n} \]
    \[ k = \sqrt[3]{n} - 1 \]
    
    \item Comportamento del ciclo for annidato:
    
    Viene eseguito ad ogni iterazione del ciclo while (dunque $\sqrt[3]{n} - 1$ volte). Al suo interno, inoltre, vengono eseguite $p$ iterazioni, dove il valore di $p$ è $(2+2\log_5(n))^2$.
    
    \item Costo finale:
    \[ T(n) = \Theta(1) + \log_5(n) \cdot \Theta(1) + (\sqrt[3]{n} - 1)((2+2\log_5(n))^2 \cdot \Theta(1) + \Theta(1)) + \Theta(1) = \]
    \[ = \Theta(\log(n)) + (\sqrt[3]{n} - 1)(\Theta(\log^2(n)) + \Theta(1)) = \Theta(\log(n)) + \Theta(\sqrt[3]{n} \cdot \log^2(n)) + \Theta(\sqrt[3]{n}) = \]
    \[ = \Theta(\sqrt[3]{n} \cdot \log^2(n))\]
    
\end{itemize}

\quad

\section{Tempi di esecuzione}

Una volta appreso come poter valutare il costo di un algoritmo, possiamo effettivamente capire quanto sono grandi i \textbf{tempi di esecuzione} di un algoritmo in base al suo \textbf{costo computazionale}.

Ipotizziamo di disporre di un calcolatore in grado di effettuare una \textbf{operazione elementare in un nanosecondo} (dunque $10^9$ operazioni al secondo) e supponiamo che la dimensione dei dati in input sia $n = 10^6$.

\begin{itemize}
    \item Tempi di un algoritmo con costo $O(n)$
    \[ T = \frac{10^6 }{10^9 \text{ op/s}} = 10^{-3} = 1 \text{ millisecondo}\]
    
    \item Tempi di un algoritmo con costo $O(n \cdot \log(n))$
    \[ T = \frac{ 10^6 \cdot \log(10^6)}{10^9 \text{ op/s}} = \frac{3 \cdot \log(10)}{500} \approx 20 \text{ millisecondi}\]
    
    \item Tempi di un algoritmo con costo $O(n^2)$
    \[ T = \frac{(10^6)^2 }{10^9 \text{ op/s}} = 10^{3} = 1000 \text{ secondi} \approx 17 \text{ minuti}\]
    
\end{itemize}

Notiamo quindi che la differenza tra $O(n)$ e $O(n^2)$ è abissale. Ma che succede se il costo computazionale cresce esponenzialmente, ad esempio quando è $O(2^n)$?

È facile immaginare che il \textbf{tempo di esecuzione} esploda fino a raggiungere cifre astronomiche. Infatti, già con un input di dimensioni misere come $n = 100$, il tempo di esecuzione raggiunge una quantità inimmaginabile:

\begin{itemize}
    \item Tempi di un algoritmo con costo $O(2^n)$
    \[ T = \frac{2^{100} }{10^9 \text{ op/s}} = 10^{3} = 1,26 \cdot 10^{21} \text{ secondi} \approx 3 \cdot 10^{13} \text{ anni}\]
\end{itemize}

Dunque, possiamo concludere che un algoritmo di costo esponenziale sia \textbf{inutilizzabile}. Infatti, nonostante l'avanzamento tecnologico possa raggiungere potenzialità formidabili, non è in grado di rendere abbordabile la risoluzione di un tale problema.

\chapter{Il Problema della Ricerca}

Nell'informatica , esistono alcune tipologie di problemi particolarmente ricorrenti. In particolare, uno di essi è la \textbf{ricerca di un elemento in un insieme di dati} (es: numeri, cognomi, ...).

Tali problemi consistono in:
\begin{itemize}
    \item \textbf{Input}: un \textbf{array} $A$ di $n$ elementi ed un \textbf{valore} $v$ da cercare al suo interno
    \item \textbf{Output}: l'\textbf{indice} corrispondente alla posizione dell'elemento $v$ trovato all'interno dell'array, oppure $Null$ o $-1$ se l'elemento non viene trovato
\end{itemize}

\section{Ricerca sequenziale}

La prima tipologia di \textbf{algoritmo di ricerca} è composta da tre passaggi:
\begin{itemize}
    \item Presi in input il valore $v$ da cercare e l'array, quest'ultimo viene analizzato \textbf{elemento per elemento}
    \item Ogni elemento viene \textbf{confrontato} con $v$. Se l'elemento analizzato e $v$ \textbf{coincidono}, allora viene restituito l'indice dell'elemento, altrimenti si procede con il \textbf{prossimo elemento}
    \item Se anche l'\textbf{ultimo elemento} dell'array non coincide con $v$, allora viene restituito $-1$, indicando che l'elemento non è presente nella lista
\end{itemize}

Per via del suo funzionamento, tale algoritmo viene chiamato \textbf{Ricerca Sequenziale}.

Anche senza dover analizzare il codice, possiamo renderci facilmente conto del fatto che il \textbf{caso migliore} di tale algoritmo corrisponda al caso in cui l'elemento da cercare sia in \textbf{prima posizione} (ossia all'indice 0), mentre il \textbf{caso peggiore} corrisponda al caso in cui l'elemento \textbf{non sia presente} all'interno della lista (poiché comunque dovrebbe venir analizzata l'intera lista).

Dunque, possiamo già concludere che:

\begin{itemize}
    \item \textbf{Caso migliore}: Se $v = A[0]$, allora $\Theta(1)$
    \item \textbf{Caso peggiore}: Se $v \notin A$, allora $\Theta(n)$
\end{itemize}

Poiché non abbiamo trovato una \textbf{stima del costo} che sia valida per tutti i casi, diremo che il \textbf{costo computazionale} dell'algoritmo è $O(n)$, per evidenziare il fatto che ci sono input in cui questo valore viene \textbf{raggiunto}, ma ci sono anche input in cui il costo è \textbf{minore}.

Una versione molto semplificata di questo algoritmo può essere implementata dal seguente \textbf{codice}:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Ricerca\_Sequenziale(A, v):}

\texttt{\qquad i = 1}

\texttt{\qquad while (i < len(A)) and (A[i] $\neq$ v):  \qquad \# eseguito massimo $n$ volte}

\texttt{\qquad\qquad i += 1}

\texttt{\qquad if i < len(A):}

\texttt{\qquad\qquad return i}

\texttt{\qquad else:}

\texttt{\qquad\qquad return -1}

\par\noindent\rule{450pt}{0.3pt}

\quad

\subsection{Stima del costo medio}

Come abbiamo visto, non è possibile determinare il costo asintotico stretto di tale algoritmo poiché il caso peggiore e il caso migliore sono differenti. In questi casi, è necessario effettuare una stima del \textbf{costo medio} dell'algoritmo, ossia quello che si verifica con \textbf{più probabilità}. 

Ipotizziamo di avere un array $A$ di $n$ elementi al cui interno ogni posizione ha la \textbf{stessa probabilità} di contenere il valore $v$ da cercare (dunque non ci sono posizioni favorite).

A questo punto, possiamo dire che la \textbf{probabilità che $v$ sia in k-esima posizione} è
\[ P = \frac{1}{n}\]

Applicando tale probabilità al \textbf{numero totale di iterazioni}, otteniamo
\[ P \cdot \sum_{k=0}^{n} k = \frac{1}{n} \cdot \frac{n (n+1)}{2} = \frac{n+1}{2}\]

Dunque, possiamo dire che \textbf{in media} il ciclo viene eseguito $\frac{n+1}{2}$ volte, corrispondenti quindi ad un $\Theta(n)$. In questo caso, quindi, il \textbf{caso medio}, si \textbf{avvicina} più \textbf{al caso peggiore} rispetto che al caso minore.

In alternativa, il \textbf{costo medio} può essere trovato utilizzando il calcolo delle \textbf{permutazioni}.

Ricordando che le permutazioni di una lista (o parola) di $n$ elementi (o lettere) corrispondono a $n!$, possiamo dire che le \textbf{permutazioni totali di A} sono $p_{tot} = n!$.

All'interno di questo insieme di permutazioni, vi sono anche i seguenti sotto-insiemi:
\begin{itemize}
    \item Permutazioni in cui $v$ è in \textbf{prima posizione}
    \item Permutazioni in cui $v$ è in \textbf{seconda posizione}
    \item Permutazioni in cui $v$ è in \textbf{terza posizione}
    \item ...
    \item Permutazioni in cui $v$ è in \textbf{ultima posizione}
\end{itemize}

\textbf{Ognuno di tali sotto-insiemi}, corrisponde ad una \textbf{permutazione di $n-1$ elementi}, ossia $p_k = (n-1)!$. Dunque, il numero medio di iterazioni del ciclo corrisponderà a

\[ \sum_{k=0}^{n} \left ( k \cdot \frac{p_k}{p_{tot}} \right ) = \sum_{k=0}^{n} \left ( k \cdot \frac{(n-1)!}{n!} \right ) = \sum_{k=0}^{n} \left ( k \cdot \frac{1}{n} \right ) = \frac{1}{n} \cdot \sum_{k=0}^{n} k = \frac{1}{n} \cdot \frac{n(n+1)}{2} = \frac{n+1}{2}\]

Anche in questo caso, dunque, concludiamo che il \textbf{numero medio di iterazioni del ciclo} è $\frac{n+1}{2}$, corrispondente ad un $\Theta(n)$.

Prima di procedere, è necessario mettere alla luce il vero comportamento dell'operatore "$in$" del linguaggio Python, la cui sintassi per l'utilizzo ricordiamo essere

\[ \texttt{<valore> in <lista>}\]

Di seguito un esempio di codice per ricordare meglio il funzionamento:

\par\noindent\rule{450pt}{0.3pt}

\texttt{if v in A:}

\texttt{\qquad print("Il valore v è dentro A")}

\texttt{else:}

\texttt{\qquad print("Il valore v non è dentro A")}

\par\noindent\rule{450pt}{0.3pt}

Sulla superficie, tale istruzione può sembrare un semplice \textbf{costo} pari a $\Theta(1)$, poiché si tratta di una \textbf{singola istruzione}. Tuttavia, in realtà tale operatore corrisponde ad una scrittura \textbf{estremamente abbreviata} fornita dal linguaggio Python corrispondente ad una \textbf{ricerca sequenzale}, il cui costo medio sappiamo essere $\Theta(n)$.

\section{Ricerca binaria}

Nella vita di tutti i giorni, tuttavia, noi esseri umani non utilizziamo \textbf{mai} una ricerca di tipo sequenziale: se volessimo cercare una parola all'interno di un dizionario per saperne il significato, non leggeremmo mai l'intero dizionario parola per parola.

Ciò che il nostro cervello svolge (inconsciamente) è un'altra tipologia di algoritmo di ricerca, chiamata \textbf{ricerca binaria}: una volta aperto il dizionario ad una pagina casuale, abbiamo tre opzioni:
\begin{itemize}
    \item La parola cercata si trova \textbf{nella pagina} che abbiamo aperto.
    \item La parola cercata si trova \textbf{prima della pagina} che abbiamo aperto. Dunque, il passo successivo sarà scegliere un'\textbf{altra pagina casuale all'interno solo delle pagine precedenti}.
    \item La parola cercata si trova \textbf{dopo la pagina} che abbiamo aperto. Dunque, il passo successivo sarà scegliere un'\textbf{altra pagina casuale all'interno solo delle pagine successive}.
\end{itemize}

Tale procedimento viene ripetuto fino a quando non troviamo la parola che stiamo cercando. Come sappiamo, tale ricerca ci risulta estremamente comoda e rapida, tant'è che il nostro cervello è in grado di sfruttarla senza fatica.

Possiamo quindi definire in modo più rigoroso l'algoritmo di \textbf{ricerca binaria}:
\begin{enumerate}
    \item Viene ispezionato l'elemento centrale dell'array (che chiameremo $m$ per comodità)
    \begin{itemize}
        \item Se corrisponde al valore $v$ che stiamo cercando (dunque $v = m$), viene restituito l'indice della posizione trovata
        \item Se $v < m$, allora $v$ si troverà nella \textbf{metà inferiore della lista}, dunque l'algoritmo verrà ripetuto solo su essa
        \item Se $m < v$, allora $v$ si troverà nella \textbf{metà superiore della lista}, dunque l'algoritmo verrà ripetuto solo su essa
    \end{itemize}
    \item \textbf{Ripetere} il passaggio finché la lista non si sarà ridotta ad \textbf{un solo elemento}
    \begin{itemize}
        \item Se l'\textbf{unico elemento rimasto} dopo le riduzioni della lista effettuate corrisponde a $v$, allora verrà restituito l'indice trovato
        \item Altrimenti, verrà restituito $-1$, indicando che l'elemento non è nella lista
    \end{itemize}
\end{enumerate}

\quad

\begin{center}
    \includegraphics[scale=0.8]{resources/images/chapter_3/bin_search.png}
    
    \textit{Un \textbf{esempio grafico} dell'algoritmo in cui viene ricercato il valore $23$}
\end{center}

Tuttavia, è necessario sottolineare un \textbf{requisito necessario} per poter applicare l'algoritmo di ricerca binaria: al contrario della ricerca sequenziale, nella ricerca binaria \textbf{l'array deve essere \underline{obbligatoriamente} ordinato in modo crescente}, altrimenti è impossibile utilizzare tale algoritmo.

\quad

Vediamo ora l'implementazione in codice di tale algoritmo:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Ricerca\_Binaria(A, v):}

\texttt{\qquad a = 0 \qquad \#primo indice di A}

\texttt{\quad b = len(A)-1 \qquad \#ultimo indice di A}

\texttt{\qquad m =(a+b)//2 \qquad \#l'indice a metà di A}

\texttt{\qquad while A[m] != v:}

\texttt{\qquad\qquad if A[m] > v:}

\texttt{\qquad\qquad\qquad b = m – 1 \qquad \#prendo la metà inferiore}

\texttt{\qquad\qquad else:}

\texttt{\qquad\qquad\qquad a = m + 1 \qquad \#prendo la metà superiore}

\texttt{\qquad\qquad if a > b: \qquad \#si verifica solo se v non è in A}

\texttt{\qquad\qquad\qquad return -1}

\texttt{\qquad\qquad m=(a+b)//2 \qquad \#calcolo di nuovo valore di m}

\texttt{\qquad return m}

\par\noindent\rule{450pt}{0.3pt}

\newpage

\textbf{Analisi del ciclo while:}

\begin{itemize}
    \item Ipotesi: supponiamo che l'elemento $v$ venga trovato alla k-esima iterazione del ciclo
    \item Comportamento del ciclo:
    
    \begin{center}
        \begin{tabular}{c | c | c | c | c | c | c}
            n. Iterazione & 1 & 2 & 3 & 4 & ... & k\\
            \hline
            Lunghezza di $A$ & $\frac{n}{2}$ &  $\frac{n}{2^2}$ &  $\frac{n}{2^3}$ &  $\frac{n}{2^4}$ & ... &  $\frac{n}{2^k}$\\
        \end{tabular}
    \end{center}
    
    finché $\frac{n}{2^k} = 1$ (condizione necessaria a terminare il while, poiché nel caso peggiore solo $v$ rimane nell'array)
    
    Dunque, il numero di iterazioni sarà
    
    \[ \frac{n}{2^k} = 1 \]
    \[ n = 2^k\]
    \[ k = \log_2(n) \]
    
    
    \item Costo finale del caso peggiore:
    
    \[ T(n)_{peggiore} = \Theta(1) + \log_2(n) \cdot \Theta(1) + \Theta(1) = \Theta(\log(n))\]
\end{itemize}

Il costo computazionale del \textbf{caso peggiore} della \textbf{ricerca binaria}, quindi, è $\Theta(\log(n))$, che è nettamente migliore rispetto al $\Theta(n)$ del caso peggiore della ricerca sequenziale.

Il \textbf{caso migliore}, invece, corrisponde ovviamente al caso in cui, appena avviata l'applicazione dell'algoritmo, si verifica che $v = A[m]$, dunque abbiamo un $\Theta(1)$

Come per la ricerca sequenziale, anche in questo caso il caso peggiore e il caso migliore discordano. Dunque, è necessario calcolare il \textbf{caso medio}.

\textbf{Assunzioni:}

\begin{itemize}
    \item Il numero di elementi dell'array è una \textbf{potenza di 2} (per semplicità di calcolo)

    \item \textbf{$v$ è presente nell'array} (dunque non si ricade nel caso peggiore)
    
    \item Tutte le posizioni dell'array hanno la \textbf{stessa probabilità} di contenere $v$
\end{itemize}

\textbf{Analisi delle posizioni raggiungibili}
\begin{enumerate}
    \item Alla prima iterazione dell'algoritmo, le posizioni raggiungibili sono solo \textbf{una}, ossia quella centrale.
    \item Alla seconda iterazione, le posizioni raggiungibili sono \textbf{due}:
    \begin{itemize}
        \item Quella al centro della metà inferiore
        \item Quella al centro della metà superiore
    \end{itemize}
    
    \item Alla terza iterazione, le posizioni raggiungibili sono \textbf{quattro}:
    \begin{itemize}
        \item Quella al centro della metà inferiore della prima metà inferiore
        \item Quella al centro della metà superiore della prima metà inferiore
        \item Quella al centro della metà inferiore della prima metà superiore
        \item Quella al centro della metà superiore della prima metà superiore
    \end{itemize}
    
    \item ...
\end{enumerate}

Dunque, concludiamo che le \textbf{posizioni raggiungibili} da ogni \textbf{k-esima iterazione} sono $n(k) = 2^{k-1}$.

Poiché abbiamo assunto che ogni posizione è \textbf{equiprobabile}, dunque otteniamo che la probabilità di ogni posizione è 
\[ \frac{n(k)}{n} = \frac{2^{k-1}}{n}\]

Di conseguenza, il numero medio di iterazioni sarà

\[ \sum_{k=0}^{\log(n)} \left ( k \cdot \frac{2^{k-1}}{n} \right ) = \frac{1}{n} \cdot \sum_{k=0}^{\log(n)} k \cdot 2^{k-1} = \frac{(\log(n)-1)2^{\log(n)} +1}{n} = \log(n) - 1 + \frac{1}{n} = \Theta(\log(n))\]

\chapter{La ricorsione}

Fino ad ora abbiamo trattato algoritmi in forma \textbf{iterativa}, ossia basati sull'uso di \textbf{cicli iterativi} (ciclo for, ciclo while, ...). In particolare, nel capitolo precedente, abbiamo visto una formulazione \textbf{iterativa} dell'algoritmo di \textbf{ricerca binaria}, dove veniva impiegato l'uso di un ciclo while. La ricerca binaria, tuttavia, è un esempio perfetto di caso in cui è possibile strutturare un algoritmo in modo \textbf{ricorsivo}.

Per capire cosa intendiamo, vediamo la seguente riformulazione ricorsiva dell'algoritmo:

\begin{itemize}
    \item Ispeziona l'\textbf{elemento centrale} dell'array
    \item Se è \textbf{uguale} a $v$, restituisci il suo \textbf{indice}
    \item Se è \textbf{maggiore} di $v$, riduci l'array alla sua \textbf{metà inferiore} ed \textbf{ri-esegui la ricerca binaria} su di essa
    \item Se è minore di $v$, riduci l'array alla sua \textbf{metà superiore} e \textbf{ri-esegui la ricerca binaria} su di essa
\end{itemize}

L'aspetto cruciale di questa formulazione risiede nel fatto che l'\textbf{algoritmo risolve il problema "riapplicando” se stesso su un sotto-problema}, ossia una versione \textbf{"più semplice"} di esso (in questo caso su una delle due metà dell'array). Questa tecnica viene chiamata \textbf{ricorsione}.

Le \textbf{funzioni ricorsive} possono essere trovate anche nell'ambito matematico. Esempio tipico di ciò è il \textbf{fattoriale} di un numero:

$$
n! = \left \{ \begin{array}{ll}
    n \cdot (n-1)! & \text{se } n > 0 \\
    1 & \text{se } n = 0 \\
\end{array} \right .
$$

\[ n! = n \cdot (n-1)! = n \cdot (n-1) \cdot (n-2) \cdot ... \cdot 1! \cdot 0! = n \cdot ... \cdot 1\]

Il fattoriale di $n$, dunque, è composto dal prodotto tra $n$ stesso e il fattoriale  di $(n-1)$, a sua volta composto dal prodotto tra $(n-1)$ e il fattoriale di $(n-2)$. Tale catena viene ripetuta fino a raggiungere il fattoriale di 0, che, per definizione, \textbf{equivale ad 1}.

Possiamo quindi considerare il fattoriale di 0 il \textbf{caso base} del problema, ossia il suo \textbf{sotto-problema minimo}, corrispondente alla versione più semplice possibile del problema stesso.

All'interno di un problema ricorsivo, dunque, la \textbf{catena di ricorsione viene ripetuta finché non viene raggiunto il caso base} del problema. Per questo motivo, è obbligatoria la presenza di \textbf{almeno un caso base} all'interno di un algoritmo ricorsivo, poiché in sua assenza la catena di ricorsione andrebbe avanti all'infinito.

\begin{frameddefn}{Algoritmo ricorsivo}
Un algoritmo è detto \textbf{ricorsivo} quando è espresso in termini di se stesso.

Un algoritmo ricorsivo ha sempre queste proprietà:
    \begin{itemize}
        \item La soluzione del problema complessivo è costruita risolvendo ricorsivamente uno o più \textbf{sottoproblemi di dimensione minore}, combinando le soluzioni ottenute
        \item la successione dei sottoproblemi, che sono sempre più piccoli, deve sempre convergere ad un sottoproblema che costituisca un \textbf{caso base}, il quale termina la ricorsione.
    \end{itemize}
\end{frameddefn}

\quad

\subsubsection{Esempi di codice ricorsivo}

\begin{itemize}
    \item Algoritmo ricorsivo per il calcolo del \textbf{fattoriale di $n$}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \texttt{def fattoriale(n): \qquad \#n = intero non negativo}

    \texttt{\qquad if (n == 0) return 1}

    \texttt{\qquad return n * fattoriale(n – 1)}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \quad
    
    \item Algoritmo ricorsivo della \textbf{ricerca sequenziale}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \texttt{def Ricerca\_seq\_ric(A, v, n=len(A)-1):}
    
    \texttt{\qquad if (A[n]==v)}
    
    \texttt{\qquad\qquad return n}

    \texttt{\qquad if (n==0):}

    \texttt{\qquad\qquad return -1}
    
    \texttt{\qquad else:}
    
    \texttt{\qquad\qquad return Ricerca\_seq\_ric(A, v, n - 1)}

    \par\noindent\rule{425pt}{0.3pt}
    
    \item Algoritmo ricorsivo della \textbf{ricerca binaria}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    
    \texttt{def Ricerca\_bin\_ric (A, v, i\_min=0, i\_max=len(A)):}
    
    \texttt{\qquad if (i\_min > i\_max):}
    
    \texttt{\qquad\qquad return -1}
    
    \texttt{\qquad m =(i\_min + i\_max)//2}

    \texttt{\qquad if (A[m] = v):}
    
    \texttt{\qquad\qquad return m}

    \texttt{\qquad elif (A[m] > v):}

    \texttt{\qquad\qquad return Ricerca\_bin\_ric (A, v, i\_min, m - 1)}

    \texttt{\qquad else:}

    \texttt{\qquad\qquad return Ricerca\_bin\_ric (A, v, m + 1, i\_max)}

    \par\noindent\rule{425pt}{0.3pt}
    
\end{itemize}

\quad

\section{Iterazione $vs$ Ricorsione}

Nella sezione precedente, abbiamo visto come sia possibile realizzare algoritmi iterativi anche in forma di algoritmi ricorsivi. Difatti, tale regola vale per ogni algoritmo: \textbf{qualsiasi problema risolvibile con un algoritmo ricorsivo può essere risolto anche con un algoritmo iterativo}.

Ma allora perché preferire la ricorsione all'iterazione e viceversa? Non potremmo semplicemente svolgere tutto in maniera iterativa? A tali domande è possibile rispondere con i tre seguenti punti:

\begin{itemize}
    \item Utilizziamo un algoritmo ricorsivo quando si può formulare la soluzione del problema in un modo \textbf{aderente alla natura del problema stesso} (es: il fattoriale di un numero), mentre la soluzione iterativa è molto più complicata o addirittura non evidente
    
    \item Utilizziamo un algoritmo iterativo se esiste una soluzione iterativa altrettanto semplice e chiara quando paragonata alla sua versione ricorsiva
    
    \item Utilizziamo un algoritmo iterativo quando l'efficienza è un requisito primario
\end{itemize}

In particolare, bisogna porre attenzione sull'ultimo punto, poiché \textbf{ogni funzione}, sia essa ricorsiva o no, richiede per la sua esecuzione una certa \textbf{quantità di memoria}, per:

\begin{itemize}
    \item Caricare in memoria il suo codice
    \item Passare i parametri e ritornare i valori calcolati
    \item Memorizzare i valori delle sue variabili locali
\end{itemize}

Dunque, poiché le \textbf{funzioni ricorsive} per loro natura stessa richiamano se stesse un elevato numero di volte, ne segue direttamente che esse abbiano un \textbf{consumo elevato in termini di memoria}.

Perciò, generalmente preferiamo l'iterazione alla ricorsione, a meno che il problema proposto non sia intuitivamente risolvibile come algoritmo ricorsivo, come nel seguente esempio:

\begin{itemize}
    \item Progettare un algoritmo in grado di calcolare l'n-esimo numero di Fibonacci, definito come
    
    \begin{itemize}
        \item $F(0) = 0$
        \item $F(1) = 1$
        \item $F(n) = F(n - 1) + F(n-2)$ se $n > 1$
    \end{itemize}
    
    In questo caso, viene naturale progettare l'algoritmo in termini di \textbf{ricorsione}, poiché l'n-esimo numero di Fibonacci viene calcolato tramite altri due numeri di Fibonacci.
    
    Il codice da implementare, dunque, risulta estremamente semplice nella sua \textbf{versione ricorsiva}: 
    
    \par\noindent\rule{425pt}{0.3pt}
    
    \texttt{def Fibonacci(n):}

    \texttt{\qquad if (n <= 1): return n}
    
    \texttt{\qquad return Fibonacci(n – 1) + Fibonacci(n – 2)}
    
    \par\noindent\rule{425pt}{0.3pt}
    
    Una \textbf{versione iterativa} del problema, invece, risulta particolarmente complessa da interpretare e sviluppare (si consiglia al lettore di effettuare un tentativo nella progettazione dell'algoritmo iterativo).
    
\end{itemize}

Analizziamo ora lo sviluppo della \textbf{catena di ricorsione} nel caso in cui volessimo calcolare il \textbf{quinto numero di Fibonacci}:

\begin{center}
    \includegraphics[scale=0.75]{resources/images/chapter_4/fib_5.png}
\end{center}

Notiamo velocemente alcune \textbf{osservazioni}:
\begin{itemize}
    \item Nonostante l'input di piccole dimensioni, il \textbf{numero di chiamate ricorsive} effettuate è \textbf{enorme}, occupando un'elevata quantità di risorse
    \item Molti \textbf{calcoli} vengono \textbf{ripetuti} numerose volte ($Fib(1)$ viene eseguito 5 volte, $Fib(2)$ 3 volte, ...)
\end{itemize}

Di seguito viene proposta una \textbf{versione iterativa} dell'algoritmo:

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Fibonacci\_iter(n):}

\texttt{\qquad if (n <= 1): return n}

\texttt{\qquad fib\_prec\_prec = 0}

\texttt{\qquad fib\_prec = 1}

\texttt{\qquad for i in range(2,n+1):}

\texttt{\qquad\qquad fib\_prec\_prec, fib\_prec = fib\_prec, fib\_prec\_prec + fib\_prec}

\texttt{\qquad return fib\_prec}

\par\noindent\rule{450pt}{0.3pt}

Siamo facilmente in grado di calcolare il \textbf{costo computazionale} di tale algoritmo, corrispondente a $\Theta(n)$. Ma qual è invece il costo della versione ricorsiva dell'algoritmo?

Considerando $T(n)$ come il costo computazionale della funzione, possiamo facilmente intuire che il \textbf{costo del primo sotto-problema} sarà $T(n-1)$, mentre quello del \textbf{secondo sotto-problema} sarà $T(n-2)$ 

\newpage

\par\noindent\rule{450pt}{0.3pt}

\texttt{def Fibonacci(n):}

\texttt{\qquad if (n <= 1): return n \qquad\qquad\qquad\qquad\qquad\qquad\, $\Theta(1)$}

\texttt{\qquad return Fibonacci(n – 1) + Fibonacci(n – 2) \qquad T(n-1) + T(n-2)}

\par\noindent\rule{450pt}{0.3pt}

Il costo computazionale sarà quindi equivalente a

$$
T = \left \{
\begin{array}{l l}
    T(n) = \Theta(1) + T(n-1) + T(n-2) & \text{se } n > 1 \\
    T(1) = \Theta(1) & \text{se } n \leq 1 
\end{array} \right .
$$

Con gli strumenti attuali, non siamo ancora in grado di calcolare il costo effettivo di tale algoritmo, poiché necessario introdurre il concetto di \textbf{equazione di ricorrenza}.

\addtocontents{toc}{\protect\newpage}
\chapter{Equazioni di ricorrenza}

Come abbiamo visto, nell'ambito del calcolo del costo computazionale gli \textbf{algoritmi ricorsivi}, per loro natura stessa, danno vita ad una f\textbf{unzione matematica anch'essa ricorsiva}.

La funzione matematica ricorsiva che esprime il costo è anche detta \textbf{equazione di ricorrenza}.

Riprendiamo l'esempio del calcolo del fattoriale di un numero.


\par\noindent\rule{450pt}{0.3pt}

\texttt{def fattoriale(n):}

\texttt{\qquad if (n == 0) return 1 \qquad\qquad\qquad $\Theta(1)$}

\texttt{\qquad return n * fattoriale(n – 1) \qquad $T(n-1)$}

\par\noindent\rule{450pt}{0.3pt}

Il costo computazionale di tale algoritmo risulta essere
$$
T = \left \{
\begin{array}{l l}
    T(n) = \Theta(1) + T(n-1)& \text{se } n > 0 \\
    T(0) = \Theta(1) & \text{se } n = 0 
\end{array} \right .
$$

La parte generale dell'equazione di ricorrenza che definisce $T(n)$ deve essere sempre costituita dalla \textbf{somma di almeno due addendi}, di cui \textbf{almeno uno contiene la parte ricorsiva} (nell'esempio $T(n-1)$, mentre uno rappresenta il costo computazionale di tutto ciò che viene eseguito al di fuori della chiamata ricorsiva (in questo caso il $\Theta(1)$).     Inoltre, così come per l'algoritmo ricorsivo, anche nell'equazione di ricorrenza \textbf{deve sempre essere presente un caso base} (in questo caso $T(0)$).

\newpage

\section{Metodo iterativo}

L'idea alla base del \textbf{metodo iterativo} è molto semplice: l'equazione di ricorrenza viene sviluppata in modo da essere espressa come \textbf{somma di termini} dipendenti dal \textbf{caso generico} e dal \textbf{caso base}. Per la sua natura stessa, tuttavia, tale metodo spesso risulta inefficiente per l'elevata quantità di calcoli da effettuare.

Consideriamo la seguente equazione di ricorrenza

$$
T = \left \{
\begin{array}{l l}
    T(n) = T(n-1) + \Theta(1)\\
    T(1) = \Theta(1)
\end{array} \right .
$$

Proviamo a \textbf{sviluppare} il termine $T(n)$ come \textbf{somma dei suoi sotto-termini}: se $T(n)$ è definito come $T(n-1) + \Theta(1)$, allora $T(n-1)$ sarà definito come $T(n-2) + \Theta(1)$ e così via

\begin{enumerate}
    \item Per definizione abbiamo
    \[ T(n) = T(n-1) + \underbrace{\Theta(1)}_{\text{Una volta}} = T(n-2) + 1 \cdot \Theta(1) \]
    
    \item Sviluppando $T(n-1)$, otteniamo che
    \[ T(n) = T(n-2) + \underbrace{\Theta(1) + \Theta(1)}_{\text{Due volte}} = T(n-2) + 2 \cdot \Theta(1) \]
    
    \item Sviluppando $T(n-2)$, otteniamo che
    \[ T(n) = T(n-3) + \underbrace{\Theta(1) + \Theta(1) + \Theta(1)}_{\text{Tre volte}} = T(n-3) + 3 \cdot \Theta(1)\]
    
    \item Sviluppando $T(n-(k-1))$, otteniamo che
    \[ T(n) = T(n-k) + \underbrace{\Theta(1) + \Theta(1) + ... + \Theta(1) + \Theta(1)}_{\text{$k$ volte}} = T(n-k) + k \cdot \Theta(1)\]
\end{enumerate}

Abbiamo quindi ottenuto una \textbf{forma generalizzata} del caso generico $T(n)$ sviluppando i suoi sotto-termini $k$ volte. Come sappiamo, però, dopo un \textbf{determinato numero di ricorsioni}, l'equazione di ricorrenza raggiungerà il suo \textbf{caso base} (nell'esempio $T(1)$), di cui sappiamo per certo il costo computazionale.

La catena di ricorsione, quindi, procederà finché $n-k = 1$, in modo che $T(n-k)$ corrisponda a $T(1)$. Dunque, da ciò ne segue che
\[ n-k = 1 \Longrightarrow k = n - 1\]

Una volta trovato il valore di $k$, ci basterà sostituirlo all'interno dell'equazione per trovare il \textbf{costo computazionale generico}

\[ T(n) = T(n-k) + k \cdot \Theta(1) = T(n-(n-1)) + (n-1) \cdot \Theta(1) = \]
\[ = T(1) + \Theta(n) = \Theta(1) + \Theta(n) = \Theta(n)  \]

\subsubsection{Ulteriori esempi}

\begin{itemize}
    \item Consideriamo la seguente equazione di ricorrenza
    
    $$
    T = \left \{
    \begin{array}{l l}
        T(n) = T(\frac{n}{2}) + \Theta(1)\\
        T(1) = \Theta(1)
    \end{array} \right .
    $$
    
    Sviluppando $T(n)$ otteniamo che
    \[ T(n) = T \left (\frac{n}{2} \right ) + \Theta(1) = T \left (\frac{n}{2^2} \right ) + \Theta(1) + \Theta(1) = T \left (\frac{n}{2^3} \right ) + \Theta(1) + \Theta(1)  + \Theta(1) = ...\]
    
    Generalizzando l'equazione, otteniamo
    
    \[ T(n) = T \left (\frac{n}{2^k} \right ) + k \cdot \Theta(1)\]
    
    Sappiamo che il caso base è $T(1)$ e che viene raggiunto quando
    \[ \frac{n}{2^k} = 1 \Longrightarrow k = \log_2(n)\]
    
    Dunque ne segue che
    \[ T(n) = T \left (\frac{n}{2^k} \right ) + k \cdot \Theta(1) = T(1) + \log_2(n) \cdot \Theta(1) = \Theta(1) + \Theta(\log(n)) = \Theta(\log(n))\]

    \item Consideriamo la seguente equazione di ricorrenza
    
    $$
    T = \left \{
    \begin{array}{l l}
        T(n) = 2T(\frac{n}{2}) + \Theta(1)\\
        T(1) = \Theta(1)
    \end{array} \right .
    $$
    
    Sviluppando $T(n)$ otteniamo che
    \[ T(n) = 2T \left (\frac{n}{2} \right ) + \Theta(1) = 2 \left ( 2T \left (\frac{n}{2^2} \right ) + \Theta(1) \right ) + \Theta(1) = \]
    \[ = 2 \left ( 2 \left ( 2T \left (\frac{n}{2^3} \right ) + \Theta(1) \right ) + \Theta(1) \right ) + \Theta(1) = ...\]
    
    Generalizzando l'equazione, otteniamo
    
    \[ T(n) = 2^k T \left (\frac{n}{2^k} \right ) + \sum_{i=0}^{k-1} 2^i \cdot \Theta(1)\]
    
    Sappiamo che il caso base è $T(1)$ e che viene raggiunto quando
    \[ \frac{n}{2^k} = 1 \Longrightarrow k = \log_2(n)\]
    
    \newpage
    Dunque ne segue che
\end{itemize}

\[ T(n) = 2^k T \left (\frac{n}{2^k} \right ) + \sum_{i=0}^{k-1} 2^i \cdot \Theta(1) = 2^{\log_2(n)} \cdot T(1) + \sum_{i=0}^{\log_2(n)-1} 2^i \cdot \Theta(1) =\]
\[\Theta(n) + \Theta \left (\frac{2^{\log_2(n)}-1}{2-1} \right ) = \Theta(n) \]

\subsubsection{Casi particolari}

\begin{itemize}
    \item Consideriamo la seguente equazione di ricorrenza relativa al calcolo dell'n-esimo numero di Fibonacci
    
    $$
    T = \left \{
    \begin{array}{l}
        T(n) = T(n-1)+T(n-2)+\Theta(1)\\
        T(1) = \Theta(1)\\
    \end{array}
    \right .
    $$
    
    Sviluppando $T(n)$ otteniamo che
    
    \[ T(n) = T(n-1)+T(n-2) + \Theta(1) = T(n-2)+2T(n-3)+T(n-4) + 3\Theta(1) =\]
    \[=T(n-3)+3T(n-4)+3T(n-5)+T(n-6) + 6\Theta(1)= ...\]
    
    Notiamo come in un caso del genere \textbf{non riusciamo a generalizzare il problema,} dunque non possiamo calcolare il costo asintotico in modo analogo ai casi precedenti.
    
    In una situazione del genere, possiamo usare le \textbf{maggiorazioni} e le \textbf{minorazioni}, cercando di calcolare il \textbf{costo O grande} e il \textbf{costo $\Omega$}:
    
    \begin{itemize}
        \item Considerando $T(n) = T(n-1)+T(n-2) + \Theta(1)$, possiamo certamente dire che
        
        \[T(n-1)+T(n-2) + \Theta(1) \leq T(n) = T(n-1)+T(n-1) + \Theta(1)\]
        
        Dunque, poiché l'equazione di ricorrenza con cui stiamo comparando la nostra equazione originale è \textbf{minore} di quest'ultima, calcolando il \textbf{costo $\Theta$ della nuova equazione} otteniamo anche il \textbf{costo O grande dell'equazione iniziale}:
        
        $$
        T_1 = \left \{
        \begin{array}{l}
            T_1(n) = T_1(n-1)+T_1(n-1)+\Theta(1) = 2T_1(n-1) + \Theta(1)\\
            T_1(1) = \Theta(1)\\
        \end{array}
        \right .
        $$
    
        Sviluppando $T_1(n)$ otteniamo che
        
        \[ T_1(n) = 2T_1(n-1) + \Theta(1) =  2[2T_1(n-2) + \Theta(1)] + \Theta(1) =\]
        \[ = 2[2[2T_1(n-3) + \Theta(1)] + \Theta(1)] + \Theta(1) = ...\]
        
        Generalizzando l'equazione, otteniamo
        
        \[ T_1(n) = 2^kT_1(n-k)+\sum_{i=0}^{k-1} 2^i\Theta(1)\]
        
        Sappiamo che il caso base è $T(1)$ e che viene raggiunto quanto
        \[ n-k = 1 \Longrightarrow k = n-1\]
        
        Dunque ne segue che
        \[ T_1(n) = 2^{n-1}T_1(1)+\sum_{i=0}^{n-2} 2^i\Theta(1) = \Theta(2^n) + (2^{n-1} -1) \cdot \Theta(1) = \Theta(2^n)\]
        
        Quindi, poiché $T(n) \leq T_1(n)$, ne segue che
        \[ T(n) \leq T_1(n)\]
        \[ T(n) \leq \Theta(2^n)\]
        \[ T(n) = O(2^n)\]
        
        \item Considerando $T(n) = T(n-1)+T(n-2) + \Theta(1)$, possiamo certamente dire che
        
        \[T(n-1)+T(n-2) + \Theta(1) \geq T(n) = T(n-2)+T(n-2) + \Theta(1)\]
        
        Dunque, poiché l'equazione di ricorrenza con cui stiamo comparando la nostra equazione originale è \textbf{maggiore} di quest'ultima, calcolando il \textbf{costo $\Theta$ della nuova equazione} otteniamo anche il \textbf{costo $\Omega$ dell'equazione iniziale}:
        
        $$
        T_2 = \left \{
        \begin{array}{l}
            T_2(n) = T_2(n-2)+T_2(n-2)+\Theta(1) = 2T_2(n-2) + \Theta(1)\\
            T_2(1) = \Theta(1)\\
        \end{array}
        \right .
        $$
    
        Sviluppando $T_2(n)$ otteniamo che
        
        \[ T_2(n) = 2T_2(n-2) + \Theta(1) =  2[2T_2(n-4) + \Theta(1)] + \Theta(1) =\]
        \[ = 2[2[2T_2(n-6) + \Theta(1)] + \Theta(1)] + \Theta(1) = ...\]
        
        Generalizzando l'equazione, otteniamo
        
        \[ T_2(n) = 2^kT_2(n-2k)+\sum_{i=0}^{k-1} 2^i\Theta(1)\]
        
        Sappiamo che il caso base è $T(1)$ e che viene raggiunto quanto
        \[ n-2k = 1 \Longrightarrow k = \frac{n-1}{2} \approx \frac{n}{2}\]
        
        Dunque ne segue che
        \[ T_2(n) = 2^{\frac{n}{2}}T_2(1)+\sum_{i=0}^{\frac{n}{2}-1} 2^i\Theta(1) = \Theta(2^{\frac{n}{2}}) + (2^{\frac{n}{2}}-1)\Theta(1) = \Theta(2^{\frac{n}{2}}) = \Theta(\sqrt{2^n})\]
        
        Quindi, poiché $T(n) \geq T_2(n)$, ne segue che
        \[ T(n) \geq T_2(n)\]
        \[ T(n) \geq \Theta(\sqrt{2^n})\]
        \[ T(n) = \Omega(\sqrt{2^n})\]
    \end{itemize}
    
    Dunque, poiché il costo asintotico di \textbf{limite inferiore} e quello di \textbf{limite superiore} differiscono, \textbf{non possiamo decretare un costo asintotico stretto} per tale equazione di ricorrenza
    \[ T(n) = \Omega(\sqrt{2^n}), \;\;\;  T(n) = O(2^n)\]
    
    dunque, con opportune costanti $c_1$ e $c_2$, otteniamo
    \[ c_1 \cdot \sqrt{2^n}\leq T(n) \leq c_2 \cdot 2^n\]
    
\end{itemize}

\quad

\section{Metodo di sostituzione}

L'idea alla base del \textbf{metodo di sostituzione} risulta più astratta ed "azzardata" rispetto al metodo iterativo: viene \textbf{ipotizzata una soluzione} per l'equazione di ricorrenza data e si \textbf{dimostra} tale ipotesi tramite l'\textbf{induzione}. Lo svantaggio di tale metodo risulta essere quello di poter procedere solo per \textbf{tentativi} tra varie \textbf{maggiorazioni} e \textbf{minorazioni}.

Consideriamo ancora la seguente equazione

$$
T = \left \{
\begin{array}{l}
        T(n) = T(n-1) + \Theta(1)\\
        T(1) = \Theta(1)
\end{array}\right .
$$

\begin{enumerate}
    \item \textbf{Ipotizziamo la soluzione} $T(n) = O(n)$, ossia che $T(n) \leq k \cdot n$ per una certa costante $k$ indeterminata

    \item \textbf{Eliminiamo la notazione asintotica} dall'equazione, sostituendo i due $\Theta(1)$ con due costanti $c$ e $d$ fissate
    
    $$
    T = \left \{
    \begin{array}{l}
            T(n) = T(n-1) + c\\
            T(1) = d
    \end{array}\right .
    $$
    
    \item \textbf{Analizziamo il caso base}, ossia quando $n=1$
    \[ T(1) \leq k \cdot 1 \]
    Poiché sappiamo che $T(1) = d$, otteniamo che la disuguaglianza è vera se e solo se
    \[ d \leq k\]
    
    \item \textbf{Consideriamo il passo induttivo}, ossia per un $n$ generico
    \[ T(n) \leq kn \]
    Poiché sappiamo che $T(n) = T(n-1) + c$, otteniamo che
    \[ T(n-1) + c \leq kn\]
    Tuttavia, per \textbf{ipotesi}, sappiamo anche che $T(n-1) \leq k(n-1)$, dunque riscriviamo la diseguaglianza come
    \[ k(n-1) + c \leq kn\]
    \[ kn - k + c \leq kn\]
    \[-k + c \leq 0\]
    \[ c \leq k\]
    
    Dunque la diseguaglianza è vera se e solo se $c \leq k$
    
    \item Poiché esiste sempre un valore $k$ tale che $k \geq c$ e $k \geq d$, la \textbf{soluzione ipotizzata $T(n) \leq kn$ è vera,} dunque $T(n) = O(n)$
    
    \item \textbf{Ipotizziamo la soluzione} $T(n) = \Omega(n)$, ossia che $T(n) \geq h \cdot n$ per una certa costante $h$ indeterminata
    
    \item \textbf{Analizziamo il caso base}, ossia quando $n=1$
    \[ T(1) \geq h \cdot 1 \]
    Poiché sappiamo che $T(1) = d$, otteniamo che la disuguaglianza è vera se e solo se
    \[ d \geq h\]
    
    \item \textbf{Consideriamo il passo induttivo}, ossia per un $n$ generico
    \[ T(n) \geq hn \]
    Poiché sappiamo che $T(n) = T(n-1) + c$, otteniamo che
    \[ T(n-1) + c \geq hn\]
    Tuttavia, per \textbf{ipotesi}, sappiamo anche che $T(n-1) \geq h(n-1)$, dunque riscriviamo la diseguaglianza come
    \[ h(n-1) + c \geq hn\]
    \[ hn - h + c \geq hn\]
    \[-h + c \geq 0\]
    \[ c \geq h\]
    
    Dunque la diseguaglianza è vera se e solo se $c \geq h$
    
    \item Poiché esiste sempre un valore $h$ tale che $h \leq c$ e $h \leq d$, la \textbf{soluzione ipotizzata $T(n) \geq kn$ è vera,} dunque $T(n) = \Omega(n)$
    
    \item Poiché \textbf{$T(n)$ è sia in $O(n)$ sia in $\Omega(n)$}, possiamo concludere che $T(n) = \Theta(n)$
\end{enumerate}

\quad

È necessario sottolineare che, in tale caso, avremmo potuto \textbf{ipotizzare e verificare} anche le soluzioni $O(n^2), O(2^n), ...$ e le soluzioni $\Omega(\sqrt{n}), \Omega(\log(n)), ...$. Poiché esistono più soluzioni, è necessario fare molta attenzione alle ipotesi effettuate, dato che, ovviamente, l'obiettivo è \textbf{stimare i costi asintotici il più stretti possibile}.

\quad

\section{Metodo dell'albero}

Il \textbf{metodo dell'albero} corrisponde esattamente alla \textbf{rappresentazione grafica del metodo iterativo}, rendendolo di interpretazione più semplice rispetto alla sua controparte scritta.

Consideriamo la seguente equazione di ricorrenza:

$$
T = \left \{
\begin{array}{l}
    T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n^2) \\
    T(1) = \Theta(1)
\end{array}\right .
$$

\quad

\begin{itemize}
\item Utilizzando il \textbf{metodo iterativo}, rappresenteremmo $T(n)$ nella forma:

\[ T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n^2)\]

\newpage
Invece, la rappresentazione grafica del \textbf{metodo dell'albero} corrisponde a:
\begin{center}
    \includegraphics[scale=0.8]{resources/images/chapter_6/tree_m1.png}
\end{center}

\quad

\item Sviluppando la \textbf{seconda iterazione}, otteniamo

\[ T(n) = 2 \left (2T \left ( \frac{n}{2^2} \right ) + \Theta \left ( \left (\frac{n}{2} \right ) ^2 \right ) \right ) + \Theta(n^2) = 4T \left ( \frac{n}{2^2} \right ) + 2\Theta \left ( \left ( \frac{n}{2} \right )^2 \right ) + \Theta(n^2)\]

\begin{center}
    \includegraphics[scale=0.8]{resources/images/chapter_6/tree_m2.png}
\end{center}

\item Sviluppando la \textbf{terza iterazione}, otteniamo

\[ T(n) = 2 \left (2 \left ( 2T \left ( \frac{n}{2^3} \right ) + \Theta(n^2) \right ) + \Theta(n^2) \right ) + \Theta(n^2) = \]
\[ =  8T \left ( \frac{n}{2^3} \right ) + 4\Theta \left ( \left ( \frac{n}{2^2} \right )^2 \right ) + 2\Theta \left ( \left ( \frac{n}{2} \right )^2 \right ) + \Theta(n^2)\]

\begin{center}
    \includegraphics[scale=0.8]{resources/images/chapter_6/tree_m3.png}
\end{center}


\item A questo punto, effettuiamo il passaggio di \textbf{generalizzazione} del metodo iterativo in vista della \textbf{k-esima iterazione}

\[ T(n) = 2^{k-1}T \left (\frac{n}{2^k} \right ) + \left ( \sum_{i=0}^{k} 2^i \cdot \Theta \left (\left ( \frac{n}{2^i} \right )^2 \right )\right ) =
2^{k-1}T \left (\frac{n}{2^k} \right ) + \left ( \sum_{i=0}^{k-1} \Theta \left ( \frac{n^2}{2^i} \right )\right ) =\]
\[= 2^{k-1}T \left (\frac{n}{2^k} \right ) + \Theta(n^2) \cdot \sum_{i=0}^{k-1}\frac{1}{2^i}\]

Tali iterazioni vengono eseguite finché
\[ \frac{n}{2^k} = 1 \Longrightarrow k = \log_2(n)\]

Sostituendo $k$ otteniamo

\[T(n) = 2^{\log_2(n)-1} \cdot \Theta(1) + \Theta(n^2) \cdot  \sum_{i=0}^{\log(n)-1} \frac{1}{2^i} = \frac{1}{2}n \cdot \Theta(1) + \Theta(n^2) \cdot \left ( 2-\frac{1}{2^{\log(n)+1}} \right ) =  \Theta(n^2)\]

\newpage

\item Analizziamo ora invece cosa accade ad ogni \textbf{livello dell'albero} che siamo andati a creare

\begin{center}
    \includegraphics[scale=0.8]{resources/images/chapter_6/tree_m4.png}
\end{center}

\item \textbf{Sommando tutti i livelli}, otteniamo che

\[ T(n) = \sum_{i=0}^{k-1} 2^{i} \cdot \Theta \left ( \left ( \frac{n}{2^{i}} \right )^2 \right ) = \Theta \left ( \sum_{i=0}^{k-1} \frac{n^2}{2^i} \right )\]

\item Il numero di iterazioni totali può essere trovato analogamente al metodo iterativo (ossia trovando il valore di $k$, che abbiamo detto essere $k = \log(n)$)

\[ T(n)= \Theta \left ( \sum_{i=0}^{\log(n)-1} \frac{n^2}{2^i} \right )= \Theta(n^2) \cdot \left( 2 - \frac{1}{2^{\log(n)+1}} \right ) = \Theta(n^2)\]
\end{itemize}

\quad

Notiamo quindi che utilizzando entrambi i metodi abbiamo ottenuto lo stesso identico risultato tramite gli \textbf{stessi identici calcoli}. Il metodo dell'albero, quindi, si conferma essere nient'altro che una rappresentazione grafica del metodo iterativo.

\newpage

\section{Metodo principale}

L'idea dietro al metodo principale consiste nell'avere una \textbf{formula} (o "ricetta") in grado di calcolare in modo estremamente rapido il costo computazionale di un determinato algoritmo. L'unico svantaggio di tale metodo, è la sua \textbf{estrema limitazione}, poiché esso \textbf{funziona solo con equazioni nel formato}

\[ T(n) = \alpha \cdot \Theta \left ( \frac{n}{\beta} \right ) + f(n)\]

dove $T(1) = \Theta(1)$ e dove $f(n)$ può essere un qualsiasi costo $\Theta$ 

L'\textbf{enunciato} del teorema del metodo principale afferma che

\begin{framedthm}{Metodo principale}
    Dati $\alpha \geq 1, \beta > 1$, una funzione asintoticamente positiva $f(n)$ ed un'equazione di ricorrenza di forma 
    
    $$
    T = \left \{
    \begin{array}{l}
        T(n) = \alpha \cdot T \left ( \frac{n}{\beta} \right ) + f(n) \\
        T(1) = \Theta(1)
    \end{array}
    \right .
    $$
    
    abbiamo che:
    
    \begin{itemize}
        \item Se per $f(n)$ vale che 
        \[ f(n) = O(n^{\log_{\beta}(\alpha)-\varepsilon})\]
        per qualche costante $\varepsilon > 0$, allora
        \[ T(n) = \Theta(n^{\log_{\beta}(\alpha)})\]
        
        \item Se per $f(n)$ vale che 
        \[ f(n) = \Theta(n^{\log_{\beta}(\alpha)})\]
        allora
        \[ T(n) = \Theta(n^{\log_{\beta}(\alpha)}\cdot \log(n))\]
        
        \item Se per $f(n)$ vale che 
        \[ f(n) = \Omega(n^{\log_{\beta}(\alpha)+\varepsilon})\]
        per qualche costante $\varepsilon > 0$ e se
        \[ \alpha \cdot f \left ( \frac{n}{\beta} \right ) \leq c \cdot f(n)\]
        per qualche costante $0 < c < 1$ e per $n$ abbastanza grande, allora
        \[ T(n) = \Theta(f(n))\]
    \end{itemize}
\end{framedthm}

\newpage

Traducendo il tutto in termini semplici, all'interno del teorema principale possono verificarsi \textbf{tre casi possibili} dovuti al confronto tra $f(n)$ e $n^{\log_{\beta}(\alpha)}$:

\begin{itemize}
    \item \textbf{Caso 1}: Se il \textbf{più grande dei due è $n^{\log_{\beta}(\alpha)}$}, allora il costo finale è
    \[ T(n) = \Theta(n^{\log_{\beta}(\alpha)})\]
    
    \item \textbf{Caso 2}: Se sono \textbf{uguali}, allora il costo finale è
    \[ T(n) = \Theta(f(n) \cdot \log(n)) = \Theta(n^{\log_{\beta}(\alpha)} \cdot \log(n))\]
    
    \item \textbf{Caso 3}: Se il \textbf{più grande dei due è $f(n)$}, allora il costo finale è
    \[ T(n) = \Theta(f(n))\]
\end{itemize}

\textbf{ATTENZIONE}: è necessario sottolineare che in questo contesto con la terminologia "più grande" si intende il valore \textbf{polinomialmente più grande}, per via della presenza del fattore $\varepsilon$

\quad

\subsubsection{Esempi}

\textit{Ai fini di dimostrare le applicazioni del metodo principale, in tutti i seguenti esempi verrà dato per assunto che  $T(1) = \Theta(1)$}

\quad

\begin{itemize}
    \item Consideriamo la seguente equazione ricorsiva
    
    \[ T(n) = 9T \left ( \frac{n}{3} \right ) + \Theta(n)\]
    
    \begin{itemize}
        \item $\alpha=9$, $\beta = 3$
        \item $f(n) = \Theta(n)$
        \item $n^{\log_{\beta}(\alpha)} = n^{\log_{3}(9)} = n^2$
        \item Verifichiamo facilmente che ci troviamo all'interno del \textbf{Caso 1}, poiché ponendo $\varepsilon = 1$ otteniamo
        \[ f(n) = O(n^{\log_{\beta}(\alpha) - \varepsilon}) = O(n^{\log_{3}(9) - 1}) = O(n)\]
        
        \item Dunque, il costo asintotico sarà
        \[ T(n) = \Theta(n^{\log_{\beta}(\alpha)}) = \Theta(n^2)\]
        
    \end{itemize}
    
    \newpage
    
    \item Consideriamo la seguente equazione ricorsiva
    
    \[ T(n) = T \left ( \frac{2n}{3} \right ) + \Theta(1)\]
    
    \begin{itemize}
        \item $\alpha=1$, $\beta = \frac{3}{2}$
        \item $f(n) = \Theta(1)$
        \item $n^{\log_{\beta}(\alpha)} = n^{\log_{\frac{3}{2}}(1)} = n^0 = 1$
        \item Verifichiamo facilmente che ci troviamo all'interno del \textbf{Caso 2}, poiché 
        \[ f(n) = n^{\log_{\beta}(\alpha)}\]
        \[ \Theta(1) = \Theta(1)\]
        
        \item Dunque, il costo asintotico sarà
        \[ T(n) = \Theta(f(n) \cdot \log(n)) = \Theta(\log(n))\]
        
    \end{itemize}
    
    \item Consideriamo la seguente equazione ricorsiva
    
    \[ T(n) = 3T \left ( \frac{n}{4} \right ) + \Theta(n \cdot \log(n))\]
    
    \begin{itemize}
        \item $\alpha=3$, $\beta = 4$
        \item $f(n) = \Theta(n \cdot \log(n))$
        \item $n^{\log_{\beta}(\alpha)} = n^{\log_{4}(3)} \approx n^{0.7}$
        
        \item Verifichiamo facilmente che ci troviamo all'interno del \textbf{Caso 3}, poiché ponendo $\varepsilon = 0.1$ otteniamo
        \[ f(n) = \Omega(n^{\log_{\beta}(\alpha)+\varepsilon})\]
        \[ f(n) = \Omega(n^{\log_{4}(3)+0.1}) \approx n^{0.8}\]
        
        \item Tuttavia, ricordiamo che è necessario prima \textbf{verificare} che
        \[ \alpha \cdot f\left( \frac{n}{\beta} \right ) \leq c \cdot f(n)\]
        \[ 3 \cdot \frac{n}{4} \cdot \log \left ( \frac{n}{4} \right ) \leq c \cdot n \cdot \log(n)\]
        
        \[ 3 \cdot \frac{n}{4} \cdot \log \left ( \frac{n}{4} \right ) \leq c \cdot n \cdot \log(n)\]
        
        \item Notiamo facilmente come porre $c = \frac{3}{4}$ sia sufficiente a soddisfare la disequazione
        \[ \frac{3n}{4} \cdot \log \left ( \frac{n}{4} \right ) \leq \frac{3n}{4} \cdot \log(n)\]
        \[ \log \left ( \frac{n}{4} \right ) \leq \log(n) \]
        
        \item Dunque, il costo asintotico sarà
        \[ T(n) = \Theta(f(n)) = \Theta(n \cdot \log(n))\]
        
    \end{itemize}
    
    \item Consideriamo la seguente equazione ricorsiva
    
    \[ T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n \cdot \log(n))\]
    
    \begin{itemize}
        \item $\alpha=2$, $\beta = 2$
        \item $f(n) = \Theta(n \cdot \log(n))$
        \item $n^{\log_{\beta}(\alpha)} = n^{\log_{2}(2)} = n$
        
        \item In questo caso, notiamo come \textbf{non rientriamo in nessuno dei tre casi}, poiché $f(n) \neq n^{\log_{\beta}(\alpha)}$ (dunque non siamo nel caso 2) e non esiste un valore di $\varepsilon$ che possa verificare il caso 1 o il caso 3. Di conseguenza, \textbf{non possiamo applicare il metodo principale}, richiedendo l'uso di uno degli altri tre metodi
        
        \item \textbf{ATTENZIONE:} è necessario sottolineare che per \underline{tale versione enunciata} del Teorema Principale non è possibile risolvere tale equazione di ricorrenza.
        
        Un enunciato più complesso di tale teorema permette invece di risolvere tale equazione di ricorrenza, dando un costo pari a $T(n) = n \cdot \log^2(n)$
    \end{itemize}
\end{itemize}

\chapter{Il Problema dell'Ordinamento}

Assieme al problema della ricerca, il \textbf{problema dell'ordinamento} è in assoluto uno dei problemi più ricorrenti nell'informatica, poiché ha un'importanza fondamentale per le applicazioni di tutti i giorni, tant'è che si stima che una parte rilevante del tempo di calcolo complessivo consumato nel mondo sia relativa all'esecuzione di \textbf{algoritmi di ordinamento}.

Un \textbf{algoritmo di ordinamento} è in grado di ordinare gli elementi di un insieme sulla base di una certa \textbf{relazione d'ordine}, definita sull'insieme stesso. Per semplificare la comprensione, nelle seguenti sezioni supporremo che gli $n$ elementi da ordinare siano dei numeri interi contenuti all'interno di un array.

Tuttavia, nei problemi reali, i dati da ordinare sono strutturati in \textbf{record}, ossia in gruppi di \textbf{informazioni} non sempre omogenee \textbf{relative allo stesso soggetto}, i quali vengono ordinati rispetto ad un'informazione specifica in grado di identificare univocamente tale record, chiamata \textbf{chiave}. Per ordinare un insieme di record, dunque, è necessario ordinarli in base alle loro chiavi.

Gli algoritmi di ordinamento che vedremo nelle sezioni successive sono:
\begin{itemize}
    \item \textbf{Insertion Sort}
    \item \textbf{Selection Sort}
    \item \textbf{Bubble Sort}
    \item \textbf{Merge Sort}
    \item \textbf{Quicksort}
    \item \textbf{Heap Sort}
    \item \textbf{Counting Sort}
\end{itemize}

\newpage

\section{Insertion Sort}

L'algoritmo di \textbf{Insertion Sort} può essere paragonato all'\textbf{ordinamento di un mazzo di carte} nella vita reale: scorrendo il mazzo analizziamo ogni carta e la mettiamo nella sua posizione corretta, inserendola in mezzo al mazzo di carte già ordinate.

In particolare, gli step dell'insertion sort sono:

\begin{itemize}
    \item Gli elementi da ordinare sono inizialmente contenuti in un array
    \item Viene \textbf{estratto l'elemento della posizione $i$}, così da liberare la sua posizione corrente
    \item Tutti gli elementi alla s\textbf{ua sinistra che sono maggiori di esso} si spostano di una posizione \textbf{verso destra}, finché non viene trovato un elemento minore (o uguale) all'elemento estratto
    \item A quel punto, l'elemento estratto viene \textbf{inserito nella posizione liberatasi} dallo scorrimento degli altri elementi
    \item \textbf{INVARIANTE}: ad ogni passo $i$, gli elementi con indice $<i$, ossia a sinistra, sono \textbf{già ordinati}, mentre quelli con indice $>i$, ossia a destra, sono \textbf{ancora da processare}
\end{itemize}

\textit{Nota: con il termine "Invariante" si intende un predicato che, nonostante la manipolazione effettuata dall'algoritmo alla k-esima iterazione, rimane sempre vero}

\quad

\begin{center}
    \includegraphics[scale=0.525]{resources/images/chapter_7/insertion_sort.png}
\end{center}

Lo pseudocodice corrispondente di tale algoritmo risulta quindi essere:

\begin{verbatim}
def Insertion_Sort(A)
    for j in range(1,len(A)):
        x = A[j]
        i = j – 1
        while (i>=0 and A[i]>x)
            A[i+1] = A[i]
            i = i – 1
        A[i+1] = x
\end{verbatim}

Il numero di iterazioni del ciclo for esterno risulta quindi essere $n-1$, mentre il numero di iterazioni del ciclo while interno si suddivide in un caso migliore ed un caso peggiore:
\begin{itemize}
    \item \textbf{Caso migliore}: Se il primo elemento precedente a quello estratto è minore di esso (una sola operazione)
    
    \item \textbf{Caso peggiore}: Se ogni elemento precedente a quello estratto è maggiore di esso ($j$ operazioni, dove $j$ è l'indice dell'elemento estratto)
\end{itemize}

Dunque, nel caso in cui il \textbf{ciclo while rientri sempre nel caso migliore} (possibile solo quando la lista è già ordinata), il costo computazionale sarà

\[ T(n) = (n-1) \cdot \Theta(1) = \Theta(n)\]

Nel caso in cui il \textbf{ciclo while rientri sempre nel caso peggiore,} invece, il costo computazionale sarà:
\[ T(n) = \sum_{j=0}^{n-1} [\Theta(1) + \Theta(j)] = \Theta(n) + \Theta(n^2) = \Theta(n^2)\]

\quad

\section{Selection Sort}

Il \textbf{Selection Sort}, risulta essere il più intuitivo tra tutti gli algoritmi di ordinamento per via della sua estrema semplicità:

\begin{itemize}
    \item Viene \textbf{ricercato il minimo} all'interno dell'array
    \item Il minimo trovato viene \textbf{scambiato di posizione} con il primo elemento dell'array
    \item Successivamente, l'algoritmo viene ripetuto sul \textbf{sotto-array di dimensione n-1} (poiché il primo elemento è già ordinato), fino a quando il sotto-array non sarà vuoto
\end{itemize}

\quad

\begin{center}
    \includegraphics[scale=0.525]{resources/images/chapter_7/selection_sort.png}
\end{center}

\quad

Lo pseudocodice di tale algoritmo risulta quindi di facile implementazione:

\begin{verbatim}
def Selection_Sort(A)
    for i in range(len(A)-1):
        m = i
        for j in range(i+1,len(A)):
            if (A[j] < A[m])
                m = j
        A[m],A[i]=A[i],A[m]
\end{verbatim}

A differenza dell'Insertion Sort, tale algoritmo non possiede un caso peggiore ed un caso migliore, bensì \textbf{il suo costo corrisponde sempre a $\Theta(n^2)$}:
\[ T(n) = \sum_{j=0}^{n-2} [\Theta(1) + (n-j) \cdot \Theta(1)  + \Theta(1)] = \Theta(n) + \Theta(n^2) = \Theta(n^2)\]

\section{Bubble Sort}

L'algoritmo di Bubble Sort è strutturato in fasi:

\begin{itemize}
    \item Partendo \textbf{da destra verso sinistra}, vengono analizzate tutte le \textbf{coppie di elementi adiacenti}
    \item Se l'\textbf{elemento più a destra è minore del suo elemento precedente}, allora le loro posizioni vengono \textbf{scambiate}, altrimenti no
    \item Una volta comparata la \textbf{coppia più a sinistra}, l'elemento minore dell'intero array risulterà trasportato nella sua posizione finale proprio come una \textbf{bolla}
    \item Successivamente, l'algoritmo viene ripetuto sul sotto-array composto dagli altri elementi dell'array
\end{itemize}

\quad

\begin{center}
    \includegraphics[scale=0.6]{resources/images/chapter_7/bubble_sort.png}
\end{center}

\newpage

L'implementazione in pseudocodice di tale algoritmo risulta essere:

\begin{verbatim}
def Bubble_Sort(A)
    for i in range(len(A)):
        for j in range(len(A)-1,i,-1):
            if (A[j] < A[j - 1])
                A[j], A[j - 1]=A[j-1],A[j] 
\end{verbatim}

Come per il Selection Sort, anche il Bubble Sort non possiede casi migliori o peggiori, bensì il \textbf{suo costo computazione risulta sempre essere} $\Theta(n^2)$:

\[ T(n) = \sum_{j=0}^{n-1} [\Theta(1) + (n-j) \cdot \Theta(1)  + \Theta(1)] = \Theta(n) + \Theta(n^2) = \Theta(n^2)\]

\quad

\section{Complessità minima di un ordinamento}
\label{min_cost_ord}

Dopo aver visto quelli che possono essere considerati i tre algoritmi di ordinamento più \textit{naif} ed aver osservato che ognuno di essi abbia un costo computazionale pari a $\Theta(n^2)$, ci viene naturale chiederci quale possa essere l'algoritmo di ordinamento avente il \textbf{costo minimo possibile}.

In parole povere, quindi, vogliamo stabilire un \textbf{limite minimo di costo computazionale} al di sotto del quale \textbf{nessun algoritmo di ordinamento basato su confronti fra coppie} di elementi possa andare.

A tale scopo, risulta particolarmente utile l'\textbf{albero di decisione}, ossia uno strumento in grado di rappresentare \textbf{tutte le strade che la computazione di uno specifico algoritmo può intraprendere}, sulla base dei possibili esiti dei test previsti dall'algoritmo stesso.

Poiché siamo in ambito di algoritmi di ordinamento basati su confronti, \textbf{ogni decisione} dell'albero corrisponderà solo a \textbf{due possibili esiti}: $a \leq b$ oppure $a > b$.

L'\textbf{albero di decisione} relativo a un qualunque algoritmo di \textbf{ordinamento basato su confronti} ha queste proprietà:

\begin{itemize}
    \item Corrisponde ad un \textbf{albero binario} che rappresenta tutti i possibili confronti che vengono effettuati dall'algoritmo, dunque ogni \textbf{nodo} possiede due figli, mentre ogni \textbf{foglia} non ne possiede alcuno
    \item Ogni \textbf{nodo rappresenta un singolo confronto}, ed i due figli del nodo sono relativi ai due possibili esiti di tale confronto. Il figlio sinistro di ogni nodo corrisponderà ad un esito $a \leq b$, mentre il figlio destro corrisponderà ad un esito $a > b$.
    \item Ogni \textbf{foglia rappresenta una possibile soluzione} del problema, la quale è una specifica \textbf{permutazione della sequenza in ingresso}.
\end{itemize}

\newpage

Ad esempio, l'albero decisionale di un algoritmo di ordinamento basato su confronti eseguito su un \textbf{array di 3 elementi} corrisponde a:

\begin{center}
    \includegraphics[scale=0.65]{resources/images/chapter_7/decision_tree.png}
\end{center}

Cerchiamo quindi di determinare tale \textbf{limitazione minima del costo computazionale} dell'algoritmo:

\begin{enumerate}
    \item Il \textbf{percorso più lungo} corrisponde al numero di confronti effettuati dall'algoritmo nel caso peggiore
    \item Dato che la soluzione del problema può corrispondere ad una qualunque delle \textbf{permutazioni della sequenza in ingresso}, le \textbf{soluzioni possibili} del problema  sono $n!$ preso in input un array di $n$ elementi
    \item Un \textbf{albero binario di altezza $h$ non può contenere più di $2^h$ foglie}
\end{enumerate}

\quad

Dalle precedenti tre osservazioni, quindi, ne segue che l'altezza $h$ deve essere un valore tale per cui:
\[ 2^h \geq n! \quad \Longrightarrow \quad h \geq \log(n!) \]

Come abbiamo già visto nella sezione \ref{log_n_factorial}, sappiamo che $\log(n!) = \Theta(n \, \log(n))$, dunque possiamo affermare che

\[ h \geq \log(n!)\]
\[ h \geq \Theta(n \, \log(n)) \]
\[ h = \Omega(n \, \log(n)) \]

Dunque, possiamo stipulare il seguente teorema

\begin{framedthm}{Costo di un ordinamento con confronti}
    Il costo computazionale di \textbf{qualsiasi algoritmo di ordinamento basato su confronti} è $\Omega(n \, \log (n))$
\end{framedthm}

\newpage

\section{Merge Sort}

L'algoritmo \textbf{Merge Sort} è un algoritmo di ordinamento basato sull'utilizzo della \textbf{tecnica algoritmica} chiamata \textbf{divide et impera}:

\begin{itemize}
    \item Il problema originale viene scomposto in sotto-problemi di dimensione inferiore \textbf{(divide)}
    \item I sotto-problemi vengono risolti richiamando ricorsivamente l'algoritmo stesso \textbf{(impera)}
    \item Le soluzioni dei sotto-problemi vengono utilizzate per comporre la soluzione del problema originale \textbf{(combina)}
\end{itemize}

Questa tecnica algoritmica non è fine solo a questo algoritmo, bensì essa risulta estremamente impiegata all'interno dello sviluppo di algoritmi complessi.

Nell'ambito del Merge Sort, l'approccio utilizzato è il seguente:

\begin{itemize}
    \item \textbf{Divide}: la sequenza di $n$ elementi viene divisa in due sotto-sequenze di $\frac{n}{2}$ elementi
    \item \textbf{Impera}: viene richiamato l'algoritmo ricorsivamente su entrambe le sotto-sequenze
    \item \textbf{Caso base}: la ricorsione termina quando la sotto-sequenza contiene un solo elemento, dunque è già ordinata
    \item \textbf{Combina}: le due sotto-sequenze, che sono già ordinate ricorsivamente, vengono fuse in un'unica sequenza, fino a tornare alla sequenza di dimensione $n$
\end{itemize}

\begin{center}
    \includegraphics[scale=0.7]{resources/images/chapter_7/merge_sort.png}
\end{center}

\quad

Possiamo notare come, in base agli step svolti dall'algoritmo, il vero ordinamento avvenga all'interno dello step \textbf{combina}, dove un \textbf{sotto-algoritmo} interno al Merge Sort stesso, che chiameremo \textbf{"Fondi"} per comodità, unisce le due sotto-sequenze creandone una ordinata.

\newpage

L'implementazione del Merge Sort, escludendo il sotto-algoritmo Fondi, risulta quindi essere estremamente facile poiché l'intero "lavoro sporco" viene svolto autonomamente dalla \textbf{ricorsione}:

\begin{verbatim}
def Merge_sort (A, ind_primo = 0, ind_ultimo = len(A)-1):
    if (ind_primo < ind_ultimo)
        ind_medio = (ind_primo+ind_ultimo)//2
        Merge_sort (A, ind_primo, ind_medio)
        Merge_sort (A, ind_medio + 1, ind_ultimo)
        Fondi (A, ind_primo, ind_medio, ind_ultimo)
\end{verbatim}

Poiché ancora non sappiamo il comportamento del sotto-algoritmo Fondi, esprimeremo il costo computazionale come:

$$
T(n) = \left \{ \begin{array}{l}
        T(n) = \Theta(1) + 2T \left ( \frac{n}{2} \right ) + S(n) \\
        T(1) = \Theta(1)
\end{array}
\right .
$$

dove $S(n)$ è il costo computazionale di Fondi.

\quad

\subsubsection{L'algoritmo Fondi}

Prima di vedere il funzionamento dell'algoritmo Fondi, è necessario sottolineare che, per via della \textbf{natura stessa del Merge Sort}, è possibile utilizzare un qualsiasi algoritmo in grado di restituire una sequenza ordinata partendo dalle due sotto-sequenze. Tuttavia, ovviamente l'algoritmo Fondi utilizzato nel Merge Sort risulta essere \textbf{estremamente ottimizzato} per questo scopo, risultando in un costo computazionale ridotto al minimo.

L'algoritmo Fondi sfrutta una condizione data per assunta, ossia il fatto che \textbf{entrambe le sotto-sequenze siano ordinate}. Ovviamente, ciò risulta sempre verificato, poiché sappiamo che l'algoritmo di Merge Sort viene richiamato ricorsivamente su entrambe le sotto-sequenze ordinandole.

\begin{itemize}
    \item Poiché le due sotto-sequenze sono ordinate, ne segue logicamente che il \textbf{valore minimo} tra il \textbf{primo elemento della prima sotto-sequenza} e il \textbf{primo elemento della seconda sotto-sequenza} sia il minimo complessivo della sequenza originale, venendo quindi spostato in quest'ultima.
    \item Una volta rimosso l'elemento scelto tra i due minimi, l'algoritmo verrà ripetuto, confrontando il \textbf{primo elemento della sotto-sequenza rimanente} dopo aver rimosso l'elemento precedentemente scelto e il \textbf{primo elemento dell'altra sotto-sequenza} da cui non è stato rimosso l'elemento
    \item Non appena una delle due sotto-sequenze ha \textbf{terminato i suoi elementi}, tutti gli elementi rimanenti nell'altra sotto-sequenza vengono direttamente \textbf{appesi uno ad uno in coda alla sequenza originale}, poiché essi sono già ordinati tra di loro
\end{itemize}

\newpage

Graficamente, l'algoritmo può essere interpretato in questo modo:

\begin{center}
    \includegraphics[scale=0.45]{resources/images/chapter_7/fuse_algorithm.png}
\end{center}

\quad

\quad

Lo pseudocodice dell'algoritmo Fondi sarà quindi:

\begin{verbatim}
def Fondi (A,ind_primo,ind_medio,ind_ultimo):
    i,j = indice_primo, indice_medio+1
    B=[]
    
    while ((i <= ind_medio) and (j <= ind_ultimo))
        if (A[i] <= A[j]):
            B.append(A[i])
            i += 1
        else:
            B.append(A[j])
            j += 1
    
    //eseguito solo se il primo sotto-vettore non è terminato
    while (i <= ind_medio) 
        B.append(A[i])
        i += 1
    
    //eseguito solo se il secondo sotto-vettore non è terminato
    while (j <= ind_ultimo)
        B.append(A[j])
        j += 1
        
    for i in range(len(B)):
        A[primo+i] = B[i]
\end{verbatim}

\newpage

Notiamo come il numero di iterazioni del primo ciclo possa variare da un \textbf{minimo di $\frac{n}{2}$} (ossia il caso in cui tutti i minimi si trovino in una sola delle due sotto-sequenze) ad un \textbf{massimo di $n$} (ossia il caso le due sotto-sequenze terminano gli elementi in contemporanea). \textbf{Il suo costo sarà quindi $\Theta(n)$.}

Quanto al secondo e al terzo ciclo, invece, è necessario sottolineare che, per via delle loro condizioni, è impossibile che entrambi i cicli vengano eseguiti, dunque essi sono considerabili come un ciclo unico (poiché svolgono le stesse identiche operazioni, ma utilizzando valori diversi).

Inoltre, poiché i due cicli si occupano di trasportare tutti gli elementi rimanenti nella sotto-sequenza non terminata, il numero di iterazioni varia da \textbf{un minimo di }$1$ (ossia quando solo un elemento è rimasto nella sotto-sequenza) ad \textbf{un massimo di }$\frac{n}{2}$ (coincidente con il caso migliore del primo ciclo). \textbf{Il suo costo sarà quindi $O(n)$.}

Infine, l'ultimo ciclo ha un \textbf{costo fisso di }$\Theta(n)$ facilmente intuibile a questo punto del corso.

Concludiamo quindi che il \textbf{costo computazionale dell'algoritmo Fondi} è:

\[ S(n) = \Theta(1) + \Theta(n) + O(n) + \Theta(n) = \Theta(n)\]

\quad

\subsubsection{Costo computazionale del Merge Sort}

Una volta conosciuto il costo computazionale del sotto-algoritmo Fondi, possiamo affermare che l'equazione di ricorrenza del Merge Sort sia:

$$
T(n) = \left \{ \begin{array}{l}
        T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n) \\
        T(1) = \Theta(1)
\end{array}
\right .
$$

Utilizzando il \textbf{metodo principale} (o un qualsiasi altro metodo) otteniamo che il \textbf{costo computazionale finale del Merge Sort} corrisponde a $\Theta(n \, \log(n))$, risultando quindi nel \textbf{costo più efficiente possibile} per un algoritmo di ordinamento basato su confronti (sezione \ref{min_cost_ord}).

\begin{itemize}
    \item $\alpha = 2$
    \item $\beta = 2$
    \item $f(n) = \Theta(n)$
    \item $n^{\log_{\beta}(\alpha)} = n^{\log_2(2)} = n$
    
    \item Siccome $f(n) = n^{\log_{\beta}(\alpha)}$, rientriamo nel \textbf{caso 2}. Dunque, ne segue che
    
    \[ T(n) = f(n) \cdot \log(n) = \Theta(n \, \log(n))\]
\end{itemize}

\newpage

\subsubsection{Osservazioni ed Ottimizzazioni del Merge Sort}

L'\textbf{operazione di fusione non può essere effettuata “in loco”} ( ossia aggiornando direttamente l'array su cui viene applicato senza dover utilizzare un ulteriore array di supporto) senza incorrere in un \textbf{peggioramento del costo computazionale}.

Difatti, all'interno dell'array originale bisognerebbe \textbf{fare spazio via via al minimo successivo}, ma questo costringerebbe a spostare di una posizione tutta la sotto-sequenza rimanente per ogni nuovo minimo, il che avrebbe un costo di $\Theta(n)$ per ciascun elemento da inserire, \textbf{aumentando così il costo dell'algoritmo Fondi} da $\Theta(n)$ a $\Theta(n^2)$.

A sua volta, ciò andrebbe a modificare l'equazione di ricorrenza del Merge Sort in

$$
T(n) = \left \{ \begin{array}{l}
        T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n^2) \\
        T(1) = \Theta(1)
\end{array}
\right .
$$

il cui costo computazionale finale corrisponde a $\Theta(n^2)$, un \textbf{netto peggioramento} rispetto al costo originale $\Theta(n \, \log(n))$.

Di conseguenza, quindi, per mantenere il costo computazionale del Merge Sort al minimo possibile, è necessaria un'\textbf{elevata quantità di memoria}, poiché ad ogni richiamo dell'algoritmo Fondi è necessario creare un nuovo array (e ricordiamo che si tratta di un algoritmo ricorsivo).

Tuttavia, nonostante il Merge Sort abbia un costo di $\Theta(n \,\log(n))$ mentre l'Insertion Sort abbia un costo di $O(n^2)$, è necessario puntualizzare che le costanti per i limiti asintotici sono tali che l'\textbf{Insertion Sort è più veloce del Merge Sort per valori piccoli di $n$}.

\quad

Dunque, possiamo ipotizzare che abbia senso \textbf{usare l'Insertion Sort all'interno del Merge Sort quando i sotto-problemi diventano sufficientemente piccoli}.

Ipotizziamo quindi il seguente algoritmo, dove $k$ è il \textbf{limite} che stabilisce se \textbf{continuare} la catena di ricorsione o \textbf{interromperla} richiamando l'Insertion Sort:

\begin{verbatim}
    def Merge_Insertion (A, k, primo, ultimo, dim):
        if dim>k:
            medio =(primo+ultimo)//2
            Merge_Insertion (A,k, primo, medio, medio-primo+1)
            Merge_Insertion (A,k,medio+1, ultimo, ultimo-primo)
            Fondi(primo, medio, ultimo)
        else:
            InsertionSort(primo, ultimo)
\end{verbatim}

L'equazione di ricorrenza di tale algoritmo, poiché il caso base coincide con il richiamo dell'Insertion Sort, sarà quindi:

$$
T(n) = \left \{ \begin{array}{l}
        T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n) \\
        T(k) = O(k^2)
\end{array}
\right .
$$

\newpage

Calcoliamo quindi il suo costo utilizzando il metodo iterativo:

\[ T(n) = 2T \left ( \frac{n}{2} \right ) + \Theta(n) = 2 \left [ 2T \left ( \frac{n}{4} \right ) + \Theta \left ( \frac{n}{2} \right ) \right ] + \Theta(n) = ...\]

\[ T(n) = 2^h \cdot T \left ( \frac{n}{2^h} \right )+ \sum_{i=0}^{h-1} 2^i \cdot \Theta \left ( \frac{n}{2^i} \right ) = 2^h \cdot T \left ( \frac{n}{2^h} \right )+ \sum_{i=0}^{h-1} \Theta(n)\]

Poiché il \textbf{caso base} viene raggiunto quando $\frac{n}{2^h} = k$, ne segue che $h = \log(\frac{n}{k}$. Dunque l'equazione diventerà:

\[ T(n) = 2^{\log(\frac{n}{k})} \cdot \Theta(k^2) + \sum_{i=0}^{\log(\frac{n}{k})-1} \Theta (n) = \frac{n}{k} \cdot \Theta(k^2) + \Theta \left ( n \, \log \left (\frac{n}{k} \right ) \right ) =\]
\[ = \Theta(n \, k) + \Theta(n \, \log(n)) - \Theta(n \, \log(k))\]

Se $k = O(\log(n))$ (dunque $k \leq c \cdot \log(n)$), otteniamo che

\[ T(n) = \Theta(n \, k) + \Theta(n \, \log(n)) - \Theta(n \, \log(k)) = \]
\[ = \Theta(n \, \log(n)) + \Theta(n \, \log(n)) - \Theta(n \, \log(\log(n))) = \Theta(n \, \log(n))\]

Concludiamo quindi che se per valori $n \leq c \cdot \log(n)$ viene utilizzato l'Insertion Sort internamente al Merge Sort, otteniamo un \textbf{costo computazionale invariato ma una riduzione notevole del costo in termini di memoria}, poiché l'Insertion Sort è in grado di lavorare in loco.

\newpage

\section{Quicksort}
L'algoritmo \textbf{Quicksort} riunisce i vantaggi del Selection sort (ossia l'\textbf{ordinamento in loco}) e del Merge sort (il\textbf{ ridotto tempo di esecuzione}). Tuttavia, nonostante il suo costo computazionale sia nel caso migliore sia nel caso medio sia pari a $\Theta(n \, \log(n))$, il suo \textbf{svantaggio} è l'elevato costo computazionale nel suo \textbf{caso peggiore} (pari $O(n^2)$).

Come il Merge Sort, anche il Quicksort è basato sulla tecnica del \textbf{divide et impera}:

\begin{itemize}
    \item \textbf{Divide}: nella sequenza di $n$ elementi viene selezionato un valore come \textbf{pivot} dell'algoritmo. Solitamente, il pivot scelto corrisponde a il primo, all'ultimo o al valore a metà dell'array su cui si applica l'algoritmo.
    
    \item La sequenza viene quindi divisa in due sotto-sequenze, la prima contenente tutti gli elementi minori del pivot e la seconda contenente tutti gli elementi maggiori o uguali al pivot
    
    \item \textbf{Impera}: le due sotto-sequenze vengono ordinate richiamando ricorsivamente l'algoritmo su di esse
    
    \item \textbf{Caso base}: la ricorsione termina quando la sotto-sequenza contiene un solo elemento, dunque è già ordinata
    
\end{itemize}

Notiamo come, a differenza del Merge Sort, le sotto-sequenze non vengono fuse tra di loro. Difatti, il vero e proprio ordinamento viene svolto nella \textbf{fase di continua divisione ordinata ricorsiva} effettuata dall'algoritmo, ossia il \textbf{Partizionamento}.

\begin{center}
    \includegraphics[scale=0.6]{resources/images/chapter_7/quicksort.png}
\end{center}

\quad

Lo pseudocodice del Quicksort corrisponde a:
\begin{verbatim}
def Quick_sort (A, ind_primo = 0, ind_ultimo = len(A)-1)
    if (ind_primo < ind_ultimo):
        ind_medio = Partiziona(A,ind_primo,ind_ultimo)
        Quick_sort (A,ind_primo,ind_medio)
        Quick_sort (A,ind_medio+1,ind_ultimo)
\end{verbatim}

Come per il Merge Sort, quindi, gran parte del "lavoro sporco" viene svolto da un sotto-algoritmo. A differenza dell'algoritmo Fondi, tuttavia, l'implementazione dell'\textbf{algoritmo Partiziona} non risulta particolarmente analoga alla sua interpretazione concettuale descritta precedentemente, per via della necessità stretta di dover svolgere le \textbf{operazioni in loco} e non su un array di appoggio:

\begin{verbatim}
def Partiziona(A, ind_primo, ind_ultimo):
    i, j = ind_primo - 1, ind_ultimo + 1
    pivot = A[ind_primo]
    
    while True:
    
        i += 1
        while A[i] < pivot:
            i += 1

        j -= 1
        while A[j] > pivot:
            j -= 1

        if i < j:
            A[i], A[j] = A[j], A[i]     #inverti A[i] e A[j]
        else:
            return j
        
\end{verbatim}

Nella pratica, una volta considerata la \textit{"magia della ricorsione"}, il comportamento di tale algoritmo corrisponde con il suo modello concettuale.

Per calcolare il \textbf{costo computazionale} di tale algoritmo, è necessario valutare il costo del \textbf{ciclo while esterno}, il quale è strettamente dipendete da ciò che avviene al suo interno: notiamo come ciascuno dei due cicli repeat-untill vada ad \textbf{avvicinare l'uno all'altro i due indici $i$ e $j$}, finché essi non si \textbf{incrociano}.

Ipotizzando che l'indice $i$ effettui $k$ scorrimenti, ne segue quindi che $j$ vada effettuare $n-k$ scorrimenti. La somma dei due, quindi, corrisponde ad \textbf{un totale di $n$ operazioni}, dunque ad un costo complessivo di $\Theta(n)$. Il ciclo while esterno, quindi, sarà eseguito un massimo di $n$ iterazioni, risultando in un costo complessivo dell'algoritmo pari a:

\[ P(n) = \Theta(1) + n \cdot \Theta(1) = \Theta(n)\]

Quanto al costo computazionale del Quicksort, invece, è necessario considerare che l'indice restituito dall'algoritmo Partiziona vada a \textbf{suddividere la sequenza originale in due sotto-sequenze}, dove la prima conterrà \textbf{$k$ elementi}, mentre la seconda conterrà \textbf{$n-k$ elementi}. L'equazione di ricorrenza sarà quindi:

$$
T(n) = \left \{ \begin{array}{l}
        T(n) = \Theta(1) + P(n) + T(k) + T(n-k) \\
        T(1) = \Theta(1)
\end{array}
\right .
$$

\newpage

Poiché non possiamo calcolare un costo strettamente asintotico di tale equazione tramite alcun metodo, possiamo ipotizzare il suo \textbf{caso peggiore}, \textbf{caso migliore} e \textbf{caso medio}:

\begin{itemize}
    \item \textbf{Caso migliore}: corrisponde al caso in cui ad ogni ricorsione ogni sotto-array venga diviso in esattamente due metà, ossia quando $k = n - k = \frac{n}{2}$:
    
    \[ T(n) = \Theta(1) + P(n) + T(k) + T(n-k) = \Theta(1) + P(n) + 2T \left (  \frac{n}{2} \right )\]
    
    Poiché sappiamo che $P(n) = \Theta(n)$, ne segue quindi che l'equazione di ricorrenza del caso migliore coincida con quella del Merge Sort, che sappiamo già essere $\Theta(n \, \log(n))$
    
    \[ T(n) = 2T \left (  \frac{n}{2} \right ) + \Theta(n) = \Theta(n \, \log(n))\]
    
    \item \textbf{Caso peggiore}: corrisponde al caso in cui ad ogni ricorsione ogni sotto-array venga diviso in un sotto-array contente 1 elemento ed uno contente $n-1$ elementi, ossia quando $k = 1$:
    
    \[ T(n) = \Theta(1) + P(n) + T(k) + T(n-k) = \Theta(1) + P(n) + T(1) + T(n-1) \]
    
    Sapendo che $T(1) = \Theta(1)$ e che $P(n) = \Theta(n)$, ne segue che l'equazione di ricorrenza del caso peggiore sia:
    
    \[ T(n) = T(n-1) + \Theta(n) = \Theta(n^2)\]
    
    il cui costo è facilmente calcolabile utilizzando il \textbf{metodo iterativo}.
    
    \item \textbf{Caso medio}: ipotizziamo che il valore del pivot suddivida con \textbf{uguale probabilità}, pari a $\frac{1}{n-1}$, la sequenza da ordinare in due sotto-sequenze di dimensioni $k$ ed $n-k$, per \textbf{tutti i valori di $k$ tra $1$ ed $n-1$}.
    
    Ne segue, quindi, che l'equazione di ricorrenza sia
    
    \[ T(n) = \frac{1}{n-1} \left [ \sum_{k=0}^{n-1} (T(k) - T(n-k)) \right ] + P(n)\]
    
    il cui \textbf{costo computazionale} è riconducibile, tramite il \textbf{metodo di sostituzione}, a $O(n \, \log(n))$ (per questioni di praticità verrà omessa tale dimostrazione, poiché contenente calcoli molto complessi, lunghi e futili da seguire).
    
    Dunque, poiché sappiamo che tale equazione è $O(n \, \log(n))$ e che il \textbf{teorema della complessità di un algoritmo di ordinamento basato sui confronti} impone che esso sia anche $\Omega(n \, \log(n))$, possiamo concludere che il caso medio sia $\Theta(n \, \log(n))$.
    
    \newpage
    
    \subsubsection{Osservazioni e Ottimizzazioni del Quicksort}
    
    Concludiamo quindi che il costo computazionale dell'\textbf{algoritmo di Quicksort} sia quasi sempre pari a $\Theta(n \, \log(n))$, risultando quindi l'\textbf{algoritmo ideale per input di grandi dimensioni}, poiché a differenza del Merge Sort esso svolga le operazioni in loco.
    
    A volte, però, l'\textbf{ipotesi di equiprobabilità non è soddisfatta} (ad esempio quando i valori in input sono \textbf{“poco disordinati”}), riducendo notevolmente le prestazioni dell'algoritmo. Difatti, il \textbf{caso peggiore} viene raggiunto proprio quando la \textbf{lista è già ordinata} e il \textbf{pivot} selezionato corrisponde al \textbf{primo elemento dell'array}, poiché ovviamente esso sarebbe minore di tutti i valori a suo seguito, andando ogni volta a dividere l'array in due sotto-array di lunghezza $1$ e $n-1$, risultando quindi in un costo pari a $\Theta(n^2)$.
    
    Per ovviare a tale inconveniente si possono adottare delle \textbf{tecniche volte a randomizzare la sequenza da ordinare}, in modo da eliminarne l'eventuale regolarità interna. Tali tecniche mirano a rendere l'algoritmo indipendente dall'input, consentendo di ricadere nel caso medio:
    
    \begin{itemize}
        \item Prima di avviare l'algoritmo d'ordinamento, \textbf{la sequenza viene randomizzata}, evitando che l'array iniziale sia parzialmente ordinato
        \item Durante il partizionamento \textbf{viene scelto un pivot casuale} all'interno della sequenza e non sistematicamente il valore più a sinistra, più a destra o nel mezzo
    \end{itemize}
    
    \quad
    
    \subsubsection{Materiale aggiuntivo}
    
    Poiché l'algoritmo di partizionamento può risultare inizialmente complesso, \textbf{è consigliata la visione di questi due video} contenenti una spiegazione passo passo dell'algoritmo:
    \begin{itemize}
        \item \href{https://www.youtube.com/watch?v=MZaf_9IZCrc}{Quicksort: Partitioning an array} (KC Ang)
        \item \href{https://www.youtube.com/watch?v=ZHVk2blR45Q}{Sorts 8 Quick Sort} (RobEdwards)
    \end{itemize}
\end{itemize}

\newpage

\section{Heap Sort}

L'algoritmo \textbf{Heap Sort}, a differenza del Merge Sort e del Quicksort, sfrutta una \textbf{opportuna organizzazione dei dati}, ossia una \textbf{struttura dati}, che garantisce una o più specifiche proprietà, il cui mantenimento è essenziale per il corretto funzionamento dell'algoritmo.

\subsection{La struttura dati Heap}
\label{heap}

Tale struttura dati viene detta \textbf{Heap}, ossia un \textbf{albero binario} completo o quasi completo, in cui tutti i livelli, eccetto l'ultimo, sono pieni e in cui i nodi sono addensati a sinistra. La proprietà fondamentale che mette in relazione tali nodi è l'\textbf{ordinamento verticale}: la \textbf{chiave di ogni nodo} corrisponde ad un \textbf{valore maggiore o uguale} alle \textbf{chiavi dei due figli} di ogni nodo.

\begin{center}
    \includegraphics[scale=0.65]{resources/images/chapter_7/heap.png}
\end{center}

Nonostante la sua struttura ad albero, un Heap viene comunque \textbf{implementato tramite un array}, i cui indici vanno da 0 fino al numero di nodi dell'Heap, ossia l'\textbf{Heap Size}, decrementato di 1. Tuttavia, tale array, affinché possa essere considerato una \textbf{traslazione dell'Heap dal modello teorico al modello pratico}, dovrà rispettare alcune caratteristiche:

\begin{itemize}
    \item L'array è \textbf{riempito a partire da sinistra} e, se contiene più elementi del valore indicato dall'Heap Size, allora gli elementi di indice $\geq heap\_size$ non fanno parte dell'Heap
    \item Ogni nodo dell'albero binario corrisponde a uno e un solo elemento dell'array
    \item La \textbf{radice dell'albero} corrisponde ad $A[0]$ e, poiché ogni chiave padre deve essere maggiore o uguale delle chiavi figlie, corrisponde al \textbf{valore massimo dell'Heap} e, di conseguenza, può essere trovato in tempo $\Theta(1)$
    \item Considerato il \textbf{nodo generico} $A[i]$, il suo \textbf{figlio sinistro}, se esiste, corrisponde all'\textbf{elemento }$A[2i+1]$, mentre il suo \textbf{figlio destro} corrisponde all'\textbf{elemento} $A[2i+2]$
    \item Di conseguenza, considerato il n\textbf{odo generico }$A[i]$, il suo \textbf{padre} sarà l'\textbf{elemento} $A \lfloor (i-1)/2 \rfloor$
    \item Poiché \textbf{ogni livello dell'Heap contiene $2^h$ nodi}, ne segue che l'\textbf{altezza} sia $h = \log(n)$, dove $n$ è il numero di nodi
\end{itemize}

\newpage

Paragonando il modello teorico al modello pratico dell'Heap, quindi, otteniamo che:

\begin{center}
    \includegraphics[scale=0.7]{resources/images/chapter_7/heap2.png}
\end{center}

Per poter utilizzare l'\textbf{algoritmo di Heap Sort} su un vettore, quindi, è prima \textbf{necessario modificare tale vettore} in modo che vada a rispettare le \textbf{proprietà della struttura dati Heap}. In particolare, ciò viene realizzato utilizzando due funzioni ausiliarie:

\begin{itemize}
    \item La funzione \textbf{Heapify}
    \item La funzione \textbf{Buildheap}
\end{itemize}

\quad

\subsubsection{La funzione Heapify}

La \textbf{funzione Heapify} ha lo scopo di \textbf{mantenere la proprietà di Heap}, dando per assunta l'\textbf{ipotesi} che nell'albero su cui viene fatta lavorare \textbf{sia garantita la proprietà di heap per entrambi i sotto-alberi} (sinistro e destro) della radice.

Di conseguenza l'unico nodo che può \textbf{violare la proprietà di Heap} è la radice dell'albero, che può essere minore di uno o di entrambi i figli.

La funzione, quindi, \textbf{opera sulla radice confrontandola coi suoi figli} e, se necessario, la \textbf{scambia col maggiore} di suoi figli. Dopo lo scambio, viene verificato se la violazione si sia \textbf{\textit{"trasferita"} sul figlio scambiato} e, se necessario, si ripete \textbf{ricorsivamente} l'operazione su tale nodo.

\begin{center}
    \includegraphics[scale=0.6]{resources/images/chapter_7/heapify.png}
\end{center}

L'implementazione in pseudocodice della \textbf{funzione Heapify} corrisponde a:

\begin{verbatim}
def Heapify (A, i, heap_size)
    L = A[2i+1]
    R = A[2i+2]
    indice_max = i
    
    if (L < heap_size) and (A[L] > A[i]):
        indice_max=L
    if (R <= heap_size) and (A[R] > A[indice_max]):
        indice_max=R
        
    //se viene violata la proprietà
    if (indice_max != i)
        A[i], A[indice_max] = A[indice_max], A[i]
        Heapify (A, indice_max, heap_size)
\end{verbatim}

Notiamo come tutte le operazioni svolte all'interno della funzione corrispondano ad un $\Theta(1)$, fatta eccezione per la chiamata ricorsiva. Dunque, l'equazione di ricorrenza della funzione sarà

$$
T(n) = \left \{ \begin{array}{l}
        T(n) = \Theta(1) + T(n') \\
        T(1) = \Theta(1)
\end{array}
\right .
$$

dove $n'$ è il \textbf{numero di nodi del sotto-albero più grande} tra figlio destro e figlio sinistro. Poiché ogni livello contiene $2^h$ nodi, tale numero \textbf{non può essere più grande di $\frac{2}{3} n$}, situazione che accade quando l'ultimo livello è pieno esattamente a metà:

\begin{center}
    \includegraphics[scale=0.7]{resources/images/chapter_7/heapify2.png}
\end{center}

Dunque, l'equazione di ricorrenza diventa:

\[ T(n) = T\left ( \frac{2}{3} n \right ) + \Theta(1)\]

\begin{itemize}
    \item $\alpha = 1$, $\beta = \frac{3}{2}$
    \item $f(n) = \Theta(1)$
    \item $n^{\log_{\beta}(\alpha)} = n^{\log_{\frac{3}{2}}(1)} = n^0 = 1$
    \item Siamo nel \textbf{caso 2}, dunque $T(n) = f(n) \cdot \log(n) = \Theta(\log(n))$
\end{itemize}

\subsubsection{La funzione Buildheap}

La \textbf{funzione Buildheap} si occupa di \textbf{trasformare qualunque vettore} contenente $n$ elementi \textbf{in un Heap}, chiamando ripetutamente la \textbf{funzione Heapify} sugli opportuni nodi dello Heap.

Tuttavia, poiché la funzione Heapify dà per assunto che entrambi i sotto-alberi della radice siano a loro volta degli Heap, essa deve essere \textbf{richiamata} scorrendo l'albero per livelli \textbf{dal basso verso l'alto}.

Inoltre, poiché \textbf{ogni foglia è già un Heap} (dato che non ha figli), possiamo \textbf{ridurre il numero di chiamate a }$\frac{n}{2}$, poiché, per via delle proprietà dell'Heap, la seconda metà dell'array sarà sempre occupata solo e soltanto dalle foglie.

\begin{center}
    \includegraphics[scale=0.7]{resources/images/chapter_7/buildheap.png}
\end{center}

\newpage

Lo pseudocodice della funzione Buildheap corrisponde a

\begin{verbatim}
def Build_heap (A):
    for i in revered(range(len(A)//2)):
        Heapify (A, i,heap_size)
\end{verbatim}

il cui \textbf{costo computazionale} è pari a
\[ T(n) = \frac{n}{2} \cdot \Theta(\log(n)) = \Theta(n \, \log(n)) \]

Con un \textbf{calcolo più accurato del costo}, considerando le varie proprietà dell'Heap, è possibile ricondurre il \textbf{costo computazionale a }$\Theta(n)$. Tuttavia, tale dimostrazione verrà omessa, poiché, come vedremo in seguito, la differenza tra il costo immediato $\Theta(n \, \log(n))$ e il costo accurato $\Theta(n)$ \textbf{non influisce sul costo finale dell'algoritmo di Heap Sort}.

\quad

\subsubsection{L'algoritmo di Heap Sort}

Una volta comprese le \textbf{proprietà} della struttura dati Heap ed aver definito due \textbf{funzioni ausiliarie} che ci permettono di manipolarlo, andiamo a vedere il funzionamento dell'\textbf{algoritmo di ordinamento Heap Sort}:

\begin{itemize}
    \item Viene richiamata la \textbf{funzione Buildheap} sul vettore dato in input
    \item Poiché il \textbf{massimo dell'Heap è in posizione }$A[0]$, per metterlo nella corretta posizione è necessario \textbf{scambiarlo con }$A[n-1]$, ossia l'ultimo elemento
    \item Di seguito, la \textbf{dimensione dell'Heap viene ridotta ad }$n-1$, poiché possiamo \textbf{ignorare l'ultimo elemento} dato che esso si trova già nella posizione finale. 
    \item Poiché i due sotto-alberi della radice sono ancora degli Heap, viene richiamata la \textbf{funzione Heapify sulla radice}, che ricordiamo non essere più il massimo dell'Heap per via dello scambio, in modo da ripristinare a pieno le \textbf{proprietà dell'Heap} di dimensione ridotta
    \item Successivamente, il procedimento verrà \textbf{riapplicato sul nuovo massimo dell'Heap}, andando man mano a ridurre l'Heap Size fino al minimo. 
\end{itemize}

\begin{center}
    \includegraphics[scale=0.45]{resources/images/chapter_7/heapsort.png}
\end{center}

Lo pseudocodice dell'Heap Sort corrisponde a:
\begin{verbatim}
def Heapsort (A):
    Build_heap(A)
    for x in reversed(range(1,len(A))):
        A[0],A[x]=A[x],A[0]
        Heapify(A, 0, x)
\end{verbatim}

Notiamo come il ciclo for venga eseguito un totale di $n-1$ volte. Dunque, per via del richiamo della f\textbf{unzione Heapify} al suo interno (che sappiamo essere $\Theta(\log(n))$), il suo costo sarà pari a $\Theta(n \, \log(n))$.

A questo punto, il costo della \textbf{funzione Buildheap}, che ricordiamo essere $\Theta(n))$ con un calcolo accurato, non andrà ad influenzare il \textbf{costo finale dell'algoritmo di Heap Sort}, che sarà pari a:
\[ T(n) = \Theta(n) + (n-1) \cdot \Theta(\log(n)) = \Theta(n \, \log(n))\]

\newpage

\section{Ordinamenti lineari}

\subsection{Counting sort}
Abbiamo già visto il teorema secondo cui \textbf{ogni algoritmo di ordinamento che opera per confronti ha un costo computazionale di $\Omega(n \log(n))$} (sezione \ref{min_cost_ord}). Vediamo ora quindi un algoritmo \underline{non} basato sui confronti e che, di conseguenza, "sfugge" al limite inferiore imposto da tale teorema, riuscendo ad arrivare ad un \textbf{costo computazionale lineare $\Theta(n)$}.

Ipotizziamo di avere un array di numeri interi da ordinare. Ciascuno degli $n$ elementi da ordinare è \textbf{un intero di valore compreso in un intervallo} $[0..k]$, dove $k$ corrisponde al \textbf{valore massimo dell'array}.

A questo punto, possiamo utilizzare \textbf{un secondo array di supporto}, i cui valori corrispondono esattamente al \textbf{numero di occorrenze dell'indice stesso}.

Infine, verrà usato il numero di occorrenze contate per riscrivere nell'array iniziale i valori \textbf{in ordine di indice}.


\begin{itemize}
    \item Viene trovato il \textbf{valore massimo }$k$ dell'array $A$
    \item Viene creato un\textbf{ secondo array }$C$ di larghezza $k+1$
    \item Viene \textbf{conteggiato} ogni elemento di $A$, \textbf{incrementando di uno il corrispettivo indice nell'array }$C$. Esempio: se $A[i] = 6$, allora $C[6]$ viene incrementato di uno.
    \item Viene sovrascritto l'array $A$ scorrendo $C$, ricopiando in $A$ \textbf{ciascun indice di $C$ tante volte quanto è il valore in $C$ di quell'indice}.
\end{itemize}

\begin{center}
    \includegraphics[scale=0.5]{resources/images/chapter_7/counting_sort.png}
\end{center}

\newpage

Lo pseudocodice di tale algoritmo corrisponde a:

\texttt{def counting\_sort (A):}

\texttt{\qquad k = max(A) \qquad\qquad\qquad\qquad\, \#$\Theta(n)$}

\texttt{\qquad n = len(A)}

\texttt{\qquad C = [0]*(k+1) \qquad\qquad\qquad\quad \#$\Theta(k)$}
\begin{verbatim}
for j in range(n):          # n volte
    C[A[j]] +=1
    #C[i] ora contiene il numero di elementi uguali a i

j = 0
for i in range(k+1):          # k volte
    while (C[i] > 0)        # C[i] volte
        A[j]=i
        j+=1
        C[i]–=1
\end{verbatim}

A questo punto, è necessario puntualizzare che:

\begin{itemize}
    \item Il costo computazionale della funzione $max()$ è $\Theta(n)$ poiché scorriamo l'array $A$
    \item La \textbf{somma di tutti i valori contenuti in } $C$, una volta finito il conteggio, corrisponde a $n$
    \[ \sum_{i=0}^{k} C[i] = n\]
\end{itemize}

A questo punto, siamo in grado di scrivere l'equazione descrivente il \textbf{costo dell'algoritmo}:

\[ T(n) = \Theta(n) + \Theta(n) + \Theta(1) \cdot \sum_{i=0}^{k} C[i] = \Theta(n) + \Theta(k) = \Theta(n+k)\]

Notiamo quindi che il costo computazionale si divide in \textbf{due casi}:

\begin{itemize}
    \item Se $k \leq n$, allora $T(n) = \Theta(n)$
    \item Se $k > n$, allora $T(n) = \Theta(k)$
\end{itemize}

Questa \textbf{versione del Counting Sort}, tuttavia, è \textbf{adeguata solamente se non vi sono dati satellite}. Infatti, il ciclo che ricopia i valori contati in C nel vettore A di fatto ne \textbf{sovrascrive i dati}.

\newpage

\subsection{Counting Sort con Dati Satellite}

Se ci dovessero essere \textbf{dati satellite}, oltre ai vettori $A$ e $C$ si deve introdurre un \textbf{nuovo vettore $B$ di $n$ elementi}, che alla fine conterrà la sequenza ordinata:

\begin{itemize}
    \item Dopo aver conteggiato gli elementi in $C$, si fa una \textbf{seconda passata su $C$}, da sinistra a destra, partendo da $C[2]$, nella quale a \textbf{ogni elemento $C[i]$ si somma il precedente}
    
    \item Alla fine di tale fase si ha che:
    
    \begin{itemize}
        \item $C[i]$ indica la \textbf{posizione corretta}, nel vettore ordinato, per l'elemento di valore pari a i più a destra fra quelli contenuti nel vettore $A$
        \item $C[i] - C[i-1]$ indica \textbf{quanti elementi ci sono con valore pari a $C[i]$}
    \end{itemize}
    
    \item Si scorre quindi il vettore $A$ da destra a sinistra e:
    \begin{itemize}
        \item Si \textbf{copia in $B$ ogni elemento $A[j]=k$} nella posizione giusta che è $C[k]$
        \item Si \textbf{decrementa} di 1 il valore $C[k]$.
    \end{itemize}
\end{itemize}

\quad

Lo pseudocodice di questa \textbf{seconda versione} corrisponde a:

\begin{verbatim}
    def counting_sort_2 (A):
        k = max(A)
        n = len(A)
        C = [0]*(k+1)
        B = [0]*n
        
        for j in range(n)
            C[A[j]]+=1
            #in C[i] ora c'è il num. di elem. = i
            
        for i in range(1,k)
            C[i]+=C[i-1]
            #in C[i] ora c'è il num. di elem. <= i
            
        for j in range(n,-1, -1)
            B[C[A[j]]-1]=A[j]
            C[A[j]]-=1
        return B
\end{verbatim}

Nonostante le modifiche rispetto alla versione precedente, il \textbf{costo computazionale rimane invariato}:

\[ T(n) = \Theta(n) + \Theta(k) + \Theta(n) + \Theta(n) + \Theta(k) +\Theta(n) = \Theta(n) + \Theta(k) = \Theta(n+k)\]

\newpage

\chapter{Strutture Dati}

In un linguaggio di programmazione, un \textbf{dato} è un valore che una variabile può assumere, mentre il \textbf{tipo di un dato} rappresenta una collezione di valori e un insieme di operazioni ammesse su questi valori (intero, carattere, stringa, ...)

Le \textbf{strutture dati}, invece, sono particolari tipi di dato,  caratterizzate dall'\textbf{organizzazione dei dati}, più che da tipo di dato stesso. (lista, pila, coda, albero, ...)

Le strutture dati, quindi, sono:

\begin{itemize}
    \item Un \textbf{modo sistematico di organizzare i dati}
    \item Un \textbf{insieme di operatori} che permettono di manipolare la struttura
    \item In grado di \textbf{memorizzare e manipolare insiemi dinamici} (composti solitamente da una \textbf{chiave}, ossia un valore in grado di identificare l'insieme in modo univoco, ed da \textbf{dati satelliti}, ossia qualsiasi altro dato legato all'insieme stesso), i cui elementi possono variare nel tempo.
    \item \textbf{Lineari o non lineari} (a seconda che esista o no una sequenzializzazione dei valori)
    \item \textbf{Statiche o dinamiche} (a seconda che possano variare la dimensione nel tempo)
    \item \textbf{Omogenee o disomogenee} (rispetto ai tipi di dati contenuti)
\end{itemize}

Le \textbf{operazioni} che vengono svolte su ogni struttura dati possono essere racchiuse in \textbf{due categorie}:

\begin{itemize}
    \item \textbf{Operazioni di interrogazione}:
    
    \begin{itemize}
        \item \textbf{Search(S, k)}: recuperare l'elemento con chiave di valore k, se è presente in S, restituire un valore speciale nullo altrimenti
        \item \textbf{Min(S)/Max(S)}: recuperare il minimo/massimo valore presente in S
        \item \textbf{Predecessor(S, k)/Successor(S, k)}: recuperare l'elemento presente in S che precederebbe/seguirebbe quello di valore k se S fosse ordinato
    \end{itemize}
    
    \item \textbf{Operazioni di manipolazione}
    
    \begin{itemize}
        \item \textbf{Insert(S, k)}: inserire un elemento di valore k in S
        \item \textbf{Delete(S, k)}: eliminare da S l'elemento di valore k
    \end{itemize}
\end{itemize}

Le differenti strutture dati che vedremo, se da un lato hanno in comune la \textbf{capacità di memorizzare insiemi dinamici}, dall'altro differiscono anche profondamente fra loro per le \textbf{proprietà che le caratterizzano}.

Sono proprio le proprietà della struttura dati ad essere
l'\textbf{elemento determinante nella scelta della struttura} da effettuare quando si deve progettare un algoritmo per risolvere un
problema.

Un esempio lo abbiamo già incontrato: la struttura dati
Heap (sezione \ref{heap}) , senza la quale l'algoritmo Heapsort non potrebbe
nemmeno essere pensato.

\quad

\section{Array Ordinati e Disordinati}

Abbiamo già visto numerose volte nei capitoli precedenti l'uso degli array come \textbf{lista statica di elementi}, sulla quale abbiamo svolto operazioni di ordinamento e di ricerca. Tuttavia, le varie operazioni di interrogazione e di manipolazione assumono un costo ben diverso a seconda dell'ordinamento dell'array stesso:

\begin{itemize}
    \item \textbf{Array Disordinato}:
    \begin{itemize}
        \item \textbf{Search}: è necessario scorrere l'array per trovare l'elemento, dunque un costo pari a $\Theta(n)$
        \item \textbf{Min/Max}: è necessario scorrere l'array per trovare l'elemento, dunque un costo pari a $\Theta(n)$
        \item \textbf{Predecessor/Successor}: è necessario scorrere l'array per trovare l'elemento, dunque un costo pari a $\Theta(n)$
        \item \textbf{Insert}: inserimento nella prima posizione libera, dunque un costo pari a $\Theta(1)$
        \item \textbf{Delete}: eliminazione e scambio con l'ultimo, dunque un costo pari a $\Theta(1)$
    \end{itemize}
    
    \item \textbf{Array Ordinato}:
    \begin{itemize}
        \item \textbf{Search}: essendo ordinato, possiamo usare la ricerca binaria, dunque un costo pari a $O(\log(n))$
        \item \textbf{Min/Max}: si prende il primo o ultimo elemento, dunque un costo pari a $\Theta(1)$
        \item \textbf{Predecessor/Successor}: si prende l'elemento precedente o seguente, dunque un costo pari a $\Theta(1)$
        \item \textbf{Insert}: è necessario ricercare la posizione in cui effettuare l'inserimento, per poi scorrere a destra gli elementi maggiori, dunque un costo pari a $\Theta(n)$
        \item \textbf{Delete}: è necessario ricercare la posizione in cui effettuare l'inserimento, per poi scorrere a sinistra gli elementi maggiori, dunque un costo pari a $\Theta(n)$
    \end{itemize}
\end{itemize}

Già da questo semplice confronto delle \textbf{due strutture dati più semplici in assoluto}, si può dedurre come \textbf{non abbia senso dire che una struttura è migliore di un'altra}, poiché le strutture dati possono essere più o meno adatte ad un certo algoritmo a seconda delle necessità dell'algoritmo stesso.

\section{Liste puntate}

Un \textbf{puntatore} è una variabile che, a differenza delle normali variabili, assume come valore un \textbf{indirizzo di memoria}, e non un valore in se e per se. Una normale variabile fa quindi riferimento ad un valore \textbf{direttamente}, mentre un puntatore lo fa \textbf{indirettamente}, puntando all'indirizzo in memoria che effettivamente contiene tale valore.

\begin{center}
    \includegraphics[scale=0.65]{resources/images/chapter_8/pointers.png}
\end{center}

\quad

Una \textbf{lista puntata} è una struttura dati nella quale gli elementi sono organizzati in \textbf{successione} e dove ogni elemento è un \textbf{record a due campi}:

\begin{itemize}
    \item \textbf{Un campo Key}: contenente l'\textbf{informazione vera e propria} dell'elemento. Viene rappresentato con la notazione \textbf{p $\to$ key}.
    \item \textbf{Un campo Next}: contiene un \textbf{puntatore all'elemento successivo della lista}. Nel caso dell'ultimo elemento della lista, il puntatore sarà un \textbf{valore null} (rappresentato col simbolo \textbackslash). Viene rappresentato con la notazione \textbf{p $\to$ next}.
\end{itemize}

\begin{center}
    \includegraphics[scale=0.65]{resources/images/chapter_8/linked_list.png}
\end{center}

Gli \textbf{array}, ordinati e non, godono di un \textbf{accesso diretto}, dove per accedere ad un dato basta conoscerne la posizione nell'array, risultando quindi in un costo di $\Theta(1)$.

Nelle \textbf{liste puntate}, invece, la successione degli elementi viene implementata mediante un collegamento esplicito da un elemento ad un altro (ossia il puntatore), rendendo quindi possibile solo l'\textbf{accesso sequenziale}. Dunque, l'accesso a qualsiasi dato in una lista ha un costo proporzionale alla sua posizione, che nel caso peggiore è $\Theta(n)$.

Poiché l'unico accesso possibile è quello sequenziale, è intuitivo pensare che la \textbf{maggior parte delle operazioni svolte su una lista puntata abbia un costo} $O(n)$:

\begin{itemize}
    \item \textbf{Search}: nel caso peggiore, l'elemento cercato si trova in fondo alla lista, dunque con un costo di $O(n)$
    
    \begin{verbatim}
def Search (p: punt. alla testa; k: valore):
p_corr = p
while ((p_corr != NULL) and (p_corr -> key != k))
    p_corr = p_corr -> next
return p_corr
    \end{verbatim}
    
    \item \textbf{Insert in testa}: poiché si tratta di un inserimento in una posizione qualsiasi, l'elemento viene aggiunto direttamente in testa alla lista, dunque con un costo di $\Theta(1)$
    
    \begin{verbatim}
def Insert_in_testa (p: punt. alla testa; k: punt. a elem. da ins.):
if (k != None)
    k->next = p
p = k
return p
    \end{verbatim}
    
    \item \textbf{Insert dopo un elemento}: inserimento nella posizione successiva ad da un puntatore già presente nella lista. Nel caso peggiore, il puntatore dopo cui inserire l'elemento si trova in ultima posizione, dunque con un costo di $O(n)$
    
    \newpage

    \begin{verbatim}
def Insert_Dopo_d(p: punt. alla testa; k: punt. a elem. da ins.;
                d: punt. dopo cui ins.):
    
if d != None:
    k->next = d->next;
    d->next = k;
    return p
else:
    return None
    \end{verbatim}
    
    \item \textbf{Delete}: per cancellare un elemento, è prima necessario trovarlo nella lista, dunque il costo sarà lo stesso del Search, ossia $O(n)$.
    
    \begin{verbatim}
def Delete (p: punt. alla testa; k: punt. a elem. da canc.):
if (k != None):         #se k=null non c'è niente da cancellare
    if k = p:           #cancella 1° elem
        p = p->next
        return p
    p_corr = p
    while (p_corr-> next != k)
        p_corr = p_corr-> next      #p_corr punta a elem. che prec. k
    p_corr-> next = k-> next
return p
    \end{verbatim}
\end{itemize}

Siccome le \textbf{liste puntate sono strutture dati inerentemente ricorsive}, tutti gli algoritmi proposti possono essere anche in \textbf{versione ricorsiva}.

Ad esempio, la cancellazione può essere implementata come:

\begin{verbatim}
def Delete_Ric(p: punt. alla testa; k: punt. a elem. da canc.)
    if p==k:
        p = p->next
    else:
        p->next = Delete_Ric(p->next, k)
    return p
\end{verbatim}

\quad

\subsubsection{Liste doppiamente puntate}

Alcuni problemi riscontrati nelle \textbf{liste puntate semplici }(ad esempio complessità lineare nella cancellazione) possono essere risolti organizzando la struttura dati in modo che da ogni suo elemento si possa accedere \textbf{sia all'elemento che lo segue che a quello che lo precede nella lista}, quando essi esistono.

Tale struttura dati si chiama \textbf{lista doppia o lista doppiamente puntata}, dove il puntatore all'elemento precedente viene rappresentato con la notazione \textbf{p -> prev}.

\begin{center}
    \includegraphics[scale=0.65]{resources/images/chapter_8/linked_list_2.png}
\end{center}

\section{Pile e Code}

Una \textbf{pila} è una struttura dati basata sul comportamento \textbf{LIFO (The last in is the first out)}, ossia l'ultimo entrato è il primo ad uscire.

In altre parole, in una pila gli elementi vengono prelevati nell'\textbf{ordine inverso rispetto a quello col quale vi sono stati inseriti}, proprio come accadrebbe in una pila di piatti o di sedie: per prendere un elemento in fondo alla pila dobbiamo prima rimuovere tutti quelli al di sopra di esso.

Un esempio di utilizzo di questa struttura dati è la \textbf{pila di sistema} (ossia lo \textbf{stack di memoria}), con la quale vengono gestite le chiamate a funzione (ricorsive e non).

\begin{tabular}{m{0.5 \textwidth} p{0.2 \textwidth} p{0.3 \textwidth}}
Per sua natura stessa, su una pila possono essere eseguite \textbf{solo due operazioni}, entrambe svolte sul \textbf{top} della pila, ossia la sua cima:

    \begin{itemize}
        \item \textbf{Push}, ossia l'inserimento in cima di un nuovo elemento, pertanto il suo costo sarà $\Theta(1)$.
        
        \begin{verbatim}
def Push(top: punt.; e: punt. a e. da ins.):
e->next = top
top = e
return top
        \end{verbatim}
        
        \item \textbf{Pop}, ossia la rimozione dalla cima dell'ultimo elemento inserito, pertanto il suo costo sarà $\Theta(1)$.
        
        \begin{verbatim}
def Pop (top: punt):
if (top==None):
    return None
e = top
top = e->next
e->next=None
return e, top
        \end{verbatim}
    \end{itemize}
    
    &&
    
    \begin{tabular}{c}
        \includegraphics[scale=0.7]{resources/images/chapter_8/stack.png}
    \end{tabular}
\end{tabular}

\quad

Diversamente dalla pila, una \textbf{coda} una struttura dati basata sul comportamento \textbf{FIFO (The first in is the first out)}, ossia il primo entrato è il primo ad uscire.

In altre parole, in una coda gli elementi vengono prelevati nell'\textbf{ordine d'inserimento}, proprio come accadrebbe in una coda di persone in attesa ad uno sportello.

Come per le pile, anche sulle code possono essere eseguite solo due operazioni:
\begin{itemize}
    \item \textbf{Equeue}, ossia l'inserimento in coda di un nuovo elemento (\textbf{tail})
    
    \begin{verbatim}
def Enqueue (head: punt.; tail: punt.; e: punt. a elem. da ins.):
if (tail == NULL):  #la coda è vuota
    tail = e
    head = e
else:
    tail->next = e
    tail = e
return head, tail
    \end{verbatim}
        
    \item \textbf{Dequeue}, ossia l'estrazione dell'elemento in testa (\textbf{head})
    
    \begin{verbatim}
def Dequeue (head: punt.; tail: punt.):
if (head == None):  #la coda è vuota
    return none
e = head
head = e->next
if (head == None):
    tail = None
return head, tail, e
    \end{verbatim}
\end{itemize}

\begin{center}
    \includegraphics[scale=0.6]{resources/images/chapter_8/queue.png}
\end{center}

\quad

La \textbf{coda con priorità} è una variante della coda.
Come nella coda, l'inserimento avviene ad un'estremità e l'estrazione avviene all'estremità opposta.

A differenza della coda, la \textbf{posizione di ciascun elemento} non dipende dal momento in cui è stato inserito, ma \textbf{dipende dal valore di una determinata grandezza, detta priorità}, la quale in generale è associata ad uno dei campi presenti nell'elemento stesso.

Quindi, gli elementi di una coda con priorità sono collocati in \textbf{ordine crescente} (o decrescente, a seconda dei casi) \textbf{rispetto alla grandezza considerata come priorità}.

\newpage

\section{Alberi}

Un \textbf{albero} è una struttura dati estremamente \textbf{versatile}, utile per modellare una grande quantità di situazioni reali e progettare le relative soluzioni algoritmiche.

Per dare la definizione formale di albero è necessario prima fornire alcune definizioni relative ad un'altra struttura dati, il \textbf{grafo}:

Un \textbf{grafo} $G = (V, E)$ è costituito da una \textbf{coppia di insiemi}:
\begin{itemize}
    \item Un insieme finito $V$ di \textbf{nodi} o \textbf{vertici}
    \item Un insieme finito $E \subseteq V \times V$ di coppie non ordinate di nodi, dette \textbf{archi} o \textbf{spigoli}
    \item Un \textbf{cammino} è una sequenza $(v_1, v_2, ..., v_k)$ di nodi distinti di $V$ tale che $(v_i, v_{i+1})$ sia un arco di $E$ per ogni $1 \leq i \leq k-1$
    \item Se nel cammino $(v_1, v_2, ..., v_k)$ i nodi $v_k$ e $v_1$ coincidono, si parla di \textbf{ciclo}.
    \item Si dice che un grafo è \textbf{connesso} se, per ogni coppia di nodi $(u, v)$, esiste un cammino tra $u$ e $v$.
\end{itemize}

Una volta definita la struttura dati grafo, possiamo definire il suo diretto discendente, ossia la \textbf{struttura albero}, corrispondente ad un \textbf{grafo aciclico e connesso}.

\begin{center}
    \begin{tabular}{c c c}
    \includegraphics[scale=0.55]{resources/images/chapter_8/graph.png}
    & \qquad \qquad &
    \includegraphics[scale=0.55]{resources/images/chapter_8/tree_1.png}
    \\
    Grafo \textbf{ciclico} e \textbf{connesso} && Grafo \textbf{aciclico} e \textbf{connesso} (\textbf{Albero})
    \end{tabular}
\end{center}

\quad

\subsubsection{Alberi radicati}

\begin{tabular}{m{0.4625 \textwidth} p{0.0125 \textwidth} p{0.4 \textwidth}}
    Un \textbf{albero radicato} è un albero in cui si distingue un nodo particolare tra gli altri, detto \textbf{radice}. L'albero radicato si può rappresentare in modo tale che i cammini da ogni nodo alla radice seguano un percorso dal \textbf{basso verso l'alto}, come se l'albero venisse, in qualche modo, "appeso" per la radice.
    I \textbf{nodi} sono organizzati in \textbf{livelli} e l'\textbf{altezza} di un albero radicato è la lunghezza del cammino più lungo dalla radice ad una foglia.
    
    & &
    \begin{tabular}{c}
        \includegraphics[scale=0.6]{resources/images/chapter_8/tree_2.png}
    \end{tabular}
\end{tabular}

In un albero radicato distinguiamo i vari \textbf{nodi} con varie terminologie:

\begin{itemize}
    \item Dato un qualunque nodo $V$ di un albero radicato che non sia la radice, il \textbf{primo nodo che si incontra sul cammino da $V$ alla radice} viene detto \textbf{padre di $V$}
    
    \item I nodi che hanno lo \textbf{stesso padre} sono detti \textbf{fratelli}, mentre tutti i nodi che ammettono $V$ come padre sono detti \textbf{figli di $V$}. Se un nodo non ha figli, esso viene detto \textbf{foglia}
    
    \item Ogni nodo sul cammino da $V$ alla radice viene detto \textbf{antenato di $V$}, mentre tutti i nodi che ammettono $V$ come antenato vengono detti \textbf{discendenti di $V$}.
\end{itemize}

\begin{center}
    \includegraphics[scale=0.65]{resources/images/chapter_8/tree_3.png}
\end{center}

Inoltre, un albero radicato viene detto \textbf{ordinato}, se viene attribuito \textbf{un qualche ordine ai figli di ciascun nodo}: se un nodo ha $k$ figli, allora vi è un figlio che viene considerato primo, uno che viene considerato secondo, ..., uno che viene considerato $k$-esimo.

\quad

\subsection{Alberi binari}

Una particolare \textbf{sottoclasse di alberi radicati e ordinati} è quella degli \textbf{alberi binari}, dove o\textbf{gni nodo ha massimo due figli}. Poiché sono alberi ordinati, i due figli di ciascun nodo si distinguono in \textbf{figlio sinistro} e \textbf{figlio destro}.

Un albero binario nel quale \textbf{tutti i livelli, tranne l'ultimo, contengono due figli} è chiamato \textbf{albero binario completo}, dove:

\begin{itemize}
    \item L'\textbf{altezza} dell'albero è $h$ 
    \item Il \textbf{numero dei nodi di ogni livello} è $2^i$, dove $i$ è il livello
    \item Il \textbf{numero di foglie} è $2^h$
    \item Il \textbf{numero di nodi interno} (ossia i nodi non foglie) è
    \[ \sum_{k=0}^{h-1} 2^k = \frac{2^h -1}{2-1} = 2^{h}-1\]
    \item Il \textbf{numero totale di nodi} dell'albero è
    \[ 2^h + 2^h - 1 = 2^{h+1}-1\]
\end{itemize}

Considerando $n$ come il \textbf{numero totale di nodi} dell'albero, otterremo che l'\textbf{altezza $h$ di un albero completo} sarà:

\[ n = 2^{h+1}-1\]
\[ \log(n+1) = h+1\]
\[ \log(n+1)-1 = h\]
\[ \log(n+1) - \log(2) = h\]
\[ h = \log \left (\frac{n+1}{2} \right )\]

Quindi, possiamo dire che l'\textbf{altezza di un albero completo} sia $h = \Theta(\log(n))$. Nel caso in cui l'albero non sia completo, invece, l'altezza sarà $O(n)$ (si pensi ad un \textbf{albero pienamente sbilanciato}, ossia con tutti i nodi agglomerati sul lato sinistro o destro).

\newpage

\subsubsection{Rappresentazione e memorizzazione degli alberi}

Vediamo adesso i \textbf{tre modi} con cui è possibile \textbf{rappresentare} e \textbf{memorizzare} il seguente albero binario:

\begin{center}
    \includegraphics[scale=0.6]{resources/images/chapter_8/tree_4.png}
\end{center}

\begin{itemize}
    \item \textbf{Tramite puntatori}:
    
    Il modo più naturale di \textbf{rappresentare} e \textbf{gestire} gli alberi binari è per mezzo dei \textbf{puntatori}, dove ogni singolo nodo è costituito da un \textbf{record} contenente:
    
    \begin{itemize}
        \item \textbf{Key}: le opportune informazioni pertinenti al nodo stesso
        \item \textbf{Left}: il puntatore al figlio sinistro (oppure \textbf{null} se il nodo non ha figlio sinistro)
        \item \textbf{Right}: il puntatore al figlio destro (oppure \textbf{null} se il nodo non ha figlio destro)
    \end{itemize}
    
    Dunque, similmente alla testa delle liste, si accede all'albero per mezzo del \textbf{puntatore alla radice}.
    
    \begin{center}
        \includegraphics[scale=0.5]{resources/images/chapter_8/tree_5.png}
    \end{center}
    
    \newpage

    \item \textbf{Tramite indici posizionali}:
    Come nella struttura dati \textbf{heap}, i nodi vengono memorizzati in un \textbf{array}, nel quale la \textbf{radice} occupa la posizione di indice 0 ed i \textbf{figli sinistro e destro del nodo in posizione $i$} si trovano rispettivamente nelle \textbf{posizioni $2i+1$ e $2i+2$}.
    
    Tale rappresentazione, tuttavia richiede di conoscere in anticipo l'\textbf{altezza massima} dell'albero, poiché un array è una struttura dati \textbf{statica}.
    
    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_8/tree_6.png}
    \end{center}

    \item \textbf{Tramite vettore dei padri}:
    
    Vengono utilizzati \textbf{due array}: \textbf{uno contenente i nodi dell'albero $A$} ed \textbf{uno contenente il padre di ciascun nodo $P$.}
    
    In questo modo, l'elemento $A[i]$ conterrà il \textbf{valore di un nodo}, mentre l'elemento $P[i]$ conterrà \textbf{l'indice del padre del nodo $i$ nell'albero}. 
    
    Questo metodo di memorizzazione funziona senza alcuna modifica anche per \textbf{alberi n-ari}, in cui cioè ogni nodo può avere un \textbf{numero qualunque di figli}.

    \begin{center}
        \includegraphics[scale=0.6]{resources/images/chapter_8/tree_7.png}
    \end{center}
\end{itemize}

\newpage

\subsection{Visite di un albero}

Tipico degli alberi è l'\textbf{accesso progressivo a tutti i nodi}, uno dopo l'altro, al fine di poter effettuare una specifica operazione su ciascuno di essi.

A differenza di array e liste, tale operazione non può essere effettuata con una semplice iterazione per via della struttura più articolata degli alberi. Questa operazione viene chiamata \textbf{visita dell'albero}.

Facendo riferimento all'\textbf{ordine} col quale si accede ai nodi dell'albero, è evidente che esiste \textbf{più di una possibilità} a seconda del \textbf{momento in cui si svolge l'operazione richiesta} sul nodo stesso.

Nell'ambito degli \textbf{alberi binari}, possiamo identificare \textbf{tre tipi di visite}:

\begin{itemize}
    \item \textbf{Visita in pre-order}: si accede al nodo prima di proseguire la visita nei suoi sottoalberi
    
    \begin{verbatim}
def visita_pre_order(p):
    if (p != None)
        fun(p) #accesso al nodo e operazioni conseguenti
        visita_pre_order(p->left)
        visita_pre_order(p->right)
    return
    \end{verbatim}
    
    \item \textbf{Visita in-order}: il nodo è visitato dopo la visita del sottoalbero sinistro e prima di quella del sottoalbero destro
    
    \begin{verbatim}
def visita_in_order(p):
    if (p != None)
        visita_in_order(p->left)
        fun(p) #accesso al nodo e operazioni conseguenti
        visita_in_order(p->right)
    return
    \end{verbatim}
    
    \item \textbf{Visita post-order}: il nodo è visitato dopo entrambe le visite dei sottoalberi.
    
    \begin{verbatim}
def visita_post_order(p):
    if (p != None)
        visita_post_order(p->left)
        visita_post_order(p->right)
        fun(p) #accesso al nodo e operazioni conseguenti
    return
    \end{verbatim}
\end{itemize}

\newpage

Il \textbf{costo di tutte e tre le visite è lo stesso} ed riconducibile alla seguente equazione, dove $k$ è il numero di nodi dell'sottoalbero sinistro, mentre $n-k-1$ è il numero nodi di quello destro:

\[ T(n) = T \left ( k \right ) + T \left ( n-k-1 \right )+ \Theta(1)\]

Analizziamo quindi il suo \textbf{caso migliore} e il suo \textbf{caso peggiore}:
\begin{itemize}
    \item \textbf{Caso migliore:} si tratta di un \textbf{albero completo}, dunque ogni nodo (tranne le foglie) possiede \textbf{esattamente due figli}. In questo caso, ogni sottoalbero sinistro e ogni sottoalbero destro avranno $\frac{n-1}{2}$ nodi, dunque:
    
    \[ T(n) = T \left ( \frac{n-1}{2} \right ) +  T \left ( \frac{n-1}{2} \right ) + \Theta(1) =  2T \left ( \frac{n-1}{2} \right ) + \Theta(1)\]
    
    Poiché ricadiamo nel \textbf{caso 1 del metodo principale}, il costo del caso migliore sarà $T(n) = \Theta(n)$
    
    \quad
    
    \item \textbf{Caso peggiore:} si tratta di un \textbf{albero estremamente sbilanciato}, dove tutti i nodi discendenti della radice sono agglomerati nel sottoalbero sinistro, dunque quando $n-k-1 = 0$, o nel sottoalbero destro, dunque quando $k = 0$.
    
    \begin{itemize}
        \item Se $k = 0$ si ha $n-k-1 = n-1$:
        \[ T(n) = T(k) + T(n-k-1) + \Theta(1) = T(n-1) + \Theta(1)\]
        
        \item Se $n-k-1 = 0$ si ha $k = n-1$:
        \[ T(n) = T(k) + T(n-k-1) + \Theta(1) = T(n-1) + \Theta(1)\]
    \end{itemize}
    
    In entrambi i casi, possiamo utilizzare il metodo iterativo per ricondurre facilmente il costo a $\Theta(n)$.
    
    \quad
    
    \item \textbf{Caso generale:} poiché si ha che l'equazione è sia $O(n)$ (per via del \textbf{caso peggiore}), sia $\Omega(n)$ (per via del \textbf{caso migliore}), concludiamo che il suo costo sia sempre $\Theta(n)$. Difatti, poiché una visita consiste nell'\textbf{analizzare tutti i nodi di un albero} e poiché $n$ corrisponde al \textbf{numero di nodi totali}, viene logico che il \textbf{costo di una visita sia sempre $\Theta(n)$}.
\end{itemize}

\quad

E se volessimo effettuare una \textbf{visita per livelli}, dalla radice in giù? Nessuna delle visite ricorsive che abbiamo illustrato per gli alberi implementati mediante puntatori permette di farlo. È necessario utilizzare una \textbf{coda d'appoggio}, nella quale inserire opportunamente i nodi, estraendoli poi per visitarli.

\begin{verbatim}
def visita_per_livelli (r; head, tail):
    if (r == None): return
    Enqueue(head, tail, r)
    while (!CodaVuota(head)):
        p = Dequeue(head, tail)
        fun(p) #accesso al nodo e operazioni conseguenti
        if (p->left != None): Enqueue(head, tail, p->left)
        if (p->right != None): Enqueue(head, tail, p->right)
    return
\end{verbatim}

\quad

\subsubsection{Esempi di visita:}

\begin{enumerate}
    \item Consideriamo il seguente albero già visto precedentemente:
    \begin{center}
        \includegraphics[scale=0.55]{resources/images/chapter_8/tree_4.png}
    \end{center}
    
    Immaginiamo di voler stampare tutti i nodi di questo albero. A seconda della \textbf{tipologia di visita} applicata otteniamo un \textbf{output diverso}:
    
    \begin{itemize}
        \item \textbf{Visita pre-order}:
        \[ \text{A B C D E F G H I}\]
        
        \item \textbf{Visita in-order}:
        \[ \text{C B D A G F E H I}\]
        
        \item \textbf{Visita post-order}:
        \[ \text{C D B G F I H E A}\]
    \end{itemize}
    
    \quad
    
    \item Vogliamo contare il numero di nodi di un albero binario.
    
    Poiché il numero di nodi di un sottoalbero corrisponde al \textbf{numero di nodi dei due sotto-sottoalberi sinistro e destro sommati alla radice stessa del sottoalbero}, ne deduciamo che sia prima necessario chiamare ricorsivamente la funzione su entrambi i due sottoalberi, per poi effettuare la somma.
    
    \newpage

    Il risultato, quindi, è l'applicazione di una \textbf{visita post-order}:
    
    \begin{verbatim}
def Calcola_n (p):
    if (p != None):
        num_l = Calcola_n (p->left)
        num_r = Calcola_n (p->right)
        num = num_l + num_r + 1     #accesso al nodo
        return num
    return 0
    \end{verbatim}
    
    Che possiamo anche comprimere nel seguente codice:
    \begin{verbatim}
def Calcola_n (p):
    if (p != None):
        return Calcola_n (p->left) + Calcola_n (p->right) + 1
    return 0
    \end{verbatim}
    
    \item Vogliamo ricercare un nodo in un albero binario.
    
    Immaginando di star effettuando noi la ricerca, ci viene logico pensare che, \textbf{prima} di controllare i nodi successivi, venga controllato il \textbf{nodo attuale}.
    
    Il risultato, quindi, è l'applicazione di una \textbf{visita pre-order}:
    
    \begin{verbatim}
def Cerca (p):
    if (p != None):
        if p->info == k return TRUE
    else:
        if Cerca(p->left,k) == TRUE:
            return TRUE
    else:
        return Cerca(p->right,k)
    return FALSE
    \end{verbatim}
    
    \item Vogliamo calcolare l'altezza di un albero binario.
    
    Poiché l'altezza di un sottoalbero corrisponde all'\textbf{altezza dei suoi sotto-sottoalberi incrementata di uno}, ne deduciamo che sia necessario chiamare ricorsivamente la funzione prima di effettuare l'incremento.
    
    Il risultato, quindi, è l'applicazione di una \textbf{visita post-order}:
    
    \begin{verbatim}
def Calcola_h (p):
    if (p==None):
        return -1   #albero vuoto
    if (p->left==None) and (p->right==None):
        return 0
    h = max(Calcola_h(p->left), Calcola_h(p->right))
    return h + 1\end{verbatim}
\end{enumerate}

\newpage

\subsection{Alberi binari di ricerca}

Un \textbf{albero binario di ricerca (ABR)} è una versione specifica dell'albero binario nel quale vengono mantenute le seguenti \textbf{proprietà aggiuntive}:

\begin{itemize}
    \item Il valore della chiave contenuta in ogni nodo è \textbf{maggiore} della chiave contenuta in \textbf{ciascun nodo del suo sottoalbero sinistro (se esiste)}. Dunque, il \textbf{valore minimo assoluto} dell'albero è il nodo \textbf{più a sinistra}.
    \item Il valore della chiave contenuta in ogni nodo è \textbf{minore} della chiave contenuta in \textbf{ciascun nodo del suo sottoalbero destro (se esiste)}. Dunque, il \textbf{valore massimo assoluto} dell'albero è il nodo \textbf{più a destra}.
\end{itemize}

\textit{Non è detto che il minimo e il massimo siano necessariamente una foglia dell'albero}

\quad

\subsubsection{Ordinamento dei nodi}

Immaginiamo di voler stampare i nodi di un albero binario di ricerca (ABR) in \textbf{ordine crescente}.
    
Ricordando che, secondo le \textbf{proprietà degli alberi binari di ricerca}, i valori minori di un nodo sono nel suo sottoalbero sinistro e mentre i valori maggiori sono nel sottoalbero destro, possiamo applicare una semplice \textbf{visita in-order} per stampare l'intero albero in ordine crescente:

\begin{verbatim}
def Stampa_crescente(p):
    if (p != None)
        Stampa_crescente(p->left)
        print(p->val)
        Stampa_crescente(p->right)
\end{verbatim}

Potenzialmente, quindi, potremmo considerare l'uso di un ABR come un \textbf{algoritmo di ordinamento}, costituito dalla costruzione dell'ABR (il cui costo ancora non conosciamo) e dalla sua visita in-order (il cui costo sappiamo essere $\Theta(n)$).

\begin{center}
    \includegraphics[scale=0.6]{resources/images/chapter_8/tree_8.png}
\end{center}

\subsubsection{Ricerca di un nodo}

Per via delle sue proprietà, inoltre, notiamo come una \textbf{ricerca su un ABR} sia concettualmente simile alla \textbf{ricerca binaria}: si esegue una discesa dalla radice che viene guidata da un \textbf{confronto (maggiore o minore) tra i valori memorizzati nei nodi} che si incontrano lungo il cammino:

\begin{verbatim}
def ABR_search(p,k):
    if (p == None) or (p->key = k):
        return p
    if (k < p->key):
        return ABR_searchRic(p->left, k)
    else:
        return ABR_searchRic(p->right, k)\end{verbatim}

\quad

Tuttavia, a differenza della ricerca binaria, in questo caso \textbf{non riusciamo a garantire un costo computazionale pari a $O(\log(n))$}, poiché il costo dell'algoritmo di ricerca è $O(h)$, dove $h$ è l'\textbf{altezza dell'ABR}:

\begin{itemize}
    \item Nel \textbf{caso peggiore}, ossia un albero pienamente sbilanciato (dove tutti i nodi sono agglomerati a sinistra o a destra), l'altezza corrisponde a $n-1$, dunque il costo sarà $O(n)$
    \item Nel \textbf{caso migliore}, ossia un albero completo, l'altezza corrisponde a $\log(n+1)-1$, dunque il costo $\Omega(\log(n))$
\end{itemize}

Quindi, per \textbf{garantire} che il costo computazionale della ricerca su un ABR sia $O(\log(n))$, è necessario mettere in campo qualche tecnica che ci permetta di tenere sotto controllo la crescita dell'altezza, ossia il \textbf{bilanciamento in altezza}.

\begin{center}
    \includegraphics[scale=0.85]{resources/images/chapter_8/tree_9.png}
\end{center}

\quad

\subsubsection{Inserimento di un nodo}

Come nella ricerca, si esegue una discesa che viene guidata dai \textbf{confronti sulle chiavi} memorizzate nei nodi che si incontrano lungo il cammino: quando si arriva al punto di voler proseguire la discesa verso un \textbf{puntatore vuoto} (None), si interrompe la discesa e si \textbf{aggiunge il nuovo nodo} contenente la chiave da inserire in quella posizione.

\begin{verbatim}
def ABR_insert (p,k):
    y,x = None,p        #y punta sempre al padre di x
    z = NodoABR(k)      #nuovo nodo con k come chiave
    
    while (x != None)   #discesa alla prima pos. disponib.
        y = x
        if z->key < x->key:
            x = x->left
        else:
            x = x->right
            
    if (y==None):       #se l'albero era inizialmente vuoto
        p = z
    else:
        if (z->key < y->key):
            y->left = z
        else:
            y->right = z
            
    z->parent = y       #collegam. padre - nodo da inser.
    return p            #p potrebbe essere cambiato
\end{verbatim}

\newpage

\subsubsection{Eliminazione di un nodo}

Nell'eliminazione di un nodo da un ABR, si possono verificare \textbf{tre casi}:

\begin{itemize}
    \item \textbf{Caso 1}: se il nodo è una \textbf{foglia}, lo si elimina ponendo a None l'opportuno campo nel suo nodo padre

    \item \textbf{Caso 2}: se il nodo ha \textbf{un solo figlio}, lo si elimina collegando direttamente suo padre e il suo unico figlio, sovrascrivendo il collegamento tra suo padre e il nodo eliminato stesso
    
    \item \textbf{Caso 3}: se il nodo ha \textbf{entrambi i figli}, è necessario \textbf{riaggiustare l'albero} significa trovare un nodo da collocare al posto del nodo che va eliminato, così da \textbf{mantenere l'albero connesso} e da \textbf{garantire il mantenimento delle proprietà fondamentali} degli ABR.
    
    Per ottenere ciò, lo si sostituisce col \textbf{predecessore} (o col \textbf{successore}), che va quindi eliminato dalla sua posizione originale (operazione che questa volta ricade in uno dei due casi precedenti).
    
\end{itemize}

\subsection{Alberi rosso-neri}

Come abbiamo già visto, per ottenere il costo computazionale minore nello svolgere operazioni su un ABR è necessario che l'albero sia il \textbf{più bilanciato possibile}, in modo che la sua altezza sia $h = O(\log(n))$.

Le \textbf{tecniche di bilanciamento} sono tutte basate sull'idea di \textbf{riorganizzare la struttura dell'albero} se essa, a seguito di un'operazione di inserimento o di eliminazione di un nodo, viola determinati requisiti.

In particolare, il requisito da controllare è che, per ciascun nodo dell'albero, l'altezza dei suoi due sottoalberi non sia \textbf{"troppo differente"}. Ciò che rende non banali queste tecniche è che si vuole aggiungere agli ABR una proprietà \textbf{senza peggiorarne il costo computazionale delle operazioni}.

\quad

Un \textbf{albero rosso-nero (A-RB)} è un ABR i cui nodi hanno un campo aggiuntivo, ossia il \textbf{colore}, che può essere \textbf{solo rosso o nero}.

All'albero vengono aggiunte \textbf{foglie fittizie} (che non contengono chiavi) in modo che tutti i nodi "veri" dell'albero abbiano \textbf{esattamente due figli}.

Un A-RB, quindi, è un ABR che soddisfa le seguenti \textbf{proprietà aggiuntive}:

\begin{itemize}
    \item Ciascun nodo è \textbf{rosso} o \textbf{nero}
    \item Ciascuna \textbf{foglia fittizia è nera}
    \item Se un nodo è \textbf{rosso} i suoi figli sono \textbf{entrambi neri}
    \item \textbf{Ogni cammino} da un \textbf{nodo} a ciascuna delle \textbf{foglie} del suo sottoalbero contiene lo \textbf{stesso numero di nodi neri}
    \item La \textbf{radice} è sempre \textbf{nera}
    \item Nessun cammino dalla radice ad una foglia può essere lungo più del doppio di un cammino dalla radice ad una qualunque altra foglia
\end{itemize}

\begin{center}
    \includegraphics[scale=0.8]{resources/images/chapter_8/arb_1.png}
\end{center}

Definiamo quindi come \textbf{black-height} di un \textbf{nodo x} il numero di nodi neri sui cammini dal nodo x (non incluso) alle sue foglie discendenti. La BH di un A-RB è la \textbf{BH della sua radice}, mentre il sottoalbero radicato in un qualsiasi nodo x contiene \textbf{almeno $2^{BH(x)}-1$ nodi interni}.

Tramite le proprietà degli A-RB, possiamo garantire il seguente teorema:

\begin{framedthm}{}
    Un A-RB con $n$ nodi interni ha \textbf{sempre un'altezza $h$}
    \[ h \leq 2 \log(n + 1) \]
\end{framedthm}
\textit{Dimostrazione:}

Sia $h$ l'altezza dell'A-RB ed $r$ la sua radice. Sappiamo già che $h \leq 2BH(r)$, ossia $BH(r) \geq \frac{h}{2}$. Sappiamo inoltre che il numero di nodi interni dell'A-RB, pari al numero di nodi interni nel sottoalbero radicato in $r$ è:

\[ n \geq 2^{BH(r)} - 1 \geq 2^{\frac{h}{2}} - 1 \]
da cui
\[ n + 1 \geq 2^{\frac{h}{2}} \quad \Longrightarrow \quad 2 \log(n + 1) \geq h \]

Tale teorema, quindi, \textbf{garantisce} che le operazioni di ricerca di una chiave, ricerca del massimo o del minimo, ricerca del predecessore o del successore, inserimento di un nodo ed eliminazione di un nodo siano tutte eseguite con un \textbf{costo computazionale} $O(\log (n))$.

\subsubsection{Rotazioni di un A-RB}

L'esigenza di \textbf{mantenere le proprietà di A-RB} implica che, dopo un inserimento o una eliminazione, la struttura dell'albero debba essere \textbf{riaggiustata}, in termini di:

\begin{itemize}
    \item Colori assegnati ai nodi
    \item Collocazione dei nodi nell'albero
\end{itemize}

A tal fine sono definite apposite operazioni, dette \textbf{rotazioni}, che permettono di ripristinare in tempo $O(\log n)$ le proprietà del A-RB dopo un inserimento o una eliminazione.

Le \textbf{rotazioni} possono essere \textbf{destre} o \textbf{sinistre} e sono operazioni \textbf{locali} che non modificano l'ordinamento delle chiavi secondo la visita in-order.

Ogni rotazione viene effettuata su un \textbf{nodo x}, il quale funge da "\textbf{perno}" su cui viene effettuata l'intera rotazione: in una \textbf{rotazione a sinistra}, il \textbf{sottoalbero sinistro $\beta$ del figlio destro del nodo x} (che chiameremo nodo y), diventa il \textbf{nuovo sottoalbero destro del nodo x stesso}, mentre il nodo x diventa il \textbf{figlio sinistro del nodo y}.

La \textbf{rotazione a destra} risulta specchiata a quella a sinistra, invertendo nodi e sottoalberi sinistri con quelli destri.

\begin{center}
    \includegraphics[scale=0.475]{resources/images/chapter_8/arb_2.png}
\end{center}

\quad

\begin{verbatim}
def A_RB_rotaz_sinistra (p, x): #x = il nodo su cui si ruota
    y = x->right
    x->right = y->left
    
    if x->right != None:
        x->right->parent = x
    y->left = x
    y->parent = x->parent
    
    if x->parent == None:
        p = y
    else:
        if x == x->parent->left:
            x->parent->left = y
        else:
            x->parent->right = y
    x->parent = y
    
    return p
\end{verbatim}

Il \textbf{costo computazionale} di una rotazione risulta essere $\Theta(1)$, poiché vengono effettuati solo scambi costanti tra i nodi.

\textbf{Nota}: una visita in-order su un A-RB effettuata prima e dopo una rotazione mantiene lo stesso ordine di accesso ai nodi.

\subsubsection{Inserimento in un A-RB}

Per via delle sue strette regole, l'inserimento di un nodo in un A-RB risulta complesso e strutturato in \textbf{più fasi e casi}:

\begin{itemize}
    \item \textbf{Fase preliminare}: ogni nodo viene inserito  come in un normale albero binario di ricerca e gli viene attribuito il \textbf{colore rosso}
    \item \textbf{Fase di aggiustamento}: è necessario se e solo se il nodo inserito risulta essere il \textbf{figlio di un altro nodo rosso}. A questo punto, si può rientrare in una di \textbf{quattro casistiche}:
    
    \begin{enumerate}
        
        \item[0] \textbf{Caso 0 - Il nodo x inserito risulta essere radice dell'albero}:
        
        \begin{itemize}
            \item Poiché la radice deve essere sempre nera, è necessario semplicemente \textbf{cambiare il colore del nodo da rosso a nero}.
            
        \end{itemize}
        
        \quad
        
        \item[1] \textbf{Caso 1 - Il nodo x inserito risulta essere figlio di un nodo rosso e il suo zio è rosso}:
        
        \begin{itemize}
            \item È necessario cambiare i colori di alcuni nodi: si colorano \textbf{di nero il padre e lo zio del nodo x} e \textbf{di rosso il nonno di x}. 
            \item La violazione risulta quindi \textbf{eliminata} oppure è stata \textbf{portata più in alto}, ricadendo sul nonno di x.
        \end{itemize}
        
        \begin{center}
            \includegraphics[scale=0.7]{resources/images/chapter_8/arb_4.png}
        \end{center}
        
        \item[2] \textbf{Caso 2 - Il nodo x inserito risulta essere figlio destro di un nodo rosso e il suo zio è nero}:
        
        \begin{itemize}
            \item Si effettua una \textbf{rotazione a sinistra sul padre di x}
        \end{itemize}
        
        \begin{center}
            \includegraphics[scale=0.7]{resources/images/chapter_8/arb_5.png}
        \end{center}
        
        \item[3] \textbf{Caso 3 - Il nodo x inserito risulta essere figlio sinistro di un nodo rosso e il suo zio è nero}:
        
        \begin{itemize}
            \item L'applicazione del Caso 2 \textbf{riconduce sempre al Caso 3}
            \item Si effettua una \textbf{rotazione a destra} e si \textbf{cambiano alcuni colori}, garantendo sempre la soluzione della violazione.
        \end{itemize}
        
        \begin{center}
            \includegraphics[scale=0.7]{resources/images/chapter_8/arb_6.png}
        \end{center}
    \end{enumerate}
    
    \quad
    
    \subsubsection{Esempio di inserimento completo su un A-RB}
    
    \begin{center}
        \includegraphics[scale=0.7825]{resources/images/chapter_8/arb_7.png}
    \end{center}
    
    \begin{enumerate}
        \item In ogni passaggio dell'inserimento etichettiamo come $z$ \textbf{il nodo che viola le proprietà degli A-RB}
        \item Inserendo il nodo $z$ si ricade nel \textbf{Caso 1}, dunque cambiamo i colori necessari
        \item La risoluzione del Caso 1 precedente da vita al \textbf{Caso 2}, dunque si effettua una rotazione a sinistra sul padre del nodo $z$
        \item Poiché la risoluzione del Caso 2 da sempre vita al \textbf{Caso 3}, effettuiamo la rotazione finale che risolve la violazione
    \end{enumerate}
    
    Notiamo come, dopo l'inserimento, l'A-RB risulti essere ancora \textbf{quasi completamente bilanciato}.
\end{itemize}
    
    \section{Dizionari}
    
    Un \textbf{dizionario} è una struttura dati che permette di gestire un insieme dinamico di dati, che di norma è un \textbf{insieme totalmente ordinato}, tramite queste tre sole operazioni:
    
    \begin{itemize}
        \item \textbf{Insert}: si inserisce un elemento
        \item \textbf{Search}: si ricerca un elemento
        \item \textbf{Delete}: si elimina un elemento
    \end{itemize}
    
    Quando l'esigenza è quella di realizzare un dizionario, si ricorre quindi a soluzioni specifiche:
    \begin{itemize}
        \item \textbf{Tabelle ad indirizzamento diretto}
        \item \textbf{Tabelle hash}
        \item \textbf{Alberi binari di ricerca}
    \end{itemize}
    
    Prima di parlare di esse, è necessario prima introdurre delle \textbf{assunzioni} e delle \textbf{nomenclature}:
    
    \begin{itemize}
        \item \textbf{Insieme U}: è l'insieme universo dei \textbf{valori che le chiavi possono assumere} ed è costituito da \textbf{valori interi}
        \item \textbf{Valore m}: è il \textbf{numero delle posizioni a disposizione} nella struttura dati
        \item \textbf{Valore n}: è il \textbf{numero degli elementi da memorizzare nel dizionario} e i valori delle chiavi degli elementi da memorizzare sono tutti diversi fra loro.
    \end{itemize}
    
    \quad
    
    \subsection{Tabelle ad indirizzamento diretto}

    Un dizionario costituito da una \textbf{tabella ad indirizzamento diretto}, è un vettore nel quale \textbf{ogni indice corrisponde al valore della chiave} dell'elemento da memorizzare in tale posizione.
    
    \begin{center}
        \includegraphics[scale=0.675]{resources/images/chapter_8/dictionaries_1.png}
    \end{center}
    
    Affinché tale tipologia di dizionario possa funzionare, \textbf{è necessario che $n \leq m = |U|$}, dove $|U|$ indica il numero di elementi dell'insieme U, assolvendo perfettamente al compito di dizionario con \textbf{grande efficienza}.
    
    Infatti, tutte e tre le operazioni necessarie hanno costo computazionale pari a $\Theta(1)$:
    
    \begin{verbatim}
    def Insert_Indirizz_Diretto (A; e: elem da ins.)
        A[e->key] = e
        return
        
    def Search_Indirizz_Diretto (A; k: chiave da cerc.)
        return A[k]
        
    def Delete_Indirizz_Diretto (A; k: chiave da canc.)
        A[k] = None
        return
    \end{verbatim}
    
    Sebbene sia estremamente efficiente quanto a costo computazionale, l'indirizzamento diretto risulta essere il meno efficiente tra tutti per quanto riguarda il \textbf{costo in memoria}:
    \begin{itemize}
        \item Poiché si ha $m = |U|$, un insieme U enorme rende impraticabile l'\textbf{allocazione in memoria} di un array lungo $m$
        \item Il numero delle chiavi effettivamente utilizzate può essere molto più piccolo di $|U|$: in tal caso vi è un \textbf{rilevante spreco di memoria}
    \end{itemize}
    
    Perciò, si ricorre spesso a differenti implementazioni dei dizionari, a meno che non ci si trovi nelle condizioni che permettono l'uso dell'indirizzamento diretto, in particolare il mantenimento di un insieme U ridotto.
    
    \quad
    
    \subsection{Tabelle hash}
    
    Per risolvere il problema principale degli indirizzamenti diretti, si ricorre all'uso di una \textbf{funzione di hash}, in grado di calcolare la posizione finale di un elemento sulla base del valore della sua chiave, il quale può essere \textbf{un intero molto più grande del valore generato dalla funzione di hash}.
    
    \begin{center}
        \includegraphics[scale=0.65]{resources/images/chapter_8/dictionaries_2.png}
    \end{center}
    
    Tuttavia, anche se le chiavi da memorizzare sono meno di $m$, non si può escludere che due chiavi $k_1 \neq k_2$ siano tali per cui $hash(k_1) = hash(k_2)$, ossia che la \textbf{funzione hash restituisca lo stesso valore per entrambe le chiavi}, che quindi andrebbero memorizzate nella stessa posizione della tabella, dando vita a quello che viene definito come \textbf{collisione di hash}.
    
    Tale fenomeno è \textbf{matematicamente inevitabile}. Dunque, possiamo solo tentare di evitarle il più possibile e risolverle con alcune strategie:
    
    \begin{itemize}
        \item Una buona funzione hash deve essere tale da rendere il \textbf{più equiprobabile possibile il valore risultante} dall'applicazione della funzione: tutti i valori fra 0 e $m - 1$ dovrebbero essere restituiti con uguale probabilità.
        
        \item In altre parole, la funzione dovrebbe far apparire come "casuale" il valore risultante, disgregando qualunque regolarità della chiave.
        
        \item Inoltre, la funzione deve essere \textbf{deterministica}: se applicata più volte alla stessa chiave deve fornire sempre lo stesso risultato.
        
        \item La situazione ideale è quella in cui ciascuna delle $m$ posizioni della tabella è scelta deterministicamente con la stessa probabilità: \textbf{ipotesi di uniformità semplice della funzione hash}.
    \end{itemize}
    
    L'i\textbf{potesi di uniformità semplice} (di cui ometteremo le specifiche per poterla ottenere) minimizza il numero di collisioni, ma queste possono comunque avvenire: per quanto sia progettata bene la funzione hash, è \textbf{impossibile evitare del tutto le collisioni}, poiché, siccome si ha che $|U| > m$, è inevitabile che esistano chiavi diverse che producono una collisione.
    
    \quad
    
    \subsection{Tabelle a liste di trabocco}
    
    Questa tecnica prevede di inserire \textbf{tutti gli elementi le cui chiavi mappano nella stessa posizione in una lista puntata}, detta \textbf{lista di trabocco}. Invece degli elementi in se, dunque, nell'array del dizionario vengono immagazzinati i puntatori alle teste delle liste di trabocco.
    
    \begin{center}
        \includegraphics[scale=0.7]{resources/images/chapter_8/dictionaries_3.png}
    \end{center}
    
    \newpage
    
    Poiché tali dizionari vengono implementati tramite l'uso di liste puntate, ne segue che il costo delle operazioni sia \textbf{strettamente legato ad esse}:
    
    \begin{itemize}
        \item \textbf{Inserimento}: il costo rimane $\Theta(1)$, poiché possiamo effettuare un inserimento in testa sulle liste
        
        \begin{verbatim}
def Insert_Liste_Trabocco (A; e: elem. da ins.):
    lista_di_trabocco = A[hash(e->key)]
    insert_in_testa(lista_di_trabocco, e)
    return
        \end{verbatim}
        
        \item \textbf{Ricerca}: poiché la ricerca di un elemento in una lista puntata ha un \textbf{costo pari a $O(i)$}, dove $i$ è la \textbf{lunghezza della lista}, ne segue che il costo di una ricerca in un dizionario di questo tipo nel \textbf{caso peggiore} risulti essere $O(n)$, dovuto all'inserimento di tutti gli elementi nella stessa identica lista di trabocco.
        
        Nel \textbf{caso medio}, il costo della ricerca risulta essere $O(\alpha)$, dove $\alpha$ è il \textbf{fattore di carico della tabella}. Considerando l'\textbf{ipotesi di uniformità semplice}, otteniamo che $\alpha = \frac{n}{m}$, dunque il costo della ricerca risulta essere $O \left (\frac{n}{m} \right )$
        
        \begin{verbatim}
def Search_ListeTrabocco (A; k: chiave da cercare):
    lista_di_trabocco = A[hash(k)]
    elem = ricerca_elem(lista_di_trabocco, k)
    return elem
        \end{verbatim}
        
        \item \textbf{Eliminazione}: dipende strettamente dall'implementazione delle liste di trabocco e valgono, pertanto, \textbf{tutte le osservazioni fatte} per il costo dell'operazione di cancellazione nelle liste concatenate.
        
        \begin{verbatim}
def Delete_ListeTrabocco (A; k: chiave da canc.)
    lista_di_trabocco = A[hash(k)]
    elimina_elem(lista_di_trabocco, k)
    return
        \end{verbatim}
        
    \end{itemize}
    
    \newpage

    \subsection{Tabelle ad indirizzamento aperto}
    
    Al contrario delle liste di trabocco, l'idea alla base di tale tecnica, prevede l'inserimento solo all'interno dell'array stesso, calcolando la \textbf{sequenza delle posizioni da esaminare}. La \textbf{funzione hash dipende ora da 2 parametri}: la chiave $k$ e il \textbf{numero di collisioni già trovate}. L'implementazione di tale nuova funzione hash verrà vista in seguito.
    
    \begin{center}
        \includegraphics[scale=0.625]{resources/images/chapter_8/dictionaries_4.png}
    \end{center}
    
    Questa tecnica è applicabile solo quando:
    \begin{itemize}
        \item $m \geq n$ (quindi il fattore di carico $\alpha = \frac{n}{m}$ non è mai maggiore di 1)
        \item $|U| >> m$, ossia il numero di elementi dell'insieme U è molto più grande di $m$
    \end{itemize}
    
    \subsubsection{Operazioni sul dizionario}
    
    \begin{itemize}
        \item \textbf{Inserimento}: se la posizione calcolata con $h(k, 0)$ (dove 0 è il numero di collisioni generate durante l'inserimento attuale) è \textbf{già occupata}, allora viene \textbf{ricalcolato l'hash} incrementando di 1 il numero di collisioni (dunque $h(k, 1)$), generando un valore completamente diverso dal precedente. Se anche la nuova posizione è già occupata, verrà ripetuto ancora il processo, \textbf{fino ad un massimo di $h(k, m-1)$}.
        
        Nel caso peggiore, il costo di tale operazione risulta essere $O(n)$ e si verifica quando tutti gli $n$ elementi inseriti restituiscono lo stesso $h(k, 0)$. Considerando l'ipotesi di uniformità semplice, invece, il caso medio risulta essere $O\left ( \frac{1}{1-\alpha} \right ) = O \left (\frac{m}{m-n} \right )$
        
        \item \textbf{Ricerca}: si analizza la tabella mediante la stessa \textbf{sequenza di funzioni hash} utilizzata per l'inserimento fino a quando si incontra l'elemento cercato oppure una casella vuota, deducendo che l'elemento non è presente. Ne segue quindi che i suoi costi computazionali risultino analoghi a quelli dell'inserimento.
        
        \item \textbf{Eliminazione}: tale operazione risulta essere molto problematica, poiché nel caso in cui si vada ad eliminare l'elemento lasciando la casella vuota, ciò \textbf{spezzerebbe la catena di sequenza della funzione hash} di un qualunque elemento già inserito nella tabella.
        
        Una soluzione proponibile sarebbe quella di eliminare un elemento marcandolo con un apposito valore \textit{\textbf{deleted}}, tuttavia il costo computazionale della ricerca e dell'inserimento non dipenderebbero più esclusivamente dal fattore di carico ma \textbf{anche dal numero di posizioni marcate}.
        
        Per queste ragioni, di solito la cancellazione \textbf{non è supportata con l'indirizzamento aperto}.
    \end{itemize}

    \newpage
    
    Garantire l'\textbf{ipotesi di uniformità semplice} risulta essere quasi impossibile, tuttavia, alcune tecniche si avvicinano molto ad essa, in particolare l'\textbf{hashing doppio}.
    
    L'idea alla base è quella di usare \textbf{due diverse funzioni hash}, una per determinare l'a\textbf{ccesso iniziale} alla tabella e l'altra per determinare il \textbf{passo di scansione }(ossia la ripetizione con l'incremento in base alla collisione):
    
    \[ h(k, i) = [h_1(k) + i \cdot h_2(k)] \texttt{ mod } m \qquad \text{ dove } i \in [0, m-1]\]
    
    Se le due funzioni sono ben progettate, è \textbf{estremamente improbabile che due chiavi $k_1 \neq k_2$ producano una collisione su entrambe le funzioni hash}.

\end{document}
