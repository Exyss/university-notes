\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%

\def\useItalian{0}  % 1 = Italian, 0 = English

\def\courseName{Cryptography}

\def\coursePrerequisites{Algebra and Computational Complexity}

\def\book{"Introduction to Modern Cryptography", J. Katz, Y. Lindell}

\def\authorName{Simone Bianco}
\def\email{bianco.simone@outlook.it}
\def\github{https://github.com/Exyss/university-notes}
\def\linkedin{https://www.linkedin.com/in/simone-bianco}

% \def\authorName{Alessio Bandiera}
% \def\email{alessio.bandiera02@gmail.com}
% \def\github{https://github.com/aflaag-notes}
% \def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../packages/Nyx/nyx-packages}
\usepackage{../../packages/Nyx/nyx-styles}
\usepackage{../../packages/Nyx/nyx-frames}
\usepackage{../../packages/Nyx/nyx-macros}
\usepackage{../../packages/Nyx/nyx-title}
\usepackage{../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../packages/Nyx/logo.png}

\if\useItalian1
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} UniversitÃ  di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \ifdefined\book
        \subtitle{Appunti integrati con il libro \book}
    \fi
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \ifdefined\book
        \subtitle{Lecture notes integrated with the book \book}
    \fi
    \author{\textit{Author}\\\authorName}
\fi


\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%

    \chapter{Introduction to modern cryptography}

    \section{Definitions, assumptions and notation}

    Cryptography is the branch of computer science that practices and studies techniques for secure communication in the presence of adversarial behavior (e.g. altering the integrity of the message, reading the content of the message, \dots). Cryptography focuses on two main goals:
    \begin{enumerate}
        \item \textbf{Confidential communication}: the property of making the original message impossible to read even if an outsider finds out the \textit{ciphertext}, i.e. the encrypted message
        
        \begin{figure}[H]
            \centering

            \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
                \node[] (1) []{Alice};
                \node[] (x) [right of = 1, xshift = 50]{};
                \node[] (2) [right of = x, xshift = 50]{Bob};
                \node[] (3) [below of = x, yshift = -10]{Eve};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge[-] node {$m \in \mathcal{M}$} (x.center)
                    (x.center) edge (2)
                    (x.center) edge (3)
                    ;
            \end{tikzpicture}

            \caption{Without an encryption scheme, Eve -- an evildoer -- may be able to read the message that Alice is sending to Bob.}
        \end{figure}

        \item \textbf{Message integrity}: the capability of ensuring that the original message has not been altered even if an outsider intercepts the ciphertext
        
        \begin{figure}[H]
            \centering

            \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
                \node[] (1) []{Alice};
                \node[] (x) [right of = 1, xshift = 50]{};
                \node[] (2) [right of = x, xshift = 50]{Bob};
                \node[] (3) [below of = x, yshift = -10]{Eve};
                \node[] (y) [below of = 2, yshift = -10]{};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge[-] node {$m \in \mathcal{M}$} (x.center)
                    (x.center) edge[dashed] (2)
                    (x.center) edge (3)
                    (3) edge[-] node {$m' \in \mathcal{M}$} (y.center)
                    (y.center) edge (2)
                    ;
            \end{tikzpicture}

            \caption{Without an encryption scheme, Eve may be able to intercept and alter the message that Alice is sending to Bob.}
        \end{figure}
    \end{enumerate}


    Before the 50's, cryptography was considered an \textit{art} for geniouses capable of encrypting and decrypting messages written by other geniouses of the field. In modern days, cryptography became a \textit{mathematical science} with precise definitions and proofs. These mathematical tools separate in two types: \textbf{unconditional proofs} and \textbf{conditional proofs}.
    
    The former type referres to proofs where no assumptions are made, focusing on showing that something is possible without caring about it being inefficient (hence we have theoretically infinite resources at our disposal). The latter, instead, uses real-world assumptions that are believed to be true in order to prove real-world results. The typical example is the $\mathsf{P} \neq \mathsf{NP}$ assumption, i.e. that there are some problems that are verifiable in polynomial time but not solvable in polynomial time. Many cryptosystems are based on the assumption that prime factorization is hard to solve but easy to verify. This is equivalent to assuming that $\mathrm{FACTORING} \in \mathsf{NP} - \mathsf{P}$ (only $\mathrm{FACTORING} \in \mathsf{NP}$ has been proven), making the assumption $\mathsf{P} \neq \mathsf{NP}$ fundamental. Making a crytosystem easy to verify allows us to make it impossible to access a resource without using knowing the solution to the authentication phase.

    \begin{framedthm}{}
        Every cryptosystem based on prime factorization is \curlyquotes{secure} if $\mathrm{FACTORING} \notin \mathsf{P}$
    \end{framedthm}
    
    \begin{proof}[Proof (sketch).]
        By contrapositive, suppose that there is a cryptosystem $\Pi$ that is not \curlyquotes{secure}, meaning that there is a polynomial time algorithm $A$ that is capable of \curlyquotes{breaking} $\Pi$. Then, for each number $n \in \N$ we can forge a message $m$ based on $n$ and use the output $A(m)$ to find the prime factors of $n$, concluding that $\mathsf{FACTORING} \in \mathsf{P}$.
    \end{proof}
    
    These two goals will be discussed under two main types of cryptosystems:
    \begin{itemize}
        \item \textbf{Symmetric cryptography}: both ends of the communication share a single common key that is random and unknown to any outsider.
        \item \textbf{Asymmetric cryptography}: both ends of the communication have each their own key pair $(pk, sk)$ where $pk$ is \textit{public}, i.e. known by everyone, and $sk$ is \textit{secret}, i.e. known only by the owner. 
    \end{itemize}

    Throughout this work we'll use the following notation to talk about cryptographic systems:
    \begin{itemize}
        \item $\mathcal{M}$ is the message space, i.e. the set of all strings of messages that can be sent between two parties.
        \item $\mathcal{C}$ is the ciphertext space, i.e. the set of all strings of ciphertexts that can be produced by a cryptosystem.
        \item $\mathcal{K}$ is the key space, i.e. the set of all strings of keys that can be used in a cryptographic system.
        \item $\mathcal{H}$ is the hashing function space, i.e. the set of all hash functions that can be used by a cryptosystem.
    \end{itemize}

    \chapter{Information-theoretic cryptography}

    \section{Perfect secrecy and Shannon's theorem}

    For now, we'll focus on symmetric cryptography, i.e. cryptosystems where a shared secret key is used for both encryption and decryption. It is widely used due to its speed and efficiency, especially in encrypting large volumes of data. A core model of this approach is \textbf{Symmetric-key Encryption (SKE)}, which includes three main components:
    \begin{itemize}
        \item A shared \textit{secret key} $K \in_{R} \mathcal{K}$ chosen uniformly at random
        \item An \textit{encryption function} $\mathrm{Enc} : \mathcal{K} \times \mathcal{M} \to \mathcal{C}$ that transforms plaintext into ciphertext
        \item A \textit{decryption function} $\mathrm{Dec} : \mathcal{K} \times \mathcal{C} \to \mathcal{M}$ that transforms a ciphertext into plaintext
    \end{itemize}

    In order to be functional, SKEs must be \textbf{correct}, meaning that if a message $m \in \mathcal{M}$ gets encrypted with $K \in_R \mathcal{K}$ obtaining the ciphertext $c \in \mathcal{C}$, the decryption process over $c$ using the same key $K$ must give back the original message.

    \begin{frameddefn}{Correctness in SKEs}
        An SKE $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ is said to be correct if $\forall m \in \mathcal{M}, \forall K \in_R \mathcal{K}$ it holds that:
        \[\mathrm{Dec}(K, \mathrm{Enc}(K, m)) = m\]
    \end{frameddefn}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{Alice};
            \node[draw, rectangle, minimum width=40] (2) [right of = 1, xshift = 75]{$\mathrm{Enc}(K,m)$};
            \node[] (x) [right of = 2, xshift = 50]{};
            \node[draw, rectangle, minimum width=40] (3) [right of = x, xshift = 50]{$\mathrm{Dec}(K,c)$};
            \node[] (4) [right of = 3, xshift = 75]{Bob};
            \node[] (5) [below of = x, yshift = -10]{Eve};
            \node[] (6) [below of = 2, yshift = -10]{$K$};
            \node[] (7) [below of = 3, yshift = -10]{$K$};

            \path[every node/.style={font=\sffamily\small}]
                (1) edge node{$m$} (2)
                (2) edge[-] node{$c$} (x.center)
                (x.center) edge (3)
                (x.center) edge (5)
                (3) edge node{$m$} (4)
                (6) edge (2)
                (7) edge (3)
                ;
        \end{tikzpicture}

        \caption{Example of SKE with perfect secrecy. Even if Eve intercepts the ciphertext, she cannot obtain the original message since she doesn't know the key.}
    \end{figure}

    Originally proposed in the 19th century by the homonym cryptographer, \textbf{Kerckhoffs's principle} is a foundational concept in cryptography which asserts that the security of a cryptographic system should depend \underline{only} on the secrecy of the key. In other words, a system should be secure even if everything about the system is known publicly, except for the secret key. The principle was later succinctly restated by Claude Shannon as \curlyquotes{the enemy knows the system}.

    During his work, Shannon also proposed a formal definition of \textbf{perfect secrecy}, i.e. a property that fully respects the concept behind Kerckhoff's principle. Shannon's formulation states that the probability of $m$ being the communicated message is equal to the probability of $m$ being the communicated message even when the corresponding ciphertext $c$ is known. In other words, no ciphertext reveals additional information over any message.

    \begin{frameddefn}{Perfect secrecy}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be an SKE. Let $M$ be a random variable over $\mathcal{M}$ and let $C$ be another random variable defined as $C = \mathrm{Enc}(K,M)$, for some key $K \in_R \mathcal{K}$. We say that $\Pi$ has perfect secrecy when $\forall m \in \mathcal{M}$ and $\forall c \in \mathcal{C}$ with $\Pr[C = c] > 0$ it holds that:
        \[\Pr[M = m] = \Pr[M = m \mid C = c]\]
    \end{frameddefn}
    
    Shannon proved that such definition is achievable by some cryptosystems, but it comes with inheritent practical limitations. Suprisingly, even a simple SKE as the \textbf{One Time Pad (OTP)} system has perfect secrecy. In this system we assume that everything is a binary string of the same length, i.e. that $\mathcal{M} = \mathcal{K} = \mathcal{C} = \{0,1\}^n$ for some $n \in \N$. The encryption and decryption functions are defined as follows:
    \[\mathrm{Enc}(K, m) = K \oplus m \qquad\qquad \mathrm{Dec}(K,c) = K \oplus c\]
    
    By properties of the bit-wise XOR function, it's easy to see that OTP is complete:
    \[\mathrm{Dec}(K,\mathrm{Enc}(K,m)) = K \oplus (K \oplus m) = m\]

    In order to prove that OTP has perfect secrecy, we start by proving two equivalent definitions of perfect secrecy. In particular, states that for every pair of messages every ciphertext has the same probability of being the output of the encoding function when applied of both messages. 

    \begin{framedlem}[label={perfect_secrecy}]{Perfect secrecy (eq. definitions)}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be an SKE. Let $M$ be a random variable over $\mathcal{M}$ and let $C$ be another random variable defined as $C = \mathrm{Enc}(K,M)$, for some key $K \in_R \mathcal{K}$. The following statements are equivalent:
        \begin{enumerate}
            \item $\Pi$ has perfect secrecy
            \item $M$ and $C$ are independent
            \item $\forall m, m' \in \mathcal{M}$ and $\forall c \in \mathcal{C}$ it holds that:
            \[\Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K,m) = c] = \Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K,m') = c]\]
        \end{enumerate}
    \end{framedlem}

    \begin{proof}
        \quad

        \begin{enumerate}
            \item[1. $\implies$ 2.] Suppose that $\Pi$ has perfect secrecy. Then, through definition of conditional probability we get that:
            \[\Pr[M = m] = \Pr[M = m \mid C = c] = \frac{\Pr[M = m, C = c]}{\Pr[C = c]}\]
            which implies that:
            \[\Pr[M = m] \cdot \Pr[C = c] = \Pr[M = m, C = c]\]
            concluding that $M$ and $C$ are independent

            \item[2. $\implies$ 3.] Suppose that $M$ and $C$ are independent. Fix $m, m' \in \mathcal{M}$ and $c \in \mathcal{C}$. Through event manipulation and independency of $M$ and $C$ we obtain that:
            \[\begin{split}
                \Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K,m) = c] &= \Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K,M) = c \mid M = m] \\
                &= \Pr_{K \in_R \mathcal{K}}[C = c \mid M = m]\\
                &= \Pr_{K \in_R \mathcal{K}}[C = c]\\
            \end{split}\]

            Proceding in the same way with $\mathrm{Enc}(K,m')$, we conclude that:
            \[\Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K,m) = c] = \Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K,m') = c]\]

            \item[3. $\implies$ 1.] Assume the third statement holds. Fix $m \in \mathcal{C}$ and $c \in \mathcal{C}$.
            
            \textbf{Claim}: $\Pr[C = c] = \Pr[C = c \mid M = m]$

            \begin{proof}[Proof of the claim]
                Through the probability sum principle, we get that:
                \[\begin{split}
                    \Pr[C = c] &= \sum_{m' \in \mathcal{M}} \Pr[C = c, M = m] \\
                    &= \sum_{m' \in \mathcal{M}} \Pr[C = c \mid M = m'] \cdot \Pr[M = m'] \\
                    &= \sum_{m' \in \mathcal{M}} \Pr[\mathrm{Enc}(K,M') = c \mid M = m'] \cdot \Pr[M = m'] \\
                    &= \sum_{m' \in \mathcal{M}} \Pr[\mathrm{Enc}(K,m') = c] \cdot \Pr[M = m'] \\
                \end{split}\]

                Through the hypothesis we also get that:
                \[\begin{split}
                    \Pr[C = c] &= \sum_{m' \in \mathcal{M}} \Pr[\mathrm{Enc}(K,m') = c] \cdot \Pr[M = m'] \\
                    &= \sum_{m' \in \mathcal{M}} \Pr[\mathrm{Enc}(K,m) = c] \cdot \Pr[M = m'] \\
                    &= \Pr[\mathrm{Enc}(K,m) = c] \cdot \sum_{m' \in \mathcal{M}} \Pr[M = m'] \\
                    &= \Pr[\mathrm{Enc}(K,m) = c] \cdot 1\\
                    &= \Pr[\mathrm{Enc}(K,M) = c \mid M = m]\\
                    &= \Pr[C = c \mid M = m]\\
                \end{split}\]
            \end{proof}

            By definition of conditional probability, we know that:
            \[ \Pr[M = m \mid C = c] \cdot \Pr[C = c] = \Pr[M = m, C = c] = \Pr[C = c \mid M = m] \cdot \Pr[M = m]\]
            
            which implies that:
            \[\Pr[M = m] = \frac{\Pr[M = m \mid C = c] \cdot \Pr[C = c]}{\Pr[C = c \mid M = m]}\]

            Finally, through the claim we conclude that:
            \[\Pr[M = m] = \frac{\Pr[M = m \mid C = c] \cdot \Pr[C = c]}{\Pr[C = c \mid M = m]} = \Pr[M = m \mid C = c]\]
        \end{enumerate}
    \end{proof}

    \begin{framedprop}{}
        OTP has perfect security.
    \end{framedprop}

    \begin{proof}
        We'll prove that the third definition of \Cref{perfect_secrecy} holds for OTP. Fix two messages $m,m' \in \mathcal{M}$ and a ciphertext $c \in \mathcal{C}$. Through definition of the OTP system and by the properties of the XOR function, we have that:
        \[\Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K, m) = c] = \Pr_{K \in_R \mathcal{K}}[K \oplus m = c] = \Pr_{K \in_R \mathcal{K}}[K = m \oplus c] = 2^{-n}\]
        
        Through the same argument, we also get that:
        \[\Pr_{K \in_R \mathcal{K}}[\mathrm{Enc}(K, m') = c] = 2^{-n}\]
    \end{proof}

    We'll now show the inheritent limitations of perfect secrecy. The key idea is simple: in order for $M$ and $C$ to be independent, the key cannot be shorter than the message. Otherwise, there will be some ciphertexts that are unreachable by some messages, revealing information on the system.

    \begin{framedthm}{Shannon's perfect secrecy theorem}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a non-trivial perfectly secret SKE. Then, it holds that $\abs{\mathcal{K}} \geq \abs{\mathcal{M}}$
    \end{framedthm}

    \begin{proof}
        Suppose that $\Pi$ is a non-trivial perfectly secret. Fix any $c \in \mathcal{C}$ such that $\Pr[C = c] > 0$ (this is where non-triviality is required). Let $\mathcal{M}'$ be the set of possible decryptions over $c$, i.e. $\mathcal{M}' = \{\mathrm{Dec}(K,c) \mid K \in \mathcal{K}\}$. We observe that $\mathcal{M}'$ contains at most one decryption for each key (some keys may yield the same decryption). By way of contradiction, suppose that $\abs{\mathcal{K}} < \abs{\mathcal{M}}$. Then, we get that $\abs{\mathcal{M}'} \leq \abs{\mathcal{K}} < \abs{\mathcal{M}}$.

        This implies that $\exists m \in \mathcal{M} - \mathcal{M}'$. Thus, there is a message that cannot be the result of applying the decryption function on $c$, meaning that $\Pr[M = m \mid C = c] = 0$. However, we know that, when no additional information is given, every message is uniform, hence $\Pr[M = m] = \frac{1}{\mathcal{M}}$, contradicting the fact that $\Pi$ has perfect secrecy.
    \end{proof}

    \section{Message authentication codes}

    We'll now focus on the second key goal of cryptography: \textit{message integrity}. The simplest way to reach such goal is through \textbf{Message Authentication Codes (MACs)}, an additional piece of information sent with the message that enables the receiver to assert that the message hasn't been altered.
    
    For now, we'll start with a simple model that \underline{doesn't care} about secrecy, but only about integrity. This type of MACs use a deterministic \textbf{tagging function} (usually implemented as an \textit{hash function}) $\mathrm{Tag} : \mathcal{K} \times \mathcal{M} \to \mathcal{T}$, where $\mathcal{T}$ is the tag space, i.e. the set of all tag strings.

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{Alice};
            \node[draw, rectangle, minimum width=40] (2) [right of = 1, xshift = 75]{$\mathrm{Tag}(K,m)$};
            \node[] (4) [right of = 2, xshift = 75]{Bob};
            \node[] (6) [below of = 2, yshift = -10]{$K$};

            \path[every node/.style={font=\sffamily\small}]
                (1) edge node{$m$} (2)
                (2) edge node{$m,\tau$} (4)
                (6) edge (2)
                ;
        \end{tikzpicture}

        \caption{Example of an integrity-only MAC.}
    \end{figure}

    The idea behind tagging functions is simple: once the message and the tag have been received, Bob can re-compute the tag using the same key-message pair and compare it to the received tag. If the two tags are equal, Bob is sure that the message hasn't been altered. However, this simple idea can only work under the assumption of \textbf{unforgeability}: 
    \begin{itemize}
        \item It should be hard to forge a valid tag $\tau$ for a message $m$ when the key $K$ is not known
        \item It should be hard to forge a valid tag $\tau$ for a message $m$ even when a pair $(m',\tau')$ is known
    \end{itemize}

    In other words, unforgeability states that no pair should reveal no information about how the tags are computed. Without this property, an adversarial entity may be able to infer information on the shared key and/or the tagging function, using them to forge valid tags. If an entity is capable of forging a valid tag, it may intercept the message-tag pair, alter the message, forge a valid tag for the new message and send a new pair, fooling the receiver. We give a formal definition of a property that reflects this unforgeability aspect.

    \begin{frameddefn}{$t$-time $\varepsilon$-statistical security}
        We say that a MAC $\Pi = (\mathrm{Tag})$ has $t$-time $\varepsilon$-statistical security when $\forall m, m_1, \ldots, m_t \in \mathcal{M}$ and $\forall \tau, \tau_1,\ldots, \tau_t \in\mathcal{T}$, where $m \neq m_i$ and $m_i \neq m_j$ for each $i \neq j$, it holds that:
        \[\Pr_{K \in_R \mathcal{K}} [\mathrm{Tag}(K,m) = \tau \mid \mathrm{Tag}(K,m_1) = \tau_1, \ldots, \mathrm{Tag}(K,m_t) = \tau_t] \leq \varepsilon\]
    \end{frameddefn}

    In simple terms, the above property states that, even when $t$ message-tag pairs $(m_1, \tau_1), \ldots,$ $(m_t, \tau_t)$ are known, the probability of a message-tag pair $(m,\tau)$ being possible is at most $\varepsilon$. Optimally, we want $\varepsilon$ to be as small as possible and $t$ to be as large as possible. However, it's easy to see that $\forall t \in \N$ it is impossible to get $\varepsilon = 0$ since a random $\tau \in \mathcal{T}$ always has probability at least $\frac{1}{\abs{\mathcal{T}}}$ of being correct. Just as we did with perfect secrecy, we'll show that the notion of good statistical security is achievable, but it's highly inefficient in terms of key size. 

    \begin{framedthm}{}
        Any $t$-time $2^{-\lambda}$-statistically secure MAC must have a key of size $(t+1)\lambda$
    \end{framedthm}

    \begin{proof}
        Omitted.
    \end{proof}

    A good enough $1$-time statistically secure MAC is achievable through \textbf{pairwise independent hash functions}, i.e. a family of hash functions where each pair of functions forms a pair of independent random variables. This idea can also be expressed through joint uniform distributions, as in the below definition.

    \begin{frameddefn}{Pairwise independent hash functions}
        Let $\mathcal{H} = \{h_K : \mathcal{M} \to \mathcal{T}\}_{K \in \mathcal{K}}$ be a family of hash functions. We say that $\mathcal{H}$ is pairwise independent if $\forall m, m' \in \mathcal{M}$ with $m \neq m'$ it holds that the distribution $(h_K(m), h_K(m'))$ is uniform over $\mathcal{T} \times \mathcal{T}$ when $K \in_R \mathcal{K}$.

        \textit{Note}: $h_K(m)$ and $h_K(m')$ denote two random variables in this context.
    \end{frameddefn}

    \begin{framedthm}{}
        Let $\mathcal{H} = \{h_K : \mathcal{M} \to \mathcal{T}\}_{K \in \mathcal{K}}$ be a family of pairwise indepependent hash functions. Then, $\mathcal{H}$ induces a $1$-time $\frac{1}{\abs{\mathcal{T}}}$-statistically secure MAC.
    \end{framedthm}

    \begin{proof}
        Fix $m \in \mathcal{M}$ and $\tau \in \mathcal{T}$. Let $\Pi = (\mathrm{Tag})$ be the MAC defined as $\mathrm{Tag}(K,m) = h_K(m)$. Since the joint probability of each pair of hash functions in $\mathcal{H}$ is pairwise uniformly distributed, we get that each hash function is also individually uniformly distributed. Hence, we derive that:
        \[\Pr[\mathrm{Tag}(K,m) = \tau] = \Pr[h_K(m) = \tau] = \frac{1}{\abs{\mathcal{T}}}\]

        Similarly, by pairwise independence $\forall m, m' \in \mathcal{M}$ with $m \neq m'$ and $\forall \tau, \tau' \in \mathcal{T}$ it holds that:
        \[\begin{split}
            \Pr_{K \in_R \mathcal{K}}[\mathrm{Tag}(K,m') = \tau', \mathrm{Tag}(K,m) = \tau] &= \Pr_{K \in_R \mathcal{K}}[\mathrm{Tag}(K,m') = \tau'] \cdot \Pr_{K \in_R \mathcal{K}}[\mathrm{Tag}(K,m) = \tau] \\
            &= \Pr_{K \in_R \mathcal{K}}[h_K(m') = \tau'] \cdot \Pr_{K \in_R \mathcal{K}}[h_K(m) = \tau] \\
            &= \frac{1}{\abs{\mathcal{T}}} \cdot \frac{1}{\abs{\mathcal{T}}}
        \end{split}\]

        Putting the two results together we get that:
        \[\Pr[\mathrm{Tag}(K,m') = \tau' \mid \mathrm{Tag}(K,m) = \tau] = \frac{\Pr_{K \in_R \mathcal{K}}[\mathrm{Tag}(K,m') = \tau', \mathrm{Tag}(K,m) = \tau]}{\Pr[\mathrm{Tag}(K,m) = \tau]} = \frac{1}{\abs{\mathcal{T}}}\]
    \end{proof}

    After proving that pairwise independent hash function families form a good enough MAC, we're left with proving that such families exist. Suprisingly, these families can be easily constructed through prime numbers.
    
    \begin{framedprop}{}
        Given a prime $p \in \Primes$, let $\mathcal{M} = \mathcal{T} = \Z_p$ and $\mathcal{K} = \Z_p^2$. Then, the family $\mathcal{H} = \{h_{(a,b)}\}_{(a,b) \in \Z_p^2}$ where $h_{(a,b)}(m) = am+b \pmod{p}$ is pairwise independent.
    \end{framedprop}

    \begin{proof}
        Fix $m, m' \in \Z_p$ with $m \neq m'$ and $\tau, \tau' \in \Z_p$. We'll show that the joint probability over the hash functions is uniform. First, we observe that:
        \[\begin{split}
            \Pr_{(a,b) \in_R \Z_p^2}[h_{(a,b)}(m) = \tau, h_{(a,b)}(m') = \tau']
            &= \Pr_{(a,b) \in_R \Z_p^2}[am+b = \tau, am' + b = \tau'] \\
            &= \Pr_{(a,b) \in_R \Z_p^2}\sbk{\rmat{m & 1 \\ m' & 1}\rmat{a \\ b} = \rmat{\tau \\ \tau'}}
        \end{split}\]

        Since $m \neq m'$, we know that $\det\rmat{m & 1 \\ m' & 1} = m-m' \neq 0$, thus the matrix is invertible. This allows us to further manipulate the event and rewrite it in terms of the key:
        \[\begin{split}
            \Pr_{(a,b) \in_R \Z_p^2}[h_{(a,b)}(m) = \tau, h_{(a,b)}(m') = \tau']
            &= \Pr_{(a,b) \in_R \Z_p^2}\sbk{\rmat{m & 1 \\ m' & 1}\rmat{a \\ b} = \rmat{\tau \\ \tau'}} \\
            &= \Pr_{(a,b) \in_R \Z_p^2}\sbk{\rmat{a \\ b} = \rmat{m & 1 \\ m' & 1}^{-1} \rmat{\tau \\ \tau'}} \\
            &= \frac{1}{\abs{\Z_p^2}}
        \end{split}\]

        concluding that $\mathcal{H}$ is pairwise independent.
    \end{proof}

    \section{Randomness extraction}

    We discussed how randomness can be used to generate secret keys over a probability distribution, but how a random value be generated? As we will discuss later, randomness is a \underline{crucial} concept for secure cryptography. Clearly, there is no such concept as \textit{real randomness}: even if the whole universe may appear chaotic, everything is technically deterministic. The process of generating a random-enough value is called \textbf{randomness extraction} and it is typically achieved by measuring physical quantities (noise, air humidity, \dots) in order to produce a short unpredictable sequence of bits (e.g. 256 bits), which is expensive to generate and not necessarly uniform. This short \curlyquotes{truly random} sequence gets usually expanded to any desired length -- as long as it is polynomial with respect to the original lenght -- through the use of a \textbf{pseudorandom generator (PRG)}. However, this process requires some strict computational assumptions, which we'll discuss later.
    
    For now, we'll focus on understanding how to extract randomness from an unpredictable secure variable $X$. The first \textbf{extractor} that sparked the idea is Von Neumann's Extractor, which yields a fair random coin from an unpredictable unfair one. Let $B \in \{0,1\}$ be the random variable describing the unpredictable unfair coin, where $\Pr[B = 0] = p < \frac{1}{2}$. Let $Y \in \{0,1\}$ be the random variable describing our new coin. The value of $Y$ is determined by the following procedure. Sample two values $b_1, b_2$ from $B$ at different times. If $b_1 = b_2$, $Y$ assumes no value (marked as $Y = ?$) and we repeat the sampling process. If not, $Y = 1$ if $b_1 = 0$ and $b_2 = 1$, otherwise $Y = 0$ if $b_1 = 1$ and $b_2 = 0$.

    If the sampling process succeeds, i.e. $Y$ assumes a value, we have that $\Pr[Y = 0] = p(1-p)$ and $\Pr[Y = 1] = (1-p)p$, thus $\Pr[Y = 0] = \Pr[Y = 1]$. Moreover, we observe that the probability of $Y= = \, ?$ being true for $m$ consecutive tries is at most:
    \[\Pr[Y = ? \text{ for $m$ tries}] = (\Pr[Y= = \, ?])^m = (1-\Pr[Y = 0 \cup Y = 1])^m \leq (1-2p(1-p))^m\] 

    As $m$ grows to infinity, the latter probability goes to 0, making the even negligible. Implying that $\Pr[Y=0]$ and $\Pr[Y = 1]$ tend to be $\frac{1}{2}$ due to them having the same probability and them being the only two outcomes. This makes $Y$ a fair enough coin.

    Our goal is to generalize this concept to any desider value, i.e. design an extractor $\mathrm{Ext}$ that uses a random variable $X$ to output any desided uniform distribution $\mathrm{Ext}(X)$. A good eye may recognize that this is clearly impossible to achieve unless the source is already truly unpredictable, which makes the extractor useless since we already have a truly random source. As for many computational and probabilistic concepts, we can only hope to achieve something that is good-enough for our purposes. This goodness is measured through a concept known as \textbf{min-entropy}, that being the largest value $m$ having the property that each observation of $X$ provides at least $m$ bits of information.

    \begin{frameddefn}{Min-entropy}
        Given a random variable $X$, we define the min-entropy of $X$ as:
        \[H_{\infty}(X) = - \log \max_{x} \Pr[X = x]\]
    \end{frameddefn}

    One way to justify the name of this operator is to compare it with the more standard definition of \textit{entropy}, i.e. the expected value of $\log\rbk{\frac{1}{p_i}}$ over a distribution $P$:
    \[H(P) = \sum_{i} p_i \log\rbk{\frac{1}{p_i}}\]

    In the min-entropy, instead, we take take the minimum value of $\log\rbk{\frac{1}{p_i}}$, where:
    \[\min_i \log\rbk{\frac{1}{p_i}} = \log \rbk{\min_i \frac{1}{p_i}} = - \log \max_i p_i\]

    Consider a random variable $X \sim \mathcal{U}_n$, where $\mathcal{U}_n$ is an uniform distribution over $\{0,1\}^n$. In this case, we have that $\Pr[X = x] = 2^{-n}$ for all value $x$ assumable by $X$. Hence, the min-entropy is:
    \[H_{\infty}(X) = -\log \max_x \Pr[X = x] = - \log (2^{-n}) = n\]

    This concludes that each observation of $X$ provides at least $n$ bits of information. Consider now a constant random variable $X'$, i.e. $\Pr[X' = x^*] = 1$ for some fixed value $x^*$ and $\Pr[X' = x] = 0$ for every $x^* \neq x$. It's easy to see that:
    \[H_{\infty}(X') = -\log \max_{x'} \Pr[X' = x] = -\log 1 = 0\]

    Concluding that each observation of $X'$ provides at least 0 bits of information. This information-theoretic measure opens a new question regarding extractors: is there an extractor $\mathrm{Ext}^*$ that for any random variable $X$ outputs an uniform distribution $Y = \mathrm{Ext}(X)$ such that $H_{\infty}(X) \geq k$ for some value $k > 0$? Even under these constraints, the answer to this question is still negative. In particular, it's impossible to define such extractor even if we restrict our interest to extracting only one bit while revealing at least one bit less than the input length.
    
    \begin{framedprop}{}
        There is no extractor $\mathrm{Ext}$ such that for every random variable $X$ over $\{0,1\}^n$ with $H_{\infty}(X) \geq n-1$ it holds that $\mathrm{Ext}(X)$ is a uniform distribution over $\{0,1\}$.
    \end{framedprop}

    \begin{proof}
        Let $\mathrm{Ext} : \{0,1\}^n \to \{0,1\}$ be any extractor and let $b \in \{0,1\}$ be the output maximizing the cardinality of the preimage of the extractor, i.e. the set of inputs for which the extractor outputs $b$.
        \[b = \argmax_{b' \in \{0,1\}} \abs{\mathrm{Ext}^{-1}(b)}\]

        By the pidgeonhole principle, we have that $\abs{\mathrm{Ext}^{-1}(b)} \geq \frac{\abs{\{0,1\}^n}}{2} = 2^{n-1}$. Let $X$ be a random variable uniform over $\mathrm{Ext}^{-1}(b)$. Since $X$ is uniform, we have that $H_{\infty}(X) \geq n-1$. However, we know that $\mathrm{Ext}(X)$ isn't uniform due to the output being always $b$. This concludes that any extractor has always a bad input uniform random variable $X$ that returns a non-uniform distribution $\mathrm{Ext}(X)$.
    \end{proof}

    Since we can't define an extractor that yields an uniform distribution, the best we can hope for is a distribution that is close enough to an uniform one. We use a standard measure for distribution similarity, called \textbf{$\varepsilon$-closeness}.

    \begin{frameddefn}{$\varepsilon$-closeness}
        Let $X,X'$ be two random variables defined over the same set. We say that $X$ and $X'$ are $\varepsilon$-close, written as $X \sim_\varepsilon X'$ when their \textit{statistical distance} $\mathrm{SD}(X; X')$ is at most $\varepsilon$, where:
        \[\mathrm{SD}(X; X') = \frac{1}{2} \sum_{x} \abs{\Pr[X=x] - \Pr[X'=x]}\]
    \end{frameddefn}

    The concept of $\varepsilon$-closeness between two random variables is equivalent to saying that every \textit{unbounded adversary} $A$, i.e. an entity with unlimited computational power that wants to break our system, cannot distinguish whether a value $x$ has been sampled from $X$ or $X'$.
    \[\abs{\Pr[A(x) = 1 : x \in_R X] - \Pr[A(x) = 1 : x \in_R X']} \leq \varepsilon\]
    
    \begin{frameddefn}{Deterministic extractor}
        Let $S$ be a random variable, referred to as \textit{seed}. We say that $\mathrm{Ext} : \{0,1\}^d \times \{0,1\}^n \to \{0,1\}^\ell$ is a $(k,\varepsilon)$-extractor if for every random variable $X$ with $H_{\infty}(X) \geq k$ it holds that $(S, \mathrm{Ext}(S,X)) \sim_{\varepsilon} (S, \mathcal{U}_\ell)$ when $S \sim \mathcal{U}_d$.
    \end{frameddefn}

    We notice that the condition $(S, \mathrm{Ext}(S,X)) \sim_{\varepsilon} (S, \mathcal{U}_\ell)$ implies that the seed must be \textit{public}. This requirement is forced in order to avoid trivial extractors such as $\mathrm{Ext}(S,X) = S$. The idea is to use a source of randomness with high min-entropy in order to force that the output of a \curlyquotes{good} hash function is statistically close to uniform -- even if the input seed wasn't. This result is known as the \textbf{left-over hash lemma}.
    
    The goodness of the hash functions is measured in terms of \textbf{low collision probability}, defined as the expected value that a random variable $Y$ over a set $\mathcal{Y}$ and a copy $Y'$ of $Y$, giving two i.i.d (identical and independend distributions) variables, return the same result:
    \[\mathrm{Col}(Y) = \Pr[Y = Y']\]

    In particular, we observe that since $Y$ and $Y'$ are i.i.d. we get that:
    \[\begin{split}
        \mathrm{Col}(Y) &= \Pr[Y = Y'] = \sum_{y \in \mathcal{Y}} \Pr[Y = y, Y' = y] = \sum_{y \in \mathcal{Y}} \Pr[Y = y]^2 \\
    \end{split}\]
    
    Before proving the left-over hash lemma, we prove the following relationship commonly used in statistical analysis.

    \begin{framedprop}{}
        Let $Y$ be a random variable over a set $\mathcal{Y}$ and assume $\mathrm{Col}(Y) = \frac{1}{\abs{\mathcal{Y}}}(1+4\varepsilon^2)$ for some $\varepsilon \in \R_{> 0}$. Then, it holds that $\mathrm{SD}(Y, U) \leq \varepsilon$, where $U$ is the uniform distribution over $\mathcal{Y}$.
    \end{framedprop}

    \begin{proof}
        We start by observing that:
        \[\mathrm{SD}(Y; U) = \frac{1}{2} \sum_{y \in \mathcal{Y}} \abs{\Pr[Y = y] - \Pr[U = y]} = \frac{1}{2} \sum_{y \in \mathcal{Y}} \abs{\Pr[Y = y] - \frac{1}{\mathcal{Y}}}\]

        For each $y \in \mathcal{Y}$, fix $q_y = \Pr[Y = y] - \frac{1}{\abs{Y}}$ and $s_y = \mathrm{sign}(q_y)$. Let $\vec q = \smat{q_{y_1} & \cdots & q_{y_{\abs{Y}}}}$ and  $\vec s = \smat{s_{y_1} & \cdots & s_{y_{\abs{Y}}}}$. We notice that:
        \[\mathrm{SD} = \frac{1}{2} \sum_{y \in \mathcal{Y}} \abs{\Pr[Y = y] - \frac{1}{\mathcal{Y}}} = \frac{1}{2} \sum_{y \in \mathcal{Y}} q_y s_y = \frac{1}{2} \abk{\vec q, \vec s}\]

        By the \textit{Cauchy-Swartz inequality} (which we won't prove), we get that:
        \[\mathrm{SD}(Y; U) = \frac{1}{2} \abk{\vec q, \vec s} \leq \frac{1}{2} \sqrt{\abk{\vec q, \vec q} \abk{\vec s, \vec s}} = \frac{1}{2} \sqrt{\rbk{\sum_{y \in \mathcal{Y}} q_y^2} \abs{\mathcal{Y}}}\]

        \textbf{Claim}: $\displaystyle \sum_{y \in \mathcal{Y}} q_y^2 \leq \frac{4\varepsilon^2}{\abs{Y}}$

        \begin{proof}[Proof of the claim]
            Through algebraic manipulation we get that:
            \[\begin{split}
                \sum_{y \in \mathcal{Y}} q_y^2 &= \sum_{y \in \mathcal{Y}} \rbk{\Pr[Y = y] - \frac{1}{\mathcal{Y}}}^2 \\
                &= \sum_{y \in \mathcal{Y}} \rbk{\Pr[Y = y]^2 - \frac{2}{\abs{\mathcal{Y}}} \Pr[Y = y] + \frac{1}{\abs{\mathcal{Y}}^2}} \\
                &= \sum_{y \in \mathcal{Y}} \Pr[Y = y]^2 - \sum_{y \in \mathcal{Y}} \frac{2}{\abs{\mathcal{Y}}} \Pr[Y = y] + \sum_{y \in \mathcal{Y}} \frac{1}{\abs{\mathcal{Y}}^2} \\
                &= \sum_{y \in \mathcal{Y}} \Pr[Y = y]^2 - \frac{2}{\abs{\mathcal{Y}}} + \frac{1}{\abs{\mathcal{Y}}} \\
                &= \mathrm{Col}(Y) - \frac{1}{\mathcal{Y}}
            \end{split}\]

            By hypothesis, we know that the collision probability of $Y$ is bounded, concluding that:
            \[\sum_{y \in \mathcal{Y}} q_y^2 = \mathrm{Col}(Y) - \frac{1}{\mathcal{Y}} \leq \frac{1}{\abs{\mathcal{Y}}}(1+4\varepsilon^2) - \frac{1}{\mathcal{Y}} = \frac{4\varepsilon^2}{\abs{\mathcal{Y}}}\]
        \end{proof}

        Through the claim we directly conclude that:
        \[\mathrm{SD}(Y;U) = \frac{1}{2} \sqrt{\rbk{\sum_{y \in \mathcal{Y}} q_y^2} \abs{\mathcal{Y}}} \leq \frac{1}{2} \sqrt{\frac{4\varepsilon^2}{\abs{\mathcal{Y}}} \abs{Y}} = \varepsilon\]
    \end{proof}

    \begin{framedlem}{Left-over hash lemma}
        Let $\mathcal{H} = \{h_S : \{0,1\}^n \to \{0,1\}^\ell\}_{S \in \{0,1\}^d}$ be a pairwise independent hash function. Let $X$ be a random variable with $H_{\infty}(X) \geq k$. Then, for each $S \in \{0,1\}^d$ it holds that $\mathrm{Ext}(S,X) = h_S(X)$ is a $(k,\varepsilon)$-extractor for $k \geq \ell + 2\log \rbk{\frac{1}{\varepsilon}} - 2$
    \end{framedlem}

    \begin{proof}
        Fix $S \in \{0,1\}^d$ and two random variables $X,X'$. Let $Y = (S, \mathrm{Ext}(S,X))$ and let $Y'$ be a copy of $Y$ defined as and $Y' = (S', \mathrm{Ext}(S',X'))$. We observe that:
        \[\begin{split}
            \mathrm{Col}(Y) &= \Pr[Y = Y'] \\
            &= \Pr[S = S', \mathrm{Ext}(S,X) = \mathrm{Ext}(S',X')] \\
            &= \Pr[S = S', h_S(X) = h_{S'}(X')]
        \end{split}\]

        Since $S$ and $S'$ are independend from $X$ and $X'$, we get that:
        \[\begin{split}
            \mathrm{Col}(Y) &= \Pr[S = S', h_S(X) = h_{S'}(X')] \\
            &= \Pr[S = S', h_S(X) = h_S(X')] \\
            &= \Pr[S = S'] \cdot \Pr[h_S(X) = h_{S}(X')] \\
            &= 2^{-d} \cdot \Pr[h_S(X) = h_{S}(X')] \\
            &= 2^{-d} \cdot (\Pr[X = X', h_S(X) = h_{S}(X')] + \Pr[X \neq X', h_S(X) = h_{S}(X')])\\
            &= 2^{-d} \cdot (\Pr[X = X'] + \Pr[X \neq X', h_S(X) = h_{S}(X')])\\
            &= 2^{-d} \cdot (\Pr[X = X'] + 2^{-\ell})\\
        \end{split}\]
        
        By hypothesis, we know that $H_{\infty}(X) \geq k$. This implies that $\Pr[X = X'] \leq 2^{-k}$ since an adversary cannot guess whether a sample comes from $X$ or $X'$ with probability greater than $2^{-k}$.
        \[\begin{split}
            \mathrm{Col}(Y) &= 2^{-d} \cdot (\Pr[X = X'] + 2^{-\ell})\\
            &= 2^{-d} \cdot (2^{-k} + 2^{-\ell})\\
            &= \frac{1}{2^{d+\ell}}(2^{\ell-k} + 1)
        \end{split}\]

        Finally, by hypothesis we have conclude that:
        \[\mathrm{Col}(Y) \leq \frac{1}{2^{d+\ell}}(2^{\ell-k} + 1) \leq \frac{1}{2^{d+\ell}}\rbk{2^{2-2\log \rbk{\frac{1}{\varepsilon}}} + 1} = \frac{4\varepsilon^2 + 1}{\abs{\mathcal{Y}}}\]
    \end{proof}

    \section{Solved exercises}

    \begin{framedprob}{}
        Prove or disprove: a SKE $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ has perfect secrecy if and only if $\forall c_0, c_1 \in \mathcal{C}$ it holds that $\Pr[C = c_0] = \Pr[C = c_1]$.
    \end{framedprob}

    \begin{proof}
        The claim is false. To disprove it, consider the SKE $\Pi$ such that $\mathcal{M} = \mathcal{K} = \{0,1\}^n$ and $\mathcal{C} = \{0,1\}^{n+1}$, where:
        \[\mathrm{Enc}(K, m) = 0 \mid\mid (m \oplus K) \qquad \mathrm{Dec}(K, b \mid\mid \widehat c) = \widehat c \oplus K\]

        Let $(*)$ be the special property of the claim. We observe that $\forall \widehat c \in \{0,1\}^n$ it holds that $\Pr[C = 1 \mid\mid \widehat C] = 0$, while $\Pr[C = 0 \mid\mid \widehat C] > 0$, implying that $(*)$ doesn't hold. Nonetheless, we can easily prove that $\Pi$ has perfect secrecy.

        Fix any pair $m, c \in \mathcal{M} \times \mathcal{C}$ such that $\Pr[C = c] > 0$. Since $\Pr[C = c] > 0$, it must hold that $c = 0 \mid\mid \widehat c$ for some $\widehat c \in \{0,1\}^n$. Therefore, we have that:
        \[\Pr[M = m \mid C = c] = \Pr[M = m \mid C = 0 \mid\mid \widehat c] = \Pr[M = m]\]
        since $c = 0 \mid\mid \widehat c$ doesn't reveal any additional information for the plaintext due to how $\mathrm{Enc}$ is defined.
    \end{proof}

    \begin{framedprob}{}
        Prove that a SKE $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ has perfect secrecy if and only if for every unbounded adversary $A$ it holds that $\mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 0) \equiv \mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 1)$
    \end{framedprob}

    \begin{proof}
        We observe that $\mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 0) \equiv \mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 1)$ if and only if for all adversaries $A'$ it holds that:
        \[\begin{split}
            & \abs{\Pr[A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 0)] - \Pr[A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 1)]} = 0 \\
            \iff & \Pr[A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 0)] = \Pr[A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi,A}^{\text{1-time}}(\lambda, 1)] = \frac{1}{2}
        \end{split}\]

        This can happen if and only if for all choices $m_0,m_1 \in \mathcal{M}$ made by any adversary $A$ and for all possible answers $c^* \in \mathcal{C}$ that may be returned by the challenger it holds that $\Pr[\mathrm{Enc}(K,m_0) = c^*] = \Pr[\mathrm{Enc}(K, m_1) = c^*]$, which is exactly the third equivalent definition of perfect secrecy.
    \end{proof}

    \begin{framedprob}{}
        Given a prime $p \in \Primes$, let $\mathcal{M} = \mathcal{T} = \Z_p$ and $\mathcal{K} = \Z_p^2$. Prove that the hash family $\mathcal{H} = \{h_{(a,b)}\}_{(a,b) \in \Z_p^2}$, where $h_{(a,b)}(m) = am+b \pmod{p}$, cannot be a $2$-time statistically secure MAC
    \end{framedprob}

    \textit{Solution}. Suppose that the values $h_{(a,b)}(m_1) = \tau_1$ and $h_{(a,b)}(m_2) = \tau_2$, where $m_1 \neq m_2$, are known to an adversary. Hence, the adversary knows that:
    \[\soe{l}{
        am_1 + b \equiv \tau_1 \pmod{p} \\
        am_2 + b \equiv \tau_2 \pmod{p} \\
    } \implies a(m_1-m_2) \equiv \tau_1 - \tau_2 \pmod{p}\]

    Since $m_1 \neq m_2$, the value $m_1 - m_2$ is invertible in $\Z_p$, the value $Q \equiv (\tau_1 - \tau_2)(m_1 - m_2)^{-1} \pmod{p}$ (notice that $a \equiv Q \pmod{p}$) can be easily computed. Once $Q$ is known, $b$ can be easily computed as $b \equiv \tau_1 - Qm_1 \pmod{p}$. Therefore, we have that:
    \[\Pr_{K \in_R \Z_p}[h_K(m) = \tau \mid h_K(m_1) = \tau_1, h_K(m_2) = \tau_2] = \soe{ll}{
        1 & \text{if } \tau \equiv Q(m -m_1)+ \tau_1 \pmod{p} \\
        0 & \text{otherwise}
    }\]
    
    making $\mathcal{H}$ statistically insecure for every value $0 < \varepsilon < 1$.

    \begin{framedprob}{}
        Construct a 3-wise independent hash function family and prove it's correctness.
    \end{framedprob}

    \textit{Solution}. Given a prime $p \in \Primes$, let $\mathcal{M} = \mathcal{T} = \Z_p$ and $\mathcal{K} = \Z_p^3$. Consider the hash family $\mathcal{H} = \{h_{(a,b,c)}\}_{(a,b,c) \in \Z_p^2}$, where $h_{(a,b,c)}(m) = am^2+bm+c \pmod{p}$.

    Fix three values $x_1, x_2, x_3 \in \Z_p$ such that $x_i \neq x_j$ for all $i \neq j$. Fix $\tau_1, \tau_2, \tau_3 \in \Z_p$. We observe that:
    \[\begin{split}
        \Pr_{(a,b,c) \in_R \Z_p^3}\smat{\begin{array}{c}
            h_{(a,b,c)}(x_1) = \tau_1\\
            h_{(a,b,c)}(x_2) = \tau_2\\
            h_{(a,b,c)}(x_3) = \tau_3
        \end{array}} &= \Pr_{(a,b,c) \in_R \Z_p^3}\smat{\begin{array}{c}
            ax_1^2+bx_1+c = \tau_1\\
            ax_2^2+bx_2+c = \tau_2\\
            ax_3^2+bx_3+c = \tau_3
        \end{array}}  \\
        &= \Pr_{(a,b,c) \in_R \Z_p^3} \sbk{\rmat{x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \\ x_3^2 & x_3 & 1} \rmat{a \\ c \\ b} = \rmat{\tau_1 \\ \tau_2 \\ \tau_3}}
    \end{split}\]

    Since $x_i \neq x_j$ for all $i \neq j$, we observe that the matrix is invertible:
    \[\begin{split}
        \det \rmat{x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \\ x_3^2 & x_3 & 1} &= x_1^2 \det \rmat{x_2 & 1 \\ x_3 & 1} - x_2^2 \det \rmat{x_1 & 1 \\ x_3 & 1} + x_3^2 \det \rmat{x_1 & 1 \\ x_2 & 1} \\
        &= x_1^2x_2 - x_1^2x_3 - x_2^2x_1 + x_2^2 x_3 + x_3^2 x_1 - x_3^2x_2 \\
        &= (x_1-x_2)(x_1-x_3)(x_2-x_3) \\
        &\neq 0
    \end{split}\]

    concluding that $\mathcal{H}$ is 3-wise independent:

    \[\begin{split}
        \Pr_{(a,b,c) \in_R \Z_p^3}\smat{\begin{array}{c}
            h_{(a,b,c)}(x_1) = \tau_1\\
            h_{(a,b,c)}(x_2) = \tau_2\\
            h_{(a,b,c)}(x_3) = \tau_3
        \end{array}} &= \Pr_{(a,b,c) \in_R \Z_p^3} \sbk{\rmat{x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \\ x_3^2 & x_3 & 1} \rmat{a \\ c \\ b} = \rmat{\tau_1 \\ \tau_2 \\ \tau_3}}\\
        &= \Pr_{(a,b,c) \in_R \Z_p^3} \sbk{\rmat{a \\ c \\ b} = \rmat{x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \\ x_3^2 & x_3 & 1}^{-1} \rmat{\tau_1 \\ \tau_2 \\ \tau_3}} \\
        &= \frac{1}{\abs{\Z_p^3}}
    \end{split}\]

    \chapter{Computational security}

    \section{One-way functions and Impagliazzo's worlds}

    In the previous chapter we discussed how, without computational assumptions, symmetric encryption and random generation can be achieved with some strong limitations. For privacy and integrity, we saw how the message length and the key length must be at least equal in order to guarantee a 1-time security (no guarantees for $t$-time security with $t > 1$). For randomness, instead, we saw how we can't extract more than $k$ bits when the min-entropy is $k$.

    Our next objective is to overcome all these limitation (or at least reduce them). This can be achieved at the price of two very strong assumptions:
    \begin{enumerate}
        \item The adversary is \textit{computationally bounded}, i.e. it has limited resources
        \item There are \textit{hard} problems
    \end{enumerate} 
    
    In other words, we'll prove results of the following form. \curlyquotes{if problem $X$ is hard against efficient solvers then cryptosystem $\Pi$ is secure against efficient adversary}. The direct consequence of these results is their contrapositive: if the system $\Pi$ were ever to be proved insecure, there would be an efficient solution for problem $X$. Depending on the \curlyquotes{level of hardness} of problem $X$, this would have groundbreaking consequences (e.g. $X$ could be a problem that implies $\mathsf{P} = \mathsf{NP}$, factoring is easy, discrete-log is easy, \dots).

    Someone may ask \curlyquotes{can't we just assume that $\mathsf{P} \neq \mathsf{NP}$ and prevent all of this?} The answer is \textit{yes}, we could, but this assumption would be \textit{too weak} for our necessities. In fact, to achieve \underline{good security} we require something that isn't directly implied by $\mathsf{P} \neq \mathsf{NP}$: the existence of \textbf{one-way functions (OWF)}. In simple terms, a functions $f : \{0,1\}^n \to \{0,1\}^m$ is said to be \textit{one-way} when it can be efficiently computed but hard to invert (meaning that $f^{-1}$ cannot be efficiently computed).
    
    It's easy to see that assuming the existence of OWFs implies that $\mathsf{P} \neq \mathsf{NP}$. This is due to the contrapositive statement: if $\mathsf{P} = \mathsf{NP}$ then OTWs cannot exist since for each function $f$ we can efficiently verify if $f(x) = y$ in order to compute $f^{-1}(y) = x$. However, the converse statement doesn't hold: assuming $\mathsf{P} \neq \mathsf{NP}$ could still imply that OWFs don't exist. In the article \href{https://dx.doi.org/10.1109/SCT.1995.514853}{A Personal View of Average-Case Complexity} presented at the 1995 Complexity Conference, Russell Impagliazzo describes \textbf{five possible worlds} that we may be living in and their implications to computer science:
    \begin{itemize}
        \item \textit{Algorithmica}: there are no hard problems ($\mathsf{P} = \mathsf{NP}$) and OWFs cannot exist.
        \item \textit{Heuristica}: there are hard problems ($\mathsf{P} \neq \mathsf{NP}$) but can they solved efficiently on average
        \item \textit{Pessiland}: there are hard problems ($\mathsf{P} \neq \mathsf{NP}$) but OWFs don't exist
        \item \textit{Minicrypt}: one-way functions exist but public-key cryptography is impossible 
        \item \textit{Cryptomania}: one-way functions exist and public-key cryptography is possible
    \end{itemize}

    Impagliazzo does not guess which world we live in, but describes \textit{Pessiland} as the worst one out of all possible worlds: in this scenario, not only can we not solve hard problems on average but we apparantly do not get any cryptographic advantage from the hardness of these problems. Today, most computer scientists believe (and hope) that our world corresponds to either \textit{Cryptomania} or \textit{Minicrypt}.

    All the results will be discussed with respect to a fixed standard computational model: the \textit{Turing machine} (TM). From now on, \textit{efficient computation} will refer to a computation made under a polynomial amount of steps with respect to the input size, i.e. if the input $x$ has length $n$ then $\poly(n)$-time referres to at most $n^{c}$ steps, for some $c \in \N$.
    
    As mentioned before, the adversary we'll also assume that the adversary has limited resources. However, we'll be generous with them: the adversary is allowed to use a $\poly(n)$ amount of time and a $\poly(n)$ amount of \textit{random bits}. This implies that their computational model is a \textit{probabilistic polynomial time (PPT)} TM. To formally define the concept of OWFs, we'll use \textbf{negligible functions}, i.e. functions $\varepsilon : \N \to \R$ such that $\forall c \in \N$ there is a value $\lambda_0 \in \N$ for which $\forall \lambda > \lambda_0$ it holds that
    \[\varepsilon(\lambda) < \frac{1}{\lambda^c}\]

    \begin{frameddefn}{One-way functions}
        We say that a function $f : \{0,1\}^n \to \{0,1\}^m$ is a one-way function if:
        \begin{itemize}
            \item $f$ is computable in $\poly(n)$-time
            \item for every PPT algorithm $A$ there is a negligible function $\negl(n)$ such that:
            \[\Pr_{x \in_R \mathcal{U}_n}[f(x') = y : y = f(x); x' \gets A(y)] \leq \negl(n)\]
        \end{itemize}

        \textit{Note}: $x \gets A(y)$ is read as \curlyquotes{$x$ is the output of $A(y)$}
    \end{frameddefn}

    \section{Pseudorandom generators}

    In the previous chapter we discussed how randomness is essential for good cryptography, even though true randomness cannot be achieved. To get a good enough approximation of true randomness, modern systems use \textbf{pseudo-randomness}, i.e. efficient deterministic algorithms that generate a sequence of bits that look unpredictable but aren't truly random. By definition, if enough time and resources are permitted an adversary is clearly able to learn how a pseudorandom algorithm work through bruteforce. Hence, we'll restrict that our adversary doesn't have such time and resources, i.e. their computation must also be efficient.
    
    The idea behind pseudo-randomness is to generate a \curlyquotes{non-random} probability distribution (this doesn't really mean anything, but you get the idea) that is \textbf{computationally indistinguishable} from a truly random probability distribution. The formal definition of computational indistinguishability refers to \textit{probability ensembles}, i.e. infinite sequences of probability distributions $\mathcal{X} = \{X_n\}_{n \in \N}$, where each $X_n$ is a distribution over $\{0,1\}^n$. This multi-length definition takes into account the fact that the distribution may change over the input length.

    \begin{frameddefn}{Computational indistinguishability}
        Let  $\mathcal{X} = \{X_n\}_{n \in \N}$ and  $\mathcal{Y} = \{Y_n\}_{n \in \N}$ be two probability ensambles. We say that $\mathcal{X}$ and $\mathcal{Y}$ are computationally indistinguishable, written as $\mathcal{X} \approx_c \mathcal{Y}$, if for every PPT algorithm $D$ there is a negligible function $\negl(n)$ such that $\forall n \in \N$ it holds that:
        \[\abs{\Pr[D(z) = 1 : z \in_R X_n] - \Pr[D(z) = 1 : z \in_R Y_n]} \leq \negl(n)\]
    \end{frameddefn}

    We observe that we already discussed a definition similar to the one above when we talked about $\varepsilon$-closeness. In fact, this can be seen as a practical computational counterpart of that definition (notice how the adversary is now restricted to PPT algorithms and the distributions vary over input lengths). Moreover, it's esy to see that the relation $\approx_c$ is \textit{transitive}, i.e. if $\mathcal{X} \approx_c \mathcal{Y}$ and $\mathcal{Y} \approx_c \mathcal{Z}$ then $\mathcal{X} \approx_c \mathcal{Z}$, and closed under \textit{reductions}, i.e. for any PPT function $f$ if $\mathcal{X} \approx_c \mathcal{Y}$ then $f(\mathcal{X}) \approx_c f(\mathcal{Y})$.

    \begin{frameddefn}{Pseudorandom generator (PRG)}
        Let $G : \{0,1\}^n \to \{0,1\}^{n+\ell}$ be a function where $\ell \geq 1$. We say that $G$ is a pseudorandom generator (PRG) with expansion factor $\ell$ if:
        \begin{itemize}
            \item $G$ can be computed in $\poly(n)$-time
            \item $G(\mathcal{U}_n) \approx_c \mathcal{U}_{n+\ell}$.
        \end{itemize}
    \end{frameddefn}

    The above definition requires a bit of discussion in order to be fully understood. First, we observe that $G$ is \underline{deterministic}, making $G(\mathcal{U}_n) \approx_c \mathcal{U}_{n+\ell}$ a very strong property. Then, we observe that the idea behind a PRG is to generate new bits that look like they are truly random, as long as the initial seed is already truly random (in practice, we use a seed that is computationally indistinguishable from a truly random one). Hence, we can use a randomness extractor to get an initial seed and then pass it to a PRG in order to \curlyquotes{expand the seed} up to any desired length: after constructing a simple PRG $G : \{0,1\}^n \to \{0,1\}^{n+1}$, it can be used to construct a new PRG $G : \{0,1\}^n \to \{0,1\}^{n+\ell}$ up to any polynomial value $\ell > 0$.

    To prove this result, we use a technique known as \textbf{hybrid argument}. We define a sequence $H_0, \ldots, H_\ell$ of distributions such that:
    \begin{itemize}
        \item $H_0 \approx_c G^\ell(\mathcal{U}_n)$ and $H_\ell \approx_c \mathcal{U}_{n+\ell}$
        \item $H_i \approx_c H_{i+1}$ for each $i \in [0, \ell-1]$
        \item Each $H_i$ is an hybrid of truly random and pseudorandom bits, where the number of truly random bits increases as $i$ grows, getting replacing the pseudorandom bits
    \end{itemize}

    \begin{framedthm}{PRG stretching}
        Let $G : \{0,1\}^n \to \{0,1\}^{n+1}$ be a PRG. Then, for any $\ell = \poly(n)$ there is a PRG $G^\ell : \{0,1\}^n \to \{0,1\}^{n+\ell}$.
    \end{framedthm}

    \begin{proof}
        For each $i \in [0, \ell]$, let $f_i(s) = b_1 \concat \ldots \concat b_\ell \concat s_\ell$ -- where $\concat$ is the string concatenation operator -- be the function computed through the following algorithm (see \Cref{prg_stretching_1}).

        $f_i$ = "On input $s \in \{0,1\}^n$: 
        \begin{enumerate}
            \item Set $s_i = s$
            \item For each $j \leq i$ set $b_j$ as a truly random bit, i.e. $b_j \in_R \mathcal{U}_1$
            \item For each $j > i$ set $s_j$ and $b_j$ as the values such that $G(s_{j-1}) = s_j \concat b_j$, where $s_j \in \{0,1\}^n$ and $b_j \in \{0,1\}$.
            \item Return $b_1 \mid\mid \ldots \mid\mid b_\ell \mid\mid s_\ell$.
        \end{enumerate}

        It's easy to see that each $i \in [0,\ell]$ the function $f_i$ is PPT-computable since $G$ is a PPT algorithm by hypothesis and $\ell = \poly(n)$. Moreover, $f_0$ is deterministic since it doesn't generate random bits. Let $H_0, \ldots, H_\ell$ be the distributions over the outputs of the functions $f_0, \ldots, f_\ell$. We observe that $H_\ell = \mathcal{U}_{n+\ell}$ since every bit is truly random in $f_\ell$. Hence, by letting $G^\ell = f_0$ we get that $H_0 = G^\ell(\mathcal{U}_n)$ and $H_\ell = \mathcal{U}_{n+\ell}$.

        \textbf{Claim:} For each $i \in [0,\ell-1]$ it holds that $H_i \approx_c H_{i+1}$.

        \begin{proof}[Proof of the claim.]
            Fix $i \in [0,\ell-1]$. We prove the claim by reducing the distinguishability of $G(\mathcal{U}_n)$ from $\mathcal{U}_{n+1}$ to the distinguishability of $H_i$ from $H_{i+1}$. By way of contradiction, suppose that $H_i \not\approx_c H_{i+1}$, meaning that there is a distinguisher $D_i$ such that:
            \[\abs{\Pr[D_i(z) = 1 : z \in_R H_i] - \Pr[D_i(z) = 1 : z \in_R H_{i+1}]} > \frac{1}{n^c}\]
            for some $c > 0$. Let $D$ be the algorithm defined as follows:

            $D$ = "On input $w \in \{0,1\}^{n+1}$:
            \begin{enumerate}
                \item Set $s$ and $b$ as the values such that $w = s \concat b$, where $s \in \{0,1\}^n$ and $b \in \{0,1\}$.
                \item Compute $f_{i+1}(s) = b_1 \concat \ldots \concat b_\ell \concat s_\ell$.
                \item Set $z' = b_1 \concat \ldots \concat b_{i-1} \concat \, b\,  \concat b_{i+1} \concat \ldots \concat b_\ell \concat s_\ell$.
                \item Return $D_i(z_w)$."
            \end{enumerate}

            We observe that $H_i$ and $H_{i+1}$ differ from each other only by one bit: in $f_i$, the bit $b_i$ is the last bit of $G(s_{i-1})$, while in $f_{i+1}$ it is a truly random bit. Hence, when $D$ is given an input $w \in G(\mathcal{U}_n)$, the last bit of $w$ is generated through $G$, making the string $z'$ built by $D$ an input sampled from $H_i$. Vice versa, when $D$ is given an input $w \in_R \mathcal{U}_{n+1}$, the last bit of $w$ is truly random, making the string $z'$ built by $D$ an in input sampled from $H_{i+1}$. Therefore, we get that:
            \[\begin{split}
                &\abs{\Pr[D(w) = 1 : w \in_R G(\mathcal{U}_n)] - \Pr[D(w) = 1 : w \in_R \mathcal{U}_{n+1}]}\\
                = &\abs{\Pr[D_i(z') = 1 : z' \in_R H_i] - \Pr[D_i(z') = 1 : z' \in_R H_{i+1}]} \\
                > &\frac{1}{n^c}
            \end{split}\]
            concluding that $D$ is a distinguisher for $G$, raising a contradiction over $G$ being a PRG.
        \end{proof}

        By transitivy of $\approx_c$ and the above claim, we conclude that $G^\ell(\mathcal{U}_n) = H_0 \approx_c \ldots \approx_c H_\ell = \mathcal{U}_{n+\ell}$, meaning that $G^\ell$ is a PRG.
    \end{proof}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1.5cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (10) []{$f_0:$};
            \node[] (11) [right of = 10, xshift = 20, color = red]{$s_0$};
            \node[rectangle, draw] (12) [right of = 11]{$G$};
            \node[] (13) [right of = 12]{$s_1$};
            \node[rectangle, draw] (14) [right of = 13]{$G$};
            \node[] (15) [right of = 14]{$s_2$};
            \node[rectangle, draw] (16) [right of = 15]{$G$};
            \node[] (17) [right of = 16]{$\cdots$};
            \node[rectangle, draw] (18) [right of = 17]{$G$};
            \node[] (19) [right of = 18]{$s_\ell$};
            \node[] (110) [below of = 12]{$b_1$};
            \node[] (111) [below of = 14]{$b_2$};
            \node[] (112) [below of = 16]{$b_3$};
            \node[] (113) [below of = 18]{$b_\ell$};

            \node[] (20) [below of = 10, yshift = -50]{$f_1:$};
            \node[] (21) [right of = 20, xshift = 20]{};
            \node[] (22) [right of = 21]{};
            \node[] (23) [right of = 22, color = red]{$s_1$};
            \node[rectangle, draw] (24) [right of = 23]{$G$};
            \node[] (25) [right of = 24]{$s_2$};
            \node[rectangle, draw] (26) [right of = 25]{$G$};
            \node[] (27) [right of = 26]{$\cdots$};
            \node[rectangle, draw] (28) [right of = 27]{$G$};
            \node[] (29) [right of = 28]{$s_\ell$};
            \node[] (210) [below of = 22, color = red]{$b_1$};
            \node[] (211) [below of = 24]{$b_2$};
            \node[] (212) [below of = 26]{$b_3$};
            \node[] (213) [below of = 28]{$b_\ell$};

            \node[] (30) [below of = 20, yshift = -50]{$f_2:$};
            \node[] (31) [right of = 30, xshift = 20]{};
            \node[] (32) [right of = 31]{};
            \node[] (33) [right of = 32]{};
            \node[] (34) [right of = 33]{};
            \node[] (35) [right of = 34, color = red]{$s_2$};
            \node[rectangle, draw] (36) [right of = 35]{$G$};
            \node[] (37) [right of = 36]{$\cdots$};
            \node[rectangle, draw] (38) [right of = 37]{$G$};
            \node[] (39) [right of = 38]{$s_\ell$};
            \node[] (310) [below of = 32, color = red]{$b_1$};
            \node[] (311) [below of = 34, color = red]{$b_2$};
            \node[] (312) [below of = 36]{$b_3$};
            \node[] (313) [below of = 38]{$b_\ell$};

            \path[every node/.style={font=\sffamily\small}]
                (11) edge (12)
                (12) edge (13)
                (13) edge (14)
                (14) edge (15)
                (15) edge (16)
                (16) edge (17)
                (17) edge (18)
                (18) edge (19)
                (12) edge (110)
                (14) edge (111)
                (16) edge (112)
                (18) edge (113)

                (23) edge (24)
                (24) edge (25)
                (25) edge (26)
                (26) edge (27)
                (27) edge (28)
                (28) edge (29)
                (24) edge (211)
                (26) edge (212)
                (28) edge (213)

                (35) edge (36)
                (36) edge (37)
                (37) edge (38)
                (38) edge (39)
                (36) edge (312)
                (38) edge (313)
            ;
        \end{tikzpicture}

        \caption{The idea behind the construction of the functions $f_0, \ldots, f_\ell$. Red strings represent truly random bits.}
        \label{prg_stretching_1}
    \end{figure}

    \newpage

    \section{Hard-core predicates}

    In the previous chapter we saw how a PRG with stretch $1$ is -- in theory -- sufficient to construct a PRG with stretch $\mathrm{poly}(n)$. The next natural objective is to build such PRG with stretch $1$.
    In theoretical terms, we'll see that such PRG can be constructed from any OWF. However, it can be proven that the existence of OWF and PRG is equivalent, i.e. that there exists an OWF if and only if there is a PRG. This theoretical result makes building PRG as hard as building a OWF, which we aren't even sure it exists. To bypass (only partially) this problem, in practical applications we build good-enough PRGs using heuristic constructions.

    \begin{framedthm}{From PRGs to OWFs}
        Let $G : \{0,1\}^n \to \{0,1\}^{2n}$ be a PRG. Then, there is a OWF $f : \{0,1\}^{2n} \to \{0,1\}^{2n}$.
    \end{framedthm}

    \begin{proof}
        Let $f$ be the function that runs $G$ on the first half of the input:
        \[f(x_1 \concat \ldots \concat x_{2n}) = G(x_1 \concat \ldots \concat x_n)\]
        
        By way of contradiction, suppose that $f$ is not a OWF, meaning that there is an adversary $A$ such that:
        \[\Pr_{x \in_R \mathcal{U}_n}[f(x') = y : y = f(x); x' \gets A(y)] \geq \frac{1}{n^c}\]
        for some $c > 0$. We build an adversary $D$ defined as:

        $D$ = "On input $z \in \{0,1\}^{2n}$:
        \begin{enumerate}
            \item Compute $A(z) = b_1 \concat \ldots \concat b_{2n}$
            \item If $G(b_1 \concat \ldots \concat b_n) = z$ accept, otherwise reject."
        \end{enumerate}

        If $z \in_R \mathcal{U}_{2n}$, the adversary $D$ accepts with probability at most $2^{-n}$ since the first half of the output $A(z)$ is also uniform at random. If $z \in_R G(\mathcal{U}_{n})$, instead, the adversary $D$ accepts with probability at least $n^{-c}$ since it first inverts the output and then recomputes it to distinguish it. Hence, we conclude that:
        \[\abs{\Pr[D(z) = 1 : z \in_R G(\mathcal{U}_n)] - \Pr[D(z) = 1 : z \in_R \mathcal{U}_{2n}]} \geq \frac{1}{n^c} - \frac{1}{2^n} \geq \frac{1}{n^c}\]
        contradicting the assumption of $G$ being a PRG.
    \end{proof}

    To prove that the implication also holds from right to left, thus that OWF imply PRGs, we need a fundamental theoretical concept called \textbf{hard-code bits}. Given a function $f$, these bits contain information about an input $x$ that is hard to obtain from the output $f(x)$, meaning that any adversary $A$ cannot do better than guessing with 50\% chance -- up to some negligible error -- if the bit comes from $x$ or not. Hard-core bits are given by a function $h$ -- called \textit{predicate} -- that extract an hard-core bit from any input $x$.

    \begin{frameddefn}{Hard-core predicate}
        Let $f : \{0,1\}^n \to \{0,1\}^m$ be a PPT-computable function. We say that a function $h : \{0,1\}^n \to \{0,1\}$ is a hard-core predicate for $f$ if for every PPT adversary $A$ it holds that:
        \[\Pr[A(y) = h(x) : x \in_R \mathcal{U}_n, y = f(x)] \leq \frac{1}{2} + \negl(n)\]
    \end{frameddefn}
    
    From the very definition that we just gave, it's easy to get an equivalent formulation: $h$ is hard-core for $f$ if and only if $(f(x), h(x)) \approx_c (f(x), \mathcal{U}_1)$ with $x \in_R \mathcal{U}_n$ (proof is left as an exercise).
    
    We observe that both definitions we gave don't require that $f$ is a OWF. However, hard-core predicates are -- intuitively -- what allows a function to be one-way: if we can establish an hard-core bit for every input $x$, any adversary will have a hard time inverting $f(x)$ due to this very bit. From these observation, we ask three questions:
    \begin{enumerate}
        \item Is there a universal hard-core predicate that is hard-core for every OWF?
        \item Is every function with an hard-core predicate also one-way?
        \item Does every one-way function have an hard-core predicate?
    \end{enumerate}

    For the first question, the answer is \textit{no}. Suppose that such $h^*$ exists. Fix a OWF $f$ and let $\widetilde f$ be defined $\widetilde f(x) = h^*(x) \concat f(x)$. It's easy to see that the adversary $A^*$ that always returns the first bit of the output $\widetilde f(x)$ is such that:
    \[\Pr[A^*(y) = h^*(x) : x \in_R \mathcal{U}_n, y = \widetilde f(x)] = 1\]
    concluding that $h^*$ is not an hard-core predicate for $\widetilde f$. By way of contradiction, suppose that $\widetilde f$ is not a OWF, i.e. there is an adversary $\widetilde A$ such that:
    \[\Pr_{x \in_R \mathcal{U}_n}[\widetilde f(x') = y : y = \widetilde f(x); x' \gets \widetilde A(y)] \geq \frac{1}{n^c}\]
    for some $c > 0$. We build an adversary $A$ such that $A(y) = \widetilde A(b \concat y)$, where $b \in_R \mathcal{U}_1$. By construction, we have that:
    \[\begin{split}
        &\Pr_{x \in_R \mathcal{U}_n}[f(x') = y : y = f(x); x' \gets A(y)]\\
        \geq\,& \Pr_{x \in_R \mathcal{U}_n}[b = h^*(x), f(x') = y : y = f(x); x' \gets \widetilde A(b \concat y); b \in_R \mathcal{U}_1] \\
        =\,& \frac{1}{2}\Pr_{x \in_R \mathcal{U}_n}[f(x') = y : y = f(x); x' \gets \widetilde A(h^*(x) \concat y)] \\
        =\,& \frac{1}{2}\Pr_{x \in_R \mathcal{U}_n}[\widetilde f(x') = h^*(x') \concat y : y = f(x); x' \gets \widetilde A(h^*(x) \concat y)] \\
        =\,& \frac{1}{2n^c}
    \end{split}\]
    which contradicts the assumption of $f$ being OWF. Hence, $\widetilde f$ must be an OWF without $h^*$ as an hard-predicate.

    For the second question, the answer is also \textit{no}. Let $f$ be the function such that drops the last bit of the input, i.e. $f(x_1 \concat \ldots \concat x_n) = x_1 \concat \ldots \concat x_{n-1}$. Let $h$ be the function that returns only the last bit of the input, i.e. $h(x_1 \concat \ldots \concat x_n) = x_n$. By construction, $h$ is trivially an hard-core predicate for $f$ since the last bit is independent from the others when $x \in_R \mathcal{U}_n$. However, $f$ is not a OWF since an adversary that picks a random bit $b \in_R \mathcal{U}_1$ has 50\% chance of guessing $b = x_n$, inverting the output.

    What about the third question? In this case, the answer is \textit{yes, kinda}. Goldreich and Levin showed that every OWF can be trivially modified to obtain a new OWF that has a specific hard-core predicate. For any OWF $f : \{0,1\}^n \to \{0,1\}^m$, the \textbf{Goldreich-Levin alteration} of $f$ is the function $g : \{0,1\}^{2n} \to \{0,1\}^{m+n}$ defined as $g(x,r) = f(x) \concat r$, where $r$ is supposed to be taken as $r \in_R \mathcal{U}_n$. We omit the proof due to it being too convoluted.

    \begin{framedthm}{The Goldreich-Levin theorem}
        Let $f$ be an OWF and let $g$ be obtained through the Goldreich-Levin construction. Then, $g$ is an OWF function with an hard-core predicate $h(x,r) = \bigoplus_{i = 1}^n x_i r_i$.
    \end{framedthm}

    \begin{proof}
        Omitted.
    \end{proof}

    Surprisingly, OWFs can be combined with an hard-core predicate in order to generate a PRG. The easiest case of such property is given by \textbf{One-way Permutations (OWPs)}, that being bijective OWFs.
    
    \begin{framedprop}{From OWP to PRG}
        Let $f$ be a OWP with an hard-core predicate $h$. Then, $G(s) = f(s) \concat h(s)$ is a PRG with expansion factor $1$.
    \end{framedprop}

    \begin{proof}
        Consider the first $n$ bits of the output $G(s)$, i.e. the bits $f(s)$. When $s$ is taken uniformly at random, the output $f(s)$ will also be uniform at random due to $f$ being a permutation. For the last $n+1$ bit of the output, instead, we know that $h(s)$ is by definition computationally hard to distinguish from a bit $b \in_R \mathcal{U}_1$. This concludes that:
        \[G(\mathcal{U}_n) \equiv (f(\mathcal{U}_n), h(\mathcal{U}_n)) \approx_c (\mathcal{U}_n, \mathcal{U}_1) \equiv \mathcal{U}_{n+1} \]
    \end{proof}

    The above proposition can be strenghtened by generalizing to every OWF instead of OWPs (the idea is to mix multiple outputs of $f$ with their hard-core bits).

    \begin{framedthm}{From OWF to PRG}
        For every OWF $f$ there is a PRG $G$ with expansion factor $1$.
    \end{framedthm}

    \begin{proof}
        Omitted.
    \end{proof}

    \section{Solved exercises}

    \begin{framedprob}{}
        For $m > n$, let $G : \{0,1\}^n \to \{0,1\}^m$ be a secure PRG. Consider $G'$ defined as $G'(s) = G(s) \oplus (0^{m-n} \mid\mid s)$. Prove or disprove that $G'$ is a secure PRG.
    \end{framedprob}

    \textit{Solution}:

    $G'$ is not necessarily a secure PRG. Let $G(s) = G^*(s_{1:n-1}) \mid\mid s_n$, where $G^* : \{0,1\}^n \to \{0,1\}^{m-1}$ is a PRG, $s_{1:n-1}$ denotes the first $n-1$ bits of $s$ and $s_n$ is the last bit of $s$. It can be proven that this chosen $G$ is indeed a PRG, making it a valid choice. Now, we observe that:
    \[\begin{split}
        G'(s) &= G(s) \oplus (0^{m-n} \mid\mid s) \\
        &= (G^*(s_{1:n-1})_{1:m-n} \oplus 0^{m-n}) \mid\mid (G^*(s_{1:n-1})_{m-n+1:m-1} \oplus s_{1:n-1}) \mid\mid (s_n \oplus s_n) \\
        &= G^*(s_{1:n-1})_{1:m-n} \mid\mid (G^*(s_{1:n-1})_{m-n+1:m-1} \oplus s_{1:n-1}) \mid\mid 0 \\
    \end{split}\]

    In other words, the last bit of $G'(s)$ will always be 0. Consider now the adversary $A$ that checks the input $z \in \{0,1\}^{m}$ and returns 1 if $z_m = 0$ and 0 otherwise. Then:
    \[\abs{\Pr[A(z) = 1 : z \in_R G'(\mathcal{U}_n)] - \Pr[A(z) = 1 : z \in_R \mathcal{U}_m]} = \abs{1-\frac{1}{2}} = \frac{1}{2}\]

    concluding that $A$ is an adversary that distinguishes $G'(\mathcal{U}_n)$ from $\mathcal{U}_m$ with non-negligible probability.

    \begin{framedprob}{}
        Let $f_1, f_2, f_3 : \{0,1\}^n \to \{0,1\}^n$ be arbitrary functions. You know that at least one of them is a OWF, but you don't know which one. Design a OWF $f$ using $f_1,f_2,f_3$ in a black-box manner.
    \end{framedprob}

    \textit{Solution}:

    Let $f : \{0,1\}^{3n} \to \{0,1\}^{3n}$ be defined by $f(x_1 \mid\mid x_2 \mid\mid x_3) = f_1(x_1) \mid\mid f_2(x_2) \mid\mid f_3(x_3)$. By way of contradiction, suppose that there is an adversary $A^{\text{OWF}}_f$ that inverts $f$ with non-negligible probability. We define three adversaries $A^{\text{OWF}}_{f_1}, A^{\text{OWF}}_{f_2}, A^{\text{OWF}}_{f_3}$ as follows (with $i \in [3]$ fixed):
    \begin{enumerate}
        \item $C^{\text{OWF}}_{f}$ sends the output $y$ to $A^{\text{OWF}}_{f_i}$
        \item $A^{\text{OWF}}_{f_i}$ sets $y_i' = y$ and $y_j' = 0^n$ for $j \in [3]-\{i\}$. They then send $y_1' \mid\mid y_2' \mid\mid y_3'$ to $A^{\text{OWF}}_{f}$
        \item $A^{\text{OWF}}_{f}$ returns $x_1' \mid\mid x_2' \mid\mid x_3'$ and $A^{\text{OWF}}_{f_i}$ forwards $x_i'$ to $C^{\text{OWF}}$
    \end{enumerate}

    Since at least one of $f_1, f_2, f_3$ is a OWF, there is at least one index $i$ such that $A^{\text{OWF}}_{f_i}$ forms a reduction from the OWF game of $f$ to the OWF game of $f_i$, raising a contradiction. Therefore, $f$ must be an OWF.

    \chapter{Symmetric-key encryption}

    \section{Games as security proofs}

    In Chapter 2 we gave the initial definitions of secure cryptographic schemes, without caring about the computation being achieved efficiently. In Chapter 3, instead, we discussed the basics of secure efficient computation. Now, we're ready to merge the two ideas.

    We start by discussing secure SKEs. Using the concept of pseudorandom generator (PRG) that we introduced in the previous chapter, we can define a \textit{PRG One Time Pad (PRG-OTP)}. Given $\mathcal{K} = \{0,1\}^\lambda$ and $\mathcal{M} = \mathcal{C} = \{0,1\}^{\lambda+\ell}$, we define:
    \[\mathrm{Enc}(K,m) = G(K) \oplus m \qquad \mathrm{Dec}(K,c) = G(K) \oplus c\]

    where $G : \{0,1\}^\lambda \to \{0,1\}^{\lambda+\ell}$ is a PRG and $\ell = \poly(\lambda)$. The use of a PRG allows us to \curlyquotes{bypass} Shannon's key-length constraint since the \curlyquotes{pseudorandom key} generated through $G(K)$ is still at least as long as the message. However, this implies that our SKE doesn't have real perfect secrecy, otherwise the theorem would be false. This idea moves us into defining a new concept of \textbf{computationally secure SKE}.
    
    In this and many other cases, security definitions will be given in terms of \textbf{games} between two entities: a challenger and an adversary. The two players play the game sending messages to each other. The challger's objective is proving that the scheme is secure by answering the questions posed the adversary, while the latter's objective is to break the scheme. The challenger wins if it is capable of responding to any possible series of questions without making the adversary able to break the scheme. If the challenger has a strategy to win the game, we consider the scheme to be secure. We assume that the two players are allowed only efficient computation.

    Consider the following example. Let $\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, b)$ be the 2-player game defined as follows:
    \begin{enumerate}
        \item The adversary $A$ sends two messages $m_0, m_1 \in \mathcal{M}$ to the challenger $C$
        \item $C$ generates a key $K \in \mathcal{K}$ and sends $\mathrm{Enc}_\Pi(K, m_b) = c$ back to $A$
        \item $A$ analyzes $c$ and tries to guess if it was obtained through $m_0$ or $m_1$ by sending a bit $b' \in \{0,1\}$
        \item If $b' \neq b$, the challenger wins. Otherwise, the adversary wins. 
    \end{enumerate}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (3) edge[near start] node{$m_0, m_1 \in \mathcal{M}$} (4)
                (6) edge[swap, near start] node{$\mathrm{Enc}_\Pi(K, m_b)$} (5)
                (7) edge[near start] node{$b' \in \{0,1\}$} (8)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, b)$}
    \end{figure}

    By the very definition of the game, it's easy to see that if, for every pair of messages, the adversary is not capable of distinguishing which message got encrypted by the challenger, i.e. if they are playing $\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, 0)$ or $\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, 1)$, then the scheme can be considered \textbf{1-time computationally secure SKE}.

    \begin{frameddefn}{1-time computational security}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE. We say that $\Pi$ is 1-time computationally secure if:
        \[\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, 0) \approx_c \mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, 1)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    To see how this game can be used for specific cryptosystems, we prove that PRG-OTP has 1-time computationally security.

    \begin{framedprop}{}
        PRG-OTP has 1-time computational security.
    \end{framedprop}

    \begin{proof}
        Let $\ell = \poly(n)$ be the expansion factor of the PRG $G$ used inside PRG-OTP. We use an argument similar to the hybrid argument. Let $\mathrm{Hyb}(\lambda, b)$ be the 2-player game defined as follows:
        \begin{enumerate}
            \item The adversary $A$ sends two messages $m_0, m_1 \in \{0,1\}^\lambda$ to the challenger $C$
            \item $C$ picks $z \in_R \mathcal{U}_{\lambda+\ell}$ and sends $z \oplus m_b = c$ back to $A$
            \item $A$ analyzes $c$ and tries to guess if it was obtained through $m_0$ or $m_1$ by sending a bit $b' \in \{0,1\}$
            \item If $b' \neq b$, the challenger wins. Otherwise, the adversary wins. 
        \end{enumerate}

        We observe that the difference between $\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, b)$ and $\mathrm{Hyb}(\lambda, b)$ is given by $G(K)$ being replaced by a uniform at random string $z \in_R \mathcal{U}_{\lambda+\ell}$. Since $G(\mathcal{U}_\lambda) \approx_c \mathcal{U}_{\lambda+\ell}$ by choice of $G$, the hybrid game and the original game are expected to also be indistinguishable from each other.
        
        \textbf{Claim}: For each $b \in \{0,1\}$ it holds that $\mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, b) \approx_c \mathrm{Hyb}(\lambda, b)$

        \begin{proof}[Proof of the claim.]
            By way of contradiction, assume that there is an adversary $A'$ such that:
            \[\abs{\Pr\sbk{A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi, A}^{\text{1-time}}(\lambda, b)} - \Pr\sbk{A'(b') = 1 : b' \gets \mathrm{Hyb}(\lambda, b)}} \geq \frac{1}{\lambda^c}\]

            for some $c > 0$. We build an adversary $D$ defined as follows:

            $D$ = "On input $z \in \{0,1\}^{\lambda+\ell}$.
            \begin{enumerate}
                \item $D$ challenges $A'$ to play $\mathrm{Hyb}(\lambda, b)$ (thus $D$ is the challenger and $A'$ is the adversary). $D$ will use $z$ to compute $m_b \oplus z = c$ instead of choosing a random string.
                \item $D$ accepts if $A'$ wins, otherwise $D$ rejects."
            \end{enumerate}

            Since $A'$ can distinguish the two games, it is also able to distinguish if the string $c$ passed by $D$ was computed with $z$ taken from $G(\mathcal{U}_\lambda)$ or $\mathcal{U}_{\lambda+\ell}$. This implies that:
            \[\Pr[D(z) = 1 : z \in_R G(\mathcal{U}_\lambda)] = \Pr\sbk{A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi, A'}^{\text{1-time}}(\lambda, b)}\]

            and:
            \[\Pr[D(z) = 1 : z \in_R \mathcal{U}_{\lambda+\ell}] = \Pr\sbk{A'(b') = 1 : b' \gets \mathrm{Hyb}(\lambda, b)}\]

            concluding that:
            \[\begin{split}
                &\abs{\Pr[D(z) = 1 : z \in_R G(\mathcal{U}_\lambda)] - \Pr[D(z) = 1 : z \in_R \mathcal{U}_{\lambda+\ell}]} \\
                =& \abs{\Pr\sbk{A'(b') = 1 : b' \gets \mathrm{Game}_{\Pi, A'}^{\text{1-time}}(\lambda, b)} - \Pr\sbk{A'(b') = 1 : b' \gets \mathrm{Hyb}(\lambda, b)}} \\
                \geq& \frac{1}{\lambda^c}
            \end{split}\]

            and thus contradicting the fact that $G$ is a PRG.
        \end{proof}
        
        By definition, it's easy to see that $\mathrm{Hyb}(\lambda, 0) \equiv \mathrm{Hyb}(\lambda, 1)$ since the distribution of $c$ is uniform and independent of $b$. Therefore, we conclude that:
        \[\mathrm{Game}_{\Pi, A'}^{\text{1-time}}(\lambda, 0) \approx_c \mathrm{Hyb}(\lambda, 0) \equiv \mathrm{Hyb}(\lambda, 1) \approx_c \mathrm{Game}_{\Pi, A'}^{\text{1-time}}(\lambda, 1)\]
    \end{proof}

    \newpage

    \section{CPA-security and pseudorandom functions}

    In the previous section we proved that PRG-OTP is 1-time computationally secure, meaning that it is secure against attackers that know only one message. However, this SKE's weakness is \textit{ciphertext attacks}. Suppose that an attacker knows a ciphertext-message pair $(m_1, c_1)$. If the challenger sends another ciphertext $c_2$ obtained from a message $m_2$, the attacker is able to retrieve the contents of $m_2$ through the SKE itself:
    \[c_1 \oplus c_2 \oplus m_1 = (G(K) \oplus m_1) \oplus (G(K) \oplus m_2) \oplus m_1 = m_2\]
    
    The generalization of this type of attack is known as \textbf{Known-plaintext Attack (KPA)} and the security counterpart is known as \textit{$t$-time computational security}. We observe that KPAs require that the attacker only knows the pairs $(m_1, c_1), \ldots, (m_t, c_t)$ but is not able to choose them. When the attacker is able to choose the pairs (e.g. they can choose messages and get their ciphertexts) we talk about \textbf{Chosen-plaintext Attack (CPA)}.
    
    In the CPA game the adversary is given access to an encryption \textbf{oracle}, i.e. a sub-procedures with unlimited power that can be queried in order to get an answer. Each query costs $\Theta(1)$ computational steps. We observe that the oracle queries made by the attacker can be seen as \curlyquotes{questions already answered}. For instance, suppose that the attacker queries an oracle $O$ capable of returning the ciphertext of a message, i.e. solves the problem $\mathrm{Enc}(K, \cdot)$. If the attacker queries $m_1, \ldots, m_t$, the oracle will answer with $c_1, \ldots, c_t$. This is equivalent to saying that the attacker has chosen the pairs $(m_1, c_1), \ldots, (m_t, c_t)$. The concept of oracle is just a computational medium to express this idea.
    
    Let $\mathrm{Game}_{\Pi,A}^{\text{CPA}}(\lambda,b)$ be the 2-player game defined as follows:
    \begin{enumerate}
        \item The challenger $C$ generates a key $K \in \mathcal{K}$.
        \item The adversary $A$ queries $m_1, \ldots, m_t \in \{0,1\}^\lambda$, where $t = \poly(\lambda)$, to an oracle for $\mathrm{Enc}_{\Pi}(K, \cdot)$. The oracle answers with $c_1, \ldots, c_t$.
        \item The adversary sends two messages $m_0^*, m_1^* \in \{0,1\}^\lambda$ to the challenger $C$
        \item $C$ sends $\mathrm{Enc}_\Pi(K, m^*_b) = c^*$ back to $A$
        \item $A$ queries $m_1', \ldots, m_s' \in \{0,1\}^\lambda$, where $s = \poly(\lambda)$, to an oracle for $\mathrm{Enc}_{\Pi}(K, \cdot)$. The oracle answers with $c_1', \ldots, c_t'$.
        \item $A$ tries to guess if it was obtained through $m_0$ or $m_1$ by sending a bit $b' \in \{0,1\}$
        \item If $b' \neq b$, the challenger wins. Otherwise, the adversary wins. 
    \end{enumerate}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \node[] (9) [below of = 7]{};
            \node[] (10) [below of = 8]{};

            \node[] (3x) [below of = 9]{};
            \node[] (4x) [below of = 10]{};

            \node[] (5x) [below of = 3x]{};
            \node[] (6x) [below of = 4x]{};

            \node[] (11) [below of = 5x]{};
            \node[] (12) [below of = 6x]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$m_i \in \{0,1\}^\lambda$} (4)
                (6) edge[swap, near start] node{$c_i = \mathrm{Enc}_\Pi(K, m_i)$} (5)
                (7) edge[near start] node{$m_0^*, m_1^* \in \{0,1\}^\lambda$} (8)
                (10) edge[swap, near start] node{$c^* = \mathrm{Enc}_\Pi(K, m_b^*)$} (9)
                (5x) edge[bend left] node {$\poly(\lambda)$}(3x)
                (3x) edge[near start] node{$m_i' \in \{0,1\}^\lambda$} (4x)
                (6x) edge[swap, near start] node{$c_i' = \mathrm{Enc}_\Pi(K, m_i')$} (5x)
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, b)$. Queries to the oracle can be substituted with queries to $C$, as if the two players already exchanged some pairs.}
    \end{figure}

    \begin{frameddefn}{CPA-security}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE. We say that $\Pi$ is CPA-secure if:
        \[\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, 0) \approx_c \mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, 1)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    It's easy to see that the PRG-OTP is not CPA-secure since the output of the encryption scheme is \textit{deterministic} (meaning that it will always return the same output when given the same input). Consider the attacker that queries two messages $m_0, m_1 \in \{0,1\}^n$. After obtaining the ciphertexts $c_0, c_1$ from $C$, the attacker sends the final queries $m_0^* = m_0$ and $m_1^* = m_1$, obtaining the final ciphertext $c^*$. Since the scheme is deterministic, we know that either $c^* = c_0$ or $c^* = c_1$ must hold (depending on the value $b$ used by $C$). Thus, the attacker returns 0 if $c^* = c_0$,, otherwise they returns 1. This allows the attacker to distinguish the two games with probability 1. Clearly, this idea is not restricted to the PRG-OTP: it holds for any deterministic SKE!

    \begin{framedprop}{}
        No deterministic SKE can be CPA-secure.
    \end{framedprop}

    In order to make a CPA-secure SKE, we need to substitute pseudorandom generators with \textbf{pseudorandom functions}, i.e. functions that are indistinguishable from a randomly selected one (thus they make their internal workings incomprehensible).

    Let $\mathcal{R}(s \to t)$ denote the uniform distribution over the set of all functions from strings of $s$ bits to strings of $t$ bits. We define the following 2-player game $\mathrm{Game}_{\mathcal{F},A}^{\text{PRF}}(\lambda, b)$:
    \begin{enumerate}
        \item The challenger $C$ generates a key $K \in \{0,1\}^\lambda$ and sets $J = F_K$ if $b = 0$, otherwise $J \in_R \mathcal{R}(n \to n)$.
        \item The adversary $A$ queries $x_1, \ldots, x_t \in \{0,1\}^n$ where $t = \poly(n)$, to an oracle for $J(\cdot)$. The oracle answers with $y_1, \ldots, y_t$.
        \item $A$ tries to guess $b$ by sending a bit $b' \in \{0,1\}$
        \item If $b \neq b'$, the challenger wins. Otherwise, the adversary wins.
    \end{enumerate}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};


            \node[] (11) [below of = 5]{};
            \node[] (12) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$x_i \in \{0,1\}^n$} (4)
                (6) edge[swap, near start] node{$y_i = J(x_i)$} (5)
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\mathcal{F}, A}^{\text{PRF}}(\lambda, b)$. Queries to the oracle can be substituted with queries to $C$, as if the two players already exchanged some pairs.}
    \end{figure}

    \begin{frameddefn}{Pseudorandom functions}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^m\}_{K \in \{0,1\}^\lambda}$ be a family of poly-time functions. We say that $\mathcal{F}$ is a pseudorandom function family (PRF) if:
        \[\mathrm{Game}_{\mathcal{F},A}^{\text{PRF}}(\lambda, 0) \approx_c \mathrm{Game}_{\mathcal{F},A}^{\text{PRF}}(\lambda, 1)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    We observe that the above definition requires that $\mathcal{F}$ is a family of efficient \underline{deterministic} functions: randomness is only used to choose a key that identifies which function of the family must be used. The uniform distribution $\mathcal{R}(n \to n)$, instead, also contains functions that aren't poly-time computable. The idea behind the PRF game is to establish that each function of the family is computationally indistinguishable from a randomly chosen function.

    In practical applications, many common encryption schemes (e.g. DES, 3DES, AES, \dots) are based on on \textit{pseudorandom permutations (PRP)}, i.e. PRFs that are invertible if they key is known. For now, we'll give the idea behind how PRFs can be used in theory to produce CPA-secure SKEs.
    
    First of all, we observe that we cannot simply swap PRGs with PRFs in order to get a good scheme since each function is deterministic. The trick here is to use a random initial value called \textbf{nonce} (short for \curlyquotes{number used once}). As their name suggests, these values are used exactly once by the cryptosystem, granting that the same input will always give a different ciphertext.
    
    Given a PRF $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$, let the \textit{PRF One Time Pad (PRF-OTP)} be the scheme defined by the following operations:
    \begin{enumerate}
        \item $\mathrm{Enc}(K,m) = (r, F_K(r) \oplus m)$, where $r \in \{0,1\}^n$ is a nonce
        \item $\mathrm{Dec}(K,(r,c)) = F_K(r) \oplus c$
    \end{enumerate}
    
    We observe that the encryption operation must also return the used nonce in order for the decryption operation to work. However, this is not an issue for CPA-security: since the nonce will never be used again by the scheme, the adversary gains no information on the key by knowing it.

    \begin{framedprop}{}
        PRF-OTP is CPA-secure.
    \end{framedprop}

    \begin{proof}
        Omitted (an hybrid argument suffices).
    \end{proof}

    \subsection{The GGM Tree}
    
    After discussing how PRFs can be used to achieve CPA-security, we prove that PRGs can be used to construct PRFs. In particular, we'll give an explicit construction known as the \textbf{Goldreich-Goldwasser-Micali Tree (GGM Tree)}.

    \begin{frameddefn}{GGM Tree}
        Given a PRG $G : \{0,1\}^\lambda \to \{0,1\}^{2\lambda}$, let $G_0, G_1 : \{0,1\}^\lambda \to \{0,1\}^\lambda$ be two functions that respectively return the first and last $\lambda$ bits of the output of $G$, meaning that $G(k) = G_0(k) \concat G_1(k)$ for all $k \in \{0,1\}^\lambda$. The GGM Tree function family is defined as $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^\lambda\}_{K \in \{0,1\}^\lambda}$, where:
        \[F_K(x_1 \concat \ldots \concat x_n) = G_{x_n}(G_{x_{n-1}}(\ldots G_{x_2}(G_{x_1}(K))))\]
    \end{frameddefn}


    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[
            edge from parent/.style={draw, -{Latex[length=2mm]}, thick},
            every node/.style={font=\small},
            level distance=1.8cm,
            level 1/.style={sibling distance=8cm},
            level 2/.style={sibling distance=4cm},
            level 3/.style={sibling distance=2cm}
        ]
            \node {$K$}
                child {node {$G_0(K)$}
                    child {node {$G_0(G_0(K))$}
                    child {node[rectangle, draw] {$F_K(000)$} edge from parent node[left] {0}}
                    child {node[rectangle, draw] {$F_K(001)$} edge from parent node[right] {1}}
                    edge from parent node[left, yshift=5] {0}
                    }
                    child {node {$G_1(G_0(K))$}
                    child {node[rectangle, draw] {$F_K(010)$} edge from parent node[left] {0}}
                    child {node[rectangle, draw, color=Red] {$F_K(011)$} edge from parent node[right] {1}}
                    edge from parent node[right, yshift=5] {1}
                    }
                    edge from parent node[left, yshift=5] {0}
                }
                child {node {$G_1(K)$}
                    child {node {$G_0(G_1(K))$}
                    child {node[rectangle, draw] {$F_K(100)$} edge from parent node[left] {0}}
                    child {node[rectangle, draw] {$F_K(101)$} edge from parent node[right] {1}}
                    edge from parent node[left, yshift=5] {0}
                    }
                    child {node {$G_1(G_1(K))$}
                    child {node[rectangle, draw] {$F_K(110)$} edge from parent node[left] {0}}
                    child {node[rectangle, draw] {$F_K(111)$} edge from parent node[right] {1}}
                    edge from parent node[right, yshift=5] {1}
                    }
                    edge from parent node[right, yshift=5] {1}
                };
        \end{tikzpicture}

        \caption{The GGM Tree for inputs of length $n = 3$. The path leading to the red rectangle represent the computation $F_K(011) = G_1(G_1(G_0(K)))$.}
    \end{figure}
    
    \begin{framedlem}[label={t_prg_uniform}]{}
        Let $G : \{0,1\}^\lambda \to \{0,1\}^{2\lambda}$ be a PRG. Given $K_1, \ldots, K_t \in_R \mathcal{U}_\lambda$, with $t = \poly(\lambda)$ and $K_i \neq K_j$ for all $i \neq j$, it holds that:
        \[(G(K_1), \ldots, G(K_t)) \approx_c (\mathcal{U}_{2\lambda}, \ldots, \mathcal{U}_{2\lambda})\] 
    \end{framedlem}

    \begin{proof}
        We define $t$ hybrid distributions $H_0, \ldots, H_t$ such that for each $i \in [t]$ it holds that:
        \[H_i = (G(K_1), \ldots, G(K_{t-i}), \underbrace{\mathcal{U}_{2\lambda}, \ldots, \mathcal{U}_{2\lambda}}_{i \text{ times}})\]

        We observe that $H_0 \equiv (G(K_1), \ldots, G(K_t))$ and $H_t \equiv (\mathcal{U}_{2\lambda}, \ldots, \mathcal{U}_{2\lambda})$.

        \textbf{Claim:} For each $i \in [0,t-1]$ it holds that $H_i \approx_c H_{i+1}$.

        \begin{proof}[Proof of the claim]
            Fix $i \in [0, t-1]$. We prove the claim by reducing the distinguishability of $G(K_{t-i})$ from $\mathcal{U}_{2\lambda}$ to the distinguishability of $H_i$ from $H_{i+1}$. By way of contradiction, suppose that $H_i \not\approx_c H_{i+1}$, meaning that there is a distinguisher $D_i$ such that:
            \[\abs{\Pr[D_i(z) = 1 : z \in_R H_i] - \Pr[D_i(z) = 1 : z \in_R H_{i+1}]} > \frac{1}{n^c}\]
            for some $c > 0$. Let $D$ be the algorithm defined as follows:

            $D$ = "On input $z \in \{0,1\}^{2\lambda}$:

            \begin{enumerate}
                \item Extract $t-i$ keys $K_1, \ldots, K_{t-i} \in_R \mathcal{U}_\lambda$
                \item Extract $i-1$ strings $w_{1}, \ldots, w_{i-1} \in_R \mathcal{U}_{2\lambda}$
                \item Return $D_i(G(K_1) \concat \ldots \concat G(K_{t-i}) \concat z \concat w_1 \concat \ldots \concat w_{i-1})$"
            \end{enumerate}
            
            We observe that $H_i$ and $H_{i+1}$ differ only on the $i$-th string of $2n$ bits: the former contains an output of $G$ while the latter contains a truly random strng. This implies that:
            \[\begin{split}
                &\abs{\Pr[D(w) = 1 : w \in_R G(K_{t-i})] - \Pr[D(w) = 1 : w \in_R \mathcal{U}_{2\lambda}]}\\
                = &\abs{\Pr[D_i(z') = 1 : z' \in_R H_i] - \Pr[D_i(z') = 1 : z' \in_R H_{i+1}]} \\
                > &\frac{1}{n^c}
            \end{split}\]
            concluding that $D$ is a distinguisher for $G$, raising a contradiction over $G$ being a PRG.
        \end{proof}
         
        By transitivity of $\approx_c$, the claim concludes that:
        \[(G(K_1), \ldots, G(K_t)) \equiv H_0 \approx_c \ldots \approx_c H_t \equiv (\mathcal{U}_{2\lambda}, \ldots, \mathcal{U}_{2\lambda})\]
    \end{proof}

    \begin{framedthm}{The GGM theorem}
        GGM Tree is a PRF when constructed using a PRG
    \end{framedthm}

    \begin{proof}
        Let $G : \{0,1\}^\lambda \to \{0,1\}^{2\lambda}$ be a PRG and let $G_0, G_1 : \{0,1\}^\lambda \to \{0,1\}^\lambda$ be the two functions such that $G(x) = G_0(x) \concat G_1(x)$. Let also $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^\lambda\}_{K \in \{0,1\}^\lambda}$ be the GGM function family.

        Fix a key $K \in \{0,1\}^\lambda$. We proceed by induction on the length $n$ of the input. When $n = 1$ it holds that $F_K(x) = G_x(K)$. Since $\mathcal{U}_{2\lambda} \approx_c G(K) \equiv (G_0(K), G_1(K))$, we know that $G_0(K) \approx_c \mathcal{U}_\lambda$ and $G_1(K) \approx_c \mathcal{U}_\lambda$. This concludes that, independently from the value of $x$, the output $F_K(x)$ is undistinguishable from a string taken from $\mathcal{U}_\lambda$, making it undistinguishable from a random function taken from $\mathcal{R}(1 \to \lambda)$. 

        We now assume that the inductive hypothesis holds for $n-1$, i.e. that the GGM family $\mathcal{F} = \{F_K : \{0,1\}^{n-1} \to \{0,1\}^{\lambda}\}_{K \in \{0,1\}^\lambda}$ is a PRF. First, we observe that for each key $K \in \{0,1\}^\lambda$, for each $x \in \{0,1\}^n$ and for each $y \in \{0,1\}^{n-1}$ it holds that $F_K(x \concat y) = G_{x}(F_K'(y))$.
        
        We define the following three hybrid distributions:
        \begin{itemize}
            \item $\mathrm{Hyb}_0$ is the distribution over the function $F_K(x \concat y) = G_x(F_K'(y))$
            \item $\mathrm{Hyb}_1$ is the distribution over the function $H(x \concat y) = G_x(R'(y))$ where $R' \in_R \mathcal{R}(n-1 \to \lambda)$.
            \item $\mathrm{Hyb}_2$ is the distribution over the function $R \in_R \mathcal{R}(n \to \lambda)$.
        \end{itemize}

        To prove that $\mathcal{F}$ is also a PRF, we'll show that $\mathrm{Hyb}_0 \approx_c \mathrm{Hyb}_1 \approx_c \mathrm{Hyb}_2$.
        
        \textbf{Claim 1}: $\mathrm{Hyb}_0 \approx_c \mathrm{Hyb}_1$
        
        \begin{proof}[Proof of Claim 1]
            We reduce the distinguishability of $\mathrm{Game}_{\mathcal{F}',A}^{\text{PRF}}(\lambda, 0)$ from $\mathrm{Game}_{\mathcal{F}',A}^{\text{PRF}}(\lambda, 1)$ to the distinguishability of $\mathrm{Hyb}_0$ from $\mathrm{Hyb}_1$. By way of contradiction, suppose that $\mathrm{Hyb}_0 \not\approx_c \mathrm{Hyb}_{1}$, meaning that there is a PPT adversary $D_{01}$ such that:
            \[\abs{\Pr[D_{01}(z) = 1 : z \in_R \mathrm{Hyb}_0] - \Pr[D_{01}(z) = 1 : z \in_R \mathrm{Hyb}_1]} > \frac{1}{n^c}\]
            for some $c > 0$. We define the attack strategy $A_{01}$ for $\mathrm{Game}_{\mathcal{F}',A}^{\text{PRF}}(\lambda, b)$ as follows. The idea is for $A_{01}$ to play two games simultaneously.

            \begin{enumerate}
                \item The challenger $C$ generates a key $K \in \{0,1\}^\lambda$ and sets $J = F'_K$ if $b = 0$, otherwise $H \in_R \mathcal{R}(n-1 \to \lambda)$.
                \item $A_{01}$ challenges $D_{01}$ to distinguish $\mathrm{Hyb}_0$ from $\mathrm{Hyb}_1$.
                \item For each query $x \concat y$ -- with $x \in \{0,1\}$ and $y \in \{0,1\}^n$ -- made by $D_{01}$, the adversary $A_{01}$ forwards $y$ to the challenger $C$.
                \item For each answer $z$ returned by $C$, the adversary $A_{01}$ forwards $G_x(z)$ to $D_{01}$.
                \item When $D_{01}$ gives the final bit $b'$, $A_{01}$ forwards the same bit to $C$.
            \end{enumerate}

            In the above strategy, $A_{01}$ plays two games simultaneously, using the challenger as a way to compute outputs of either $F'_K$ or a random function $R$. After obtaining the outputs, $A_{01}$ passes them through $G_x$ before forwarding them to $D_{01}$. If the challenger used $F'_K$, the values forwarded from $A_{01}$ will be outputs of $F_K$. Otherwise, if the challenger used $R'$, the values forwarded from $A_{01}$ will be outputs of $R$. Since $D_{01}$ can distinguish between $F_K$ and $R$, $A_{01}$ will be able to distinguish $F'_K$ from $R'$. This contradicts the fact that $\mathcal{F}'$ is a PRF, concluding that $\mathrm{Hyb}_0 \approx_c \mathrm{Hyb}_{1}$ must be true.
        \end{proof}

        \textbf{Claim 2}: $\mathrm{Hyb}_1 \approx \mathrm{Hyb}_2$

        \begin{proof}[Proof of Claim 2]
            We reduce the distinguishability of $(G(K_1), \ldots, G(K_t))$ from $(\mathcal{U}_{2\lambda}, \ldots, \mathcal{U}_{2\lambda})$ to the distinguishability of $\mathrm{Hyb}_1$ from $\mathrm{Hyb}_2$. By way of contradiction, suppose that there is an algorithm $D_{12}$ that distinguishes $\mathrm{Hyb}_1$ from $\mathrm{Hyb}_{2}$
            For each string $w \in \{0,1\}^{2\lambda}$, let $w^0, w^1 \in \{0,1\}^{\lambda}$ denote the first and last $\lambda$ bits of $w$, i.e. $w = w^0 \concat w^1$. Let $A_{12}$ be the algorithm defined as follows:

            $A_{12}$ = "On input $z = z_1 \concat \ldots \concat z_t$ with $z_i \in \{0,1\}^{2\lambda}$:

            \begin{enumerate}
                \item $A_{12}$ challenges $D_{12}$ to distinguish $\mathrm{Hyb}_1$ from $\mathrm{Hyb}_2$, allowing $D_{12}$ exactly $t$ queries.
                \item For each query $x \concat y$ -- with $x \in \{0,1\}$ and $y \in \{0,1\}^n$ -- made by $D_{12}$, the adversary $A_{12}$ answers with $z^x_{i(y)}$, where $i(y) \in [1,t]$ is either the index that was used by $A_{12}$ if $D_{12}$ already queried $y$ or the next unused index.
                \item When $D_{01}$ gives the final bit $b'$, $A_{01}$ returns $b'$
            \end{enumerate}

            When $z_1, \ldots, z_t \in \mathcal{U}_{2\lambda}$, all values returned by $A_{12}$ to $D_{12}$ will be truly random, making $D_{12}$ assume that the values are taken from the distribution $\mathrm{Hyb}_2$. When $z_1, \ldots, z_t \in G(\mathcal{U}_{\lambda})$, instead, all values will be outputs of $G$, making $D_{12}$ assume that the values are taken from the distribution $\mathrm{Hyb}_1$. This concludes that $A_{12}$ is a distinguisher for $(G(K_1), \ldots, G(K_t))$ and $(\mathcal{U}_{2\lambda}, \ldots, \mathcal{U}_{2\lambda})$, contradicting \Cref{t_prg_uniform}.
        \end{proof}
    \end{proof}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$D_{01}$};
            \node[] (2) [left of = 1, xshift = 200]{$A_{01}$};
            \node[] (2x) [left of = 2, xshift = 200]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};
            \node[] (4x) [below of = 2x]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};
            \node[] (6x) [below of = 4x]{};


            \node[] (11) [below of = 5]{};
            \node[] (12) [below of = 6]{};
            \node[] (12x) [below of = 6x]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                
                (3) edge[near start] node{$x_i \concat y_i \in \{0,1\}^{n}$} (4)
                (6) edge[swap, near start] node{$G_x(z_i)$} (5)
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)

                (4) edge[near start] node{$y_i$} (4x)
                (6x) edge[swap, near start] node{$z_i$} (6)
                (12) edge[near start] node{$b'$} (12x)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of the double-game attack used to prove Claim 1 of the GGM theorem.}
    \end{figure}


    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$D_{12}$};
            \node[] (2) [left of = 1, xshift = 200]{$A_{12}$};
            \node[] (2x) [left of = 2, xshift = 200]{$C$};
            
            \node[] (a) [below of = 1]{};
            \node[] (b) [below of = 2]{};
            \node[] (c) [below of = 2x]{};

            \node[] (3) [below of = a]{};
            \node[] (4) [below of = b]{};
            \node[] (4x) [below of = c]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};
            \node[] (6x) [below of = 4x]{};


            \node[] (11) [below of = 5]{};
            \node[] (12) [below of = 6]{};
            \node[] (12x) [below of = 6x]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$t$}(3)

                (c) edge[swap, near start] node{$z_i \concat \ldots \concat z_t \in \{0,1\}^{2\lambda t}$} (b)
                
                (3) edge[near start] node{$x_j \concat y_j \in \{0,1\}^{n}$} (4)
                (6) edge[swap, near start] node{$z^{x_j}_{i(y_j)}$} (5)
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)

                (12) edge[near start] node{$b'$} (12x)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of the double-game attack used to prove Claim 2 of the GGM theorem.}
    \end{figure}

    \section{Encryption modes for SKEs}

    After proving that OWFs can be used to create PRFs through PRGs, we're going to show that PRFs are enough to achieve practical symmetric-key encryption. As we'll see, for some practical applications we strictly require that the pseudorandom functions are invertible, i.e. that they are PRPs. Practical functions with this property are commonly called \textbf{block ciphers} (the concept of blockchipers was used way before the introduction of the concept of PRPs). From a theoretical prospective, we'll discuss how PRFs can be used to derive PRPs. From a practical prospective, instead, we'll discuss some blockchipers used in the real world.

    First of all, we observe that real-world encryption systems must work with messages of \textit{different lengths}. Formally, we say that these SKEs have \textbf{Variable Input Length (VIL)}, meaning that for each message $m$ it holds that $\abs{m} = \poly(\lambda)$, where $\lambda$ is the key length. In order to achieve security for VIL SKEs, we require a \curlyquotes{standardized way} to encrypt messages of variable length. To solve this issue, each message $m$ is split into a variable number of \textbf{blocks} of an equal fixed length $n$ (commonly is a power of 2, such as 256). In other words, each message $m$ is considered as $m = (m_1, \ldots, m_d)$ where $d \in \N$ and $m_i \in \{0,1\}^n$.
    
    The simplest encryption type is the \textbf{Electronic Code Book (ECB)} mode, where the scheme simply applies the selected PRF to every block of the message.
    
    \begin{frameddefn}{Electronic Code Book (ECB) mode}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF. Let $K \in \{0,1\}^\lambda$ be a chosen key. On input $m = (m_1, \ldots, m_d)$ the ECB encryption mode returns the ciphertext $c = (c_1, \ldots, c_d)$ constructed as:
        \begin{itemize}
            \item For all $i \in [d]$ it holds that $c_i = F_K(m_i)$
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/ECB.pdf}
        \caption{Graphical representation of ECB mode.}
    \end{figure}
    
    Clearly, the ECB mode cannot be CPA-secure since it is deterministic. As discussed in the PRF-OTP example, a nonce value is sufficient to make the ecryption non-deterministic. In encryption modes, this nonce value is usually referred to as \textbf{Initialization Vector (IV)}. In order to make decryption harder to bruteforce, common encryption modes are based on \textit{sequential encryption}: some value produced during the encryption of block $m_i$ is used for the encryption of block $m_{i+1}$. This allows us to restrict our interests to the secretness of the key and the IV. Moreover, it also makes decryption unparallelizable, further improving the security of the SKE. The first type of encryption mode based on this idea is the \textbf{Cipher Block Chaining (CBC)}.

    \begin{frameddefn}{Cipher Block Chaining (CBC) mode}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF. Let $K \in \{0,1\}^\lambda$ be a chosen key. On input $m = (m_1, \ldots, m_d)$ the CBC encryption mode returns the ciphertext $c = (c_1, \ldots, c_d)$ constructed as:
        \begin{itemize}
            \item $c_0 \in_R \mathcal{U}_n$ is a nonce value (IV)
            \item For all $i \in [d]$ it holds that $c_i = F_K(c_{i-1} \oplus m_i)$
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/CBC.pdf}
        \caption{Graphical representation of CBC mode.}
    \end{figure}

    Other types of encryption mode are pretty much based on ideas similar to CBC. The only real difference lies in the order of operations. For instance, the \textbf{Cipher Feedback (CFB)} mode XORes each block after applying the PRF to the nonce value, instead of before. We observe that this was the idea behind our PRF-OTP example, the only difference lies in the chaining of encryptions.


    \begin{frameddefn}{Cipher Feedback (CFB) mode}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF. Let $K \in \{0,1\}^\lambda$ be a chosen key. On input $m = (m_1, \ldots, m_d)$ the CFB encryption mode returns the ciphertext $c = (c_1, \ldots, c_d)$ constructed as:
        \begin{itemize}
            \item $c_0 \in_R \mathcal{U}_n$ is a nonce value (IV)
            \item For all $i \in [d]$ it holds that $c_i = F_K(c_{i-1}) \oplus m_i$
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/CFB.pdf}
        \caption{Graphical representation of CFB mode.}
    \end{figure}

    Another encryption mode extremely similar to CFB is the \textbf{Output Feedback (OFB)} mode, where the output of the PRF is directly forwarded instead of being XORed first.

    \begin{frameddefn}{Output Feedback (OFB) mode}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF. Let $K \in \{0,1\}^\lambda$ be a chosen key. On input $m = (m_1, \ldots, m_d)$ the OFB encryption mode returns the ciphertext $c = (c_1, \ldots, c_d)$ constructed as:
        \begin{itemize}
            \item $q_0 \in_R \mathcal{U}_n$ is a nonce value (IV)
            \item For all $i \in [d]$ it holds that $q_i = F_K(q_{i-1})$
            \item For all $i \in [d]$ it holds that $c_i = q_i \oplus m_i$
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/OFB.pdf}
        \caption{Graphical representation of OFB mode.}
    \end{figure}

    The last standard encryption mode we discuss is \textbf{Counter (CTR)} mode. Instead of using the previous (partial) output as the next input for the PRF, this mode gradually increases the initial nonce value and uses it for the next encryptions.

    \begin{frameddefn}{Counter (CTR) mode}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF. Let $K \in \{0,1\}^\lambda$ be a chosen key. On input $m = (m_1, \ldots, m_d)$ the CTR encryption mode returns the ciphertext $c = (c_0, c_1, \ldots, c_d)$ constructed as:
        \begin{itemize}
            \item $r \in_R \mathcal{U}_n$ is a nonce value (IV)
            \item $c_0 = r$
            \item For all $i \in [d]$ it holds that $c_i = F_K(r+i-1) \oplus m_i$
        \end{itemize}

        \textit{Note}: the sum operation is in mod 2.
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \resizebox{1\textwidth}{!}{
            \includegraphics[scale=1]{images/CTR.pdf}
        }
        \caption{Graphical representation of CTR mode.}
    \end{figure}

    CBC, CFB, OFB and CTR can all be proven to be CPA-secure for VIL. Instead of proving the security of all of them, we'll give an idea by proving the result only for CTR (which is actually harder to prove).
    
    \begin{framedthm}[label={cpa_prf}]{CPA-security through PRFs}
        CBC, CFB, OFB and CTR are CPA-secure for VIL when constructed using a PRF
    \end{framedthm}

    \begin{proof}
        We prove the theorem only for CTR. Let $G(\lambda, b) := \mathrm{Game}^{\text{CPA}}_{\Pi,A}(\lambda,b)$, where $\Pi$ is a CTR mode scheme constructed through a PRF $\mathcal{F}$.

        Recall that for $G(\lambda, b)$ the challenge is given by two messages $m_0^*, m_1^*$ such that $m_b^* = (m^*_{b,1}, \ldots, m^*_{b,d^*})$ and an answer $c^* = (c^*_0, c^*_1, \ldots, c^*_{d^*})$ such that $c^*_0 = r^* \in_R \mathcal{U}_n$ and $c^*_i = F_K(r^*+i-1) \oplus m^*_{b,i}$.

        We define an hybrid distribution for both values of $b$. Let $H(\lambda, b)$ be the distribution obtained by replacing $F_K(\cdot) \in \mathcal{F}$ with $R(\cdot) \in \mathcal{R}(n \to n)$ in the game $G(\lambda, b)$ (in other words, we're using a random function $R$ for the encryption instead of $F_K$).

        \textbf{Claim 1}: $G(\lambda, b) \approx_c H(\lambda, b)$ for $b \in \{0,1\}$

        \begin{proof}[Proof of Claim 1.]
            We reduce the distringuishability of $F_K$ from $R$ to the distinguishability of $G(\lambda, b)$ from $H(\lambda, b)$. Fix $b \in \{0,1\}$. By way of contradiction, suppose that $G(\lambda, b) \not\approx_c H(\lambda, b)$, i.e. there is an adversary $A$ that distringuished $G(\lambda, b)$ from $H(\lambda, b)$ with probability at least $n^{-c}$ for some $c > 0$.

            Let $A'$ be the adversary for $\mathcal{F}$ defined as follows:
            \begin{enumerate}
                \item The adversary $A'$ starts both games with the challenger $C$ and the other adversary $A$
                \item For $i \in [t]$ iterations, with $t = \poly(\lambda)$, $A$ sends a message $m^{(i)} = (m^{(i)}_1, \ldots, m^{(i)}_{d_i})$ to $A'$
                \item Upon receiving $m^{(i)}$, $A'$ sends $r^{(i)}, r^{(i)}+1, \ldots, r^{(i)}+d-1$ to the challenger $C$, which answers with $z^{(i)}_1, \ldots, z^{(i)}_d$
                \item $A'$ sends $c^{(i)} = (r^{(i)}, c^{(i)}_1, \ldots, c^{(i)}_{d_i})$ back to $A$, where $c^{(i)}_j = z^{(i)}_j \oplus m^{(i)}_j$
                \item After $t$ iterations, the above istructions are repeated again with the challenge messages $m_0^*, m_1^*$, getting the answer $c^*$
                \item Once $A$ receives $c^*$ it returns a final answer $b' \in \{0,1\}$ and $A'$ forwards $b'$ to $C$
                \item If $b' = b$, $A'$ wins the PRF-game with $C$, otherwise it loses.
            \end{enumerate}

            The idea behind the above simultaneous game is to use the challenger $C$ to compute the outputs of either a pseudorandom function or a truly random one (depends on the value of $b$) and use those outputs to play the game with $A$.
            
            If $b = 0$, $C$ will use a pseudorandom function to compute the outputs, thus $A$ will recognize that we're playing $G(\lambda, b)$, outputting $0$. If $b = 1$, $C$ will use a truly random function to compute the outputs, thus $A$ will recognize that we're playing $H(\lambda, b)$, outputting $1$. Therefore, $A'$ distinguishes $F_K$ from $R$ with non-negligible probability, contradicting the fact that $\mathcal{F}$ is a PRF.
        \end{proof}

        Now, show that both $H(\lambda,0)$ and $H(\lambda, 1)$ are indistinguishable from $\mathcal{U}_{n(d^*+1)}$, concluding the proof of the theorem through transitivity of $\approx_c$.

        \textbf{Claim 2}: $H(\lambda, b) \approx_c \mathcal{U}_{n(d^*+1)}$ for $b \in \{0,1\}$

        \begin{proof}[Proof of Claim 2]
            Consider the values $R(r^*), R(r^*+1), \ldots, R(r^*+d^*-1)$ computed during the challenge query of the game $H(\lambda, b)$. Similarly, consider the values $R(r^{(i)}), R(r^{(i)}+1), \ldots, R(r^{(i)}+d_i-1)$ computed during the encryption queries.

            Let $B_i$ be the binary \curlyquotes{bad event} such that $B_i = 1$ if $\exists j \in [0,d_i-1], j^* \in [0, d^*-1]$ such that $r^{(i)} + j = r^* + j^*$, otherwise $B_i = 0$. More generally, let $B = B_1 \cup \ldots \cup B_t$

            Consider the conditioning over the event $B = 0$. When $B = 0$, the two sequences never overlap over each $i \in [t]$, which means that the challenge ciphertext is distributed as:
            \begin{itemize}
                \item $c^*_0 = r^*$
                \item $c^*_{j^*} = m^*_{b,{j^*}} \oplus u_{j^*}$ with $u_{j^*} \in_R \mathcal{U}_n$ for $j^* \in [0,d^*-1]$
            \end{itemize}

            which is indistinguishable from $\mathcal{U}_{n(d^*+1)}$. As a consequence of this, we get that:
            \[\mathrm{SD}(H(\lambda, b); \mathcal{U}_{n(d+1)}) \leq \Pr[B = 1]\]

            We're left with computing $\Pr[B = 1]$. Fix $i \in [t]$. Without loss of generality, assume that $d_i = d^* = t = q$. Since the event $B_i = 1$ holds when $r^*, r^*+1, \ldots, r^*+1-1$ overlaps with $r^{(i)}, r^{(i)}+1, \ldots, r^{(i)}+q-1$, this can happen only when:
            \[r^*-q+1 \leq r^{(i)} \leq r^* + q-1\]

            Therefore, we have that:
            \[\Pr[B_i = 1] \leq \frac{1+(r^*+q-1)-(r^*-q-1)}{2^n} = \frac{2q-1}{2^n}\]

            Finally, through the union bound we conclude that:
            \[\Pr[B = 1] \leq \sum_{i = 1}^{q} \Pr[B_i = 1] = \frac{q(2q-1)}{2^n}\]

            thus $H(\lambda, b) \approx_c \mathcal{U}_{n(d^*+1)}$.
        \end{proof}

        The two claims conclude that $G(\lambda,0) \approx_c H(\lambda, 0) \approx_c \mathcal{U}_(n(d^*+1)) \approx H(\lambda, 1) \approx_c G(\lambda, 1)$.
    \end{proof}

    \section{UFCMA-security and universal hash families}

    After showing that the common encryption schemes are CPA-secure, we focus on secure message authentication. Conside any tag function $\mathrm{Tag} : \mathcal{K} \times \mathcal{M} \to \mathcal{T}$. When can this function be considered secure under a computational setting? The idea is to make it hard to forge a pair $(m^*, \tau^*)$ such that $\mathrm{Tag}(K,m^*) = \tau^*$ when $K$ is unknown. This concept is formally known as \textbf{Unforgeability under Chosen-Message Attacks (UFCMA)}.

    In $\mathrm{Game}^{\text{UFCMA}}_{\Pi, A}(\lambda)$, the adversary is given access to an oracle for the $\mathrm{Tag}$ function and it is allowed to make $t = \poly(\lambda)$ tag queries to the oracle. After completing the tag queries, the adversary returns a pair $(m^*, \tau^*)$ to the challenger (with $m^* \neq m_i$ for all $i \in [t]$). The challenger computes the tag of $m^*$ and compares it with $\tau^*$. If $\mathrm{Tag}(K, m^*) = \tau^*$ is true, the adversary wins. Otherwise, the challenger wins.

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$m_i \in \{0,1\}^\lambda$} (4)
                (6) edge[swap, near start] node{$\tau_i = \mathrm{Tag}(K, m_i)$} (5)
                (7) edge[near start] node{$m^*, \tau^* \in \{0,1\}^\lambda$} (8)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}^{\text{UFCMA}}_{\Pi, A}(\lambda)$}
    \end{figure}

    \begin{frameddefn}{UFCMA-security}
        Let $\Pi = (\mathrm{Tag})$ be a MAC. We say that $\Pi$ is UFCMA-secure if:
        \[\Pr[\mathrm{Game}^{\text{UFCMA}}_{\Pi, A}(\lambda) = 1] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    Pseudorandom functions are intuitively an easy way to get an UFCMA-secure MAC: since the function is hard to invert, the adversary won't be able to infer informations on the key through message-tag pairs. However, this works only for FIL (Fixed Input Length) schemes.
    
    \begin{framedthm}[label={ufcma_prf}]{UFCMA-security through PRFs}
        If $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ is a PRF family, $\mathrm{Tag}(K, m) = F_K(m)$ is UFCMA-secure for FIL.
    \end{framedthm}

    \begin{proof}
        Let $G(\lambda) := \mathrm{Game}_{\Pi,A}^{\text{UFCMA}}(\lambda)$. Let $H(\lambda)$ be the hybrid distribution obtained by replacing $F_K(\cdot) \in \mathcal{F}$ with $R(\cdot) \in \mathcal{R}(n \to n)$ in the game $G(\lambda)$ (in other words, we're using a randum function $R$ for the tag instead of $F_K$).

        Proceeding in the usual way with a simultaneous game, we can reduce the distringuishability of $F_K$ from $R$ to the distinguishability of $G(\lambda)$ from $H(\lambda)$ (omitted, similar to Claim 1 of \Cref{cpa_prf}). This concludes that $G(\lambda) \approx_c H(\lambda)$.
        
        Finally, since $H(\lambda)$ uses a truly random function $R$, the probability of an adversary being able to guess the correct tag for a message is at most $2^{-n}$, concluding that $\Pi$ is UFCMA-secure.
    \end{proof}

    But what about VIL tags? What about inputs of longer FIL? Both questions fall into the idea of \textbf{domain expansion} for MACs. It's easy to see that simple tricks don't work. For instance, consider the tag function that concatenes the tags of the blocks, i.e. $\mathrm{Tag}(K,m) = \tau$ such that $m = (m_1, \ldots, m_d)$ and $\tau = (\tau_1, \ldots, \tau_d)$. The UFCMA game requires that message in the challenge query is different from the messages in the tag queries. However, a simple trick suffices to \curlyquotes{break} this constraint:
    \begin{enumerate}
        \item The adversary queries $m_1 \mid\mid m_2$ and gets $\tau_1 \mid\mid \tau_2$
        \item The adversary then queries $m_3 \mid\mid m_4$ and gets $\tau_3 \mid\mid \tau_4$
        \item The adversary challenges with the pair $(m_1 \mid\mid m_3, \tau_1 \mid\mid \tau_3)$ and wins with probability 1
    \end{enumerate}

    Consider now the tag function that XORes the blocks and returns a single tag, i.e. $\mathrm{Tag}(K, \bigoplus_{i = 1}^d m_i) = \tau$. It's easy to see that this construction still doesn't work:
    \begin{enumerate}
        \item The adversary queries $m_1 \mid\mid m_1$ and gets $\tau_1$
        \item The adversary challenges with the pair $(m_2 \mid\mid m_2, \tau_1)$ and wins with probability 1
    \end{enumerate}

    It seems that our construction cannot be easily changed to allow VIL or longer FIL inputs. However, we still have a trick up our sleeve: we can hash an input of length $nd$ and reduce it to length $n$, preserving the UFCMA security of the construction. In other words, we have that $\mathrm{Tag}(K,m) = F_K(h_s(m))$ with $h_s \in \mathcal{H}$, for an hash family $\mathcal{H} = \{h_s : \{0,1\}^{nd} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$.

    Does this idea work with every type of hash family? Inuitively, no. The hash family should be such that every hash function hardly collides on different inouts, meanin that it should be hard to have two messages $m, m'$ with $m \neq m'$ such that $h_s(m) = h_s(m')$. Depending on the degree of hardness, these hash families are called \textbf{$\epsilon$-almost universal}.

    \begin{frameddefn}{Universal hash families}
        Let $\mathcal{H} = \{h_s : \{0,1\}^{nd} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ be a family of hash functions. We say that $\mathcal{H}$ is $\varepsilon$-almost universal if $\forall m,m' \in \mathcal{M}$ with $m \neq m'$ it holds that:
        \[\Pr_{s \in_R \{0,1\}^\lambda} [h_s(m) = h_s(m')] \leq \varepsilon\]
        
        When $\varepsilon = \mathrm{negl}(\lambda)$, the family is said to be \textit{almost universal (AU)}. When $\varepsilon = 2^{-n}$, the family is said to be \textit{perfectly universal (PU)}.
    \end{frameddefn}

    As the name suggests, PU hash families are considered \textit{perfect} in a sense that it is practically impossible to get a collision. Nonetheless, for our purposes AU hash families are enough. In fact, it can be proven that composing them with a PRF yields another PRF.

    \begin{framedthm}[label={prf_au}]{Composition of PRFs with AU hash families}
        Given a PRF family $\mathcal{F} = \{F_k : \{0,1\}^n \to \{0,1\}^n\}$ and an AU hash family  $\mathcal{H} = \{h_s : \{0,1\}^{nd} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$, the family $\mathcal{F}(\mathcal{H})$ defined as:
        \[\mathcal{F}(\mathcal{H}) = \{F_K \circ h_s : \{0,1\}^{nd} \to \{0,1\}^n\}_{(K,s) \in \{0,1\}^{2\lambda}}\]
        is a PRF family.
    \end{framedthm}

    \begin{proof}
        Consider the two games $G(\lambda, b) := \mathrm{Game}_{\mathcal{F}(\mathcal{H}), A}^{\text{PRF}}(\lambda, b)$ with $b \in \{0,1\}$. We recall that $G(\lambda, 0)$ is the PRF game where we use a function $F_K(h_s(\cdot)) \in \mathcal{F}(\mathcal{H})$, while $G(\lambda, 1)$ is the PRF game where we use a truly random function $R \in \mathcal{R}(nd \to n)$.

        Ler $H(\lambda)$ be the hybrid distribution that plays the PRF game with the function $R'(h_s(\cdot))$, where $R' \in \mathcal{R}(n \to n)$. It can be proven that $G(\lambda,0) \approx_c H(\lambda)$ in the standard way by reducing the distringuishability of $\mathrm{F}_K$ from $R'$ to the distringuishability of $G(\lambda,0)$ from $H(\lambda)$.

        Therefore, it remains to show that $H(\lambda) \approx_c G(\lambda,1)$. Let $m_1, \ldots, m_t$ be the encryption queries made by the adversary, where $t = \poly(\lambda)$. Let $B$ be the bad event such that $B = 1$ if $\exists i,j \in [t]$ with $i \neq j$ for which $h_s(m_i) = h_j(m_j)$. When conditioning on $B = 0$, no collisions occurr, thus :
        \[\mathrm{SD}(H(\lambda); G(\lambda, 1)) \leq \Pr[B = 1]\]

        Through the union bound, we have that:
        \[\begin{split}
            \Pr[B = 1] &= \Pr_{s \in_R \{0,1\}^R} [\exists i,j \in [t] \; i \neq j \text{ s.t. } h_s(m_i) = h_s(m_j)] \\
            &\leq \sum_{i = 1}^t \sum_{\substack{j = 0 :\\j \neq i}}^t \Pr_{s \in \{0,1\}^\lambda} [h_s(m_i) = h_s(m_j)] \\
            &= \binom{t}{2} \varepsilon \\
            &= \mathrm{negl}(\lambda)
        \end{split}\]

        Therefore, $H(\lambda) \approx_c G(\lambda,1)$.
    \end{proof}

    Surprisingly, it is very easy to construct universal hash families, in particular through \textit{Galois fields}. We give two examples:
    \begin{enumerate}
        \item Let $\F = \mathrm{GF}(2^n)$. Let $m \in \F^d$ be the input and let $s \in \F^d$ be the hash seed, where $m_i, s_i \in \F$. We define each $h_s(\cdot)$ as the scalar product between the input and the hash seed:
        \[h_s(m) = \abk{s, m} = \sum_{i = 1}^d s_i m_i\]

        Then, given $m, m' \in \F^d$ with $m \neq m'$, we have that:
        \[\begin{split}
            h_s(m) = h_s(m') &\iff \abk{s, m} = \abk{s, m'} \\
            &\iff \sum_{i = 1}^d s_i m_i = \sum_{i = 1}^d s_i m_i' \\
            &\iff \sum_{i = 1}^d s_i (m_i-m_i') = 0 \\
            &\iff \sum_{i = 1}^d s_i \delta_i = 0 \\
        \end{split}\]

        where $\delta_i = m_i - m_i'$ for each $i \in [d]$. Since $m \neq m'$, there is at least one index $j \in [d]$ such that $m_j \neq m_j'$, therefore $\delta_j \neq 0$:
        \[\begin{split}
            h_s(m) = h_s(m') &\iff \sum_{i = 1}^d s_i \delta_i = 0 \\
            &\iff s_j = - \frac{1}{\delta_j} \sum_{\substack{i = 1 :\\ j \neq i}}^d s_i\delta_i
        \end{split}\]

        which is true with probability at most $2^{-d}$. This concludes that $\mathcal{H}$ is a perfectly universal hash family.

        \item Let $\F = \mathrm{GF}(2^n)$. Let $m \in \F^d$ be the input and let $s \in \F$ be the hash seed, where $m_i, s_i \in \F$. We define each $h_s(\cdot)$ as the $d$-degree polynomial $Q_m(s) \in \F[s]_{d}$ with coefficients $m_1, \ldots, m_d$ and variable $s$:
        \[h_s(m) = Q_m(s) = \sum_{i = 1}^d m_i s^{i-1}\]

        Then, given $m, m' \in \F^d$ with $m \neq m'$, we have that:
        \[\begin{split}
            h_s(m) = h_s(m') &\iff Q_m(s) = Q_{m'}(s)\\
            &\iff \sum_{i = 1}^d m_i s^{i-1} = \sum_{i = 1}^d m_i' s^{i-1}  \\
            &\iff \sum_{i = 1}^d (m_i-m_i') s^{i-1} = 0 \\
        \end{split}\]
        which is true with probability at most $(d-1)2^{-n}$. This concludes that $\mathcal{H}$ is an almost universal hash family.
    \end{enumerate}

    A more complex type of almost universal hash family can be obtained through encryption modes. For instance, the CBC encryption mode can be modified to define an AU hash family usually referred to as \textbf{CBC-MAC}.
    
    \begin{frameddefn}{CBC-MAC}
        Let $\mathcal{F} = \{F_k : \{0,1\}^n \to \{0,1\}^{n}\}_{k \in \{0,1\}^\lambda}$ be a PRF family. We define the CBC-MAC family as $\mathcal{H}_{\text{CBC}} = \{h_s : \{0,1\}^n \to \{0,1\}^{nd}\}_{s \in \{0,1\}^\lambda}$, where:
        \[h_s(m_1, \ldots, m_d) = F_s(m_d \oplus F_s(m_{d-1} \oplus F_s(\ldots m_2 \oplus F_s(m_1)) \ldots ))\]
    \end{frameddefn}
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/CBC-MAC.pdf}
        \caption{Graphical representation of CBC-MAC.}
    \end{figure}

    The CBC-MAC family has many properties. First of all, it can be easily proven that $\mathcal{H}_{\text{CBC}}$ is also a PRF family through a reduction argument. In a similar way, it can be proven that it is also UFCMA-secure, but only for FIL. Nonetheless, \Cref{ufcma_prf} and \Cref{prf_au} still imply that $\mathcal{F}(\mathcal{H}_{\text{CBC}})$ is an UFCMA-secure MAC scheme.

    \begin{framedthm}{}
        Let $\mathcal{F}$ be a PRF family and let $\mathcal{H}_{\text{CBC}}$ be the corresponding CBC-MAC hash family. Then, it holds that:
        \begin{itemize}
            \item $\mathcal{H}_{\text{CBC}}$ is a PRF
            \item $\mathcal{H}_{\text{CBC}}$ is AU for FIL
            \item $\mathcal{F}(\mathcal{H}_{\text{CBC}})$ is a PRF
            \item $\mathcal{F}(\mathcal{H}_{\text{CBC}})$ is UFCMA-secure
        \end{itemize}
    \end{framedthm}

    \begin{proof}
        Omitted.
    \end{proof}

    \section{CCA-security and non-malleability}

    In the previous sections we gave the idea behind computationally secure encryption and computationally secure message authentication. Now, we're ready to combine the two concepts. We say that an encryption scheme is \textbf{malleable} if it is possible to transform a  ciphertext $c \in \mathcal{C}$ -- of which we don't know the original plaintext -- into another ciphertext $\widetilde c$ which decrypts to another plaintext related to the original one. Formally, given an encryption $c$ of a plaintext $m$, it is possible to generate another cyphertext $\widetilde c$ of another plaintext $\widetilde m$ that is in some way related to $m$.
    
    For instance, consider again the PRG-OTP scheme, where $\mathrm{Enc}(K,m) = G(K) \oplus$ for some PRG $G$. Suppose that we know a ciphertext $c \in \mathcal{C}$, \underline{without} knowing the original message $m$. Then, for any value $t \in \{0,1\}^n$ we can construct the ciphertext $\widetilde c$ of the message $\widetilde m = m \oplus t$ by XORing $c$ with $t$:
    \[\widetilde c = c \oplus t = \mathrm{Enc}(K,m) \oplus t = (G(K) \oplus m ) \oplus t = G(K) \oplus (m \oplus t) = \mathrm{Enc}(K, \widetilde m)\]

    Malleability is often an undesirable property in a general-purpose cryptosystem (e.g. digital auctions), since it allows an attacker to modify the contents of a message: Eve could steal the ciphertext $c$ from Alice, compute $\widetilde c$ and send it to Bob, which will decrypt it as $\widetilde m$. It's easy to see that even CPA-secure SKEs may be malleable. For instance, consider again the PRF-OTP scheme, where:
    \begin{enumerate}
        \item $\mathrm{Enc}(K,m) = (r, F_K(r) \oplus m)$, where $r \in \{0,1\}^n$ is a nonce
        \item $\mathrm{Dec}(K,(r,c)) = F_K(r) \oplus c$
    \end{enumerate} 

    for a PRF family $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$. We already discussed how this scheme is CPA-secure. Consider now the ciphertext $(r,c) \in \{0,1\}^n \times \mathcal{C}$ of an unknown message $m$, but the same trick used for PRG-OTP works even in this case. Let $\tilde c = c \oplus t$ for any $t \in \{0,1\}^n$. Then, we have that
    \[(r, \tilde c) = (r, c \oplus t) = (r, (F_k(r) \oplus m) \oplus t) = (r, F_k(r) \oplus (m \oplus t)) = \mathrm{Enc}(K, \widetilde m)\]
    where $\widetilde m = m \oplus t$.
    
    In the computational security context, \textit{malleability attacks} are known as \textbf{Chosen-cipher-} \textbf{text Attack (CCA)}. Formally, CCA-security is defined through the game $\mathrm{Game}_{\Pi,A}^{\text{CCA}}(\lambda, b)$, similar to the one for CPA-security with the addition of \textit{decryption queries} after each series of encryption queries (both before and after the sending the challenge messages). Obviously, the ciphertext sent in each decryption query must be strictly different from the challenge ciphertext, otherwise the attacker could win the game with probability 1.

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (3y) [below of = 5]{};
            \node[] (4y) [below of = 6]{};

            \node[] (5y) [below of = 3y]{};
            \node[] (6y) [below of = 4y]{};

            \node[] (7) [below of = 5y]{};
            \node[] (8) [below of = 6y]{};

            \node[] (9) [below of = 7]{};
            \node[] (10) [below of = 8]{};

            \node[] (3x) [below of = 9]{};
            \node[] (4x) [below of = 10]{};

            \node[] (5x) [below of = 3x]{};
            \node[] (6x) [below of = 4x]{};

            \node[] (3z) [below of = 5x]{};
            \node[] (4z) [below of = 6x]{};

            \node[] (5z) [below of = 3z]{};
            \node[] (6z) [below of = 4z]{};

            \node[] (11) [below of = 5z]{};
            \node[] (12) [below of = 6z]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$m_i \in \{0,1\}^\lambda$} (4)
                (6) edge[swap, near start] node{$c_i = \mathrm{Enc}_\Pi(K, m_i)$} (5)

                (5y) edge[bend left] node {$\poly(\lambda)$}(3y)
                (3y) edge[near start] node{$\widetilde c_i \in \{0,1\}^\lambda, \widetilde c_i \neq c^*$} (4y)
                (6y) edge[swap, near start] node{$\widetilde m_i = \mathrm{Dec}_\Pi(K, \widetilde c_i)$} (5y)

                (7) edge[near start] node{$m_0^*, m_1^* \in \{0,1\}^\lambda$} (8)
                (10) edge[swap, near start] node{$c^* = \mathrm{Enc}_\Pi(K, m_b^*)$} (9)

                (5x) edge[bend left] node {$\poly(\lambda)$}(3x)
                (3x) edge[near start] node{$m_i' \in \{0,1\}^\lambda$} (4x)
                (6x) edge[swap, near start] node{$c_i' = \mathrm{Enc}_\Pi(K, m_i')$} (5x)

                (5z) edge[bend left] node {$\poly(\lambda)$}(3z)
                (3z) edge[near start] node{$\widetilde c_i' \in \{0,1\}^\lambda, \widetilde c_i' \neq c^*$} (4z)
                (6z) edge[swap, near start] node{$\widetilde m_i' = \mathrm{Dec}_\Pi(K, \widetilde c_i')$} (5z)
                
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda, b)$.}
    \end{figure}

    \begin{frameddefn}{CCA-security}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE. We say that $\Pi$ is CCA-secure if:
        \[\mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda, 0) \approx_c \mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda, 1)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    Since the CCA-game is way more complex than the CPA-game due to the presence of decryption queries, we need a simpler procedure to prove that a scheme is CCA-secure. In particular, we'll prove that CPA-security together with an additional property implies CCA-security. This additional property is the computational security equivalent of \textbf{Integrity (INT)}: it must be hard for any adversary PPT to generate a \textit{valid} chipertext without knowing the key chosen by the scheme, otherwise they could use this ability to alter the original message and forge a new chipertext that decrypts into the altered message.

    In $\mathrm{Game}_{\Pi, A}^{\text{INT}}(\lambda)$, the adversary is given access to an oracle for the $\mathrm{Enc}$ function and it is allowed to make $t = \poly(\lambda)$ encryption queries to the oracle. After completing the encryption queries, the adversary returns a chipertext $c^*$ to the challenger (with $c^* \neq c_i$ for all $i \in [t]$). The challenger then computes $\mathrm{Dec}(K, c^*)$ and compares it with a special value $\bot$, which can be thought of as a \curlyquotes{fake message}. If $\mathrm{Dec}(K, c^*) \neq \bot$ is true, the adversary wins. Otherwise, the challenger wins.

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (3) [below of = 1]{};
            \node[] (4) [below of = 2]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$m_i \in \{0,1\}^\lambda$} (4)
                (6) edge[swap, near start] node{$c_i = \mathrm{Enc}(K, m_i)$} (5)
                (7) edge[near start] node{$c^* \in \{0,1\}^\lambda$} (8)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}^{\text{INT}}_{\Pi, A}(\lambda)$}
    \end{figure}

    Let's solidify the idea behind the INT game: if the adversary is able to forge a ciphertext that decrypts into a message $m$ that is different from the fake message $\bot$, we know that they are clearly capable of forging a valid chipertext even if they don't know the key, breaking the integrity of the scheme.

    \begin{frameddefn}{Integrity (in computational security)}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE. We say that $\Pi$ satisfies integrity if:
        \[\Pr[\mathrm{Game}^{\text{INT}}_{\Pi, A}(\lambda) = 1] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    \begin{framedthm}{CPA$+$INT implies CCA}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE. Then, $\Pi$ is CCA-secure if it is CPA-secure and satisfies integrity.
    \end{framedthm}

    \begin{proof}[Proof (sketch).]
        The full proof requires to be extremely formal, hence we'll only give a sketch. The idea is simple: we can reduce both the distinguishability of $\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda,b)$ and a win of $\mathrm{Game}_{\Pi,A}^{\text{INT}}(\lambda)$ to the distinguishability of $\mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda,b)$.
        
        We start with the CPA-game. Let $H(\lambda,b)$ be the game identical to the CCA-game where decryption queries are substituted by the following logic: if the attacker queries a ciphertext $\widetilde c_i$ that was the answer to a previous encryption query $m_i$, the challenger returns $m_i$, otherwise it returns the special message $\bot$.
        
        \textbf{Claim 1}: $H(\lambda, 0) \approx_c H(\lambda,1)$

        \begin{proof}[Proof of Claim 1.]
            By way of contradiction, suppose that there is an adversary $A$ that is able to distinguish the CCA-game with non-negligible probability. Let $B$ be the adversary defined as follows:
            \begin{enumerate}
                \item $B$ starts the hybrid game with $A$ and the CPA game with $C$
                \item $A$ sends a series of encryption queries to $B$. Each query $m_i$ gets forwarded to the challenger $C$, which will answer with a chipertext $c_i$. The ciphertext is then forwarded from $B$ to $A$.
                \item $A$ sends a series of decryption queries to $B$. For each query $\widetilde c_i$, $B$ returns $m_i$ if $(m_i, \widetilde c_i)$ is a query-answer pair of the previous encryption queries, otherwise it returns $\bot$
                \item $A$ sends the challenge messages $m_0^*, m_1^*$ to $B$, which then forwards them to $C$. Afterwards, $C$ replies with $c^*$, which gets forwarded back to $A$ by $B$.
                \item $A$ and $B$ play another series of encryption queries followed by decryption queries, using the same rules as the previous ones.
                \item $A$ makes a guess by sending a bit $b' \in \{0,1\}$ to $B$, which forwards it to $C$. If $b' = b$, $B$ wins the game
            \end{enumerate}

            We observe that $H(\lambda, b)$ is basically the CCA game where we \curlyquotes{fix} the decryption queries, \curlyquotes{collapsing} back to the CPA game. Therefore, the above reduction concludes that $H(\lambda, 0) \approx_c H(\lambda,1)$, otherwise we could distinguish the CPA game.
        \end{proof}

        We're now left with proving that $\mathrm{Game}_{\Pi,A}^{\text{CCA}}(\lambda, b) \approx_c H(\lambda, b)$.

        \textbf{Claim 2}: $\mathrm{Game}_{\Pi,A}^{\text{CCA}}(\lambda, b) \approx_c H(\lambda, b)$ for $b \in \{0,1\}$

        \begin{proof}[Proof of Claim 2.]
            We define a reduction identical to one of Claim 1 but with a small difference: instead of forwarding both challenge messages $m_0^*, m_1^*$, we'll forward only $m_b^*$. By way of contradiction, suppose that there is an adversary $A'$ that is able to distinguish $\mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda, b)$ from $H(\lambda, b)$ with non-negligible probability. $A'$ returns 0 when it recognizes the CCA game, otherwise it returns 1 when it recognizes the hybrid game.
            
            We define an adversary $B'$ for the INT game:
            \begin{enumerate}
                \item $B'$ starts both games with $A'$ and $C'$
                \item $A'$ sends a series of encryption queries to $B'$. Each query $m_i$ gets forwarded to the challenger $C'$, which will answer with a chipertext $c_i$. The ciphertext is then forwarded from $B'$ to $A'$.
                \item $A'$ sends a series of decryption queries to $B'$. For each query $\widetilde c_i$, $B'$ returns $m_i$ if $(m_i, \widetilde c_i)$ is a query-answer pair of the previous encryption queries, otherwise it returns $\bot$
                \item $A'$ sends the challenge messages $m_0^*, m_1^*$ to $B'$, which then forwards them to $C'$. Afterwards, $C'$ replies with $c^*$, which gets forwarded back to $A'$ by $B'$.
                \item $A'$ and $B'$ play another series of encryption queries followed by decryption queries, using the same rules as the previous ones.
                \item $A'$ makes a guess by sending a bit $b' \in \{0,1\}$ to $B'$, which forwards it to $C'$. If $b' = b$, $B'$ wins the game
            \end{enumerate}
            
            The above forms a reduction from a win of $\mathrm{Game}_{\Pi, A'}^{\text{INT}}(\lambda)$ to the distinguishability of $\mathrm{Game}_{\Pi, A'}^{\text{CCA}}(\lambda, b)$ from $H(\lambda, b)$. In particular, let $E$ be the event such that $E = 1$ if $A'$ makes a decryption query $\widetilde c$ that is both different from all the answers to the encryption queries and such that $\mathrm{Dec}(K, \widetilde c) \neq \bot$. When conditioning on $E = 0$, we get that $H(\lambda, b) \equiv \mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda, b)$, therefore for $A'$ it holds that:
            \[\Pr[\mathrm{Game}_{\Pi, A'}^{\text{IND}}(\lambda) = 1] \geq \Pr[E = 0] \Pr[\text{$A'$ causes $E = 0$} \mid E = 0] \geq \frac{1}{s} \cdot \frac{1}{n^c}\]

            where $s$ is the number of decryption queries, contradicting the fact that $\Pi$ satisfies integrity.
        \end{proof}
    \end{proof}
        

    The very next natural thing to inquire is a SKE that satisfies both CPA-security and integrity, in order to obtain a CCA-secure scheme. Not so surprisingly, combining a CPA-secure scheme with a secure MAC satisfies our needs. First, let's see in which ways we could combine these two ideas. Let $\Pi_1 = (\mathrm{Enc}, \mathrm{Dec})$ be an SKE with key space $\mathcal{K}_1$ and let $\Pi_2 = (\mathrm{Tag})$ be a MAC with key space $\mathcal{K}_2$. We define the following ways to combine the two schemes:
    \begin{enumerate}
        \item \textbf{Encrypt-and-MAC (E\&M)}: we define a new SKE $\Pi' = (\mathrm{Enc}', \mathrm{Dec}')$ as:
        \[\mathrm{Enc}'((K_1,K_2), m) = (\mathrm{Enc}(K_1, m), \mathrm{Tag}(K_2,m))\]
        \[\mathrm{Dec}'((K_1, K_2), (c, \tau)) = \soe{ll}{
            \bot & \text{if } \mathrm{Tag}(K_2, \mathrm{Dec}(K_1, m)) \neq \tau \\
            \mathrm{Dec}(K_1, m) & \text{otherwise}
        }\]
        where $(K_1, K_2) \in \mathrm{K}_1 \times \mathrm{K}_2$.

        \item \textbf{Encrypt-then-MAC (EtM)}: we define a new SKE $\Pi' = (\mathrm{Enc}', \mathrm{Dec}')$ as:
        \[\mathrm{Enc}'((K_1,K_2), m) = (\mathrm{Enc}(K_1, m), \mathrm{Tag}(K_2,\mathrm{Enc}(K_1, m)))\]
        \[\mathrm{Dec}'((K_1, K_2), (c, \tau)) = \soe{ll}{
            \bot & \text{if } \mathrm{Tag}(K_2, c) \neq \tau \\
            \mathrm{Dec}(K_1, c) & \text{otherwise}
        }\]
        where $(K_1, K_2) \in \mathrm{K}_1 \times \mathrm{K}_2$.

        \item \textbf{MAC-then-Encrypt (MtE)}: we define a new SKE $\Pi' = (\mathrm{Enc}', \mathrm{Dec}')$ as:
        \[\mathrm{Enc}'((K_1,K_2), m) = \mathrm{Enc}(K_1, m \mid\mid \mathrm{Tag}(K_2,m))\]
        \[\mathrm{Dec}'((K_1, K_2), c) = \soe{ll}{
            \bot & \text{if } \mathrm{Tag}(K_2, m) \neq \tau \\
            m & \text{otherwise}
        }\]
        where $(K_1, K_2) \in \mathrm{K}_1 \times \mathrm{K}_2$ and $\mathrm{Dec}(K_1, c) = m \mid\mid \tau$.
    \end{enumerate}  

    In real-world implementations, Encrypt-and-MAC was used by old versions of the \textit{SSL/TLS protocol}, while new versions use Encrypt-then-MAC. Mac-then-Encrypt is used by the \textit{SSH protocol}. Out of all the three constructions, only EtM guarantees that the resulting scheme satisfies both CPA-security and integrity when $\Pi_1$ is CPA-secure and $\Pi_2$ is \textbf{Strong UFCMA (SUFCMA)}. The SUFCMA game is defined in the same way as the UFCMA game, with the additional constraint imposing that $\tau^* \neq \tau_i$ for all $i \in [t]$ (we recall that the UFCMA game only imposes that $m^* \neq m_i$)
    
    \begin{frameddefn}{SUFCMA-security}
        Let $\Pi = (\mathrm{Tag})$ be a MAC. We say that $\Pi$ is SUFCMA-secure if:
        \[\Pr[\mathrm{Game}^{\text{SUFCMA}}_{\Pi, A}(\lambda) = 1] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    \begin{framedthm}{}
        Let $\Pi' = (\mathrm{Enc}', \mathrm{Dec}')$ be an EtM SKE constructed from the SKE $\Pi_1 = (\mathrm{Enc}, \mathrm{Dec})$ and the MAC $\Pi_2 = (\mathrm{Tag})$. Then, $\Pi'$ satisfies both CPA-security and integrity if $\Pi_1$ is CPA-secure and $\Pi_2$ is SUFCMA-secure.
    \end{framedthm}

    \begin{proof}
        As usual, we proceed with two reductions: one that goes from one CPA-security to the other and one that goes from integrity to SUFCMA-security.

        \textbf{Claim 1}: $\Pi'$ is CPA-secure

        \begin{proof}[Proof of Claim 1.]
            By way of contradiction, suppose that there is an adversary $A'$ such that $\mathrm{Game}_{\Pi', A'}^{\text{CPA}}(\lambda,0) \not\approx_c \mathrm{Game}_{\Pi', A'}^{\text{CPA}}(\lambda,1)$. We define a new adversary $A_1$ to distinguish $\mathrm{Game}_{\Pi_1, A_1}^{\text{CPA}}(\lambda,0)$ from $\mathrm{Game}_{\Pi_1, A_1}^{\text{CPA}}(\lambda,1)$:
            \begin{enumerate}
                \item $A_1$ picks a key $K_2 \in_R \mathcal{K_2}$. This key will be used to compute tags (observe that the challenger $C_1$ also picks a key $K_1 \in_R \mathcal{K_1}$, which will be used for encryptions)
                \item $A_1$ starts the CPA game both with $A'$ and $C_1$
                \item Each encryption query $m_i$ from $A'$ to $A_1$ is forwarded to $C_1$, which replies with the ciphertext $c_i$
                \item Upon receiving $c_i$, $A_1$ computes $\mathrm{Tag}(K_2, c_i) = \tau_i$ and returns $(c_i, \tau_i)$ to $A'$
                \item After a polynomial amount of encryption queries, $A'$ sends two challenge messages $m_0^*, m_1^*$ to $A_1$, who will forward them to $C_1$. The challenger then replies with the chipertext $c^*$.
                \item Upon receiving $c^*$, $A_1$ computes $\mathrm{Tag}(K_2, c^*) = \tau^*$ and returns $(c^*, \tau^*)$ to $A'$
                \item After another exchange of a polynomial amount of encryption queries, $A'$ guesses the value of $b$ by sending $b' \in \{0,1\}$. This bit is then forwarded by $A_1$ to $C_1$
                \item If $b' = b$, $A_1$ wins the CPA game against $C_1$, otherwise $C_1$ wins.
            \end{enumerate}

            The above reduction has no particular instruction that must be discusses: $A_1$ just acts as an intermediate between $A'$ and $C_1$, while also tagging the chipertexts sent by $C_1$. Therefore, $A_1$ is able to distinguish the CPA game for $\Pi_1$ if and only if $A'$ is able to do so for $\Pi'$, raising a contradiction on the CPA-security of $\Pi_1$.  
        \end{proof}

        \textbf{Claim 2}: $\Pi'$ satisfies integrity

        \begin{proof}[Proof of Claim 2.]
            By way of contradiction, suppose that there is an adversary $A''$ such that $\Pr[\mathrm{Game}_{\Pi', A''}^{\text{INT}}(\lambda) = 1] \geq \lambda^{-c}$ for some $c > 0$. We define a new adversary $A_2$ such that $\Pr[\mathrm{Game}_{\Pi_2, A_2}^{\text{SUFCMA}}(\lambda) = 1] \geq \lambda^{-c}$:
            \begin{enumerate}
                \item $A_2$ picks a key $K_1 \in_R \mathcal{K_1}$. This key will be used to compute encryptions (observe that the challenger $C_2$ also picks a key $K_2 \in_R \mathcal{K_2}$, which will be used for taggging)
                \item $A_2$ starts the INT game with $A''$ and the SUFCMA game with $C_2$
                \item Each encryption query $m_i$ from $A''$ to $A_2$ is forwarded to $C_2$, which replies with the tag $\tau_i$. 
                \item Upon receiving $\tau_i$, $A_2$ computes $\mathrm{Enc}(K_1, m_1) = c_i$ and returns $(c_i, \tau_i)$ to $A''$
                \item After a polynomial amount of encryption queries, $A''$ sends the challenge pair $(c^*, \tau^*)$ to $A_2$, who will forward it to $C_2$.
                \item If $\mathrm{Tag}(K_2, c^*) = \tau^*$ and $c^* \neq c_i$ for all $c_i$, $A_2$ wins the SUFCMA game against $C_2$, otherwise $C_2$ wins
            \end{enumerate}

            Unlike the previous claim, this reduction requires some discussion. First of all, we observe that in order to play a valid INT game the adversary $A''$ must pick a challenge pair $(c^*, \tau^*)$ such that $(c^*, \tau^*) \neq (c_i, \tau_i)$ for every encryption query $m_i$. However, this can hold true both when $c^* \neq c_i$ and when $\tau^* \neq \tau_i$ (or both). This is where SUFCMA-security plays a crucial role: we know that each tag $\tau_i$ returned by the challenger $C_2$ is unique. 

            Therefore, when $(c^*, \tau^*)$ is such that $\mathrm{Dec}'((K_1, K_2), (c^*, \tau^*)) \neq \bot$ we know that both $\mathrm{Tag}(K_2, c^*) = \tau^*$ (guaranteed since the decryption works) and $c^* \neq c_i$ for all $c_i$ hold (guaranteed by the uniqueness of tags), declaring the adversary $A_2$ as winner. Otherwise, if  $\mathrm{Dec}'((K_1, K_2), (c^*, \tau^*)) = \bot$ then $\mathrm{Tag}(K_2, c^*) \neq \tau^*$ must hold, declaring the challenger $C_2$ as winner.
        \end{proof}
    \end{proof}

    \begin{framedcor}{}
        Let $\Pi' = (\mathrm{Enc}', \mathrm{Dec}')$ be an EtM SKE constructed from the SKE $\Pi_1 = (\mathrm{Enc}, \mathrm{Dec})$ and the MAC $\Pi_2 = (\mathrm{Tag})$. Then, $\Pi'$ is CCA-secure if $\Pi_1$ is CPA-secure and $\Pi_2$ is SUFCMA-secure.
    \end{framedcor}

    \begin{proof}
        Follows from the two previous theorems.
    \end{proof}

    \section{Block ciphers and Feistel networks}

    We saw how to get CPA-secure SKEs and UFCMA-secure MACs through PRFs. To get SUFCMA-secure MACS, small changes to the given construction suffice (e.g. using PRFs with unique outputs, such as PRPs). By combining these two constructions in an Encrypt-then-Mac fashion, we finally get our CCA-secure construction, proving that excellent cryptography symmetric-key encryption is theoretically possible. However, we're still missing a link: we need to show that PRP can be constructed.

    PRPs are useful not only for their output uniqueness, but also for their invertibility. In particular, real-world cryptosystems are based on encryption schemes whose decryption function is exacly the inverse of the encryption function. Since the encryption must be efficient both in theory and practice, decryptions based on PRPs also achieve this property.
    
    In practical applications, PRPs are referred to as \textbf{block ciphers}. The two most common block ciphers are known as \textit{Data Encryption Standard (DES)} and \textit{Advances Encryption Standard (AES)}. We remark that neither of these block ciphers can be really proven to be secure under standard assumptions (recall that we don't know if PRFs really do exist in practice). However, their design is high inspired by standard results in provable security. The DES block cipher is based on the theoretical notion of \textbf{Feistel networks}, a way to construct a PRP family from any PRF family.

    \begin{frameddefn}{Feistel function}
        Given a function $f : \{0,1\}^n \to \{0,1\}^n$, the Feistel function over $f$ is the function $\psi_f : \{0,1\}^{2n} \to \{0,1\}^{2n}$ such that:
        \[\psi_f(x,y) = (y, x \oplus f(y))\]
        with $x,y \in \{0,1\}^n$
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/feistel_function.pdf}
        \caption{A feistel function.}
    \end{figure}

    By definition, the Feistel function is easily invertible. By setting $\psi_f^{-1}(x',y') = (y' \oplus f(x'), x')$, we get that:
    \[\psi_f^{-1}(\psi_F(x,y)) = \psi_f^{-1}(y, x \oplus f(y)) = (x \oplus f(y) \oplus f(y), y) = (x,y)\]

    Nonetheless, the Feistel function is not pseudorandom by itself even when $f = F_K$ for some PRF family $\mathcal{F} = \{F_k : \{0,1\}^n \to \{0,1\}\}_{K \in \{0,1\}^n}$: since the second half of the input is equal to the first half of the output, an attacker can easily distinguish it from a truly random function. The strength of the Feistel function comes from repeated applications, using different pseudorandom functions. These repeated applications are called \textbf{Feistel networks}.

    \begin{frameddefn}{Feistel network}
        Given $t$ functions $f_1, \ldots, f_t : \{0,1\}^n \to \{0,1\}^n$, the Feistel network over $f_1, \ldots, f_t$ is the function  $\psi_{f_1, \ldots, f_t} : \{0,1\}^{2n} \to \{0,1\}^{2n}$ such that:
        \[\psi_{f_1, \ldots, f_t}(x,y) = \psi_{f_t}(\psi_{f_{t-1}}(\ldots \psi_{f_1}(x,y)))\]
        with $x,y \in \{0,1\}^n$
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/feistel_network.pdf}
        \caption{A 3-layer Feistel network.}
    \end{figure}

    Is a 2-layer Feistel network enough over independent PRFs to get a PRP? The answer is still no, but they almost suffice. Consider two keys $K_1, K_2 \in \{0,1\}^\lambda$. Then, for any two pairs $(x,y), (x',y') \in \{0,1\}^{2n}$ we have that:
    \[\begin{split}
        &\psi_{F_{K_2}, F_{K_1}}(x,y) \oplus \psi_{F_{K_2}, F_{K_1}}(x',y') \\
        =& \psi_{F_{K_2}}(y, x \oplus F_{K_1}(y)) \oplus \psi_{F_{K_2}}(y', x' \oplus F_{K_1}(y')) \\
        =& (x \oplus F_{K_1}(y), y \oplus F_{K_2}(x \oplus F_{K_1}(y))) \oplus (x' \oplus F_{K_1}(y'), y' \oplus F_{K_2}(x' \oplus F_{K_1}(y'))) \\
        =& (x \oplus x', y \oplus y' \oplus F_{K_2}(x \oplus F_{K_1}(y)) \oplus F_{K_2}(x' \oplus F_{K_1}(y')))
    \end{split}\]

    implying that an attacker can just query $(x,y), (x',y')$ and then check if the first half of the output is $x \oplus x'$ to easily distinguish $\psi_{F_{K_2}, F_{K_1}}$ from a truly random function. Hence, we strictly require \textit{at least} 3 applications: after three applications, both halves of the output will be XORed with a PRF, making them hard to distinguish (recall that the adversary doesn't have access to the key $(K_1, K_2, K_3) \in \{0,1\}^{3\lambda}$ used by the challenger). To formally prove this result, the following lemma is required, for which we omit the proof.

    \begin{framedlem}{}
        Let $H$ and $H'$ be the two distributions defined as follows:
        \begin{itemize}
            \item $H$ is given by $\psi_{R_1, R_2}(\cdot, \cdot)$ where $R_1,R_2 \in_R \mathcal{R}(n \to n)$ 
            \item $H'$ is given by $R(\cdot, \cdot)$ where $R \in_R \mathcal{R}(2n \to 2n)$ 
        \end{itemize}

        Assuming all queries $(x_1, y_1), \ldots,$ $(x_q, y_q)$, with $q = \poly(\lambda)$, are \curlyquotes{$y$-unique}, i.e. $y_i \neq y_j$ for all $i \neq j$, for every unbounded attacker $A$ the distributions $S$ and $R$ are computationally indistinguishable.
    \end{framedlem}

    \begin{proof}
        Omitted.
    \end{proof}

    \begin{framedthm}{Luby-Rackoff theorem}
        Let $\mathcal{F} = \{F_k : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF family. Then, for any three independent keys $K_1, K_2, K_3 \in_R \{0,1\}^\lambda$ the 3-layer Feistel network $\psi_{F_{K_1}, F_{K_2}, F_{K_3}}$ is a PRP.
    \end{framedthm}

    \begin{proof}
        Let $H_1, H_2, H_3, H_4$ be the four hybrid distribution defined as follows:
        \begin{itemize}
            \item $H_1$ is given by $\psi_{F_{K_1}, F_{K_2}, F_{K_3}}(\cdot, \cdot)$ with $K_1,K_2,K_3 \in_R \{0,1\}^\lambda$
            \item $H_2$ is given by $\psi_{R_1, R_2, R_3}(\cdot, \cdot)$ with $R_1,R_2,R_3 \in_R \mathcal{R}(n \to n)$
            \item $H_3$ is given by $R(\cdot, \cdot)$ with $R \in_R \mathcal{R}(2n \to 2n)$
            \item $H_4$ is given by $P(\cdot, \cdot)$ with $P \in_R \mathcal{P}(2n \to 2n)$, where $\mathcal{P}$ is the set of truly random permutations on $\{0,1\}^{2n}$
        \end{itemize}

        By definition, $\mathrm{Game}_{\psi_{F_{K_1}, F_{K_2}, F_{K_3}}, A}^{\text{PRP}}(\lambda, 0) \equiv H_1$ and $\mathrm{Game}_{\psi_{F_{K_1}, F_{K_2}, F_{K_3}}, A}^{\text{PRP}}(\lambda, 1) \equiv H_4$.

        \textbf{Claim 1}: $H_1 \approx_c H_2$

        \begin{proof}[Proof of Claim 1.]
            Omitted (standard triple reduction to distinguishibility of PRF)
        \end{proof}

        \textbf{Claim 2}: $H_3 \approx_c H_4$.

        \begin{proof}[Proof of Claim 2.]
            Let $B$ be the event such that $B = 1$ if $\exists i,j \in [q]$ with $i \neq j$ such that $R(x_i, y_i) = R(x_j, y_j)$. When conditioning over $B = 0$, we have that $H_3 \equiv H_4$ since $R$ becomes bijective. Therefore:
            \[\mathrm{SD}(H_3; H_4) \leq \Pr[B = 1] = \sum_{i = 1}^{q} \sum_{\substack{j = 1 :\\ j \neq i}}^{q} \Pr[R(x_i, y_i) = R(x_j, y_j)] \leq \binom{q}{2}\frac{1}{2^{2n}} \leq \mathrm{negl}(n)\]
        \end{proof}

        \textbf{Claim 3}: $H_2 \approx_c H_3$

        \begin{proof}[Proof of Claim 3.]
            We'll use the previous lemma to prove this claim. For the following argument, it is more convinient to look at $H_4$ as given by $\psi_{R_1, R_2}(\psi_{R_3}(\cdot, \cdot))$. Consider the queries $(x_1, y_1), \ldots, (x_q, y_q)$, where $(x_i, y_i) \neq (x_j, y_j)$.Let $B'$ be the event such that $B = 1$ if the outputs $\psi_{R_3}(x_i, y_i) = (x_i', y_i')$ are not \curlyquotes{$y'$-unique}, i.e. there are at least two outputs whose second halves are the same.

            By conditioning over $B = 0$, $H_2$ and $H_3$ are indistinguishable thanks to the previous lemma. Therefore:
            \[\mathrm{SD}(H_2; H_3) \leq \Pr[B = 1]\]

            Fix two indices $i,j \in [q]$ such that $i \neq j$. We may assume that $y_i \neq y_j$ since otherwise if $y_i = y_j$ then $x_i \neq x_j$ since $(x_i, y_i) \neq (x_j, y_j)$ holds. Then, we have that:
            \[y_i' = x_i \oplus R_3(y_i) \neq x_j \oplus R_3(y_i) = x_j \oplus R_3(y_j) = y_j'\]

            thus these queries don't affect $\Pr[B = 1]$. Now, we observe that:
            \[\begin{split}
                y_i' = y_j' &\iff x_i \oplus R_3(y_i) = x_j \oplus R_3(y_j) \\
                &\iff x_i \oplus x_i \oplus R_3(y_i) \oplus R_3(y_j) = x_i \oplus x_j \oplus R_3(y_j) \oplus R_3(y_j)\\
                &\iff R_3 \oplus R_3(y_j) = x_i \oplus x_j
            \end{split}\]

            Therefore, we conclude that:
            \[\Pr[B = 1] = \sum_{i = 1}^{q} \sum_{\substack{j = 1 :\\ j \neq i}}^{q} \Pr[y_i' = y_j'] \leq \binom{q}{2} \frac{1}{2^n} \leq \mathrm{negl}(n)\]
        \end{proof}
    \end{proof}

    In practical implementations, the DES encryption scheme uses a 18-layer Fiestel network over an \textit{heuristic} family instead of a PRF family (no PRF family has been proven to exist!), where the keys $K_1, \ldots, K_{18}$ are generated from a single key $K$. Even thought 18-layers may seem a lot, the DES scheme is actually \underline{very} insecure due to the usage a key that is too short (56 bits), making it very easy to bruteforce by any modern computer. The easiest way to fix this issue is through multiple applications of DES (using independent keys): we're basically doubling the key length without changing the internal structure.
    
    Nonetheless, 2-DES can also be broken through a meet-in-the-middle attack (finding the two keys by simultaneously encrypting known plaintext and decrypting the corresponding ciphertext to find matching intermediate values). Intuitively, the meet-in-the-middle attack works doesn't work for $k$-DES with $k > 2$ since the intermediate keys cannot be bruteforced.
    
    The 3-DES encryption scheme is the most used out of all the variants of DES since 168 bits for the key length are enough for modern computers. However, 3-DES is very inefficient since it is basically a 54-layer Fiestel network. For this reason, the AES standard was created. This protocol solves the issues of DES by using a longer key (128, 192 or 256 bits) and a different internal network called \textit{Substitution Permutation Network (SPN)}, which is basically the repeated application of XORs and publicly known permutations.

    \section{Collision-resistant hash families}
    
    When we discussed domain expansion for secure MACs, we saw how a PRF family $\mathcal{F}$ can be combined with an hash family $\mathcal{H}$ to get a new PRF family $\mathcal{F}(\mathcal{H})$ with an input length that can be way larger than the output length. The idea was to use hash families with inputs longer than the outputs in order to \curlyquotes{compress} the input into a string whose size is compatibile with $\mathcal{F}$.
    
    Naturally, an hash family with such property is subject to \textbf{collisions}: by the pidgeonhole principle, a function $h_s : \{0,1\}^{\ell(n)} \to \{0,1\}^n$ with $\ell(n) >> n$ is guaranteed inputs with the same output (actually, a lot of them). Collisions can be used by an attacker to infer information on the structure of the seed and the hash function itself. In our $\mathcal{F}(\mathcal{H})$ construction, this is not an issue since the resulting hashes are passed to a pseudorandom function, making collisions hard to find even for \textit{unbounded} adversaries.
    
    In many cases, however, we strictly require to use compressing hash families without passing the output through pseudorandom functions. These families are clearly insecure against unbounded adversaries: even when the seed is hidden, we can just test every input with every seed until we find collisions. Therefore, we require that our compressing hash families are \textbf{collision-resistant (CRH)}, meaning that collisions are hard to find by a \textit{bounded} adversary. To ensure security, this hardness should depend \underline{only} on the structure of the hash family and \underline{not} on the secretness of the seed (in some applications, the seed is required to be public).

    Let $\mathrm{Game}_{\mathcal{H}, A}^{\text{CRH}}(\lambda)$ be the CRH game, defined as follows:
    \begin{enumerate}
        \item The challenger sends a seed $s \in_R \mathcal{U}_\lambda$
        \item The adversary returns two inputs $x,x' \in \mathcal{M}$ with $x \neq x'$
        \item If $h_s(x) = h_s(x')$, the adversary wins. Otherwise, the challenger wins.
    \end{enumerate}
    
    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (5) [below of = 1]{};
            \node[] (6) [below of = 2]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (6) edge[swap, near start] node{$s \in_R \mathcal{U}_\lambda$} (5)
                (7) edge[near start] node{$(x,x'), x \neq x'$} (8)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}^{\text{CRH}}_{\mathcal{H}, A}(\lambda)$}
    \end{figure}

    \begin{frameddefn}{Collision-resistant hash}
        Let $\mathcal{H} = \{h_s : \{0,1\}^{\ell(n)} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ be an hash family, where $\ell(n) >> n$. We say that $\mathcal{H}$ is a collision resistant hash when:
        \[\Pr[\mathrm{Game}^{\text{CRH}}_{\mathcal{H}, A}(\lambda) = 1] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$.
    \end{frameddefn}

    In the theoretical context, assuming a CRH exists, the domain extension for hash families can be achieve. In particular, we study the \textbf{Merkle-Damg\aa rd transform}. The idea is to start with a CRH family and repeatedly apply it to obtain a CRH family with arbitrary domain size, preserving efficient computability.

    \begin{frameddefn}{Merkle-Damg\aa rd transform}
        Let $\mathcal{H} = \{h_s : \{0,1\}^{2n} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ be an hash family. We define the Merkle-Damg\aa rd transform over $\mathcal{H}$ as the hash family $\mathcal{H}_{MD} : \{H_s: \{0,1\}^{dn} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$, for any chosen $d > 2$, such that $H_s$ is defined by:
        \begin{itemize}
            \item The input is $x = (x_1, \ldots, x_d)$, with $m_i \in \{0,1\}^\lambda$
            \item $y_0, \ldots, y_d$ are partial computations, where $y_0 = 0^n$
            \item For all $i \in [d]$ it holds that $y_i = h_s(x_i \mid\mid y_{i-1})$
            \item The output is $y = y_d$
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=2.5cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (y0) []{$0^n$};

            \foreach \curr [evaluate = \curr as \prev using {int(\curr-1)}] in {1,...,2}{

                \node[state,  color = white, fill=BlueLagoon] (y\curr) [right of = y\prev]{$h_s$};
                \node[state] (x\curr) [above left of = y\curr]{$x_{\curr}$};

                \path[every node/.style={font=\sffamily\small}]
                    (y\prev) edge (y\curr)
                    (x\curr) edge (y\curr)
                ;
            }

            \node[] (z) [right of = y2]{$\cdots$};
            \node[state,  color = white, fill=BlueLagoon] (yd) [right of = z]{$h_s$};
            \node[state] (xd) [above left of = yd]{$x_d$};

            \path[every node/.style={font=\sffamily\small}]
                (y2) edge (z)
                (z) edge (yd)
                (xd) edge (yd)
                (yd) edge +(2, 0)
            ;
        \end{tikzpicture}

        \caption{The Merkle-Damg\aa rd transform}
    \end{figure}

    \begin{framedprop}{}
        Let $\mathcal{H} = \{h_s : \{0,1\}^{2n} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ be an hash family. If $\mathcal{H}$ is collision-resistant, $\mathcal{H}_{MD}$ is also collision-resistant for any $d > 2$.
    \end{framedprop}

    \begin{proof}
        The idea is to basically reduce a collision for $\mathcal{H}_{MD}$ to a collision for $\mathcal{H}$. Fix a seed $s \in_R \mathcal{U}_\lambda$. Assume that $x = (x_1, \ldots, x_d)$ and $x' = (x'_1, \ldots, x'_d)$ are two inputs such that $x \neq x'$ and $H_s(x) = H_s(x')$.

        Given the partial computations $y_1, \ldots, y_d$ and $y'_1, \ldots, y'_d$, consider the largest index $i \in [d]$ such that $(x_i, y_{i-1}) \neq (x', y_{i-1})$  and $h_s(x_i, y_{i-1}) = h_s(x'_i, y'_i)$. Notice that such index always exists since $x \neq x'$ and $H_s(x) = H_s(x')$. Then, index the $i$-th application of $h_s$ gives a collision for $x_i \mid\mid y_{i-1} \neq x'\mid\mid y_{i-1}$ since $h_s(x_i \mid\mid y_{i-1}) = h_s(x'_i \mid\mid y'_i)$. Therefore, we can define a distinguisher for $\mathcal{H}$ using a distinguisher for $\mathcal{H}_{MD}$
    \end{proof}

    Observe that the above proof only works when $d > 2$ is FIL. However, it doesn't work for VIL. The flaw in the proov is given by the fact that we can't rule out that $h_s(0^{2n}) = 0^n$ may be a possibility since we don't know the structure of $h_s$. When this happens, by construction we have that:
    \[H_s(x) = H_s(0^n \mid\mid x) = H_s(0^{2n} \mid\mid x) = H_s(0^{3n} \mid\mid x) = \ldots\]

    If the construction has FIL, the number of colliding inputs is finite, thus negligible. When the construction has VIL, instead, this the number of colliding inputs becomes infinite, thus non-negligible.

    Nonetheless, the Merkle-Damg\aa rd transform can be modified to allow VIL. To avoid the previous issue, we encode each input $x$ in a way such that it isn't the suffix of another input, i.e. there is no other input $x'$ such that $x$ is a suffix of $x'$ (making the strings $0^n \mid\mid x$ and $0^{2n} \mid\mid x'$ illegal). To achieve this, it suffices to add a final round that hashes the standard output with an encoding of the number $d$ of blocks in the input, written as $\abk{d}$.

    \begin{frameddefn}{Strenghtened Merkle-Damg\aa rd transform}
        Let $\mathcal{H} = \{h_s : \{0,1\}^{2n} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ be an hash family. We define the strenghtened Merkle-Damg\aa rd transform over $\mathcal{H}$ as the hash family $\mathcal{H}_{MD}^* : \{H_s^*: \{0,1\}^{*} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ such that $H_s^*$ is defined by:
        \begin{itemize}
            \item The input is $x = (x_1, \ldots, x_d)$, with $m_i \in \{0,1\}^\lambda$ and with $d$ being any value
            \item $y_0, \ldots, y_d$ are partial computations, where $y_0 = 0^n$
            \item For all $i \in [d]$ it holds that $y_i = h_s(x_i \mid\mid y_{i-1})$
            \item The output is $y = h_s(\abk{d} \mid\mid y_d)$, where $\abk{d}$ is an encoding of $d$ using $n$ bits
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=2.5cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (y0) []{$0^n$};

            \foreach \curr [evaluate = \curr as \prev using {int(\curr-1)}] in {1,...,2}{

                \node[state,  color = white, fill=BlueLagoon] (y\curr) [right of = y\prev]{$h_s$};
                \node[state] (x\curr) [above left of = y\curr]{$x_{\curr}$};

                \path[every node/.style={font=\sffamily\small}]
                    (y\prev) edge (y\curr)
                    (x\curr) edge (y\curr)
                ;
            }

            \node[] (z) [right of = y2]{$\cdots$};
            \node[state,  color = white, fill=BlueLagoon] (yd) [right of = z]{$h_s$};
            \node[state] (xd) [above left of = yd]{$x_d$};
            \node[state,  color = white, fill=BlueLagoon] (ydd) [right of = yd]{$h_s$};
            \node[state] (xdd) [above left of = ydd]{$\abk{d}$};

            \path[every node/.style={font=\sffamily\small}]
                (y2) edge (z)
                (z) edge (yd)
                (xd) edge (yd)
                (yd) edge (ydd)
                (xdd) edge (ydd)
                (ydd) edge +(2, 0)
            ;
        \end{tikzpicture}

        \caption{The Strenghtened Merkle-Damg\aa rd transform}
    \end{figure}

    \begin{framedthm}{}
        Let $\mathcal{H} = \{h_s : \{0,1\}^{2n} \to \{0,1\}^n\}_{s \in \{0,1\}^\lambda}$ be an hash family. If $\mathcal{H}$ is collision-resistant, $\mathcal{H}_{MD}^*$ is also collision-resistant for VIL.
    \end{framedthm}

    \begin{proof}
        The proof is almost identical to the previous proposition. Assume that $x = (x_1, \ldots, x_d)$ and $x' = (x'_1, \ldots, x'_{d'})$ are two inputs such that $x \neq x'$ and $H_s^*(x) = H_s^*(x')$. We have two cases:
        \begin{itemize}
            \item If $d = d'$, we can proceed as in the previous proposition since the lengths are equal
            \item If $d \neq d'$ then the collision happens in the final round since $\abk{d} \mid\mid y_d \neq \abk{d'} \mid\mid y_{d'}$ and $h_s(\abk{d} \mid\mid y_d) = H_s^*(x) = H_s^*(x') = h_s(\abk{d'} \mid\mid y_{d'})$.
        \end{itemize}
    \end{proof}

    A good eye may notice a small problem with our last construction: since $\abk{d} \in \{0,1\}^n$, we can have at most $2^n$ blocks, but this is a huge number for real values of $n$ (e.g. 128, 256), making it enough.

    There are many other constructions for achieving CRH families with VIL, such as \textbf{Merkle trees}. The idea behind this construction is simple: instead of applying repeated hashes in a \curlyquotes{linearl-like fashion} as done in the Merkel-Damg\aa rd transform, we apply them in a \curlyquotes{tree-like fashion}.

    \begin{figure}[H]
        \centering

        \resizebox{1\textwidth}{!}{
            \begin{tikzpicture}[<-,>=stealth,shorten >=1pt,auto,node distance=2.5cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]

                \node[state,  color = white, fill=BlueLagoon] (y) []{$h_s$};
                \node[state,  color = white, fill=BlueLagoon] (y1) [below left of = y, xshift=-40]{$h_s$};
                \node[state,  color = white, fill=BlueLagoon] (y2) [below right of = y, xshift=40]{$h_s$};
                \node[state,  color = white, fill=BlueLagoon] (y12) [below of = y1]{$h_s$};
                \node[state,  color = white, fill=BlueLagoon] (y11) [left of = y12, xshift=-40]{$h_s$};
                \node[state,  color = white, fill=BlueLagoon] (y21) [below of = y2]{$h_s$};
                \node[state,  color = white, fill=BlueLagoon] (y22) [right of = y21, xshift=40]{$h_s$};

                \node[state] (y112) [below of = y11]{$x_2$};
                \node[state] (y111) [left of = y112]{$x_1$};
                \node[state] (y122) [below of = y12]{$x_4$};
                \node[state] (y121) [left of = y122]{$x_3$};
                
                \node[state] (y211) [below of = y21]{$x_5$};
                \node[state] (y212) [right of = y211]{$x_6$};
                \node[state] (y221) [below of = y22]{$x_7$};
                \node[state] (y222) [right of = y221]{$x_8$};

                \path[every node/.style={font=\sffamily\small}]
                    (y) edge (y1)
                    (y) edge (y2)

                    (y1) edge (y11)
                    (y1) edge (y12)
                    (y2) edge (y21)
                    (y2) edge (y22)

                    (y11) edge (y111)
                    (y11) edge (y112)
                    (y12) edge (y121)
                    (y12) edge (y122)
                    (y21) edge (y211)
                    (y21) edge (y212)
                    (y22) edge (y221)
                    (y22) edge (y222)
                ;
                \draw[->] (y) edge +(0,2);
            \end{tikzpicture}
        }

        \caption{A Merkle tree with 4 levels.}
    \end{figure}

    In practical applications (e.g. MD5, SHA-1, SHA-2), the Merkel-Damg\aa rd transform is adapted by replacing the initial CRH  family $\mathrm{H}$ with an heuristic equivalent (similar to what is done in DES, 3-DES and AES). In the theoretical setting, instead, the initial CRH family can indeed be constructed, but it requires additional \textit{number theoretic assumptions} (e.g. the hardness of factoring, discrete log) or \textit{post-quantum assumptions}. A middle-ground solution is obtained by using AES, where $h_s(x_1, x_2) = \mathrm{AES}(x_1, x_2) \oplus x_2$. The security of this last construction can be proven only by assuming that AES is an \textit{ideal cipher}, i.e. a truly random permutation for every choice of the key.

    Hash functions are also used to build MACs. The HMAC scheme uses $\mathrm{Tag}(K, m) = H(K \mid\mid m)$, with $H$ being hash function. Assuming that $H(\cdot) \in \mathcal{R}(\ell(n) \to n)$ and that it can be computed only by querying an oracle, the HMAC scheme is secure. If $H$ is constructed through the Merkle-Damg\aa rd transform, the scheme can be broken. The study of cryptosystems under the usage of an ideal cipher is called \textbf{Random Oracle Model (ROM)}.

    \begin{framedass}{Random Oracle Model}
        Given an hash function $H$, in the Random Oracle Model (ROM) we assume that $H$ is a truly random function and that it can be computed only through access to an oracle. 
    \end{framedass}

    \section{Solved exercises}

    \begin{framedprob}{}
        Let $\mathcal{F} = \{F_k : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF family and let $\mathcal{F}' = \{F_k : \{0,1\}^{n-1} \to \{0,1\}^{2n}\}_{K \in \{0,1\}^\lambda}$ be a family such that:
        \[F'_K(m) = F_K(0 \mid\mid m) \mid\mid F_K(m \mid\mid 1)\]
        Prove or disprove that $\mathcal{F}'$ is a PRF family.
    \end{framedprob}

    \begin{proof}
        The claim is false. To disprove it, we define an adversary $A$ as follows:

        $A$ = "On input $w \in \{0,1\}^{2n}$:
        \begin{enumerate}
            \item Query $m_1 = 0^{n-1}$ and $m_2 = 0^{n-2} \mid\mid 1$ to $C$, which will answer with $y_1 = J(m_1)$ and $y_2 = J(m_2)$
            \item Let $s_i, t_i$ be the strings $s,t \in \{0,1\}^{n}$ such that $y_i = s_i \mid\mid t_i$
            \item If $t_1 = s_2$ return 1, otherwise 0
        \end{enumerate}

        In $\mathrm{Game}_{\mathcal{F'},A}^{\text{PRF}}(\lambda, 0)$, the challenger will use $J = F'_K$ for some $K \in_R \mathcal{K}$, thus we get that:
        \[F'_K(m_1) = F'_K(0^{n-1}) = F_K(0^n) \mid\mid F_K(0^{n-1} \mid\mid 1)\]
        \[F'_K(m_2) = F'_K(0^{n-2} \mid\mid 1) = F_K(0^{n-1} \mid\mid 1) \mid\mid F_K(0^{n-2} \mid\mid 1^2)\]

        making the attacker return 1 with probability 1. In  $\mathrm{Game}_{\mathcal{F'},A}^{\text{PRF}}(\lambda, 1)$, instead, the challenger will use $J \in_R \mathcal{R}(n-1 \to 2n)$, giving no extra information to the attacker, making them return 1 with probability at most $2^{n-1}/2^{2n} = 2^{-n-1}$. Therefore, we conclude that:
        \[\abs{\Pr[A(w) = 1 : b = 0] - \Pr[A(w) = 1 : b = 1]} \geq 1-\frac{1}{2^{n+1}} \geq \frac{1}{n}\]
        thus $\mathcal{F}'$ is not a PRF family.
    \end{proof}

    \begin{framedprob}{}
        Let $\mathcal{F} = \{F_K : \{0,1\}^n \to \{0,1\}^n\}_{K \in \{0,1\}^\lambda}$ be a PRF family and let $G : \{0,1\}^n \to \{0,1\}^{n+1}$ be a PRG. For each of the following SKE with key space $\mathcal{K} = \{0,1\}^\lambda$, prove or disprove whether the scheme is CPA-secure or not:
        \begin{enumerate}[label=\Alph*]
            \item $\Pi_a$ : given a plaintext $m \in \{0,1\}^{n+1}$ and pick nonce $r \in_R \{0,1\}^n$, then output $(r, G(r) \oplus m)$
            \item $\Pi_b$ : given a plaintext $m \in \{0,1\}^{n}$, pick a key $K \in_R \{0,1\}^\lambda$, then output $F_K(0^n) \oplus m$
            \item $\Pi_c$ : given a plaintext $m \mid\mid \widehat m \in \{0,1\}^{2n}$ with $m, \widehat m \in \{0,1\}^n$, pick a nonce $r \in_R \{0,1\}^n$ and a key $K \in_R \{0,1\}^\lambda$, then output $(r, F_K(r) \oplus m, F_K(r+1) \oplus \widehat m)$, where $+$ is addition modulo $2^n$
        \end{enumerate}
    \end{framedprob}
    

    \textit{Solution}:

    \begin{enumerate}[label=\Alph*]
        \item The first scheme is not CPA-secure. In fact, it can be proven that it isn't even 1-time secure. Consider the adversary $A_{\Pi_a}^{\text{1-time}}$ defined as follows:
        \begin{itemize}
            \item The adversary $A_{\Pi_a}^{\text{1-time}}$ sends $m_0^*, m_1^*$ and the challenger $C_{\Pi_a}^{\text{1-time}}$ returns $(r^*, c^*)$
            \item If $c^* = G(r^*) \oplus m_0^*$, the adversary returns 0. Otherwise, they return 1.
        \end{itemize}

        The above adversary is able to win the 1-time game with probability 1 since they can directly decrypt the challenge ciphertext, concluding that $\Pi_a$ is not 1-time secure and thus not CPA-secure

        \item The second scheme is not CPA-secure. Consider the adversary $A_{\Pi_c}^{\text{CPA}}$ defined as follows:
        \begin{itemize}
            \item The adversary $A_{\Pi_a}^{\text{CPA}}$  queries $m$ and the challenger $C_{\Pi_a}^{\text{CPA}}$ returns $c$.
            \item The adversary sends $m_0^*, m_1^*$ and the challenger returns $(r^*, c^*)$
            \item The adversary computes $\gamma = c \oplus m$.
            \item If $c^* = m_0^* \oplus \gamma$, the adversary returns 0. Otherwise, they return 1.
        \end{itemize}

        The first encryption query allows the adversary to compute $\gamma = c \oplus m = (F_K(0^n) \oplus m) \oplus m = F_K(0^n)$, enabling them to directly decrypt the challenge chiphertext. Therefore, the above adversary is able to win the CPA game with probability 1, concluding that $\Pi_b$ is not CPA-secure.

        \item The third scheme is CPA-secure. In fact, $\Pi_c$ corresponds exactly to the CTR mode SKE with FIL set to 2. The proof of CPA-security for CTR mode can be easily adapted to show that $\Pi_c$ is CPA-secure.
    \end{enumerate}

    \begin{framedprob}{}
        Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE scheme over key space $\mathcal{K}$, message space $\mathcal{M}$ and ciphertext space $\mathcal{C}$, such that $\mathcal{K} \subseteq \mathcal{M}$. We say that $\Pi$ is circularly secure if the standard notion of CPA-security holds even when the adversary is given $\mathrm{Enc}(K,K)$, where $K$ is the key of the scheme. Give a formal definition of circular security. Prove or dispove: CPA-security implies circular security. 
    \end{framedprob}

    \textit{Solution}:

    A formal definition of circular security can be achieved by adding an intial message sent by the challenger containing $\mathrm{Enc}(K,K)$ to the standard game $\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, b)$. We refer to this modified game as the CIRC game and we say that $\Pi$ is circularly secure when $\mathrm{Game}_{\Pi, A}^{\text{CIRC}}(\lambda, b) \approx_c \mathrm{Game}_{\Pi, A}^{\text{CIRC}}(\lambda, b)$. By definition, circular security implies CPA-security, but the converse doesn't always hold.

    To prove this, we give a counterexample. Let $\Pi = (\mathrm{Enc}, \mathrm{Dec})$ be a SKE such that:
    \[\mathrm{Enc}(K,m) = \soe{ll}{
        K & \text{if } m = K \\
        \mathrm{Enc}'(K, m) & \text{if } m \neq K
    } \qquad \mathrm{Dec}(K,c) = \soe{ll}{
        K & \text{if } c = K \\
        \mathrm{Dec}'(K, c) & \text{if } c \neq K
    }\]

    where $\Pi' = (\mathrm{Enc}', \mathrm{Dec}')$ is a CPA-secure SKE. Let $m_1, \ldots, m_q$ be encryption queries, with $q = \poly(\lambda)$, and consider the event $B$ such that $B = 1$ when $\exists i \in [q]$ with $m_i = K$. When conditioning on $B = 0$, we have that $\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, b) \equiv \mathrm{Game}_{\Pi', A}^{\text{CPA}}(\lambda, b)$, therefore:
    \[\mathrm{SD}(\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, b); \mathrm{Game}_{\Pi', A}^{\text{CPA}}(\lambda, b)) \leq \Pr[B = 1] \leq \frac{q}{2^n}\]
    concluding that $\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, b) \approx_c \mathrm{Game}_{\Pi', A}^{\text{CPA}}(\lambda, b)$ and thus that $\Pi$ is also CPA-secure. However, $\Pi$ can be easily proven to not be circularly secure after receiving $y_0 = \mathrm{Enc}(K,K)$, we have full access to the key since $y_0 = K$.

    \begin{framedprob}{}
        Suppose we are given a pseudorandom permutation $P : \mathcal{K} \times \mathcal{X} \to \mathcal{X}$. We want to construct a pseudorandom permutation $P' : \mathcal{K} \times \mathcal{X}' \to \mathcal{X}'$ that operates on a smaller domain $\mathcal{X}' \subseteq \mathcal{X}'$. Given $K \in \mathcal{K}$ and $x \in \mathcal{X}$, we compute $P'(K,x)$ as follows:
        \begin{enumerate}
            \item Set $y = P(K,x)$
            \item Until $y \notin \mathcal{X}'$, set $y = P(K,y)$
            \item Output $y$
        \end{enumerate}

        To invert a given $y \in \mathcal{X}'$ one simply computes the inverse permutation until the result lies in $\mathcal{X}'$. Answer the following questions:
        \begin{itemize}
            \item Let $t = \abs{\mathcal{X}}/\abs{\mathcal{X}'}$. How many evaluations of $P$ are needed in expectation to evaluate $P'(K,x)$ as a function of $t$? When is the evaluation of $P'$ efficient?
            \item Using induction on $\abs{\mathcal{X}}- \abs{\mathcal{X}'}$ show that if $P$ is a PRP with domain $\mathcal{X}$ then $P'$ is a PRP with domain $\mathcal{X}'$.
        \end{itemize}
    \end{framedprob}


    \textit{Solution}:

    Let $I$ be the random variable denoting the number of times $P$ is evaluated to get an output. Since $I$ has geometric distribution, we have that:
    \[\Exp[I] = \sum_{i = 1}^{+\infty} i \Pr[I = i] = \sum_{i = 1}^{+\infty} i (1-t)^{i-1}t = \frac{1}{t}\]

    The evaluation of $P'$ can therefore be considered efficient when $\frac{1}{t}$ is polynomial in the length $n = \log \abs{\mathcal{X}}$ of an element of $\mathcal{X}$, i.e. when:
    \[\frac{\abs{\mathcal{X}}}{\abs{\mathcal{X}'}} = \frac{1}{t} \leq \log^c \abs{\mathcal{X}} \implies \abs{X'} \geq \frac{\abs{\mathcal{X}}}{\log^c \abs{\mathcal{X}}} = \frac{2^n}{n^c}\]
    for some $c > 0$.
    
    For the second request, we get a trivial base case when $\abs{\mathcal{X}}-\abs{\mathcal{X}'} = 0$ this is true only when $\mathcal{X} = \mathcal{X}'$. Now, assume that $\abs{\mathcal{X}}-\abs{\mathcal{X}'} > 0$. Fix $\widehat x \in \mathcal{X}-\mathcal{X}'$ and let $\widehat P$ the equivalent of $P'$ but defined on $\widehat X = \mathcal{X}' \cup \{\widehat{x}\}$. By inductive hypothesis, we know that $\widehat P$ is a PRP. By way of contradiction, suppose that there is an adversary $A^{\text{PRP}}$ for the PRP game of $P'$. We notice that $A^{\text{PRP}}$ is also a distinguisher for the PRP game of $\widehat P$:
    \[\begin{split}
        &\Pr_{x \in_R \widehat{\mathcal{X}}'}[\widehat{P}(x') = y : y = \widehat{P}(x); x' \gets A^{\text{PRP}}(y)] \\
        =\,& \Pr_{x \in_R \widehat{\mathcal{X}}}[\widehat{P}(x') = y : y = \widehat{P}(x'); x' \gets A^{\text{PRP}}(y); x \neq \widehat x] \Pr_{x \in_r \widehat{\mathcal{X}}}[x \neq \widehat x] \\
        =\,& \frac{\abs{\widehat{\mathcal{X}}}-1}{\abs{\widehat{\mathcal{X}}}} \Pr_{x \in_R \mathcal{X}'}[P'(x') = y : x = P'(y); x' \gets A^{\text{PRP}}(y)] \\
        \geq\,& \mathrm{negl}(\log \abs{\mathcal{X}})
    \end{split} \]
    contradicting the fact that $\widehat P$ is a PRP, thus $P'$ must be a PRP.

    \chapter{Number theory in cryptography}

    \section{Brush-up on number theory}

    We briefly discussed how number theoretic assumptions are required to build the theoretical tools that we have seen in the previous chapter. In this chapter, we'll introduce some of these assumptions (e.g. the hardness of factoring, discrete log and learning with erros) and use them to build valid tools.

    Number theory is the field of algebraic mathematics that studies prime numbers and modular arithmetic, i.e. the sets $\Z_n = \{0, 1, \ldots, n-1\}$ with $n \in \N$. In particular, the field studies algebraic structures such as groups (e.g. $(\Z_n, \cdot)$), rings (e.g. $(\Z_n, +, \cdot)$) and fields (e.g. $(\Z_p, +, \cdot)$ with $p \in \Primes$). We recall that:
    \begin{itemize}
        \item $(\G, +)$ is a group if $+$ satisfies aassociativity, the existence of a neutral element and the existence of an inverse element 
        \item $(\G, +, \cdot)$ is a ring if $(\G, +)$ is a group and $\cdot$ satisfies aassociativity and the existence of a neutral element
        \item $(\G, +, \cdot)$ is a field if $(\G, +)$ is a group and $\cdot$ satisfies aassociativity, the existence of a neutral element and the existence of an inverse element (except for the neutral element of $+$).
    \end{itemize}
    where $+ : \G \times \G \to \G$ and $\cdot : \G \times \G \to \G$ are two binary operations (here we'll always assume that they are the standard addition and multiplication). We also recall that in modular arithmetic we have that $a \equiv b \pmod n$ when $a = b+kn$ for some $k \in \Z$. In this section we'll state and proof a list of basic number theory results that will be used in the following sections.

    Let $Z_n^*$ denote the set of invertible elements of $\Z_n$, i.e. the elements $a \in \Z_n$ for which $\exists b \in \Z_n$ such that $ab = 1$. It can be easily proven that an element of $\Z_n$ is invertible if and only if its GCD with $n$ is exactly 1.

    \begin{framedprop}{}
        Given $a \in \Z_n$, it holds that $a \in \Z_n^*$ if and only if $\gcd(a,n) = 1$
    \end{framedprop}

    \begin{proof}
        Let $d = \gcd(a,n)$. Suppose that $a$ is invertible, that is $\exists b \in \Z_n$ such that $ab \equiv 1 \pmod n$. Then, we have that $ab = 1+kn$ for some $k \in \Z$, thus $ab - kn = 1$. By definition of GCD, we have that $d$ must divide $ab - kn$, but the only possible natural number that divides 1 is 1 itself, concluding that $d = 1$. Vice versa, suppose that $d = 1$. Then, $\exists k_1,k_2$ such that $1 = d = k_1a + k_2n$, implying that $k_1a = 1- k_2n$. Therefore, we conclude that $ka \equiv 1 \pmod n$ and thus that $k_1$ is the inverse of $a$. 
    \end{proof}

    The above proposition immediately concludes that $\Z_n^* = \{a \in \Z_n \mid \gcd(a,n) = 1\}$ and that $\abs{\Z_n^*} = \varphi(n)$, where $\varphi(n)$ is \textbf{Euler's totient function}, the function that returns the number of coprimes of $n$. In the special case of $\Z_p^*$ with $p \in \Primes$, we have that every number $\Z_p$ except zero is coprime with $p$, therefore $\Z_p^* = \{1, \ldots, p-1\}$ and $\phi(p) = p-1$. We'll see ways to compute the totient function for general $n \in \N$ in future sections.

    \begin{framedlem}{}
        Given $a,b \in \N$ such that $a \geq b > 0$, it holds that:
        \[\gcd(a,b) = \gcd(b, a \pmod b)\]
    \end{framedlem}

    \begin{proof}
        Let $a \equiv x \pmod b$ with $x$ being as small as possible. Let also $d = \gcd(a,b)$ and let $d' = \gcd(b,x)$. Since $a = x + kb$ for some $k \in \Z$, by definition of GCD we know that $d$ must divide $x$ since $x = a-kb$, implying that $x = dq$ for some $q \in \N$. Similarly, by definition of GCD we have that $d'$ divides $a$ since $a = x+kb$, implying that $a = d'q'$ for some $q \in \N$. By transitivity, we conclude that both $d$ and $d'$ divide each other, which is possible only if $d = d'$.
    \end{proof}

    The above lemma can be used to define an algorithm that iteratively computes the GCD between two numbers by reducing them little by little up until $\gcd(x, 0)$ is reached, outputting $x$. This is known as the \textbf{Euclidean algorithm}.
    
    \begin{framedalgo}{Euclidean algorithm}
        Given $a,b \in \Z$ with $0 < a \leq b$ as input, the Euclidean algorithm is defined as follows and it outputs $\gcd(a,b)$ in $O(\abs{a}+\abs{b})$:
        \begin{enumerate}
            \item Set $r_0 := b$ and $r_1 := a$. Initialize $i = 0$
            \item While $r_i \neq 0$, set $r_{i+1} := r_{i-1} \pmod{r_i}$ (i.e. $r_{i-1} = r_iq_i + r_{i+1}$ with $r_{i+1}$ as small as possible), then increment $i$
            \item Return $r_{i-1}$
        \end{enumerate} 
    \end{framedalgo}

    \begin{proof}
        Omitted.
    \end{proof}

    We give an example of computation made by the algorithm. Given $a = 14$ and $b = 10$, we have that:
    \begin{enumerate}
        \item Set $r_0 = 10$ and $r_1 = 14$
        \item $r_1 = r_0 \cdot 1 + 4$, thus $r_2 = 4$
        \item $r_2 = r_1 \cdot 2 + 2$, thus $r_3 = 2$
        \item $r_3 = r_2 \cdot 2 + 0$, thus $r_4 = 0$
        \item The output is $r_3 = 2$
    \end{enumerate}
    
    We observe that this algorithm can also be extended to express the GCD as a \textit{BÃ©zout identity}, meaning that $\gcd(a,b) = ak+bh$ for some $k,h \in \Z$. This can be achieved by simply inverting each iteration made by the algorithm. For instance, in our previous example we have that:
    \begin{enumerate}
        \item $r_2 = r_1 \cdot 2 + r_3$, thus $r_3 = r_2 - r_1 \cdot 2$
        \item $r_1 = r_0 \cdot 2 + r_2$, thus $r_2 = r_1 - r_0 \cdot 2$
        \item Combining all the found relationships we get that:
        \[r_3 = r_2 - r_1 \cdot 2 = (r_1 - r_0 \cdot 2) - r_0 \cdot 2 = r_1 - 4r_0 = (1) a + (-4) b\]
    \end{enumerate}

    Observe that when $\gcd(a,n) = 1$, the extended Euclinean algorithm finds two valus $k,h \in \Z$ such that $1 = \gcd(a,n) = ak+hn$, meaning that $ak = 1+(-h)n$ and thus that $k$ is the inverse of $a$ in $\Z_n$.

    After proving that we have a tool to efficiently compute the GCD of two numbers, we require to prove that exponentiation modulo $n$ can also be efficiently computed. Given two numbers $a,b \in \N$, let $b_tb_{t-1}\ldots b_0$ be the binary representation of $b$. Then, we have that:
    \[a^b \equiv a^{\sum_{i = 0}^t b_i 2^i} \equiv \prod_{i = 0}^t (a^{2^i})^{b_i} \pmod n\]

    Observe that the exponents $b_0, b_1, \ldots, b_t$ acts as a \curlyquotes{toggle}, meaning that we skip the inner exponentiations with $b_i = 0$ and compute only those with $b_j = 1$. The values $a, a^2, a^4, \ldots, a^{2^t} \pmod n$ can be easily computed through a recursive approach (e.g. $a^8 \equiv (a^4)^2 \equiv ((a^2)^2)^2$, thus we can compute $((a^2 \pmod n)^2 \pmod n)^2 \pmod n$) with $\log 2^t = t$ recursion levels. This concludes that exponentiation can also be computed in polynomial time with respect to $\abs{b}$.

    Now, we need one last tool: finding prime numbers. The \textbf{prime number theorem} states that the number $\pi(x)$ of primes up to $x > 0$ is bounded by:
    \[\pi(x) \geq \frac{x}{3 \log x} \approx \frac{x}{\log x}\] 

    Assuming that we can test if a number is prime or not, we can efficiently generate $\lambda$-bit primes through a random process that is guaranteed to eventually terminate. We sample $n \in_R [2^\lambda-1]$ and test if it is prime. If it's prime, we output it. Otherwise, we repeat the process. Notice that:
    \[\Pr[\text{no output after $3\lambda^2$ samples}] \leq \rbk{1-\frac{1}{3\log(2^\lambda -1)}}^{3\lambda^2} \leq \rbk{1-\frac{1}{3\lambda}}^{3\lambda^2} \leq e^{-\lambda}\]

    Therefore, we're left with proving that there is an algorithm that can test prime numbers. Almost all the currently known algorithms for primality testing are based on \textbf{Fermat's little theorem}, which states that:
    \[a^{p-1} \equiv 1 \pmod p \quad \forall p \in \Primes, a \neq 0\]

    Fermat's little theorem is a particular case of Euler's theorem, which states that:
    \[a^{\varphi(n)} \equiv 1 \pmod n \quad \forall n \in \N, a \in \Z_n^*\]

    and even more generally that:
    \[a^{b} \equiv a^{b \pmod {\varphi(n)}} \pmod n \quad \forall b,n \in \N, a \in \Z_n^*\]
    
    We start by considering the \textbf{Fermat primality test}. On input $n$, sample $a \in_r [2, n-2]$ and test if $a^{n-1} \equiv 1 \pmod n$. If it's true, we output \curlyquotes{maybe prime}. Otherwise, we output \curlyquotes{not prime}. It's easy to see that this test can only distinguish non prime numbers: if the test fails, the input cannot be a prime number since otherwise it would satisfy Fermat's little theorem. When the test succeeds, we cannot ensure that the input is prime: a prime number definitely passes the test, but some non-prime numbers may also pass it for some values of $a$. These inputs are called \textit{Fermat's liars} for $a$.
    
    An easy fix could be to iteratively apply the test until we find a nice value of $a$. However, there are some specific values of $n$ that pass the test for \underline{any} possible value of $a$. These inputs are called \textit{Carmicheal numbers}.

    Nonetheless, effective primality tests do exists. The Miller-Robin primality test uses an efficient random process, based on advanced algebra and Fermat's little theorem, that is guaranteed to eventually terminate. The Agrawal-Kayal-Saxena primality test, instead, uses a process similar to that of Miller-Robin without randomness, at the cost of some efficiency. Therefore, generating prime numbers is effectively possible!

    \section{The DL, CDH and DDH assumptions}

    We formally introduce our first number theory hardness assumptions. Consider the equation $a^x = b$ over a continuous setting such as $\R$. With common mathematical tools, this quation can easily by solved as $x = \log_a b$. Consider now the equivalent version of this problem over a discrete set such as $\Z_p$, i.e. $a^x \equiv b \pmod p$. In the continuous context, the problem  has an unique solution. In the discrete context, instead, we have infinite solutions due $\Z_p$ being a \textit{cyclic group}. For instance, for the equation $2^x \equiv 4 \pmod 5$ we have that:
    \[4 \equiv 2^2 \equiv 2^6 \equiv 2^{10} \equiv 2^{14} \equiv \ldots \pmod 5\]

    Therefore, it's very hard to find the exact value of $x$ that was originally used. This problem is known as the \textbf{Discrete Logarithm (DL)} problem. For centuries, many mathematicians tried to find an efficient way to solve this problem (observe that it can be trivially solved in exponential time through bruteforce). To this day, the best algorithms achieve only a sub-exponential time, which is still worse than polynoamial time. Over time, researchers started to assume that the problem is too hard to be solved in polynomial time even through randomness, i.e. that no PPT algorithm can solve the problem efficiently, even thought this has not yet been proven to be true. This is known as the \textit{DL assumption}.
    
    \begin{framedass}{DL assumption}
        Given a prime $p \in \Primes$, the DL assumption states that:
        \[\Pr[A(g,p,y) = x : x \in_R \Z_p, y \equiv g^x \pmod p] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$ and for a chosen value $\lambda > 0$
    \end{framedass}

    It's easy to see that, by definition, the DL assumption automatically implies the existence of an OWF (the discrete log itself). Diffie and Hellman used the DL assumption to start the public key revolution of cryptography. They proposed a \textbf{key exchange protocol} whose security depends only on this assumption.

    \begin{framedalgo}{Diffie-Hellman key exchange}
        The Diffie-Hellman key exchange allows two users Alice and Bob to generate a sharede key over a public channel without allowing third parties to infer the key:
        \begin{enumerate}
            \item Alice samples a prime $p \in_R \Primes$ and computes a generator $g$ of $\Z_p$, sharing the pair $(g,p)$ with Bob
            \item Alice samples a value from $x \in_R \Z_{p-1}$ and sends $X = g^{x} \pmod p$ to Bob
            \item Bob samples a value from $y \in_R \Z_{p-1}$ and sends $Y = g^{y} \pmod p$ to Alice
            \item Alice computes $K = Y^x = g^{xy} \pmod p$, while Bob computes $K = X^y = g^{xy} \pmod p$
            \item Both parties will use $K$ as the key
        \end{enumerate}
    \end{framedalgo}

    We recall that a \textit{generator} $g$ for a cyclic group $\G$ is a value $g \in \G$ such that $\G = \{g^{0}, g^1, g^2, \ldots, g^{o(g)}\}$, where $o(g)$ is the \textit{order} of $g$, i.e. the smallest value such that $g^{o(g)} = 1$ in $\G$.

    The protocol clearly requires some hardness assumptions to hold, otherwise an attacker could intercept the messages $(g,p), X, Y$ and derive the values of $x,y$ from them, computing the key $K = g^{xy}$. This is usually referred to as a \textbf{passive attack} since the thid party eavesdrops without changing the messages. The protocol may also suffer from another type of attack, called \textbf{active attack}, where the third party also changes the messages (also referred to as \textit{man-in-the-middle attack}).
    
    We start by analyzing passive security. To \curlyquotes{fix} this issue, we have to assume that the key is hard to compute without knowing the values of $x$ and $y$. Consider the following generalization of the DH key exchange to any cyclic group:
    \begin{enumerate}
        \item Alice chooses a cyclic group $\G$, computes a generator $g$ of $\G$ and the order $q$ of $g$ over $\G$, sharing the triple $(\G,g,q)$ with Bob
        \item Alice samples a value from $x \in_R \Z_q$ and sends $X = g^{x} \pmod p$ to Bob
        \item Bob samples a value from $y \in_R \Z_q$ and sends $Y = g^{y} \pmod p$ to Alice
        \item Alice computes $K = Y^x = g^{xy}$ in $\G$, while Bob computes $K = X^y = g^{xy}$ in $\G$
        \item Both parties will use $K$ as the key
    \end{enumerate}

    Currently, it is not known if the DL assumption suffices to make it hard to compute the key for passive attackers. This motivates a new assumption, the \textbf{Computatational Diffie-Hellman (CDH)} assumption.

    \begin{framedass}{CDH assumption}
        Given a triple $(\G,g,q)$ where $\G$ is a cyclig group, $g \in \G$ is a generator and $q$ is the order of $g$ in $\G$, the CDH assumption states that:
        \[\Pr[A(\G,g,q,g^x,g^y) = g^{xy} : x,y \in_R \Z_q] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$ and for a chosen value $\lambda > 0$
    \end{framedass}

    Clearly, the CDH assumption implies that the DL assumption is true: if one can solve the DL problem, it can also solve the CDH problem by finding $x,y$. As already stated, the converse implication is currently not known to hold. There is a lot of evidence for the CDH assumption to hold in $\Z_p$ with $p \in \Primes$, but it currenlty hasn't been problem.

    We observe that the CDH assumption is not enough to fix the issue of passive security: an attacker may still infer information about the key without inverting $X$ and $Y$ but simply by analyzing them. What we really need is for the key to be indistinguishable from a value chosen uniformly-at-random over $\G$. This is called the \textbf{Decisional Diffie-Hellman (DDH)} assumption.

    \begin{framedass}{DDH assumption}
        Given a triple $(\G,g,q)$ where $\G$ is a cyclig group, $g \in \G$ is a generator and $q$ is the order of $g$ in $\G$, the DDH assumption states that:
        \[(\G,g,q,g^x,g^y, g^{xy}) \approx_c (\G,g,q,g^x,g^y, g^{z})\]
        with $x,y,z \in_R \Z_q$, for all PPT adversaries $A$ and for a chosen value $\lambda > 0$
    \end{framedass}

    The DDH assumption implies the CDH assumption, but the converse is proven to not hold for a general triple $(\G,g,q)$. Moreover, the DDH assumption is proven to \underline{not} hold for $\G = \Z_p$. 

    \begin{framedprop}{}
        The DDH assumption doesn't hold for $\G = \Z_p$.
    \end{framedprop}

    \begin{proof}
        Consider the group $\mathbb{QR}_p$ of quadratic residues over $p$, defined as $\mathbb{QR}_p = \{y \in \Z_p \mid \exists x \in \Z_p \; y \equiv x^2 \pmod p\}$. Equivalently, we have that $\mathbb{QR}_p = \{y \in \Z_p \mid \exists k \in \Z \; y \equiv g^{2k} \pmod p\}$ 

        \textbf{Claim}: $y \in \mathbb{QP}_p$ if and only if $y^{\frac{p-1}{2}} \equiv 1 \pmod p$

        \begin{proof}
            Suppose that $y \in \mathbb{QP}_p$, meaning that $y \equiv g^{2k} \pmod p$ for some $k \in \Z$. Then, we have that:
            \[y^{\frac{p-1}{2}} \equiv (g^{2k})^{\frac{p-1}{2}} \equiv (g^{p-1})^k \equiv 1 \pmod p\]

            Vice versa, suppose that $y \notin \mathbb{QP}_p$, meaning that $y \equiv g^{2k'+1} \pmod p$ for some $k' \in \Z$. Then, we have that:
            \[y^{\frac{p-1}{2}} \equiv (g^{2k'+1})^{\frac{p-1}{2}} \equiv (g^{p-1})^k g^{\frac{p-1}{2}} \equiv g^{\frac{p-1}{2}} \not\equiv 1 \pmod p\]
            since $p-1$ is the order of $g$ in $\Z_p$.
        \end{proof}

        The above claim easily gives us a distinguisher for the DDH condition. When $Z = g^z$ with $z \in_R \Z_p$ then $Z \in \mathbb{QR}_p$ with probability $1/2$. When $Z = g^{xy}$ with $x,y \in_R \Z_p$, instead, we have that $Z \in \mathbb{QR}_p$ with probability $3/4$: either $g^x \in \mathbb{QR}_p$ or $g^y \in \mathbb{QR}_p$ or both. Therefore, we can build an attacker that distinguishes the DDH condition with probability $3/4 - 1/2 = 1/4$.
    \end{proof}

    The above implies that we require a new type of cyclic group for which the DDH assumption is believed to hold. Luckily, there are many groups for which this is true. The easiest fix is to set $\G = \mathbb{QP}_{p}$ where $p = 2q+1$ and $p,q \in \Primes$ to break the previous proposition. Another very popular choice is \textit{elliptic curve groups}. An elliptic curve is composed of solutions to an equation $y = x^3 + ax^2+bx+c$, for some coefficients $a,b,c \in \R$. The elliptic curve group modulo $p \in \Primes$ is a group whose elements are the points of an elliptic curve with coordinates modulo $p$.

    What about active security? It's easy to see that even the DDH assumption is not enough. A third party Eve could intercept the message $X = g^x$ sent by Alice and replace it with $X' = g^{x'}$, sending it to Bob. Then, Eve intercepts $Y = g^y$ from Bob and sends $Y' = g^{y'}$ to Alice. Now, Alice will use the key $K_{AE} = Y'^{x} = g^{xy'}$ and Bob will use the key $K_{EB} = X'^{y} = g^{x'y}$, while Eve can compute both keys.

    This issue cannot be fixed under any assumption by itself. Nonetheless, it can be fixed by strengthening the protocol through the use of a \textit{master key} (a key that is already shared between Alice and Bob but never used for communications) to authenticate the messages sent by Alice and Bob, using either MACs or digital signatures. We'll return to this topic when we'll extensively discuss public-key encryption.

    \section{Building crypto-tools through number theory}

    To close our discussion on the DL, CDH and DDH assumptions, let's talk about other ways in which they can be used for cryptography. We already saw that DDH $\to$ CDH $\to$ DL $\to$ OWF, from which we can then prolong our chain to get OWF $\to$ PRG $\to$ PRF $\to$ PRP $\to$ CRH, concluding that the DDH assumption is effectively enough to get secure symmetric encryption. Actually, we don't need to apply each construction of the full chain to get every single tool. In fact, each tool can be derived directly through the DDH assumption (or even the DL assumption).
    
    Let's start by building a PRG. Consider any triple $(\G,g,q)$ for which the DDH assumption holds. We define a function $G_{\G,g,q} : \Z_q \to \G^3$ as $G_{\G,g,q}(x,y) = (g^x, g^y, g^{xy})$. Under the DDH assumption, we have that $G_{\G,g,q}(\mathcal{U}_{\Z_q^2}) \approx_c (\mathcal{U}_{\G}, \mathcal{U}_{\G}, \mathcal{U}_{\G}) \approx_c \mathcal{U}_{\G^3}$. When $\G = \Z_q$, $G_{\G,g,q}$ becomes a PRG with strech 1. This construction can be further improved to a PRG $\G^t_{\G,g,q} : \Z_q^{t+1} \to \G^{2t+1}$ with stretch $t$, where:
    \[\G^t_{\G,g,q}(x, y_1, \ldots, y_t) = (g^x, g^{y_1}, g^{xy_1}, g^{y_2}, g^{xy_2}, \ldots, g^{y_t}, g^{xy_t})\]

    Now, let's build a PRF. We already saw that PRFs can be constructed through the GGM tree using a PRG whose output can be nicely split in half. Copying this idea, we can construct a nice PRF with a closed form (thus we don't have to recursively compute the two halves of the PRG).
    
    Consider the following PRG $G_a : \G \to \G^2$ where $a \in \Z_q$ and $G_a(g^b) = (g^b, g^{ab})$ (proof of pseudorandomness through the DL assumption). By setting $G_{a,0}(g^b) = g^b$ and $G_{a,0}(g^b) = g^{ab}$, we get that $G_a(g^b) = G_{a,0}(g^b) \mid\mid G_{a,1}(g^b)$. Let $\mathcal{F} = \{F_{\vec a} : \{0,1\}^n \to \G\}_{\vec a \in \Z_q^{n+1}}$ where:
    \[F_{\vec a}(x_1, \ldots, x_n) = g^{a_0 \prod_{i = 1}^n a_i^{x_i}}\]

    After some thought, we can convince ourselves that $\mathcal{F}$ is basically the GGM tree of $G$, proving that it is a PRF. After obtaining a PRF, we can easily construct a PRP through a 3-layer Feistel network.
    
    We're left with constructing a valid CRH hash family. Consider the hash function $H_{g_1,g_2}(x_1, x_2) = g_1^{x_1}g_2^{x_2}$ where $g_1, g_2 \in \G$ and $x_1,x_2 \in \Z_q$, with $q$ being the order of $\G$. Suppose that there is an adversary that finds two colliding inputs $(x_1,x_2), (x_1',x_2')$ with $(x_1,x_2) \neq (x_1',x_2')$. Given a DL input triple $(g,q,y)$, if we randomly select an index $i \in_R \{0,1\}$ and set $g_i = y$ and $g_j = g^b$, with $j \neq i$ and $b \in_R \Z_q$, we have that:
    \[g_1^{x_1}g_2^{x_2} = g_1^{x_1'} g_2^{x_2} \iff y^{x_i-x_i'} = g^{b(x_j-x_j')} \iff y = g^{b(x_i-x_j)^{-1}(x_j-x_j')}\]

    We observe that this holds only if $x_i - x_i' \neq 0$, which is possible with $1/2$ probability since $i$ is randomly selected and $(x_1,x_2) \neq (x_1',x_2')$. Since we reduced the DL game of $\G$ to the CRH game of $H_{g_1, g_2}$, the hash function is collision resistant under the DL assumption.   

    \section{Solved exercises}

    \begin{framedprob}{}
        Let $\G$ be a cyclic group of prime order $q$ generated by $g \in \G$. Let $n$ be a polynomially bounded parameter. Consider the hash family $\mathcal{H}_{\G,n} : \{H_{g_1, \ldots, g_n} : \Z_q^n \to G\}$ parametrized by the group $\G$ and by $n$ randomly chosen group elements $g_1, \ldots, g_n \in \G$ such that for $x_1, \ldots, x_n \in \Z_q$ it holds that:
        \[H_{g_1, \ldots, g_n} (x_1, \ldots, x_n) = g_1^{x_1} \cdot \ldots \cdot g_n^{x_n}\]
        Prove that $\mathcal{H}_{\G,n}$ is collision resistant assuming the DL assumption for $\G$.
    \end{framedprob}

    \begin{proof}
        By way of contradiction, suppose that there is an adversary $A^{\text{CRH}}_{\mathcal{H}_{\G,n}}$ that finds a collision with in $\mathcal{H}_{\G,n}$ with probability at least $n^{-c}$ for some $c > 0$. We define a new adversary $A^{\text{DL}}_{\G}$ as follows:
        \begin{enumerate}
            \item The challenger $C^{\text{DL}}_{\G}$ sends the input $(g,p,y)$ to $A^{\text{DL}}_{\G}$.
            \item $A^{\text{DL}}_{\G}$ samples $i \in_R \mathcal{U}_n$ and sets $g_i = y$
            \item For each $j \in [n] - \{i\}$, $A^{\text{DL}}_{\G}$ samples $r_j \in_R \Z_q$ and sets $g_j = g^{r_j}$
            \item $A^{\text{DL}}_{\G}$ sends $(g_1, \ldots, g_n)$ to $A_{\mathcal{H}}^{\text{CRH}}$, which replies with $(x_1, \ldots, x_n)$ and $(x_1',\ldots, x_n')$ such that $(x_1, \ldots, x_n) \neq (x_1',\ldots, x_n')$
            \item $A^{\text{DL}}_{\G}$ returns $(x_i-x_i')^{-1} \sum_{j = 1 : j \neq i}^n r_j (x_j-x_j')$ to $C^{\text{DL}}_{\G}$
        \end{enumerate}

        Suppose that $A_{\mathcal{H}}^{\text{CRH}}$ finds a collision for the seed $(g_1, \ldots, g_n)$, meaning that $g_1^{x_1} \cdot \ldots \cdot g_n^{x_n} = g_1^{x_1'} \cdot \ldots \cdot g_n^{x_n'}$ with $(x_1, \ldots, x_n) \neq (x_1',\ldots, x_n')$. By rearranging the equation, we get that:
        \[\begin{split}
            \prod_{k = 1}^n g_k^{x_k} = \prod_{k = 1}^n g_k^{x_k'} &\iff g_i^{x_i-x_i'} = \prod_{\substack{j = 1 :\\j \neq i}}^n g_n^{x_j-x_j'} \\
            &\iff y^{x_i-x_i'} = \prod_{\substack{j = 1 :\\j \neq i}}^n g^{r_j(x_j-x_j')} \\
            &\iff y^{x_i-x_i'} = g^{\sum_{j = 1 : j \neq i}^n r_j(x_j-x_j')}
        \end{split}\]
        
        Assuming that $x_i - x_i' \neq 0$, we get that:
        \[y = g^{(x_i-x_i')^{-1} \sum_{j = 1 : j \neq i}^n r_j(x_j-x_j')}\]
        
        Observe that the probability of this assumption to be true is at least $1/n$ since $i \in_R \mathcal{U}_n$ and $(x_1, \ldots, x_n) \neq (x_1', \ldots, x_n')$. Therefore, we conclude that:
        \[\Pr[A^{\text{DL}}_{\G}(g,p,y) = \alpha : \alpha \in_R \Z_q, y = g^\alpha] \geq \frac{1}{n} \frac{1}{n^{c}}\]
        contradicting the fact that the DL assumption holds in $\G$.
    \end{proof}

    \addtocontents{toc}{\protect\newpage}
    \chapter{Public-key encryption}

    \section{The aymmetric key paradigm}

    In the previous chapter we introduced key exchange, a basic concept of public-key cryptography. Differently from symmetric-key encryption, in \textbf{public-key encryption (PKE)} the two parties use a pair of keys $(pk, sk)$, where $pk$ is the \textit{public key} and it is used for encryption, while $sk$ is the \textit{secret key} (or \textit{private key}) and it is used for decryption.  

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{Alice};
            \node[draw, rectangle, minimum width=40] (2) [right of = 1, xshift = 75]{$\mathrm{Enc}(pk,m)$};
            \node[] (x) [right of = 2, xshift = 50]{};
            \node[draw, rectangle, minimum width=40] (3) [right of = x, xshift = 50]{$\mathrm{Dec}(sk,c)$};
            \node[] (4) [right of = 3, xshift = 75]{Bob};
            \node[] (5) [below of = x, yshift = -10]{$(pk,sk)$};
            \node[] (6) [below of = 2, yshift = -10]{};
            \node[] (7) [below of = 3, yshift = -10]{};

            \path[every node/.style={font=\sffamily\small}]
                (1) edge node{$m$} (2)
                (2) edge[-] node{$c$} (x.center)
                (x.center) edge (3)
                (3) edge node{$m$} (4)
                (5) edge[-] (6.center)
                (5) edge[-] (7.center)
                (6.center) edge (2)
                (7.center) edge (3)
                ;
        \end{tikzpicture}

        \caption{Example of public-key encryption.}
    \end{figure}

    Since we're working with two keys and one must act as the tool to invert the other, in PKE schemes the \textbf{keygen} (\textit{key generation}) algorithm must be explicitily stated (we can't just say \curlyquotes{let $(pk, sk)$ be a public-secret key pair}). Therefore, PKE schemes are formally considered as a triple $\Pi = (\mathrm{KGen}, \mathrm{Enc}, \mathrm{Dec})$. 

    As the names suggest, the public key must be shared with the other party in order to achieve encryption, meaning that we have to find a way to securely send Bob's public key to Alice without it being subject of a man-in-the-middle attack, similar to what we have seen for the Diffie-Hellman protocol. To achieve this, we'll have to introduce the concept of digital signature and public key infrastructure.

    For now, we start by translating the security concepts that we have already discussed from the symmetric setting to the asymmetric one (CPA-security, CCA-security, \dots). The main idea is to use the same identical games to show the security of PKEs, with the addition of an initial message sent by the challenger. This initial message contains the public key generated together with the secret key at the start of the game. We observe that sending the public key allows the adversary to encrypt the queries by themselves, without having to send them to the challenger. This implies that we can remove encryption queries from both the CPA game and the CCA game. In particular, we recall that the encryption algorithm is assumed to be \underline{randomized}, meaning that the adversary cannot trick the challenger by encrypting the challenge messages $m_0^*, m_1^*$ by themselves and get $c^*$ before the challenger sends it. 

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (1t) [below of = 1]{};
            \node[] (2t) [below of = 2]{};

            \node[] (7) [below of = 1t]{};
            \node[] (8) [below of = 2t]{};

            \node[] (9) [below of = 7]{};
            \node[] (10) [below of = 8]{};

            \node[] (11) [below of = 9]{};
            \node[] (12) [below of = 10]{};

            \path[every node/.style={font=\sffamily\small}]
                (2t) edge[swap, near start] node{$pk$} (1t)
                (7) edge[near start] node{$m_0^*, m_1^* \in \{0,1\}^\lambda$} (8)
                (10) edge[swap, near start] node{$c^* = \mathrm{Enc}_\Pi(pk, m_b^*)$} (9)
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\Pi, A}^{\text{CPA}}(\lambda, b)$ for public-key encryption.}
    \end{figure}

    \begin{figure}[H]
        \centering


        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (1t) [below of = 1]{};
            \node[] (2t) [below of = 2]{};

            \node[] (3y) [below of = 1t]{};
            \node[] (4y) [below of = 2t]{};

            \node[] (5y) [below of = 3y]{};
            \node[] (6y) [below of = 4y]{};

            \node[] (7) [below of = 5y]{};
            \node[] (8) [below of = 6y]{};

            \node[] (9) [below of = 7]{};
            \node[] (10) [below of = 8]{};

            \node[] (3z) [below of = 9]{};
            \node[] (4z) [below of = 10]{};

            \node[] (5z) [below of = 3z]{};
            \node[] (6z) [below of = 4z]{};

            \node[] (11) [below of = 5z]{};
            \node[] (12) [below of = 6z]{};

            \path[every node/.style={font=\sffamily\small}]
                (2t) edge[swap, near start] node{$pk$} (1t)

                (5y) edge[bend left] node {$\poly(\lambda)$}(3y)
                (3y) edge[near start] node{$\widetilde c_i \in \{0,1\}^\lambda, \widetilde c_i \neq c^*$} (4y)
                (6y) edge[swap, near start] node{$\widetilde m_i = \mathrm{Dec}_\Pi(sk, \widetilde c_i)$} (5y)

                (7) edge[near start] node{$m_0^*, m_1^* \in \{0,1\}^\lambda$} (8)
                (10) edge[swap, near start] node{$c^* = \mathrm{Enc}_\Pi(pk, m_b^*)$} (9)

                (5z) edge[bend left] node {$\poly(\lambda)$}(3z)
                (3z) edge[near start] node{$\widetilde c_i' \in \{0,1\}^\lambda, \widetilde c_i' \neq c^*$} (4z)
                (6z) edge[swap, near start] node{$\widetilde m_i' = \mathrm{Dec}_\Pi(sk, \widetilde c_i')$} (5z)
                
                (11) edge[near start] node{$b' \in \{0,1\}$} (12)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\Pi, A}^{\text{CCA}}(\lambda, b)$ for public-key encryption.}
    \end{figure}

    Thanks to our theoretical background with SKEs, we can jump straight away into constructing PKE schemes using the above two games. Our first PKE scheme is the \textbf{ElGamal scheme}.

    \begin{framedalgo}{ElGamal PKE scheme}
        Given a triple $(\G,g,q)$ where $\G$ is a cyclig group, $g \in \G$ is a generator and $q$ is the order of $g$ in $\G$, the ElGamal PKE scheme $\Pi = (\mathrm{KGen}, \mathrm{Enc}, \mathrm{Dec})$ is defined as follows:
        \begin{itemize}
            \item $\mathrm{KGen}(g,q) = (pk, sk)$ where $pk = g^x$ and $sk = x$, for some  $x \in_R \Z_q$
            \item $\mathrm{Enc}(pk, m) = (c_1, c_2)$ where $c_1 = g^r$ and $c_2 = pk^r\cdot m$, for some $r \in_R \Z_q$
            \item $\mathrm{Dec}(sk, (c_1, c_2)) = \frac{c_2}{c_1^sk}$ 
        \end{itemize}
    \end{framedalgo}

    The correctness of the scheme easily follows by simple substitution:
    \[\mathrm{Dec}(sk, (c_1, c_2)) = \frac{c_2}{c_1^sk} = \frac{pk^r \cdot m}{(g^{r})^{sk}} = \frac{g^{xr} \cdot m}{g^{rx}} = m\]

    \begin{framedthm}{}
        The ElGamal PKE scheme is CPA-secure under the DDH assumption.
    \end{framedthm}

    \begin{proof}
        Let $G(\lambda,b) = \mathrm{Game}_{\Pi,A}^{\text{CPA}}(\lambda, b)$. We define an hybrid $H(\lambda, b)$ where $pk$ is replaced by $g^z$ for some $z \in_R \Z_q$. Assume that the DDH assumption holds for $\G$ and fix $b \in \{0,1\}$. By way of contradiction, suppose that there is an adversary $A$ that distinguishes $G(\lambda,b)$ from $H(\lambda,b)$ with non-negligible probability. We define a new adversary $A'$ for the DDH assumption:
        \begin{enumerate}
            \item The challenger $C$ sends $(\G,g,q,g^x, g^y, Z)$ to $A'$, with $Z$ being either $g^{xy}$ or $g^z$ for some $z \in_R \Z_q$
            \item $A'$ sends $pk = g^x$ to $A$, which replies with $m_0^*, m_1^*$
            \item $A'$ then sends $(g^y, Z \cdot m_b)$ to $A$
            \item $A$ replies with a guessing bit $b'$, which gets forwarded to $C$
        \end{enumerate}

        Since $A'$ is able to distinguish $G(\lambda,b)$ from $H(\lambda,b)$ with non-negligible probability, they can distinguish $Z = g^{xy}$ from $Z = g^z$, meaning that $A'$ wins breaks the DDH assumption on $\G$. Now, by definition we have $H(\lambda, 0) \equiv H(\lambda, 1)$ since we don't care about which of the two messages actually gets encrypted. This concludes that $G(\lambda,0) \approx_c G(\lambda, 1)$.
    \end{proof}

    The simplicity of the ElGamal scheme comes at a price since its CCA game can be easily broken: after receiving the challenge ciphertext $(c_1^*, c_2^*)$, we can query $\mathrm{Dec}(sk, (c_1^*, c_2^* \cdot \widehat m'))$ with a fresh message $m'$ and check if the answer is $m_0^*\widehat m'$ or $m_1^* \widehat m'$.

    \section{RSA encryption and trapdoor permutations}

    We study the computational security of the ever-famous Rivest-Shamir-Adleman (RSA) public-key encryption scheme. The original PKE construction was based on the hardness of factoring a number $n$ composed of two primes, i.e. finding the primes $p,q \in \Primes$ such that $n = pq$. As for the discrete-log problem, we state the hardness of factoring through a computational assumption, the \textbf{FACTORING assumption}.

    \begin{framedass}{FACTORING assumption}
        The FACTORING assumption states that:
        \[\Pr[A(n) = (p,q) : p,q \in_R \Primes, n = pq] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$ and for a chosen value $\lambda > 0$
    \end{framedass}

    \begin{framedalgo}{RSA PKE scheme}
        The RSA PKE scheme $\Pi = (\mathrm{KGen}, \mathrm{Enc}, \mathrm{Dec})$ is defined as follows:
        \begin{itemize}
            \item $ (pk, sk) \in_R \mathrm{KGen}(1^{\lambda})$ where $pk = (n,e)$ and $sk = (n,d)$, with $n = pq$ for two primes $p,q \in \Primes$ of approximately $\lambda$ bits and $e,d$ being each other's inverse in $\Z_{\varphi(n)}$, i.e. $ed \equiv 1 \pmod{\varphi(n)}$
            \item $\mathrm{Enc}(pk, m) = m^e \pmod n$
            \item $\mathrm{Dec}(sk, c) = c^d \pmod n$ 
        \end{itemize}
    \end{framedalgo}

    The correctness of the RSA scheme comes from Euler's theorem, which we already discussed in the previous chapter. First, we observe that $\phi(n) = (p-1)(q-1)$. Second, we observe that $ed \equiv 1 \pmod{\phi(n)}$ implies that $ed = 1+k \phi(n)$ for some $k \in \Z$. Therefore, assuming $c = \mathrm{Enc}(pk, m)$, we have that:
    \[\mathrm{Dec}(sk, c) = c^d \equiv (m^{ed}) \equiv m^{k\phi(n)+1} \equiv m(m^{\phi(n)})^k \equiv m \pmod{n}\]

    It's easy to see that the scheme is not CPA-secure since the scheme is deterministic. To fix this, we encode the message $m$ into a string $\widehat m$ in a randomized -- but invertible -- fashion. The Public-key Cryptography Standard (PKCS) \#1.5 sets $\widehat m = m \mid\mid r$ with randomly chosen string $r$. Clearly, $r$ must have strictly more that $\Omega(\log \lambda)$ bits, otherwise we could just find it through bruteforce. Unfortunately, for real-world values of $r$ we can't prove the CPA-security of RSA even with PKCS \#1.5, except when $m \in \{0,1\}$. Nonetheless, this last result requires a stronger computational assumption about the RSA scheme itself.
    
    The RSA assumption states that RSA is a \textbf{trapdoor permutation (TDP)}. A trapdoor permutation is formed by two functions $f$ and $f^{-1}$ that work, respectively, with a public and a secret key.The first function is assumed to be a \textit{one-way function} even when the public key is known. The second function is an inverse of the first one, but it is must be efficiently computable only when the secret key is known (hence the name \textit{trapdoor}).

    \begin{frameddefn}{Trapdoor permutation}
        We say that a triple $(\mathrm{KGen}, f, f^{-1})$ with parameter $\lambda$ is a trapdoor permutation when:
        \begin{itemize}
            \item $(pk,sk) \in_R \mathrm{KGen}(1^\lambda)$, with $pk,sk \in \{0,1\}^\lambda$
            \item $f : \{0,1\}^\lambda \times \{0,1\}^n \to \{0,1\}^n$ is a OWF
            \item $f^{-1} : \{0,1\}^\lambda \times \{0,1\}^n \to \{0,1\}^n$ is such that $f^{-1}(sk, f(pk, x)) = x$ for all $x \in \{0,1\}^n$
            \item $f^{-1}(K, x)$ is efficiently computable only when $K = sk$ 
        \end{itemize}
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.6]{images/tdp.png}
        \caption{Graphical representation of a trapdoor permutation.}
    \end{figure}

    \begin{framedass}{RSA assumption}
        The RSA PKE scheme is a trapdoor permutation.
    \end{framedass}

    It's easy to see that the RSA assumption implies the FACTORING assumption: if we could easily solve factoring, we could also easily break RSA by finding the two primes $p,q$ forming $n$. From trapdoor permutations we can easily construct CPA-secure PKE schemes using the hard-core predicate of the encrypting function.

    \begin{framedthm}{}
        Let $(\mathrm{KGen}, f, f^{-1})$ be a TDP. Given the hard-core predicate $h$ of $f$, let $\Pi = (\mathrm{KGen}, \mathrm{Enc},$ $\mathrm{Dec})$ be the PKE defined by:
        \[\mathrm{Enc}(pk, m) = (f(pk, r), h(pk, r) \oplus m )\]
        \[\mathrm{Dec}(sk, (c_0, c_1)) = c_1 \oplus h(pk, f^{-1}(sk, c_0))\]

        with $m \in \{0,1\}$. Then, $\Pi$ is CPA-secure.
    \end{framedthm}

    \begin{proof}
        Omitted.
    \end{proof}

    We also observe that PKE CPA-secure schemes for one bit can be extended to a polynomial amount of bits. Therefore, the RSA assumption and the above TDP construction implies that we can achieve CPA-secure public-key encryption. But what about CCA-security? The PKCS \#2.0 uses a different encoding to achieve this in \textbf{Optimal Asymmetric Encryption Padding (OAEP)}. The idea is very similar to a 2-layer Feistel network using two different hash functions $H, H'$ on $m\mid\mid r$, with $r$ is chosen uniform-at-random. The CCA-security of this scheme can be proven only under the RSA assumption and in the \textit{Random Oracle Model (ROM)}: $H,H'$ are assumed to be truly random hash functions and they can be computed \underline{only} by querying an oracle.

    \begin{framedalgo}{OAEP PKE scheme}
        Let $\Pi_{\text{RSA}}= (\mathrm{KGen}', \mathrm{Enc}', \mathrm{Dec}')$ be the RSA PKE and $H,H'$ be two hash functions. The OAEP PKE scheme $\Pi_{\text{OAEP}} = (\mathrm{KGen}, \mathrm{Enc}, \mathrm{Dec})$ is defined as follows:
        \begin{itemize}
            \item Fix $\lambda_1, \lambda_2 \in \N$ and sample $r \in_R \{0,1\}^\lambda$
            \item Let $s = m \mid\mid 0^{\lambda_1} \oplus H(r)$ and $t = r \mid\mid H'(s)$
            \item $\mathrm{Enc}(pk, m) = (s \mid\mid t)^e$, with $pk = (n,e)$ being the RSA public key
        \end{itemize}
    \end{framedalgo}

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/oaep.pdf}
        \caption{The OAEP scheme.}
    \end{figure}

    \begin{framedthm}{}
        The OAEP PKE scheme is CCA-secure under the RSA assumption in the ROM.
    \end{framedthm}

    \begin{proof}
        Omitted.
    \end{proof}

    \section{Digital signatures}

    After proving that CCA-security is achievable also for public-key cryptography, we're left with constructing the equivalent of MACs for PKEs. In the context of asymmetric encryption, we talk about \textbf{Digital Signatures (DS)} schemes. The critical difference between MACs and DSs is that the signature is \underline{publicly verifiable}: we don't need to know the key that was used for tagging. This gives birth to a some-what mirror concept of public-key encryption were we use the secret key to sign the message and then use the public key to verify it, instead of using the public key for encryption and the secret key for decryption.
    
    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{Alice};
            \node[draw, rectangle, minimum width=40] (2) [right of = 1, xshift = 75]{$\mathrm{Sign}(sk,m)$};
            \node[] (x) [right of = 2, xshift = 50]{};
            \node[draw, rectangle, minimum width=40] (3) [right of = x, xshift = 50]{$\mathrm{Vrfy}(pk,\sigma)$};
            \node[] (4) [right of = 3, xshift = 75]{Bob};
            \node[] (5) [below of = x, yshift = -10]{$(pk,sk)$};
            \node[] (6) [below of = 2, yshift = -10]{};
            \node[] (7) [below of = 3, yshift = -10]{};

            \path[every node/.style={font=\sffamily\small}]
                (1) edge node{$m$} (2)
                (2) edge[-] node{$(m,\sigma)$} (x.center)
                (x.center) edge (3)
                (3) edge node{$0/1$} (4)
                (5) edge[-] (6.center)
                (5) edge[-] (7.center)
                (6.center) edge (2)
                (7.center) edge (3)
                ;
        \end{tikzpicture}

        \caption{Example of digital signature.}
    \end{figure}

    Just like MACs, digital signatures are used to authenticate a message to guarantee its integrity. Moreover, the asymmetric paradigm allows us to form some sort of hierarchy of authentications. If Alice wants to know Bob's public key without being subject of a man-in-the-middle attack, they can ask the key to the \textit{Certificate Authority (CA)} to which Bob previously sent his key in an authenticated manner. The idea here is that the CA is also authenticated by another CA of \curlyquotes{higher grade}, which is also authenticated by a CA of higher grade and so on and so forth, until we reach a final CA with the highest grade of authority (usually, a physical entity subject to very strict regulations). This is known as the Public Key Infrastructure (PKI).

    The simplest digital signature scheme can be constructed through RSA itself. The idea is to use the decryption function as the signing function and the encryption function as the verifying function. This process simple idea works thanks to how the public and secret key are inverse of each other: we don't care about the order of application.

    \begin{framedalgo}{RSA DS scheme}
        The RSA DS scheme $\Pi = (\mathrm{KGen}, \mathrm{Sign}, \mathrm{Vrfy})$ is defined as follows:
        \begin{itemize}
            \item $ (pk, sk) \in_R \mathrm{KGen}(1^{\lambda})$ where $pk = (n,e)$ and $sk = (n,d)$, with $n = pq$ for two primes $p,q \in \Primes$ of approximately $\lambda$ bits and $e,d$ being each other's inverse in $\Z_{\varphi(n)}$, i.e. $ed \equiv 1 \pmod{\varphi(n)}$
            \item $\mathrm{Sign}(sk, m) = (m, m^d \pmod n)$
            \item $\mathrm{Vrfy}(pk, (m, \sigma)) = 1$ if $m \equiv \sigma^e \pmod n$ and $0$ otherwise.
        \end{itemize}
    \end{framedalgo}

    For the computational security of digital signatures, we still use the concept of UFCMA-security. Similarly to what we did with CPA-security and CCA-security, the UFCMA game for PKEs is identical to the one for SKEs, with the addition of an initial message sent by the challenger containing the public key. 
    
    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (1t) [below of = 1]{};
            \node[] (2t) [below of = 2]{};

            \node[] (3) [below of = 1t]{};
            \node[] (4) [below of = 2t]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (2t) edge[swap, near start] node{$pk$} (1t)
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$m_i \in \{0,1\}^\lambda$} (4)
                (6) edge[swap, near start] node{$\tau_i = \mathrm{Tag}(K, m_i)$} (5)
                (7) edge[near start] node{$m^*, \tau^* \in \{0,1\}^\lambda$} (8)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}^{\text{UFCMA}}_{\Pi, A}(\lambda)$ for PKEs}
    \end{figure}
    
    It's easy to see that the standard RSA digital signature is not UFCMA-secure. Given two messages $m_1$ and $m_2$, let $\sigma_1$ and $\sigma_2$ be their signatures under RSA. Now, observe that:
    \[(\sigma_1 \sigma_2)^e \equiv \sigma_1^e \sigma_2^e \equiv m_1 m_2 \pmod n \]
    
    Therefore, an adversary can just query $m_1, m_2$ and then send the challenge pair $(m_1m_2, \sigma_1\sigma_2)$ to win with probability 1. Nonetheless, we can fix this issue with a simple trick: we can hash each message before signing it. Even more generally, we can use a trapdoor permutation instead of the RSA scheme. This is known as the \textbf{Full-Domain Hash (FDH)} signature scheme.

    \begin{framedalgo}{FDH DS scheme}
        Let $(\mathrm{KGen}, f, f^{-1})$ be a TDP and let $H$ be an hash function. The Full-Domain Hash (FDH) DS scheme $\Pi_{\text{FDH}} = (\mathrm{KGen}, \mathrm{Enc}, \mathrm{Dec})$ is defined as follows:
        \begin{itemize}
            \item $(pk, sk) \in_R \mathrm{KGen}(1^{\lambda})$ where $pk = (n,e)$ and $sk = (n,d)$, with $n = pq$ for two primes $p,q \in \Primes$ of approximately $\lambda$ bits and $e,d$ being each other's inverse in $\Z_{\varphi(n)}$, i.e. $ed \equiv 1 \pmod{\varphi(n)}$
            \item $\mathrm{Sign}(sk, m) = f^{-1}(sk, H(s))$
            \item $\mathrm{Vrfy}(pk, (m, \sigma)) = 1$ if $H(m) = f(pk, \sigma)$ and $0$ otherwise.
        \end{itemize}
    \end{framedalgo}

    \begin{framedthm}{}
        The FDH signature scheme is UFCMA-secure in the ROM when constructed through a TDP.
    \end{framedthm}

    \begin{proof}
        This will be our first proof in the Random Oracle Model. We recall that in the ROM the hash function $H$ is assumed to be truly random and that an attacker may compute it only through queries to an oracle.
        
        Let $\Pi = (\mathrm{KGen}, \mathrm{Sign}, \mathrm{Vrfy})$ be the FDH scheme constructed through a TDP $(\mathrm{KGen}, f, f^{-1})$. We build a reduction from the distinguishability of the UFCMA game of $\Pi$ to the OWF security of $f$. By way of contradiction, suppose that there is an adversary $A^H$ that distinguishes the UFCMA game of $\Pi$ with probability at least $n^{-c}$ for some $c > 0$, where $A^H$ also makes RO queries for $H$. We build a new adversary $A$ as follows:
        \begin{enumerate}
            \item $A$ will substitute the oracle for $H$, answering $A^H$ RO queries.
            \item The challenger $C$ sends the pair $(pk, y)$ to $A$, who then forwards $pk$ to $A$
            \item $A^H$ tells $A$ that they will make $q = \poly(\lambda)$ RO queries. 
            \item $A$ samples $j \in_R \mathcal{U}_{q_h}$ and sets $H'$ as the function such that $H'(m_i) = y$ when $i = j$ and $H(m_i) = f(pk, x_i)$, for $x_i \in_R \mathcal{U}_n$, when $i \neq j$
            \item $A^H$ starts the RO queries. For each query $m_i$, $A$ will answer with $y_i = H'(m_i)$. One of these RO queries will be for the future challenge message $m^*$, meaning that $m^* = m_{i^*}$ for some $i^* \in [q]$, but $A$ doesn't know which one 
            \item $A^H$ starts the sign queries. For each previously queried $m_i$ such that $m_i \neq m^*$, $A^H$ queries the signature of $m_i$ to $A$, which replies with $x_i$ if $i \neq j$ and with ABORT when $i = j$.
            \item After receiving the ABORT message, $A^H$ sends the challenge pair $(m^*, \sigma^*)$ to $A$, who then forwards $\sigma^*$ to $C$
        \end{enumerate}

        We observe that if the signature $\sigma^*$ sent by $A^H$ is valid then $\sigma^*$ is a pre-image of $y^* = H'(m^*)$. Therefore, the main trick of the reduction is to enforce with good enough probability that $y^* = y$. This is true if and only if $j = i^*$, which happens with probability $1/q$. If $j$ is guessed correcly by $A$, $A^H$ will find $y$ with non-negligible probability. Therefore, we conclude that:
        \[\Pr[A \text{ wins}] = \Pr[j = i^*] \Pr[A^H \text{ wins } \mid j = i^*] > \frac{1}{q n^c}\]
    \end{proof}

    \chapter{Identification schemes}

    \section{$\Sigma$-protocols and zero-knowledge proofs}

    An \textbf{identification scheme} is a protocol whereby Peggy the Prover proves to Victor the Verifier that she is indeed who she says she is. In practice, Peggy's identity is encoded in a private key $a$ and a public key $y$. The protocol takes the form of Peggy proving to Victor that she has knowledge of the private key $a$. For instance, suppose that private key is a value $a \in \Z_{p-1}$ and that the public key $y$ is such that $y \equiv x^a \pmod p$ for some $x \in \Z_p$ with $p \in \Primes$. Peggy can prove her to Victor by demonstrating that she knows the discrete logarithm of $y$ to the base $x$.

    In order for an identification scheme to be a valid method of identification, we require that Peggy must be able to always convince the Victor of her rightfulness (\textbf{perfect correctness}), even when they act through random choices -- meaning that they are PPT algorithms. This requirement translates to making Victor accept with probability 1 for each possible exchange of messages, namely a \textit{transcript}. We denote the set of all transcripts between a prover  $P$ and a verifier $V$ with $P(pk, sk) \rightleftarrows V(pk)$.
    
    \begin{frameddefn}{Identification scheme}
        We say that a triple $\Pi = (\mathrm{KGen}, P, V)$ is an identification scheme (IDS) when:
        \begin{itemize}
            \item $(pk,sk) \in_R \mathrm{KGen}(1^\lambda)$, with $pk,sk \in \{0,1\}^\lambda$
            \item $P$ and $V$ are two PPT algorithms, called prover and verifier, such that $P(pk,sk)$ tries to prove to $V(pk)$ that it knows $sk$
            \item $P(pk, sk) \rightleftarrows V(pk)$ denotes the set of all transcripts, i.e. sequences of messages, between $P(pk, sk)$ and $V(pk)$
            \item $\mathrm{out}(P(pk, sk) \rightleftarrows V(pk))$ is a r.v. such that $\mathrm{out}(P(pk, sk) \rightleftarrows V(pk))(\tau) = 1$ when $\tau$ is a transcript that convinces $V$ and $0$ otherwise
            \item \textit{Perfect correctness}: for all $\lambda \in \N$ and for all $(pk,sk)$ it holds that:
            \[\Pr[\mathrm{out}(P(pk, sk) \rightleftarrows V(pk)) = 1] = 1\]
            meaning that the prover always convinces the verifier with probability 1
        \end{itemize}
    \end{frameddefn}


    We observe that Peggy \underline{cannot} simply tell Victor that the value is $a$ to prove her identity in a secure way since $a$ is her private key, meaning that he could impersonate her in the future. A viable identification scheme must prevent this from happening; we require that Victor can't impersonate Peggy even if she proves her identity to him polynomially many times (\textbf{passive security}). Differently from the other constraint, this one can be enforced through a game. Let $\mathrm{Game}_{\Pi, A}^{\text{ID}}(\lambda)$ be the 2-player game defined as follows:
    \begin{enumerate}
        \item The adversary $A = (A_1, A_2)$ is composed of two separate algorithms.
        \item The challenger $C$ generates a key pair $(pk, sk)$ and sends $pk$ to the adversary $A$
        \item $A$ makes $q = \poly(\lambda)$ transcript queries to $C$, which replies with $\tau_1, \ldots, \tau_q \in_R P(pk, sk) \rightleftarrows V(pk)$
        \item $A$ computes $A_1(\tau_1, \ldots, \tau_q) = s$, where $s$ is a \curlyquotes{state}
        \item $A$ samples $\tau^* \in_R A_2(s, pk) \rightleftarrows V(pk)$
        \item $A$ wins if $\mathrm{out}(P(pk, sk) \rightleftarrows V(pk))(\tau^*) = 1$, otherwise $C$ wins
    \end{enumerate}

    \begin{figure}[H]
        \centering

        \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=1cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
            \node[] (1) []{$A$};
            \node[] (2) [left of = 1, xshift = 225]{$C$};

            \node[] (1t) [below of = 1]{};
            \node[] (2t) [below of = 2]{};

            \node[] (3) [below of = 1t]{};
            \node[] (4) [below of = 2t]{};

            \node[] (5) [below of = 3]{};
            \node[] (6) [below of = 4]{};

            \node[] (7) [below of = 5]{};
            \node[] (8) [below of = 6]{};

            \path[every node/.style={font=\sffamily\small}]
                (2t) edge[swap, near start] node{$pk$} (1t)
                (5) edge[bend left] node {$\poly(\lambda)$}(3)
                (3) edge[near start] node{$\varnothing$} (4)
                (6) edge[swap, near start] node{$\tau_i \in_R P(pk, sk) \rightleftarrows V(pk)$} (5)
                (7) edge[] node{$\tau^* \in_R A_2(A_1(\tau_1, \ldots, \tau_q), pk) \rightleftarrows V(pk)$} (8)
                ;
        \end{tikzpicture}

        \caption{Graphical representation of $\mathrm{Game}_{\Pi, A}^{\text{ID}}(\lambda)$.}
    \end{figure}

    \begin{frameddefn}{Passive security for IDSs}
        Let $\Pi = (\mathrm{KGen}, P, V)$ be an IDS. We say that $\Pi$ has passive security if:
        \[\Pr[\mathrm{Game}_{\Pi, A}^{\text{ID}}(\lambda) = 1] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$
    \end{frameddefn}

    By definition of the game itself, proving that an IDS has passive security is really hard. Similarly to what we did for CCA-security, we'll prove a result that allows us to get passive security from simpler properties. 

    We will work with a particular type of identification schemes, called \textbf{$\Sigma$-protocols}. In these IDS we add a constraint on the length of the transcripts: only 3 messages are allowed before $V$ declares the output (2 from the prover and 1 from the verifier). Each message is randomized for both parties (\textit{3-move public coins protocol}). Moreover, we also want the first message of the prover to be hard to guess to avoid trivial protocols.

    \begin{frameddefn}{$\Sigma$-protocols}
        We say that a triple $\Pi = (\mathrm{KGen}, P, V)$ is a $\Sigma$-protocol when:
        \begin{itemize}
            \item $\Pi$ is an identification scheme where $P = (P_1, P_2)$ and $V = (V_1, V_2)$
            \item The transcriptions have three randomized messages $\alpha,\beta,\gamma$:
            \begin{enumerate}
                \item The prover sends $\alpha \in_R P_1(pk,sk)$
                \item The verifier answers with $\beta \in_R V_1(pk, \alpha)$
                \item The prover replies with $\gamma \in_R P_2(pk, sk, \alpha, \beta)$
                \item The verifier outputs $V_2(pk, sk, \alpha, \beta, \gamma)$
            \end{enumerate} 
            \item \textit{Non-triviality}: it's hard to predict the first message:
            \[\forall \widehat \alpha \; \forall (pk, sk) \; \Pr[\alpha = \widehat \alpha] \leq \mathrm{negl}(\lambda)\]
        \end{itemize}
    \end{frameddefn}
    
    We give an example of a standard $\Sigma$-protocol, that is \textbf{Schnorr's protocol}.

    \begin{framedalgo}{Schnorr's $\Sigma$-protocol}
        The Schnorr $\Sigma$-protocol $\Sigma_{\text{Schorr}} = (\mathrm{KGen}, \mathrm{Sign}, \mathrm{Vrfy})$ is defined as follows:
        \begin{itemize}
            \item $(\G, g, q)$ are the protocol parameters, where $\G$ is a cyclic group generated by $g \in \G$ with order $q$
            \item $(pk, sk) \in_R \mathrm{KGen}(1^{\lambda})$ where $pk = g^x$ and $s_k = x$, with $x \in_R \Z_q$
            \item The transcripts are given by:
            \begin{enumerate}
                \item $P_1(pk,sk) = \alpha$ such that $\alpha = g^a$ for some $a \in_R \Z_q$
                \item $V_1(pk, \alpha) = \beta$ such that $\beta \in_R \Z_q$
                \item $P_2(pk,sk,\alpha, \beta) = \gamma$ such that $\gamma = \beta x + \alpha$
                \item $V_2(pk, \alpha, \beta, \gamma) = 1$ if $g^\gamma = \alpha (pk)^{\beta}$ and $0$ otherwise
            \end{enumerate}
        \end{itemize}
    \end{framedalgo}

    Through some algebraic manipulation, we can show that this protocol has perfect correctness. First, we observe that:
    \[g^\gamma = g^{\beta x + \alpha} = \alpha (pk)^\beta\]

    Then, we observe the fact that $\gamma$ can be correctly computed if and only if the prover $P$ really knows the value $x$ of the secret key. After some tought, we can also convince ourselves of the protocol's non-leakage of the secret key. In fact, Schnorr's protocol also satisfies passive security and non-triviality, meaning that it is in fact a $\Sigma$-protocol.

    To show the passive security of this protocol, we have to introduce two stronger properties that when summed up give passive security as a natural byproduct: \textbf{Honest-verifier Zero-knowledge (HVZK)} and \textbf{Special Soundness (SS)}. The first property tells us that real transcripts computed by the protocol are computationally indistinguishable from \textit{simulated transcripts} produced by an simulation algorithm. In other words, the transcripts don't give any information to the verifier.

    \begin{frameddefn}{Honest-verifier Zero-knowledge}
        Let $\Pi = (\mathrm{KGen}, P, V)$ be an IDS. We say that $\Pi$ has Honest-verifier Zero-knowledge (HVZK) if:
        \[(pk, sk, \mathrm{Sim}(pk)) \approx_c (pk,sk P(pk,sk) \rightleftarrows V(pk))\]
        for some PPT simulator $\mathrm{Sim}$
    \end{frameddefn}

    \begin{framedprop}{}
        Schnorr's protocol has HVZK.
    \end{framedprop}

    \begin{proof}
        Consider $\Sigma_{\text{Schnorr}} = (\mathrm{KGen}, P, V)$ and let $\mathrm{Sim}$ be the such that $\mathrm{Sim}(pk) = (\alpha', \beta', \gamma')$, where $\beta',\gamma' \in_R \Z_q$ and $\alpha' = g^{\gamma'} (pk)^{-\beta'}$.Given the value $a'= \gamma'-\beta' x$, we observe that $\alpha' =  g^{\gamma'-\beta' x} = g^{a'}$.
        
        Consider now a transcript $(\alpha, \beta, \gamma) \in_R P(pk,sk) \rightleftarrows V(pk)$. Since $\gamma = \beta x + \alpha$ and $a \in_R \Z_q$, we get that $(\alpha', \beta', \gamma')$ and $(\alpha, \beta, \gamma)$ have the same probability since the first and third element of both distributions constrain other in the same way.
    \end{proof}

    The second property, instead, is restricted only to $\Sigma$-protocols and it describes the impossibility of an adversary to forge valid transcripts starting from the same initial message $\alpha$ sent by $P_1$. Let $\mathrm{Game}_{\Pi, A}^{\text{SS}}(\lambda)$ be the 2-player game defined as follows:
    \begin{enumerate}
        \item The challenger $C$ generates $(pk, sk)$ and sends $pk$ to the adversary $A$
        \item $A$ returns two transcripts $\tau = (\alpha, \beta, \gamma)$ and $\tau' = (\alpha, \beta', \gamma')$
        \item $A$ wins if $\beta \neq \beta'$ and $V_2(p_k, \alpha, \beta, \gamma) = V_2(p_k, \alpha, \beta', \gamma') = 1$
    \end{enumerate}

    \begin{frameddefn}{Special Soundness}
        Let $\Sigma = (\mathrm{KGen}, P, V)$ be a $\Sigma$-protocol. We say that $\Sigma$ has special soundness if:
        \[\Pr[\mathrm{Game}_{\Pi, A}^{\text{SS}}(\lambda) = 1] \leq \mathrm{negl}(\lambda)\]
        for all PPT adversaries $A$
    \end{frameddefn}

    \begin{framedprop}{}
        Schnorr's protocol has special soundness under the DL assumption.
    \end{framedprop}

    \begin{proof}
        Consider $\Sigma_{\text{Schnorr}} = (\mathrm{KGen}, P, V)$. By way of contradiction, suppose that there is a distinguisher $A$ for the SS of $\Sigma_{\text{Schnorr}}$. Suppose that $A$ finds two transcripts $\tau = (\alpha, \beta, \gamma)$ and $\tau' = (\alpha, \beta', \gamma')$ such that $\beta \neq \beta'$ and $V_2(p_k, \alpha, \beta, \gamma) = V_2(p_k, \alpha, \beta', \gamma') = 1$. Then, we have that:
        \[g^{\gamma-\gamma'} = \alpha(pk)^{\beta} \alpha^{-1} (pk)^{-\beta'} \iff g^{\gamma-\gamma'} = (g^x)^{\beta- \beta} \iff g^x = g^{(\gamma-\gamma')(\beta-\beta')^{-1}}\]

        Therefore, we can build a new distinguisher $A'$ through $A$ that computes $x$ from $g^x$ with non-negligible probability.
    \end{proof}

    \begin{framedthm}{HVZK$+$SS implies passive security}
        Let $\Sigma = (\mathrm{KGen}, P, V)$ be a $\Sigma$-protocol. Then, $\Sigma$ has passive security if has HVZK and special soundness.
    \end{framedthm}

    \begin{proof}
        To do.
    \end{proof}
\end{document}
